2024-06-05 17:11:49,758 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:11:49,867 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:11:52,448 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:11:52,840 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:16:16,226 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:16:16,335 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:16:18,237 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:16:18,704 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:20:14,135 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:20:14,242 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:20:16,195 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:20:16,602 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:23:28,395 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:23:28,489 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:23:30,253 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:23:30,737 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:27:40,701 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:27:40,746 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:27:42,841 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:27:43,281 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:27:48,000 - __main__ - INFO - Action: [0.48201379 0.48201379 0.48201379 0.48201379], Reward: tensor([-74.0156], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:27:48,920 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-74.0156], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:27:52,745 - __main__ - INFO - Action: [0.62180397 0.62180397 0.62180397 0.62180397], Reward: tensor([-26.6078], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:27:56,404 - __main__ - INFO - Action: [0.69169906 0.69169906 0.69169906 0.69169906], Reward: tensor([-28.4593], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:27:59,844 - __main__ - INFO - Action: [0.72664661 0.72664661 0.72664661 0.72664661], Reward: tensor([-30.2449], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:04,124 - __main__ - INFO - Action: [0.8453371 0.8453371 0.8453371 0.8453371], Reward: tensor([-32.4828], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:08,515 - __main__ - INFO - Action: [0.92019592 0.92019592 0.92019592 0.92019592], Reward: tensor([-34.7442], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:12,716 - __main__ - INFO - Action: [0.95762534 0.95762534 0.95762534 0.95762534], Reward: tensor([-36.9640], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:17,093 - __main__ - INFO - Action: [0.97634005 0.97634005 0.97634005 0.97634005], Reward: tensor([-39.0883], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:21,469 - __main__ - INFO - Action: [0.97018381 0.97018381 0.97018381 0.97018381], Reward: tensor([-41.2531], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:25,729 - __main__ - INFO - Action: [0.86588898 0.86588898 0.86588898 0.86588898], Reward: tensor([-43.2977], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:29,819 - __main__ - INFO - Action: [0.91495828 0.91495828 0.91495828 0.91495828], Reward: tensor([-45.4935], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:30,740 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-432.6512], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:28:34,101 - __main__ - INFO - Action: [0.83827622 0.83827622 0.83827622 0.83827622], Reward: tensor([-47.8286], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:37,450 - __main__ - INFO - Action: [0.79993519 0.79993519 0.79993519 0.79993519], Reward: tensor([-49.0679], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:41,437 - __main__ - INFO - Action: [0.89749497 0.89749497 0.89749497 0.89749497], Reward: tensor([-51.2922], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:45,573 - __main__ - INFO - Action: [0.94627486 0.94627486 0.94627486 0.94627486], Reward: tensor([-53.3502], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:49,837 - __main__ - INFO - Action: [0.97066481 0.97066481 0.97066481 0.97066481], Reward: tensor([-55.4436], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:53,870 - __main__ - INFO - Action: [0.4853324 0.4853324 0.4853324 0.4853324], Reward: tensor([-57.0589], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:57,600 - __main__ - INFO - Action: [0.62346328 0.62346328 0.62346328 0.62346328], Reward: tensor([-59.0123], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:01,429 - __main__ - INFO - Action: [0.31173164 0.31173164 0.31173164 0.31173164], Reward: tensor([-60.8062], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:04,977 - __main__ - INFO - Action: [0.15586582 0.15586582 0.15586582 0.15586582], Reward: tensor([-62.2058], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:09,264 - __main__ - INFO - Action: [0.45872999 0.45872999 0.45872999 0.45872999], Reward: tensor([-64.7589], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:10,167 - __main__ - INFO - Epoch 0, Iteration 20: Reward: tensor([-993.4758], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:29:13,373 - __main__ - INFO - Action: [0.22936499 0.22936499 0.22936499 0.22936499], Reward: tensor([-66.5770], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:17,168 - __main__ - INFO - Action: [0.1146825 0.1146825 0.1146825 0.1146825], Reward: tensor([-68.2964], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:21,369 - __main__ - INFO - Action: [0.05734125 0.05734125 0.05734125 0.05734125], Reward: tensor([-70.3494], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:25,573 - __main__ - INFO - Action: [0.02867062 0.02867062 0.02867062 0.02867062], Reward: tensor([-72.4235], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:29,811 - __main__ - INFO - Action: [0.01433531 0.01433531 0.01433531 0.01433531], Reward: tensor([-74.5531], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:33,274 - __main__ - INFO - Action: [0.00716766 0.00716766 0.00716766 0.00716766], Reward: tensor([-76.5553], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:37,758 - __main__ - INFO - Action: [0.5011112 0.5011112 0.5011112 0.5011112], Reward: tensor([-78.9387], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:41,741 - __main__ - INFO - Action: [0.63135268 0.63135268 0.63135268 0.63135268], Reward: tensor([-81.0394], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:45,718 - __main__ - INFO - Action: [0.31567634 0.31567634 0.31567634 0.31567634], Reward: tensor([-82.9167], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:49,558 - __main__ - INFO - Action: [0.15783817 0.15783817 0.15783817 0.15783817], Reward: tensor([-84.6146], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:50,551 - __main__ - INFO - Epoch 0, Iteration 30: Reward: tensor([-1749.7399], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:29:54,453 - __main__ - INFO - Action: [0.57644646 0.57644646 0.57644646 0.57644646], Reward: tensor([-87.4627], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:58,512 - __main__ - INFO - Action: [0.66902031 0.66902031 0.66902031 0.66902031], Reward: tensor([-89.4907], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:02,327 - __main__ - INFO - Action: [0.81652394 0.81652394 0.81652394 0.81652394], Reward: tensor([-91.6264], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:06,489 - __main__ - INFO - Action: [0.78905905 0.78905905 0.78905905 0.78905905], Reward: tensor([-93.6130], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:10,882 - __main__ - INFO - Action: [0.87654332 0.87654332 0.87654332 0.87654332], Reward: tensor([-95.8886], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:14,927 - __main__ - INFO - Action: [0.43827166 0.43827166 0.43827166 0.43827166], Reward: tensor([-97.5543], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:19,160 - __main__ - INFO - Action: [0.70114962 0.70114962 0.70114962 0.70114962], Reward: tensor([-99.9066], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:23,340 - __main__ - INFO - Action: [0.35057481 0.35057481 0.35057481 0.35057481], Reward: tensor([-101.6071], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:27,517 - __main__ - INFO - Action: [0.65730119 0.65730119 0.65730119 0.65730119], Reward: tensor([-104.0234], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:31,628 - __main__ - INFO - Action: [0.82617797 0.82617797 0.82617797 0.82617797], Reward: tensor([-106.2457], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:32,456 - __main__ - INFO - Epoch 0, Iteration 40: Reward: tensor([-2717.1587], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:30:36,284 - __main__ - INFO - Action: [0.91061636 0.91061636 0.91061636 0.91061636], Reward: tensor([-108.6688], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:40,582 - __main__ - INFO - Action: [0.93732197 0.93732197 0.93732197 0.93732197], Reward: tensor([-110.8353], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:44,487 - __main__ - INFO - Action: [0.95067478 0.95067478 0.95067478 0.95067478], Reward: tensor([-112.8008], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:48,803 - __main__ - INFO - Action: [0.47533739 0.47533739 0.47533739 0.47533739], Reward: tensor([-114.4695], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:53,121 - __main__ - INFO - Action: [0.71968248 0.71968248 0.71968248 0.71968248], Reward: tensor([-116.8626], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:57,174 - __main__ - INFO - Action: [0.85736862 0.85736862 0.85736862 0.85736862], Reward: tensor([-119.1196], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:01,282 - __main__ - INFO - Action: [0.42868431 0.42868431 0.42868431 0.42868431], Reward: tensor([-120.7260], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:05,521 - __main__ - INFO - Action: [0.71186953 0.71186953 0.71186953 0.71186953], Reward: tensor([-123.0856], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:09,610 - __main__ - INFO - Action: [0.83794856 0.83794856 0.83794856 0.83794856], Reward: tensor([-125.1991], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:13,522 - __main__ - INFO - Action: [0.91650165 0.91650165 0.91650165 0.91650165], Reward: tensor([-127.2990], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:14,335 - __main__ - INFO - Epoch 0, Iteration 50: Reward: tensor([-3896.2249], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:31:18,194 - __main__ - INFO - Action: [0.83904791 0.83904791 0.83904791 0.83904791], Reward: tensor([-129.5747], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:22,201 - __main__ - INFO - Action: [0.41952395 0.41952395 0.41952395 0.41952395], Reward: tensor([-131.1516], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:26,568 - __main__ - INFO - Action: [0.70728935 0.70728935 0.70728935 0.70728935], Reward: tensor([-133.5968], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:30,410 - __main__ - INFO - Action: [0.35364468 0.35364468 0.35364468 0.35364468], Reward: tensor([-135.3506], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:34,408 - __main__ - INFO - Action: [0.65883613 0.65883613 0.65883613 0.65883613], Reward: tensor([-137.4483], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:38,793 - __main__ - INFO - Action: [0.81143185 0.81143185 0.81143185 0.81143185], Reward: tensor([-139.7997], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:42,612 - __main__ - INFO - Action: [0.88772972 0.88772972 0.88772972 0.88772972], Reward: tensor([-141.8174], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:46,570 - __main__ - INFO - Action: [0.44386486 0.44386486 0.44386486 0.44386486], Reward: tensor([-143.3605], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:50,774 - __main__ - INFO - Action: [0.22193243 0.22193243 0.22193243 0.22193243], Reward: tensor([-145.1944], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:54,827 - __main__ - INFO - Action: [0.11096621 0.11096621 0.11096621 0.11096621], Reward: tensor([-147.1341], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:55,654 - __main__ - INFO - Epoch 0, Iteration 60: Reward: tensor([-5280.6528], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:31:59,252 - __main__ - INFO - Action: [0.05548311 0.05548311 0.05548311 0.05548311], Reward: tensor([-149.3619], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:03,616 - __main__ - INFO - Action: [0.02774155 0.02774155 0.02774155 0.02774155], Reward: tensor([-151.4281], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:07,452 - __main__ - INFO - Action: [0.51139815 0.51139815 0.51139815 0.51139815], Reward: tensor([-154.0549], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:11,659 - __main__ - INFO - Action: [0.75322645 0.75322645 0.75322645 0.75322645], Reward: tensor([-156.1270], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:15,904 - __main__ - INFO - Action: [0.8741406 0.8741406 0.8741406 0.8741406], Reward: tensor([-158.4312], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:20,306 - __main__ - INFO - Action: [0.4370703 0.4370703 0.4370703 0.4370703], Reward: tensor([-160.1314], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:24,861 - __main__ - INFO - Action: [0.21853515 0.21853515 0.21853515 0.21853515], Reward: tensor([-162.2032], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:28,780 - __main__ - INFO - Action: [0.10926758 0.10926758 0.10926758 0.10926758], Reward: tensor([-164.1185], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:33,032 - __main__ - INFO - Action: [0.53664758 0.53664758 0.53664758 0.53664758], Reward: tensor([-166.6414], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:37,168 - __main__ - INFO - Action: [0.76585117 0.76585117 0.76585117 0.76585117], Reward: tensor([-168.9890], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:37,667 - __main__ - INFO - Epoch 0, Iteration 70: Reward: tensor([-6872.1392], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:32:41,356 - __main__ - INFO - Action: [0.88045296 0.88045296 0.88045296 0.88045296], Reward: tensor([-171.1259], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:45,582 - __main__ - INFO - Action: [0.92224027 0.92224027 0.92224027 0.92224027], Reward: tensor([-173.3195], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:49,883 - __main__ - INFO - Action: [0.46112013 0.46112013 0.46112013 0.46112013], Reward: tensor([-175.0023], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:53,995 - __main__ - INFO - Action: [0.71257386 0.71257386 0.71257386 0.71257386], Reward: tensor([-177.2871], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:58,281 - __main__ - INFO - Action: [0.35628693 0.35628693 0.35628693 0.35628693], Reward: tensor([-179.0268], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:02,628 - __main__ - INFO - Action: [0.17814346 0.17814346 0.17814346 0.17814346], Reward: tensor([-181.0825], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:06,961 - __main__ - INFO - Action: [0.58659911 0.58659911 0.58659911 0.58659911], Reward: tensor([-183.6661], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:11,158 - __main__ - INFO - Action: [0.29329955 0.29329955 0.29329955 0.29329955], Reward: tensor([-185.4171], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:15,455 - __main__ - INFO - Action: [0.62866357 0.62866357 0.62866357 0.62866357], Reward: tensor([-187.9830], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:19,703 - __main__ - INFO - Action: [0.81185916 0.81185916 0.81185916 0.81185916], Reward: tensor([-190.2345], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:20,567 - __main__ - INFO - Epoch 0, Iteration 80: Reward: tensor([-8676.2842], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:33:24,516 - __main__ - INFO - Action: [0.88794337 0.88794337 0.88794337 0.88794337], Reward: tensor([-192.7342], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:28,743 - __main__ - INFO - Action: [0.92598548 0.92598548 0.92598548 0.92598548], Reward: tensor([-194.8718], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:32,928 - __main__ - INFO - Action: [0.46299274 0.46299274 0.46299274 0.46299274], Reward: tensor([-196.5436], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:36,961 - __main__ - INFO - Action: [0.71351016 0.71351016 0.71351016 0.71351016], Reward: tensor([-198.8730], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:41,210 - __main__ - INFO - Action: [0.85428246 0.85428246 0.85428246 0.85428246], Reward: tensor([-201.0102], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:45,428 - __main__ - INFO - Action: [0.80793831 0.80793831 0.80793831 0.80793831], Reward: tensor([-203.1579], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:49,624 - __main__ - INFO - Action: [0.40396915 0.40396915 0.40396915 0.40396915], Reward: tensor([-204.8801], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:53,874 - __main__ - INFO - Action: [0.20198458 0.20198458 0.20198458 0.20198458], Reward: tensor([-206.7082], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:58,269 - __main__ - INFO - Action: [0.10099229 0.10099229 0.10099229 0.10099229], Reward: tensor([-208.7950], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:02,465 - __main__ - INFO - Action: [0.05049614 0.05049614 0.05049614 0.05049614], Reward: tensor([-210.8508], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:03,417 - __main__ - INFO - Epoch 0, Iteration 90: Reward: tensor([-10694.7090], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:34:07,132 - __main__ - INFO - Action: [0.02524807 0.02524807 0.02524807 0.02524807], Reward: tensor([-213.3649], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:11,161 - __main__ - INFO - Action: [0.49463783 0.49463783 0.49463783 0.49463783], Reward: tensor([-215.6177], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:15,225 - __main__ - INFO - Action: [0.74484629 0.74484629 0.74484629 0.74484629], Reward: tensor([-218.0343], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:19,208 - __main__ - INFO - Action: [0.37242314 0.37242314 0.37242314 0.37242314], Reward: tensor([-219.7157], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:23,412 - __main__ - INFO - Action: [0.18621157 0.18621157 0.18621157 0.18621157], Reward: tensor([-221.4910], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:27,753 - __main__ - INFO - Action: [0.59063316 0.59063316 0.59063316 0.59063316], Reward: tensor([-224.0187], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:32,042 - __main__ - INFO - Action: [0.77733037 0.77733037 0.77733037 0.77733037], Reward: tensor([-226.4055], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:36,579 - __main__ - INFO - Action: [0.88619256 0.88619256 0.88619256 0.88619256], Reward: tensor([-228.7117], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:40,825 - __main__ - INFO - Action: [0.82389336 0.82389336 0.82389336 0.82389336], Reward: tensor([-230.8335], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:44,847 - __main__ - INFO - Action: [0.41194668 0.41194668 0.41194668 0.41194668], Reward: tensor([-232.4606], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:45,724 - __main__ - INFO - Epoch 0, Iteration 100: Reward: tensor([-12925.3643], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:34:49,546 - __main__ - INFO - Action: [0.58677042 0.58677042 0.58677042 0.58677042], Reward: tensor([-234.9537], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:54,012 - __main__ - INFO - Action: [0.775399 0.775399 0.775399 0.775399], Reward: tensor([-237.3621], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:58,376 - __main__ - INFO - Action: [0.86971329 0.86971329 0.86971329 0.86971329], Reward: tensor([-239.6548], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:02,631 - __main__ - INFO - Action: [0.81565372 0.81565372 0.81565372 0.81565372], Reward: tensor([-241.7460], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:06,698 - __main__ - INFO - Action: [0.40782686 0.40782686 0.40782686 0.40782686], Reward: tensor([-243.3411], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:11,066 - __main__ - INFO - Action: [0.58471051 0.58471051 0.58471051 0.58471051], Reward: tensor([-245.6532], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:15,361 - __main__ - INFO - Action: [0.67315233 0.67315233 0.67315233 0.67315233], Reward: tensor([-247.8915], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:19,805 - __main__ - INFO - Action: [0.71737324 0.71737324 0.71737324 0.71737324], Reward: tensor([-250.1651], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:24,137 - __main__ - INFO - Action: [0.35868662 0.35868662 0.35868662 0.35868662], Reward: tensor([-252.0289], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:28,235 - __main__ - INFO - Action: [0.17934331 0.17934331 0.17934331 0.17934331], Reward: tensor([-253.8882], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:29,078 - __main__ - INFO - Epoch 0, Iteration 110: Reward: tensor([-15372.0508], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:35:33,095 - __main__ - INFO - Action: [0.58719903 0.58719903 0.58719903 0.58719903], Reward: tensor([-256.7513], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:37,006 - __main__ - INFO - Action: [0.79112689 0.79112689 0.79112689 0.79112689], Reward: tensor([-259.0311], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:41,181 - __main__ - INFO - Action: [0.89309082 0.89309082 0.89309082 0.89309082], Reward: tensor([-261.0426], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:45,508 - __main__ - INFO - Action: [0.44654541 0.44654541 0.44654541 0.44654541], Reward: tensor([-262.7676], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:49,776 - __main__ - INFO - Action: [0.22327271 0.22327271 0.22327271 0.22327271], Reward: tensor([-264.6800], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:54,069 - __main__ - INFO - Action: [0.49243343 0.49243343 0.49243343 0.49243343], Reward: tensor([-267.1265], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:58,260 - __main__ - INFO - Action: [0.62701379 0.62701379 0.62701379 0.62701379], Reward: tensor([-269.3263], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:02,410 - __main__ - INFO - Action: [0.81103427 0.81103427 0.81103427 0.81103427], Reward: tensor([-271.6686], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:06,501 - __main__ - INFO - Action: [0.78631421 0.78631421 0.78631421 0.78631421], Reward: tensor([-273.6430], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:10,727 - __main__ - INFO - Action: [0.89068448 0.89068448 0.89068448 0.89068448], Reward: tensor([-275.8999], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:11,353 - __main__ - INFO - Epoch 0, Iteration 120: Reward: tensor([-18033.9883], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:36:15,525 - __main__ - INFO - Action: [0.82613932 0.82613932 0.82613932 0.82613932], Reward: tensor([-278.1014], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:19,721 - __main__ - INFO - Action: [0.91059704 0.91059704 0.91059704 0.91059704], Reward: tensor([-280.3684], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:24,083 - __main__ - INFO - Action: [0.9528259 0.9528259 0.9528259 0.9528259], Reward: tensor([-282.5532], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:28,542 - __main__ - INFO - Action: [0.47641295 0.47641295 0.47641295 0.47641295], Reward: tensor([-284.3470], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:32,589 - __main__ - INFO - Action: [0.73573385 0.73573385 0.73573385 0.73573385], Reward: tensor([-286.6373], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:36,912 - __main__ - INFO - Action: [0.36786693 0.36786693 0.36786693 0.36786693], Reward: tensor([-288.3457], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:41,059 - __main__ - INFO - Action: [0.18393346 0.18393346 0.18393346 0.18393346], Reward: tensor([-290.3515], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:45,305 - __main__ - INFO - Action: [0.09196673 0.09196673 0.09196673 0.09196673], Reward: tensor([-292.3698], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:49,542 - __main__ - INFO - Action: [0.54351074 0.54351074 0.54351074 0.54351074], Reward: tensor([-294.8594], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:53,989 - __main__ - INFO - Action: [0.75376916 0.75376916 0.75376916 0.75376916], Reward: tensor([-297.3133], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:54,940 - __main__ - INFO - Epoch 0, Iteration 130: Reward: tensor([-20909.2344], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:36:58,928 - __main__ - INFO - Action: [0.37688458 0.37688458 0.37688458 0.37688458], Reward: tensor([-299.4723], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:02,927 - __main__ - INFO - Action: [0.18844229 0.18844229 0.18844229 0.18844229], Reward: tensor([-301.3159], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:07,321 - __main__ - INFO - Action: [0.59174852 0.59174852 0.59174852 0.59174852], Reward: tensor([-303.8420], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:11,540 - __main__ - INFO - Action: [0.29587426 0.29587426 0.29587426 0.29587426], Reward: tensor([-305.6257], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:14,781 - __main__ - INFO - Action: [0.14793713 0.14793713 0.14793713 0.14793713], Reward: tensor([-307.2327], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:19,143 - __main__ - INFO - Action: [0.55598236 0.55598236 0.55598236 0.55598236], Reward: tensor([-309.7164], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:23,295 - __main__ - INFO - Action: [0.27799118 0.27799118 0.27799118 0.27799118], Reward: tensor([-311.5322], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:27,602 - __main__ - INFO - Action: [0.63652297 0.63652297 0.63652297 0.63652297], Reward: tensor([-314.0059], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:31,893 - __main__ - INFO - Action: [0.69905856 0.69905856 0.69905856 0.69905856], Reward: tensor([-316.1758], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:35,977 - __main__ - INFO - Action: [0.84705666 0.84705666 0.84705666 0.84705666], Reward: tensor([-318.3874], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:36,899 - __main__ - INFO - Epoch 0, Iteration 140: Reward: tensor([-23996.5391], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:37:40,779 - __main__ - INFO - Action: [0.42352833 0.42352833 0.42352833 0.42352833], Reward: tensor([-320.3968], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:44,880 - __main__ - INFO - Action: [0.70929154 0.70929154 0.70929154 0.70929154], Reward: tensor([-322.8085], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:49,079 - __main__ - INFO - Action: [0.35464577 0.35464577 0.35464577 0.35464577], Reward: tensor([-324.4279], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:53,042 - __main__ - INFO - Action: [0.55811996 0.55811996 0.55811996 0.55811996], Reward: tensor([-326.6927], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:57,280 - __main__ - INFO - Action: [0.65985706 0.65985706 0.65985706 0.65985706], Reward: tensor([-328.9392], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:01,597 - __main__ - INFO - Action: [0.32992853 0.32992853 0.32992853 0.32992853], Reward: tensor([-330.7143], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:05,507 - __main__ - INFO - Action: [0.54576134 0.54576134 0.54576134 0.54576134], Reward: tensor([-332.9458], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:09,660 - __main__ - INFO - Action: [0.65367775 0.65367775 0.65367775 0.65367775], Reward: tensor([-335.1184], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:13,730 - __main__ - INFO - Action: [0.80885266 0.80885266 0.80885266 0.80885266], Reward: tensor([-337.3349], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:18,171 - __main__ - INFO - Action: [0.78522341 0.78522341 0.78522341 0.78522341], Reward: tensor([-339.4378], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:19,092 - __main__ - INFO - Epoch 0, Iteration 150: Reward: tensor([-27295.3555], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:38:23,089 - __main__ - INFO - Action: [0.8746255 0.8746255 0.8746255 0.8746255], Reward: tensor([-342.0060], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:27,251 - __main__ - INFO - Action: [0.43731275 0.43731275 0.43731275 0.43731275], Reward: tensor([-343.7004], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:31,411 - __main__ - INFO - Action: [0.70067016 0.70067016 0.70067016 0.70067016], Reward: tensor([-345.9879], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:34,930 - __main__ - INFO - Action: [0.84786246 0.84786246 0.84786246 0.84786246], Reward: tensor([-348.1867], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:38,717 - __main__ - INFO - Action: [0.80472831 0.80472831 0.80472831 0.80472831], Reward: tensor([-349.9105], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:42,988 - __main__ - INFO - Action: [0.78316123 0.78316123 0.78316123 0.78316123], Reward: tensor([-351.8674], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:47,112 - __main__ - INFO - Action: [0.88910799 0.88910799 0.88910799 0.88910799], Reward: tensor([-354.0722], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:51,341 - __main__ - INFO - Action: [0.444554 0.444554 0.444554 0.444554], Reward: tensor([-355.6615], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:55,650 - __main__ - INFO - Action: [0.60307408 0.60307408 0.60307408 0.60307408], Reward: tensor([-358.0182], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:59,920 - __main__ - INFO - Action: [0.30153704 0.30153704 0.30153704 0.30153704], Reward: tensor([-359.8423], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:00,856 - __main__ - INFO - Epoch 0, Iteration 160: Reward: tensor([-30804.6094], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:39:04,793 - __main__ - INFO - Action: [0.15076852 0.15076852 0.15076852 0.15076852], Reward: tensor([-362.1536], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:09,142 - __main__ - INFO - Action: [0.57291164 0.57291164 0.57291164 0.57291164], Reward: tensor([-364.7190], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:13,303 - __main__ - INFO - Action: [0.7839832 0.7839832 0.7839832 0.7839832], Reward: tensor([-367.0282], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:17,044 - __main__ - INFO - Action: [0.3919916 0.3919916 0.3919916 0.3919916], Reward: tensor([-368.6002], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:21,427 - __main__ - INFO - Action: [0.1959958 0.1959958 0.1959958 0.1959958], Reward: tensor([-370.4371], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:25,589 - __main__ - INFO - Action: [0.0979979 0.0979979 0.0979979 0.0979979], Reward: tensor([-372.5788], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:29,771 - __main__ - INFO - Action: [0.04899895 0.04899895 0.04899895 0.04899895], Reward: tensor([-374.5405], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:34,212 - __main__ - INFO - Action: [0.02449947 0.02449947 0.02449947 0.02449947], Reward: tensor([-376.6656], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:38,481 - __main__ - INFO - Action: [0.49426353 0.49426353 0.49426353 0.49426353], Reward: tensor([-379.3184], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:42,874 - __main__ - INFO - Action: [0.74465914 0.74465914 0.74465914 0.74465914], Reward: tensor([-381.7497], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:43,804 - __main__ - INFO - Epoch 0, Iteration 170: Reward: tensor([-34522.3984], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:39:47,577 - __main__ - INFO - Action: [0.37232957 0.37232957 0.37232957 0.37232957], Reward: tensor([-383.8071], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:51,959 - __main__ - INFO - Action: [0.66817858 0.66817858 0.66817858 0.66817858], Reward: tensor([-386.2524], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:56,274 - __main__ - INFO - Action: [0.83161666 0.83161666 0.83161666 0.83161666], Reward: tensor([-388.5569], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:00,447 - __main__ - INFO - Action: [0.41580833 0.41580833 0.41580833 0.41580833], Reward: tensor([-390.2329], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:04,346 - __main__ - INFO - Action: [0.58870124 0.58870124 0.58870124 0.58870124], Reward: tensor([-392.5265], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:08,771 - __main__ - INFO - Action: [0.29435062 0.29435062 0.29435062 0.29435062], Reward: tensor([-394.2473], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:13,132 - __main__ - INFO - Action: [0.52797239 0.52797239 0.52797239 0.52797239], Reward: tensor([-396.6790], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:17,206 - __main__ - INFO - Action: [0.74599998 0.74599998 0.74599998 0.74599998], Reward: tensor([-398.9883], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:21,292 - __main__ - INFO - Action: [0.37299999 0.37299999 0.37299999 0.37299999], Reward: tensor([-400.6265], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:25,636 - __main__ - INFO - Action: [0.56729707 0.56729707 0.56729707 0.56729707], Reward: tensor([-402.9913], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:26,515 - __main__ - INFO - Epoch 0, Iteration 180: Reward: tensor([-38457.3125], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:40:30,346 - __main__ - INFO - Action: [0.28364854 0.28364854 0.28364854 0.28364854], Reward: tensor([-405.0732], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:34,320 - __main__ - INFO - Action: [0.63935165 0.63935165 0.63935165 0.63935165], Reward: tensor([-407.4146], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:38,715 - __main__ - INFO - Action: [0.31967582 0.31967582 0.31967582 0.31967582], Reward: tensor([-409.2866], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:42,977 - __main__ - INFO - Action: [0.15983791 0.15983791 0.15983791 0.15983791], Reward: tensor([-411.2062], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:47,376 - __main__ - INFO - Action: [0.57744633 0.57744633 0.57744633 0.57744633], Reward: tensor([-413.8323], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:51,701 - __main__ - INFO - Action: [0.78625054 0.78625054 0.78625054 0.78625054], Reward: tensor([-416.2586], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:55,838 - __main__ - INFO - Action: [0.89065265 0.89065265 0.89065265 0.89065265], Reward: tensor([-418.3843], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:00,046 - __main__ - INFO - Action: [0.8261234 0.8261234 0.8261234 0.8261234], Reward: tensor([-420.4400], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:04,218 - __main__ - INFO - Action: [0.4130617 0.4130617 0.4130617 0.4130617], Reward: tensor([-422.1569], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:08,269 - __main__ - INFO - Action: [0.68854464 0.68854464 0.68854464 0.68854464], Reward: tensor([-424.4309], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:09,079 - __main__ - INFO - Epoch 0, Iteration 190: Reward: tensor([-42605.7930], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:41:12,951 - __main__ - INFO - Action: [0.34427232 0.34427232 0.34427232 0.34427232], Reward: tensor([-426.4258], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:17,205 - __main__ - INFO - Action: [0.66966354 0.66966354 0.66966354 0.66966354], Reward: tensor([-428.8233], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:21,427 - __main__ - INFO - Action: [0.83235915 0.83235915 0.83235915 0.83235915], Reward: tensor([-431.1330], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:25,692 - __main__ - INFO - Action: [0.89819336 0.89819336 0.89819336 0.89819336], Reward: tensor([-433.3566], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:30,168 - __main__ - INFO - Action: [0.93111047 0.93111047 0.93111047 0.93111047], Reward: tensor([-435.5843], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:34,453 - __main__ - INFO - Action: [0.46555524 0.46555524 0.46555524 0.46555524], Reward: tensor([-437.2262], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:38,717 - __main__ - INFO - Action: [0.23277762 0.23277762 0.23277762 0.23277762], Reward: tensor([-439.1897], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:42,882 - __main__ - INFO - Action: [0.61391619 0.61391619 0.61391619 0.61391619], Reward: tensor([-441.6406], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:46,737 - __main__ - INFO - Action: [0.30695809 0.30695809 0.30695809 0.30695809], Reward: tensor([-443.3694], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:50,888 - __main__ - INFO - Action: [0.15347905 0.15347905 0.15347905 0.15347905], Reward: tensor([-445.2600], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:51,734 - __main__ - INFO - Epoch 0, Iteration 200: Reward: tensor([-46967.8086], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:41:55,629 - __main__ - INFO - Action: [0.07673952 0.07673952 0.07673952 0.07673952], Reward: tensor([-447.4648], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:59,960 - __main__ - INFO - Action: [0.03836976 0.03836976 0.03836976 0.03836976], Reward: tensor([-449.5772], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:03,963 - __main__ - INFO - Action: [0.01918488 0.01918488 0.01918488 0.01918488], Reward: tensor([-451.6452], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:08,183 - __main__ - INFO - Action: [0.49160623 0.49160623 0.49160623 0.49160623], Reward: tensor([-454.2308], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:12,421 - __main__ - INFO - Action: [0.24580312 0.24580312 0.24580312 0.24580312], Reward: tensor([-456.0006], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:16,687 - __main__ - INFO - Action: [0.50369864 0.50369864 0.50369864 0.50369864], Reward: tensor([-458.4463], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:20,895 - __main__ - INFO - Action: [0.25184932 0.25184932 0.25184932 0.25184932], Reward: tensor([-460.3289], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:24,976 - __main__ - INFO - Action: [0.60793845 0.60793845 0.60793845 0.60793845], Reward: tensor([-462.7050], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:29,297 - __main__ - INFO - Action: [0.6847663 0.6847663 0.6847663 0.6847663], Reward: tensor([-464.8925], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:33,439 - __main__ - INFO - Action: [0.34238315 0.34238315 0.34238315 0.34238315], Reward: tensor([-466.6912], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:34,438 - __main__ - INFO - Epoch 0, Iteration 210: Reward: tensor([-51539.7852], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:42:37,402 - __main__ - INFO - Action: [0.55198865 0.55198865 0.55198865 0.55198865], Reward: tensor([-469.2740], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:41,681 - __main__ - INFO - Action: [0.27599433 0.27599433 0.27599433 0.27599433], Reward: tensor([-470.6411], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:46,075 - __main__ - INFO - Action: [0.13799716 0.13799716 0.13799716 0.13799716], Reward: tensor([-472.7499], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:50,358 - __main__ - INFO - Action: [0.56652596 0.56652596 0.56652596 0.56652596], Reward: tensor([-475.3058], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:54,743 - __main__ - INFO - Action: [0.66406006 0.66406006 0.66406006 0.66406006], Reward: tensor([-477.6497], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:58,981 - __main__ - INFO - Action: [0.33203003 0.33203003 0.33203003 0.33203003], Reward: tensor([-479.4310], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:03,093 - __main__ - INFO - Action: [0.66354239 0.66354239 0.66354239 0.66354239], Reward: tensor([-481.8344], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:06,883 - __main__ - INFO - Action: [0.82929857 0.82929857 0.82929857 0.82929857], Reward: tensor([-484.0639], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:11,026 - __main__ - INFO - Action: [0.41464929 0.41464929 0.41464929 0.41464929], Reward: tensor([-485.5319], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:15,191 - __main__ - INFO - Action: [0.20732464 0.20732464 0.20732464 0.20732464], Reward: tensor([-487.3548], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:16,071 - __main__ - INFO - Epoch 0, Iteration 220: Reward: tensor([-56323.6172], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:43:19,936 - __main__ - INFO - Action: [0.4844594 0.4844594 0.4844594 0.4844594], Reward: tensor([-490.0662], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:24,040 - __main__ - INFO - Action: [0.72424349 0.72424349 0.72424349 0.72424349], Reward: tensor([-492.3724], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:28,203 - __main__ - INFO - Action: [0.74291882 0.74291882 0.74291882 0.74291882], Reward: tensor([-494.5450], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:32,269 - __main__ - INFO - Action: [0.37145941 0.37145941 0.37145941 0.37145941], Reward: tensor([-496.0743], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:36,546 - __main__ - INFO - Action: [0.18572971 0.18572971 0.18572971 0.18572971], Reward: tensor([-498.0864], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:40,797 - __main__ - INFO - Action: [0.47366193 0.47366193 0.47366193 0.47366193], Reward: tensor([-500.4902], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:44,988 - __main__ - INFO - Action: [0.23683097 0.23683097 0.23683097 0.23683097], Reward: tensor([-502.3244], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:49,296 - __main__ - INFO - Action: [0.11841548 0.11841548 0.11841548 0.11841548], Reward: tensor([-504.3283], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:53,700 - __main__ - INFO - Action: [0.05920774 0.05920774 0.05920774 0.05920774], Reward: tensor([-506.4879], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:57,985 - __main__ - INFO - Action: [0.02960387 0.02960387 0.02960387 0.02960387], Reward: tensor([-508.6148], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:58,795 - __main__ - INFO - Epoch 0, Iteration 230: Reward: tensor([-61317.0039], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:44:02,561 - __main__ - INFO - Action: [0.39559901 0.39559901 0.39559901 0.39559901], Reward: tensor([-511.2826], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:06,622 - __main__ - INFO - Action: [0.69532688 0.69532688 0.69532688 0.69532688], Reward: tensor([-513.6509], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:11,045 - __main__ - INFO - Action: [0.84519082 0.84519082 0.84519082 0.84519082], Reward: tensor([-515.9870], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:14,990 - __main__ - INFO - Action: [0.9046092 0.9046092 0.9046092 0.9046092], Reward: tensor([-518.0596], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:19,353 - __main__ - INFO - Action: [0.94983198 0.94983198 0.94983198 0.94983198], Reward: tensor([-520.2553], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:23,514 - __main__ - INFO - Action: [0.47491599 0.47491599 0.47491599 0.47491599], Reward: tensor([-521.8465], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:27,294 - __main__ - INFO - Episode timed out.
2024-06-05 17:44:27,836 - __main__ - INFO - Action: [0.23745799 0.23745799 0.23745799 0.23745799], Reward: tensor([-523.7324], grad_fn=<AddBackward0>), Done: True
