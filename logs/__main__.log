2024-06-05 17:11:49,758 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:11:49,867 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:11:52,448 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:11:52,840 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:16:16,226 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:16:16,335 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:16:18,237 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:16:18,704 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:20:14,135 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:20:14,242 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:20:16,195 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:20:16,602 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:23:28,395 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:23:28,489 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:23:30,253 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:23:30,737 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:27:40,701 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-05 17:27:40,746 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-05 17:27:42,841 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-05 17:27:43,281 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-05 17:27:48,000 - __main__ - INFO - Action: [0.48201379 0.48201379 0.48201379 0.48201379], Reward: tensor([-74.0156], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:27:48,920 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-74.0156], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:27:52,745 - __main__ - INFO - Action: [0.62180397 0.62180397 0.62180397 0.62180397], Reward: tensor([-26.6078], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:27:56,404 - __main__ - INFO - Action: [0.69169906 0.69169906 0.69169906 0.69169906], Reward: tensor([-28.4593], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:27:59,844 - __main__ - INFO - Action: [0.72664661 0.72664661 0.72664661 0.72664661], Reward: tensor([-30.2449], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:04,124 - __main__ - INFO - Action: [0.8453371 0.8453371 0.8453371 0.8453371], Reward: tensor([-32.4828], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:08,515 - __main__ - INFO - Action: [0.92019592 0.92019592 0.92019592 0.92019592], Reward: tensor([-34.7442], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:12,716 - __main__ - INFO - Action: [0.95762534 0.95762534 0.95762534 0.95762534], Reward: tensor([-36.9640], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:17,093 - __main__ - INFO - Action: [0.97634005 0.97634005 0.97634005 0.97634005], Reward: tensor([-39.0883], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:21,469 - __main__ - INFO - Action: [0.97018381 0.97018381 0.97018381 0.97018381], Reward: tensor([-41.2531], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:25,729 - __main__ - INFO - Action: [0.86588898 0.86588898 0.86588898 0.86588898], Reward: tensor([-43.2977], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:29,819 - __main__ - INFO - Action: [0.91495828 0.91495828 0.91495828 0.91495828], Reward: tensor([-45.4935], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:30,740 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-432.6512], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:28:34,101 - __main__ - INFO - Action: [0.83827622 0.83827622 0.83827622 0.83827622], Reward: tensor([-47.8286], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:37,450 - __main__ - INFO - Action: [0.79993519 0.79993519 0.79993519 0.79993519], Reward: tensor([-49.0679], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:41,437 - __main__ - INFO - Action: [0.89749497 0.89749497 0.89749497 0.89749497], Reward: tensor([-51.2922], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:45,573 - __main__ - INFO - Action: [0.94627486 0.94627486 0.94627486 0.94627486], Reward: tensor([-53.3502], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:49,837 - __main__ - INFO - Action: [0.97066481 0.97066481 0.97066481 0.97066481], Reward: tensor([-55.4436], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:53,870 - __main__ - INFO - Action: [0.4853324 0.4853324 0.4853324 0.4853324], Reward: tensor([-57.0589], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:28:57,600 - __main__ - INFO - Action: [0.62346328 0.62346328 0.62346328 0.62346328], Reward: tensor([-59.0123], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:01,429 - __main__ - INFO - Action: [0.31173164 0.31173164 0.31173164 0.31173164], Reward: tensor([-60.8062], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:04,977 - __main__ - INFO - Action: [0.15586582 0.15586582 0.15586582 0.15586582], Reward: tensor([-62.2058], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:09,264 - __main__ - INFO - Action: [0.45872999 0.45872999 0.45872999 0.45872999], Reward: tensor([-64.7589], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:10,167 - __main__ - INFO - Epoch 0, Iteration 20: Reward: tensor([-993.4758], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:29:13,373 - __main__ - INFO - Action: [0.22936499 0.22936499 0.22936499 0.22936499], Reward: tensor([-66.5770], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:17,168 - __main__ - INFO - Action: [0.1146825 0.1146825 0.1146825 0.1146825], Reward: tensor([-68.2964], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:21,369 - __main__ - INFO - Action: [0.05734125 0.05734125 0.05734125 0.05734125], Reward: tensor([-70.3494], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:25,573 - __main__ - INFO - Action: [0.02867062 0.02867062 0.02867062 0.02867062], Reward: tensor([-72.4235], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:29,811 - __main__ - INFO - Action: [0.01433531 0.01433531 0.01433531 0.01433531], Reward: tensor([-74.5531], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:33,274 - __main__ - INFO - Action: [0.00716766 0.00716766 0.00716766 0.00716766], Reward: tensor([-76.5553], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:37,758 - __main__ - INFO - Action: [0.5011112 0.5011112 0.5011112 0.5011112], Reward: tensor([-78.9387], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:41,741 - __main__ - INFO - Action: [0.63135268 0.63135268 0.63135268 0.63135268], Reward: tensor([-81.0394], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:45,718 - __main__ - INFO - Action: [0.31567634 0.31567634 0.31567634 0.31567634], Reward: tensor([-82.9167], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:49,558 - __main__ - INFO - Action: [0.15783817 0.15783817 0.15783817 0.15783817], Reward: tensor([-84.6146], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:50,551 - __main__ - INFO - Epoch 0, Iteration 30: Reward: tensor([-1749.7399], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:29:54,453 - __main__ - INFO - Action: [0.57644646 0.57644646 0.57644646 0.57644646], Reward: tensor([-87.4627], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:29:58,512 - __main__ - INFO - Action: [0.66902031 0.66902031 0.66902031 0.66902031], Reward: tensor([-89.4907], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:02,327 - __main__ - INFO - Action: [0.81652394 0.81652394 0.81652394 0.81652394], Reward: tensor([-91.6264], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:06,489 - __main__ - INFO - Action: [0.78905905 0.78905905 0.78905905 0.78905905], Reward: tensor([-93.6130], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:10,882 - __main__ - INFO - Action: [0.87654332 0.87654332 0.87654332 0.87654332], Reward: tensor([-95.8886], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:14,927 - __main__ - INFO - Action: [0.43827166 0.43827166 0.43827166 0.43827166], Reward: tensor([-97.5543], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:19,160 - __main__ - INFO - Action: [0.70114962 0.70114962 0.70114962 0.70114962], Reward: tensor([-99.9066], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:23,340 - __main__ - INFO - Action: [0.35057481 0.35057481 0.35057481 0.35057481], Reward: tensor([-101.6071], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:27,517 - __main__ - INFO - Action: [0.65730119 0.65730119 0.65730119 0.65730119], Reward: tensor([-104.0234], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:31,628 - __main__ - INFO - Action: [0.82617797 0.82617797 0.82617797 0.82617797], Reward: tensor([-106.2457], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:32,456 - __main__ - INFO - Epoch 0, Iteration 40: Reward: tensor([-2717.1587], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:30:36,284 - __main__ - INFO - Action: [0.91061636 0.91061636 0.91061636 0.91061636], Reward: tensor([-108.6688], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:40,582 - __main__ - INFO - Action: [0.93732197 0.93732197 0.93732197 0.93732197], Reward: tensor([-110.8353], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:44,487 - __main__ - INFO - Action: [0.95067478 0.95067478 0.95067478 0.95067478], Reward: tensor([-112.8008], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:48,803 - __main__ - INFO - Action: [0.47533739 0.47533739 0.47533739 0.47533739], Reward: tensor([-114.4695], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:53,121 - __main__ - INFO - Action: [0.71968248 0.71968248 0.71968248 0.71968248], Reward: tensor([-116.8626], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:30:57,174 - __main__ - INFO - Action: [0.85736862 0.85736862 0.85736862 0.85736862], Reward: tensor([-119.1196], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:01,282 - __main__ - INFO - Action: [0.42868431 0.42868431 0.42868431 0.42868431], Reward: tensor([-120.7260], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:05,521 - __main__ - INFO - Action: [0.71186953 0.71186953 0.71186953 0.71186953], Reward: tensor([-123.0856], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:09,610 - __main__ - INFO - Action: [0.83794856 0.83794856 0.83794856 0.83794856], Reward: tensor([-125.1991], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:13,522 - __main__ - INFO - Action: [0.91650165 0.91650165 0.91650165 0.91650165], Reward: tensor([-127.2990], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:14,335 - __main__ - INFO - Epoch 0, Iteration 50: Reward: tensor([-3896.2249], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:31:18,194 - __main__ - INFO - Action: [0.83904791 0.83904791 0.83904791 0.83904791], Reward: tensor([-129.5747], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:22,201 - __main__ - INFO - Action: [0.41952395 0.41952395 0.41952395 0.41952395], Reward: tensor([-131.1516], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:26,568 - __main__ - INFO - Action: [0.70728935 0.70728935 0.70728935 0.70728935], Reward: tensor([-133.5968], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:30,410 - __main__ - INFO - Action: [0.35364468 0.35364468 0.35364468 0.35364468], Reward: tensor([-135.3506], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:34,408 - __main__ - INFO - Action: [0.65883613 0.65883613 0.65883613 0.65883613], Reward: tensor([-137.4483], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:38,793 - __main__ - INFO - Action: [0.81143185 0.81143185 0.81143185 0.81143185], Reward: tensor([-139.7997], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:42,612 - __main__ - INFO - Action: [0.88772972 0.88772972 0.88772972 0.88772972], Reward: tensor([-141.8174], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:46,570 - __main__ - INFO - Action: [0.44386486 0.44386486 0.44386486 0.44386486], Reward: tensor([-143.3605], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:50,774 - __main__ - INFO - Action: [0.22193243 0.22193243 0.22193243 0.22193243], Reward: tensor([-145.1944], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:54,827 - __main__ - INFO - Action: [0.11096621 0.11096621 0.11096621 0.11096621], Reward: tensor([-147.1341], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:31:55,654 - __main__ - INFO - Epoch 0, Iteration 60: Reward: tensor([-5280.6528], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:31:59,252 - __main__ - INFO - Action: [0.05548311 0.05548311 0.05548311 0.05548311], Reward: tensor([-149.3619], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:03,616 - __main__ - INFO - Action: [0.02774155 0.02774155 0.02774155 0.02774155], Reward: tensor([-151.4281], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:07,452 - __main__ - INFO - Action: [0.51139815 0.51139815 0.51139815 0.51139815], Reward: tensor([-154.0549], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:11,659 - __main__ - INFO - Action: [0.75322645 0.75322645 0.75322645 0.75322645], Reward: tensor([-156.1270], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:15,904 - __main__ - INFO - Action: [0.8741406 0.8741406 0.8741406 0.8741406], Reward: tensor([-158.4312], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:20,306 - __main__ - INFO - Action: [0.4370703 0.4370703 0.4370703 0.4370703], Reward: tensor([-160.1314], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:24,861 - __main__ - INFO - Action: [0.21853515 0.21853515 0.21853515 0.21853515], Reward: tensor([-162.2032], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:28,780 - __main__ - INFO - Action: [0.10926758 0.10926758 0.10926758 0.10926758], Reward: tensor([-164.1185], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:33,032 - __main__ - INFO - Action: [0.53664758 0.53664758 0.53664758 0.53664758], Reward: tensor([-166.6414], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:37,168 - __main__ - INFO - Action: [0.76585117 0.76585117 0.76585117 0.76585117], Reward: tensor([-168.9890], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:37,667 - __main__ - INFO - Epoch 0, Iteration 70: Reward: tensor([-6872.1392], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:32:41,356 - __main__ - INFO - Action: [0.88045296 0.88045296 0.88045296 0.88045296], Reward: tensor([-171.1259], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:45,582 - __main__ - INFO - Action: [0.92224027 0.92224027 0.92224027 0.92224027], Reward: tensor([-173.3195], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:49,883 - __main__ - INFO - Action: [0.46112013 0.46112013 0.46112013 0.46112013], Reward: tensor([-175.0023], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:53,995 - __main__ - INFO - Action: [0.71257386 0.71257386 0.71257386 0.71257386], Reward: tensor([-177.2871], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:32:58,281 - __main__ - INFO - Action: [0.35628693 0.35628693 0.35628693 0.35628693], Reward: tensor([-179.0268], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:02,628 - __main__ - INFO - Action: [0.17814346 0.17814346 0.17814346 0.17814346], Reward: tensor([-181.0825], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:06,961 - __main__ - INFO - Action: [0.58659911 0.58659911 0.58659911 0.58659911], Reward: tensor([-183.6661], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:11,158 - __main__ - INFO - Action: [0.29329955 0.29329955 0.29329955 0.29329955], Reward: tensor([-185.4171], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:15,455 - __main__ - INFO - Action: [0.62866357 0.62866357 0.62866357 0.62866357], Reward: tensor([-187.9830], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:19,703 - __main__ - INFO - Action: [0.81185916 0.81185916 0.81185916 0.81185916], Reward: tensor([-190.2345], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:20,567 - __main__ - INFO - Epoch 0, Iteration 80: Reward: tensor([-8676.2842], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:33:24,516 - __main__ - INFO - Action: [0.88794337 0.88794337 0.88794337 0.88794337], Reward: tensor([-192.7342], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:28,743 - __main__ - INFO - Action: [0.92598548 0.92598548 0.92598548 0.92598548], Reward: tensor([-194.8718], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:32,928 - __main__ - INFO - Action: [0.46299274 0.46299274 0.46299274 0.46299274], Reward: tensor([-196.5436], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:36,961 - __main__ - INFO - Action: [0.71351016 0.71351016 0.71351016 0.71351016], Reward: tensor([-198.8730], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:41,210 - __main__ - INFO - Action: [0.85428246 0.85428246 0.85428246 0.85428246], Reward: tensor([-201.0102], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:45,428 - __main__ - INFO - Action: [0.80793831 0.80793831 0.80793831 0.80793831], Reward: tensor([-203.1579], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:49,624 - __main__ - INFO - Action: [0.40396915 0.40396915 0.40396915 0.40396915], Reward: tensor([-204.8801], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:53,874 - __main__ - INFO - Action: [0.20198458 0.20198458 0.20198458 0.20198458], Reward: tensor([-206.7082], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:33:58,269 - __main__ - INFO - Action: [0.10099229 0.10099229 0.10099229 0.10099229], Reward: tensor([-208.7950], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:02,465 - __main__ - INFO - Action: [0.05049614 0.05049614 0.05049614 0.05049614], Reward: tensor([-210.8508], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:03,417 - __main__ - INFO - Epoch 0, Iteration 90: Reward: tensor([-10694.7090], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:34:07,132 - __main__ - INFO - Action: [0.02524807 0.02524807 0.02524807 0.02524807], Reward: tensor([-213.3649], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:11,161 - __main__ - INFO - Action: [0.49463783 0.49463783 0.49463783 0.49463783], Reward: tensor([-215.6177], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:15,225 - __main__ - INFO - Action: [0.74484629 0.74484629 0.74484629 0.74484629], Reward: tensor([-218.0343], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:19,208 - __main__ - INFO - Action: [0.37242314 0.37242314 0.37242314 0.37242314], Reward: tensor([-219.7157], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:23,412 - __main__ - INFO - Action: [0.18621157 0.18621157 0.18621157 0.18621157], Reward: tensor([-221.4910], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:27,753 - __main__ - INFO - Action: [0.59063316 0.59063316 0.59063316 0.59063316], Reward: tensor([-224.0187], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:32,042 - __main__ - INFO - Action: [0.77733037 0.77733037 0.77733037 0.77733037], Reward: tensor([-226.4055], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:36,579 - __main__ - INFO - Action: [0.88619256 0.88619256 0.88619256 0.88619256], Reward: tensor([-228.7117], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:40,825 - __main__ - INFO - Action: [0.82389336 0.82389336 0.82389336 0.82389336], Reward: tensor([-230.8335], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:44,847 - __main__ - INFO - Action: [0.41194668 0.41194668 0.41194668 0.41194668], Reward: tensor([-232.4606], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:45,724 - __main__ - INFO - Epoch 0, Iteration 100: Reward: tensor([-12925.3643], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:34:49,546 - __main__ - INFO - Action: [0.58677042 0.58677042 0.58677042 0.58677042], Reward: tensor([-234.9537], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:54,012 - __main__ - INFO - Action: [0.775399 0.775399 0.775399 0.775399], Reward: tensor([-237.3621], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:34:58,376 - __main__ - INFO - Action: [0.86971329 0.86971329 0.86971329 0.86971329], Reward: tensor([-239.6548], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:02,631 - __main__ - INFO - Action: [0.81565372 0.81565372 0.81565372 0.81565372], Reward: tensor([-241.7460], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:06,698 - __main__ - INFO - Action: [0.40782686 0.40782686 0.40782686 0.40782686], Reward: tensor([-243.3411], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:11,066 - __main__ - INFO - Action: [0.58471051 0.58471051 0.58471051 0.58471051], Reward: tensor([-245.6532], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:15,361 - __main__ - INFO - Action: [0.67315233 0.67315233 0.67315233 0.67315233], Reward: tensor([-247.8915], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:19,805 - __main__ - INFO - Action: [0.71737324 0.71737324 0.71737324 0.71737324], Reward: tensor([-250.1651], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:24,137 - __main__ - INFO - Action: [0.35868662 0.35868662 0.35868662 0.35868662], Reward: tensor([-252.0289], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:28,235 - __main__ - INFO - Action: [0.17934331 0.17934331 0.17934331 0.17934331], Reward: tensor([-253.8882], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:29,078 - __main__ - INFO - Epoch 0, Iteration 110: Reward: tensor([-15372.0508], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:35:33,095 - __main__ - INFO - Action: [0.58719903 0.58719903 0.58719903 0.58719903], Reward: tensor([-256.7513], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:37,006 - __main__ - INFO - Action: [0.79112689 0.79112689 0.79112689 0.79112689], Reward: tensor([-259.0311], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:41,181 - __main__ - INFO - Action: [0.89309082 0.89309082 0.89309082 0.89309082], Reward: tensor([-261.0426], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:45,508 - __main__ - INFO - Action: [0.44654541 0.44654541 0.44654541 0.44654541], Reward: tensor([-262.7676], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:49,776 - __main__ - INFO - Action: [0.22327271 0.22327271 0.22327271 0.22327271], Reward: tensor([-264.6800], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:54,069 - __main__ - INFO - Action: [0.49243343 0.49243343 0.49243343 0.49243343], Reward: tensor([-267.1265], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:35:58,260 - __main__ - INFO - Action: [0.62701379 0.62701379 0.62701379 0.62701379], Reward: tensor([-269.3263], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:02,410 - __main__ - INFO - Action: [0.81103427 0.81103427 0.81103427 0.81103427], Reward: tensor([-271.6686], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:06,501 - __main__ - INFO - Action: [0.78631421 0.78631421 0.78631421 0.78631421], Reward: tensor([-273.6430], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:10,727 - __main__ - INFO - Action: [0.89068448 0.89068448 0.89068448 0.89068448], Reward: tensor([-275.8999], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:11,353 - __main__ - INFO - Epoch 0, Iteration 120: Reward: tensor([-18033.9883], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:36:15,525 - __main__ - INFO - Action: [0.82613932 0.82613932 0.82613932 0.82613932], Reward: tensor([-278.1014], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:19,721 - __main__ - INFO - Action: [0.91059704 0.91059704 0.91059704 0.91059704], Reward: tensor([-280.3684], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:24,083 - __main__ - INFO - Action: [0.9528259 0.9528259 0.9528259 0.9528259], Reward: tensor([-282.5532], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:28,542 - __main__ - INFO - Action: [0.47641295 0.47641295 0.47641295 0.47641295], Reward: tensor([-284.3470], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:32,589 - __main__ - INFO - Action: [0.73573385 0.73573385 0.73573385 0.73573385], Reward: tensor([-286.6373], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:36,912 - __main__ - INFO - Action: [0.36786693 0.36786693 0.36786693 0.36786693], Reward: tensor([-288.3457], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:41,059 - __main__ - INFO - Action: [0.18393346 0.18393346 0.18393346 0.18393346], Reward: tensor([-290.3515], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:45,305 - __main__ - INFO - Action: [0.09196673 0.09196673 0.09196673 0.09196673], Reward: tensor([-292.3698], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:49,542 - __main__ - INFO - Action: [0.54351074 0.54351074 0.54351074 0.54351074], Reward: tensor([-294.8594], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:53,989 - __main__ - INFO - Action: [0.75376916 0.75376916 0.75376916 0.75376916], Reward: tensor([-297.3133], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:36:54,940 - __main__ - INFO - Epoch 0, Iteration 130: Reward: tensor([-20909.2344], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:36:58,928 - __main__ - INFO - Action: [0.37688458 0.37688458 0.37688458 0.37688458], Reward: tensor([-299.4723], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:02,927 - __main__ - INFO - Action: [0.18844229 0.18844229 0.18844229 0.18844229], Reward: tensor([-301.3159], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:07,321 - __main__ - INFO - Action: [0.59174852 0.59174852 0.59174852 0.59174852], Reward: tensor([-303.8420], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:11,540 - __main__ - INFO - Action: [0.29587426 0.29587426 0.29587426 0.29587426], Reward: tensor([-305.6257], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:14,781 - __main__ - INFO - Action: [0.14793713 0.14793713 0.14793713 0.14793713], Reward: tensor([-307.2327], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:19,143 - __main__ - INFO - Action: [0.55598236 0.55598236 0.55598236 0.55598236], Reward: tensor([-309.7164], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:23,295 - __main__ - INFO - Action: [0.27799118 0.27799118 0.27799118 0.27799118], Reward: tensor([-311.5322], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:27,602 - __main__ - INFO - Action: [0.63652297 0.63652297 0.63652297 0.63652297], Reward: tensor([-314.0059], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:31,893 - __main__ - INFO - Action: [0.69905856 0.69905856 0.69905856 0.69905856], Reward: tensor([-316.1758], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:35,977 - __main__ - INFO - Action: [0.84705666 0.84705666 0.84705666 0.84705666], Reward: tensor([-318.3874], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:36,899 - __main__ - INFO - Epoch 0, Iteration 140: Reward: tensor([-23996.5391], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:37:40,779 - __main__ - INFO - Action: [0.42352833 0.42352833 0.42352833 0.42352833], Reward: tensor([-320.3968], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:44,880 - __main__ - INFO - Action: [0.70929154 0.70929154 0.70929154 0.70929154], Reward: tensor([-322.8085], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:49,079 - __main__ - INFO - Action: [0.35464577 0.35464577 0.35464577 0.35464577], Reward: tensor([-324.4279], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:53,042 - __main__ - INFO - Action: [0.55811996 0.55811996 0.55811996 0.55811996], Reward: tensor([-326.6927], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:37:57,280 - __main__ - INFO - Action: [0.65985706 0.65985706 0.65985706 0.65985706], Reward: tensor([-328.9392], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:01,597 - __main__ - INFO - Action: [0.32992853 0.32992853 0.32992853 0.32992853], Reward: tensor([-330.7143], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:05,507 - __main__ - INFO - Action: [0.54576134 0.54576134 0.54576134 0.54576134], Reward: tensor([-332.9458], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:09,660 - __main__ - INFO - Action: [0.65367775 0.65367775 0.65367775 0.65367775], Reward: tensor([-335.1184], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:13,730 - __main__ - INFO - Action: [0.80885266 0.80885266 0.80885266 0.80885266], Reward: tensor([-337.3349], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:18,171 - __main__ - INFO - Action: [0.78522341 0.78522341 0.78522341 0.78522341], Reward: tensor([-339.4378], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:19,092 - __main__ - INFO - Epoch 0, Iteration 150: Reward: tensor([-27295.3555], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:38:23,089 - __main__ - INFO - Action: [0.8746255 0.8746255 0.8746255 0.8746255], Reward: tensor([-342.0060], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:27,251 - __main__ - INFO - Action: [0.43731275 0.43731275 0.43731275 0.43731275], Reward: tensor([-343.7004], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:31,411 - __main__ - INFO - Action: [0.70067016 0.70067016 0.70067016 0.70067016], Reward: tensor([-345.9879], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:34,930 - __main__ - INFO - Action: [0.84786246 0.84786246 0.84786246 0.84786246], Reward: tensor([-348.1867], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:38,717 - __main__ - INFO - Action: [0.80472831 0.80472831 0.80472831 0.80472831], Reward: tensor([-349.9105], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:42,988 - __main__ - INFO - Action: [0.78316123 0.78316123 0.78316123 0.78316123], Reward: tensor([-351.8674], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:47,112 - __main__ - INFO - Action: [0.88910799 0.88910799 0.88910799 0.88910799], Reward: tensor([-354.0722], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:51,341 - __main__ - INFO - Action: [0.444554 0.444554 0.444554 0.444554], Reward: tensor([-355.6615], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:55,650 - __main__ - INFO - Action: [0.60307408 0.60307408 0.60307408 0.60307408], Reward: tensor([-358.0182], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:38:59,920 - __main__ - INFO - Action: [0.30153704 0.30153704 0.30153704 0.30153704], Reward: tensor([-359.8423], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:00,856 - __main__ - INFO - Epoch 0, Iteration 160: Reward: tensor([-30804.6094], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:39:04,793 - __main__ - INFO - Action: [0.15076852 0.15076852 0.15076852 0.15076852], Reward: tensor([-362.1536], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:09,142 - __main__ - INFO - Action: [0.57291164 0.57291164 0.57291164 0.57291164], Reward: tensor([-364.7190], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:13,303 - __main__ - INFO - Action: [0.7839832 0.7839832 0.7839832 0.7839832], Reward: tensor([-367.0282], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:17,044 - __main__ - INFO - Action: [0.3919916 0.3919916 0.3919916 0.3919916], Reward: tensor([-368.6002], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:21,427 - __main__ - INFO - Action: [0.1959958 0.1959958 0.1959958 0.1959958], Reward: tensor([-370.4371], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:25,589 - __main__ - INFO - Action: [0.0979979 0.0979979 0.0979979 0.0979979], Reward: tensor([-372.5788], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:29,771 - __main__ - INFO - Action: [0.04899895 0.04899895 0.04899895 0.04899895], Reward: tensor([-374.5405], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:34,212 - __main__ - INFO - Action: [0.02449947 0.02449947 0.02449947 0.02449947], Reward: tensor([-376.6656], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:38,481 - __main__ - INFO - Action: [0.49426353 0.49426353 0.49426353 0.49426353], Reward: tensor([-379.3184], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:42,874 - __main__ - INFO - Action: [0.74465914 0.74465914 0.74465914 0.74465914], Reward: tensor([-381.7497], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:43,804 - __main__ - INFO - Epoch 0, Iteration 170: Reward: tensor([-34522.3984], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:39:47,577 - __main__ - INFO - Action: [0.37232957 0.37232957 0.37232957 0.37232957], Reward: tensor([-383.8071], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:51,959 - __main__ - INFO - Action: [0.66817858 0.66817858 0.66817858 0.66817858], Reward: tensor([-386.2524], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:39:56,274 - __main__ - INFO - Action: [0.83161666 0.83161666 0.83161666 0.83161666], Reward: tensor([-388.5569], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:00,447 - __main__ - INFO - Action: [0.41580833 0.41580833 0.41580833 0.41580833], Reward: tensor([-390.2329], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:04,346 - __main__ - INFO - Action: [0.58870124 0.58870124 0.58870124 0.58870124], Reward: tensor([-392.5265], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:08,771 - __main__ - INFO - Action: [0.29435062 0.29435062 0.29435062 0.29435062], Reward: tensor([-394.2473], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:13,132 - __main__ - INFO - Action: [0.52797239 0.52797239 0.52797239 0.52797239], Reward: tensor([-396.6790], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:17,206 - __main__ - INFO - Action: [0.74599998 0.74599998 0.74599998 0.74599998], Reward: tensor([-398.9883], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:21,292 - __main__ - INFO - Action: [0.37299999 0.37299999 0.37299999 0.37299999], Reward: tensor([-400.6265], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:25,636 - __main__ - INFO - Action: [0.56729707 0.56729707 0.56729707 0.56729707], Reward: tensor([-402.9913], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:26,515 - __main__ - INFO - Epoch 0, Iteration 180: Reward: tensor([-38457.3125], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:40:30,346 - __main__ - INFO - Action: [0.28364854 0.28364854 0.28364854 0.28364854], Reward: tensor([-405.0732], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:34,320 - __main__ - INFO - Action: [0.63935165 0.63935165 0.63935165 0.63935165], Reward: tensor([-407.4146], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:38,715 - __main__ - INFO - Action: [0.31967582 0.31967582 0.31967582 0.31967582], Reward: tensor([-409.2866], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:42,977 - __main__ - INFO - Action: [0.15983791 0.15983791 0.15983791 0.15983791], Reward: tensor([-411.2062], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:47,376 - __main__ - INFO - Action: [0.57744633 0.57744633 0.57744633 0.57744633], Reward: tensor([-413.8323], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:51,701 - __main__ - INFO - Action: [0.78625054 0.78625054 0.78625054 0.78625054], Reward: tensor([-416.2586], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:40:55,838 - __main__ - INFO - Action: [0.89065265 0.89065265 0.89065265 0.89065265], Reward: tensor([-418.3843], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:00,046 - __main__ - INFO - Action: [0.8261234 0.8261234 0.8261234 0.8261234], Reward: tensor([-420.4400], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:04,218 - __main__ - INFO - Action: [0.4130617 0.4130617 0.4130617 0.4130617], Reward: tensor([-422.1569], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:08,269 - __main__ - INFO - Action: [0.68854464 0.68854464 0.68854464 0.68854464], Reward: tensor([-424.4309], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:09,079 - __main__ - INFO - Epoch 0, Iteration 190: Reward: tensor([-42605.7930], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:41:12,951 - __main__ - INFO - Action: [0.34427232 0.34427232 0.34427232 0.34427232], Reward: tensor([-426.4258], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:17,205 - __main__ - INFO - Action: [0.66966354 0.66966354 0.66966354 0.66966354], Reward: tensor([-428.8233], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:21,427 - __main__ - INFO - Action: [0.83235915 0.83235915 0.83235915 0.83235915], Reward: tensor([-431.1330], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:25,692 - __main__ - INFO - Action: [0.89819336 0.89819336 0.89819336 0.89819336], Reward: tensor([-433.3566], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:30,168 - __main__ - INFO - Action: [0.93111047 0.93111047 0.93111047 0.93111047], Reward: tensor([-435.5843], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:34,453 - __main__ - INFO - Action: [0.46555524 0.46555524 0.46555524 0.46555524], Reward: tensor([-437.2262], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:38,717 - __main__ - INFO - Action: [0.23277762 0.23277762 0.23277762 0.23277762], Reward: tensor([-439.1897], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:42,882 - __main__ - INFO - Action: [0.61391619 0.61391619 0.61391619 0.61391619], Reward: tensor([-441.6406], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:46,737 - __main__ - INFO - Action: [0.30695809 0.30695809 0.30695809 0.30695809], Reward: tensor([-443.3694], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:50,888 - __main__ - INFO - Action: [0.15347905 0.15347905 0.15347905 0.15347905], Reward: tensor([-445.2600], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:51,734 - __main__ - INFO - Epoch 0, Iteration 200: Reward: tensor([-46967.8086], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:41:55,629 - __main__ - INFO - Action: [0.07673952 0.07673952 0.07673952 0.07673952], Reward: tensor([-447.4648], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:41:59,960 - __main__ - INFO - Action: [0.03836976 0.03836976 0.03836976 0.03836976], Reward: tensor([-449.5772], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:03,963 - __main__ - INFO - Action: [0.01918488 0.01918488 0.01918488 0.01918488], Reward: tensor([-451.6452], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:08,183 - __main__ - INFO - Action: [0.49160623 0.49160623 0.49160623 0.49160623], Reward: tensor([-454.2308], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:12,421 - __main__ - INFO - Action: [0.24580312 0.24580312 0.24580312 0.24580312], Reward: tensor([-456.0006], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:16,687 - __main__ - INFO - Action: [0.50369864 0.50369864 0.50369864 0.50369864], Reward: tensor([-458.4463], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:20,895 - __main__ - INFO - Action: [0.25184932 0.25184932 0.25184932 0.25184932], Reward: tensor([-460.3289], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:24,976 - __main__ - INFO - Action: [0.60793845 0.60793845 0.60793845 0.60793845], Reward: tensor([-462.7050], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:29,297 - __main__ - INFO - Action: [0.6847663 0.6847663 0.6847663 0.6847663], Reward: tensor([-464.8925], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:33,439 - __main__ - INFO - Action: [0.34238315 0.34238315 0.34238315 0.34238315], Reward: tensor([-466.6912], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:34,438 - __main__ - INFO - Epoch 0, Iteration 210: Reward: tensor([-51539.7852], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:42:37,402 - __main__ - INFO - Action: [0.55198865 0.55198865 0.55198865 0.55198865], Reward: tensor([-469.2740], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:41,681 - __main__ - INFO - Action: [0.27599433 0.27599433 0.27599433 0.27599433], Reward: tensor([-470.6411], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:46,075 - __main__ - INFO - Action: [0.13799716 0.13799716 0.13799716 0.13799716], Reward: tensor([-472.7499], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:50,358 - __main__ - INFO - Action: [0.56652596 0.56652596 0.56652596 0.56652596], Reward: tensor([-475.3058], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:54,743 - __main__ - INFO - Action: [0.66406006 0.66406006 0.66406006 0.66406006], Reward: tensor([-477.6497], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:42:58,981 - __main__ - INFO - Action: [0.33203003 0.33203003 0.33203003 0.33203003], Reward: tensor([-479.4310], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:03,093 - __main__ - INFO - Action: [0.66354239 0.66354239 0.66354239 0.66354239], Reward: tensor([-481.8344], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:06,883 - __main__ - INFO - Action: [0.82929857 0.82929857 0.82929857 0.82929857], Reward: tensor([-484.0639], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:11,026 - __main__ - INFO - Action: [0.41464929 0.41464929 0.41464929 0.41464929], Reward: tensor([-485.5319], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:15,191 - __main__ - INFO - Action: [0.20732464 0.20732464 0.20732464 0.20732464], Reward: tensor([-487.3548], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:16,071 - __main__ - INFO - Epoch 0, Iteration 220: Reward: tensor([-56323.6172], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:43:19,936 - __main__ - INFO - Action: [0.4844594 0.4844594 0.4844594 0.4844594], Reward: tensor([-490.0662], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:24,040 - __main__ - INFO - Action: [0.72424349 0.72424349 0.72424349 0.72424349], Reward: tensor([-492.3724], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:28,203 - __main__ - INFO - Action: [0.74291882 0.74291882 0.74291882 0.74291882], Reward: tensor([-494.5450], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:32,269 - __main__ - INFO - Action: [0.37145941 0.37145941 0.37145941 0.37145941], Reward: tensor([-496.0743], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:36,546 - __main__ - INFO - Action: [0.18572971 0.18572971 0.18572971 0.18572971], Reward: tensor([-498.0864], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:40,797 - __main__ - INFO - Action: [0.47366193 0.47366193 0.47366193 0.47366193], Reward: tensor([-500.4902], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:44,988 - __main__ - INFO - Action: [0.23683097 0.23683097 0.23683097 0.23683097], Reward: tensor([-502.3244], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:49,296 - __main__ - INFO - Action: [0.11841548 0.11841548 0.11841548 0.11841548], Reward: tensor([-504.3283], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:53,700 - __main__ - INFO - Action: [0.05920774 0.05920774 0.05920774 0.05920774], Reward: tensor([-506.4879], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:57,985 - __main__ - INFO - Action: [0.02960387 0.02960387 0.02960387 0.02960387], Reward: tensor([-508.6148], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:43:58,795 - __main__ - INFO - Epoch 0, Iteration 230: Reward: tensor([-61317.0039], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-05 17:44:02,561 - __main__ - INFO - Action: [0.39559901 0.39559901 0.39559901 0.39559901], Reward: tensor([-511.2826], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:06,622 - __main__ - INFO - Action: [0.69532688 0.69532688 0.69532688 0.69532688], Reward: tensor([-513.6509], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:11,045 - __main__ - INFO - Action: [0.84519082 0.84519082 0.84519082 0.84519082], Reward: tensor([-515.9870], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:14,990 - __main__ - INFO - Action: [0.9046092 0.9046092 0.9046092 0.9046092], Reward: tensor([-518.0596], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:19,353 - __main__ - INFO - Action: [0.94983198 0.94983198 0.94983198 0.94983198], Reward: tensor([-520.2553], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:23,514 - __main__ - INFO - Action: [0.47491599 0.47491599 0.47491599 0.47491599], Reward: tensor([-521.8465], grad_fn=<AddBackward0>), Done: False
2024-06-05 17:44:27,294 - __main__ - INFO - Episode timed out.
2024-06-05 17:44:27,836 - __main__ - INFO - Action: [0.23745799 0.23745799 0.23745799 0.23745799], Reward: tensor([-523.7324], grad_fn=<AddBackward0>), Done: True
2024-06-09 00:52:34,935 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-09 00:52:35,044 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-09 00:52:37,676 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-09 00:52:38,082 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-09 00:52:41,957 - __main__ - INFO - Action: [0. 0. 0. 0.], Reward: tensor([-73.2603], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:52:42,546 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-73.2603], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:52:45,321 - __main__ - INFO - Action: [0. 0. 0. 0.], Reward: tensor([-24.8542], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:52:48,201 - __main__ - INFO - Action: [0.38079708 0.38079708 0.38079708 0.38079708], Reward: tensor([-26.6333], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:52:51,035 - __main__ - INFO - Action: [0.67241233 0.67241233 0.67241233 0.67241233], Reward: tensor([-28.3483], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:52:54,096 - __main__ - INFO - Action: [0.33620616 0.33620616 0.33620616 0.33620616], Reward: tensor([-29.6363], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:52:57,693 - __main__ - INFO - Action: [0.65011687 0.65011687 0.65011687 0.65011687], Reward: tensor([-31.7459], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:00,985 - __main__ - INFO - Action: [0.82258581 0.82258581 0.82258581 0.82258581], Reward: tensor([-33.7387], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:04,964 - __main__ - INFO - Action: [0.90882028 0.90882028 0.90882028 0.90882028], Reward: tensor([-35.5900], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:08,419 - __main__ - INFO - Action: [0.95193752 0.95193752 0.95193752 0.95193752], Reward: tensor([-87.3921], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:11,768 - __main__ - INFO - Action: [0.85676584 0.85676584 0.85676584 0.85676584], Reward: tensor([-89.1436], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:14,817 - __main__ - INFO - Action: [0.42838292 0.42838292 0.42838292 0.42838292], Reward: tensor([-40.4023], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:15,442 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-500.7450], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:53:18,926 - __main__ - INFO - Action: [0.71171884 0.71171884 0.71171884 0.71171884], Reward: tensor([-42.4728], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:22,092 - __main__ - INFO - Action: [0.83787321 0.83787321 0.83787321 0.83787321], Reward: tensor([-44.2716], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:24,963 - __main__ - INFO - Action: [0.4189366 0.4189366 0.4189366 0.4189366], Reward: tensor([-45.4504], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:28,363 - __main__ - INFO - Action: [0.59026538 0.59026538 0.59026538 0.59026538], Reward: tensor([-47.0722], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:31,625 - __main__ - INFO - Action: [0.77714648 0.77714648 0.77714648 0.77714648], Reward: tensor([-49.0065], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:35,073 - __main__ - INFO - Action: [0.87058703 0.87058703 0.87058703 0.87058703], Reward: tensor([-50.7009], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:38,699 - __main__ - INFO - Action: [0.91730731 0.91730731 0.91730731 0.91730731], Reward: tensor([-52.4732], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:42,512 - __main__ - INFO - Action: [0.83945073 0.83945073 0.83945073 0.83945073], Reward: tensor([-54.5092], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:45,848 - __main__ - INFO - Action: [0.91725274 0.91725274 0.91725274 0.91725274], Reward: tensor([-56.2213], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:49,419 - __main__ - INFO - Action: [0.45862637 0.45862637 0.45862637 0.45862637], Reward: tensor([-57.5851], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:50,014 - __main__ - INFO - Epoch 0, Iteration 20: Reward: tensor([-1000.5081], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:53:52,111 - __main__ - INFO - Action: [0.61011026 0.61011026 0.61011026 0.61011026], Reward: tensor([-59.2225], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:54,413 - __main__ - INFO - Action: [0.78706892 0.78706892 0.78706892 0.78706892], Reward: tensor([-60.5896], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:56,965 - __main__ - INFO - Action: [0.89106184 0.89106184 0.89106184 0.89106184], Reward: tensor([-61.7930], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:53:59,174 - __main__ - INFO - Action: [0.9430583 0.9430583 0.9430583 0.9430583], Reward: tensor([-63.1418], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:01,879 - __main__ - INFO - Action: [0.95354294 0.95354294 0.95354294 0.95354294], Reward: tensor([-64.2254], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:04,842 - __main__ - INFO - Action: [0.85756855 0.85756855 0.85756855 0.85756855], Reward: tensor([-65.9164], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:06,751 - __main__ - INFO - Action: [0.80958135 0.80958135 0.80958135 0.80958135], Reward: tensor([-66.7569], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:09,258 - __main__ - INFO - Action: [0.90231805 0.90231805 0.90231805 0.90231805], Reward: tensor([-67.9170], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:12,228 - __main__ - INFO - Action: [0.93317282 0.93317282 0.93317282 0.93317282], Reward: tensor([-69.4262], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:14,816 - __main__ - INFO - Action: [0.46658641 0.46658641 0.46658641 0.46658641], Reward: tensor([-70.4021], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:15,159 - __main__ - INFO - Epoch 0, Iteration 30: Reward: tensor([-1649.8988], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:54:17,260 - __main__ - INFO - Action: [0.61409028 0.61409028 0.61409028 0.61409028], Reward: tensor([-71.9181], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:19,907 - __main__ - INFO - Action: [0.68784222 0.68784222 0.68784222 0.68784222], Reward: tensor([-73.0367], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:22,726 - __main__ - INFO - Action: [0.84144849 0.84144849 0.84144849 0.84144849], Reward: tensor([-74.6851], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:25,575 - __main__ - INFO - Action: [0.80152132 0.80152132 0.80152132 0.80152132], Reward: tensor([-76.1315], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:28,594 - __main__ - INFO - Action: [0.40076066 0.40076066 0.40076066 0.40076066], Reward: tensor([-77.1846], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:32,359 - __main__ - INFO - Action: [0.20038033 0.20038033 0.20038033 0.20038033], Reward: tensor([-78.5964], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:35,977 - __main__ - INFO - Action: [0.59771754 0.59771754 0.59771754 0.59771754], Reward: tensor([-80.8881], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:39,559 - __main__ - INFO - Action: [0.78087256 0.78087256 0.78087256 0.78087256], Reward: tensor([-82.8164], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:43,572 - __main__ - INFO - Action: [0.77123336 0.77123336 0.77123336 0.77123336], Reward: tensor([-84.7006], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:46,800 - __main__ - INFO - Action: [0.76641376 0.76641376 0.76641376 0.76641376], Reward: tensor([-86.6852], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:47,691 - __main__ - INFO - Epoch 0, Iteration 40: Reward: tensor([-2436.5417], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:54:51,183 - __main__ - INFO - Action: [0.38320688 0.38320688 0.38320688 0.38320688], Reward: tensor([-88.2602], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:54,782 - __main__ - INFO - Action: [0.57240052 0.57240052 0.57240052 0.57240052], Reward: tensor([-90.2342], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:54:58,402 - __main__ - INFO - Action: [0.76821405 0.76821405 0.76821405 0.76821405], Reward: tensor([-92.3011], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:01,492 - __main__ - INFO - Action: [0.7649041 0.7649041 0.7649041 0.7649041], Reward: tensor([-93.7874], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:04,776 - __main__ - INFO - Action: [0.76324913 0.76324913 0.76324913 0.76324913], Reward: tensor([-95.4136], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:07,998 - __main__ - INFO - Action: [0.86363835 0.86363835 0.86363835 0.86363835], Reward: tensor([-97.1879], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:11,599 - __main__ - INFO - Action: [0.43181918 0.43181918 0.43181918 0.43181918], Reward: tensor([-98.6812], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:13,990 - __main__ - INFO - Action: [0.21590959 0.21590959 0.21590959 0.21590959], Reward: tensor([-99.7136], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:17,521 - __main__ - INFO - Action: [0.58996858 0.58996858 0.58996858 0.58996858], Reward: tensor([-101.6551], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:21,338 - __main__ - INFO - Action: [0.79251167 0.79251167 0.79251167 0.79251167], Reward: tensor([-103.6766], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:21,899 - __main__ - INFO - Epoch 0, Iteration 50: Reward: tensor([-3397.4524], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:55:24,907 - __main__ - INFO - Action: [0.87826962 0.87826962 0.87826962 0.87826962], Reward: tensor([-105.6487], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:28,617 - __main__ - INFO - Action: [0.93666219 0.93666219 0.93666219 0.93666219], Reward: tensor([-107.5772], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:32,467 - __main__ - INFO - Action: [0.84912817 0.84912817 0.84912817 0.84912817], Reward: tensor([-109.3293], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:35,560 - __main__ - INFO - Action: [0.92209146 0.92209146 0.92209146 0.92209146], Reward: tensor([-111.0565], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:39,147 - __main__ - INFO - Action: [0.46104573 0.46104573 0.46104573 0.46104573], Reward: tensor([-112.4376], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:42,657 - __main__ - INFO - Action: [0.71253666 0.71253666 0.71253666 0.71253666], Reward: tensor([-114.5680], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:46,268 - __main__ - INFO - Action: [0.73706541 0.73706541 0.73706541 0.73706541], Reward: tensor([-116.2238], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:49,318 - __main__ - INFO - Action: [0.74932978 0.74932978 0.74932978 0.74932978], Reward: tensor([-117.8129], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:52,440 - __main__ - INFO - Action: [0.87219227 0.87219227 0.87219227 0.87219227], Reward: tensor([-119.5396], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:55,650 - __main__ - INFO - Action: [0.91810992 0.91810992 0.91810992 0.91810992], Reward: tensor([-121.1785], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:55:56,399 - __main__ - INFO - Epoch 0, Iteration 60: Reward: tensor([-4532.8247], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:55:59,917 - __main__ - INFO - Action: [0.83985204 0.83985204 0.83985204 0.83985204], Reward: tensor([-123.0650], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:02,671 - __main__ - INFO - Action: [0.41992602 0.41992602 0.41992602 0.41992602], Reward: tensor([-124.1128], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:06,211 - __main__ - INFO - Action: [0.20996301 0.20996301 0.20996301 0.20996301], Reward: tensor([-125.6703], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:09,155 - __main__ - INFO - Action: [0.1049815 0.1049815 0.1049815 0.1049815], Reward: tensor([-127.3117], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:12,033 - __main__ - INFO - Action: [0.43328783 0.43328783 0.43328783 0.43328783], Reward: tensor([-128.9287], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:15,677 - __main__ - INFO - Action: [0.21664392 0.21664392 0.21664392 0.21664392], Reward: tensor([-130.2529], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:19,156 - __main__ - INFO - Action: [0.60584933 0.60584933 0.60584933 0.60584933], Reward: tensor([-132.5931], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:21,636 - __main__ - INFO - Action: [0.78493846 0.78493846 0.78493846 0.78493846], Reward: tensor([-134.1781], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:24,881 - __main__ - INFO - Action: [0.77326631 0.77326631 0.77326631 0.77326631], Reward: tensor([-135.5060], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:28,262 - __main__ - INFO - Action: [0.76743023 0.76743023 0.76743023 0.76743023], Reward: tensor([-137.2006], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:28,763 - __main__ - INFO - Epoch 0, Iteration 70: Reward: tensor([-5831.6440], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-09 00:56:31,794 - __main__ - INFO - Action: [0.86572891 0.86572891 0.86572891 0.86572891], Reward: tensor([-139.1613], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:35,067 - __main__ - INFO - Action: [0.43286445 0.43286445 0.43286445 0.43286445], Reward: tensor([-140.3455], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:38,304 - __main__ - INFO - Action: [0.5972293 0.5972293 0.5972293 0.5972293], Reward: tensor([-142.2044], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:41,464 - __main__ - INFO - Action: [0.78062844 0.78062844 0.78062844 0.78062844], Reward: tensor([-144.0183], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:45,106 - __main__ - INFO - Action: [0.7711113 0.7711113 0.7711113 0.7711113], Reward: tensor([-145.7067], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:48,130 - __main__ - INFO - Action: [0.76635273 0.76635273 0.76635273 0.76635273], Reward: tensor([-147.2814], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:50,774 - __main__ - INFO - Action: [0.76397344 0.76397344 0.76397344 0.76397344], Reward: tensor([-148.6736], grad_fn=<AddBackward0>), Done: False
2024-06-09 00:56:53,171 - __main__ - INFO - Action: [0.38198672 0.38198672 0.38198672 0.38198672], Reward: tensor([-149.4491], grad_fn=<AddBackward0>), Done: False
2024-06-09 01:03:21,190 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-09 01:03:21,298 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-09 01:03:23,465 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-09 01:03:23,776 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-09 01:10:55,447 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-09 01:10:55,507 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-09 01:10:57,427 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-09 01:10:57,800 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-09 02:00:27,710 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-09 02:00:27,826 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-09 02:00:29,949 - __main__ - WARNING - Checkpoint does not contain required keys
2024-06-09 02:00:30,438 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 02:52:53,597 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 02:52:53,707 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 02:52:55,554 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 02:54:57,974 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 02:54:58,082 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 02:54:59,834 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 02:55:03,594 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 02:56:00,720 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 02:56:00,829 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 02:56:02,447 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 02:56:07,469 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 02:57:12,234 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 02:57:12,343 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 02:57:13,937 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 02:57:18,177 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 02:58:12,227 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 02:58:12,336 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 02:58:14,381 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 02:58:19,039 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 02:59:21,566 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 02:59:21,673 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 02:59:23,345 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 02:59:27,607 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:01:11,984 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:01:12,091 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:01:13,624 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:01:17,449 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:04:22,003 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:04:22,093 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:04:23,657 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:04:28,429 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:05:44,236 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:05:44,326 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:05:46,142 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:05:50,941 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:39:22,103 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:39:22,211 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:39:24,099 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:39:29,107 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:42:14,267 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:42:14,362 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:42:16,442 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:42:19,600 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:43:20,902 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:43:21,025 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:43:22,978 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:43:28,290 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:44:27,673 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:44:27,751 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:44:29,793 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:44:33,830 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 03:48:32,470 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 03:48:32,580 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 03:48:34,315 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 03:48:39,032 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 04:12:50,198 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:12:50,200 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true
  },
  "policy_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 15,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 3,
        "stride": 2
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:12:50,251 - __main__ - ERROR - Error initializing ICM: 'conv_layers'
2024-07-20 04:14:48,951 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:14:48,954 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true
  },
  "policy_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:14:49,128 - __main__ - ERROR - Missing key in configuration for policy network: 'image_channels'
2024-07-20 04:16:27,228 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:16:27,231 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true
  },
  "policy_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:16:27,411 - __main__ - ERROR - Missing key in configuration for critic network: 'image_channels'
2024-07-20 04:17:42,559 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:17:42,561 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true
  },
  "policy_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:33:32,851 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:33:32,855 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:33:39,361 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 04:33:39,469 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 04:33:41,457 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 04:33:46,564 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 04:35:56,627 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:35:56,628 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:36:02,847 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 04:36:02,956 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 04:36:05,083 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 04:36:10,379 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 04:46:30,085 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:46:30,088 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:46:36,122 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 04:46:36,229 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 04:46:38,116 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 04:46:42,861 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 04:49:31,822 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:49:31,825 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 128,
        "kernel_size": 3,
        "stride": 2
      },
      "conv4": {
        "out_channels": 256,
        "kernel_size": 3,
        "stride": 2
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:49:33,020 - __main__ - ERROR - Error initializing policy network: Given groups=1, weight of size [64, 64, 4, 4], expected input[1, 32, 17, 31] to have 64 channels, but got 32 channels instead
2024-07-20 04:49:33,021 - __main__ - ERROR - Error initializing AirSimEnv: Given groups=1, weight of size [64, 64, 4, 4], expected input[1, 32, 17, 31] to have 64 channels, but got 32 channels instead
2024-07-20 04:52:27,037 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:52:27,042 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:52:32,958 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 04:52:33,062 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 04:52:34,259 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 04:52:38,509 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 04:55:28,158 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:55:28,162 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:55:33,914 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 04:55:34,008 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 04:55:35,565 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 04:55:39,056 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 04:58:01,662 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 04:58:01,663 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 04:58:06,605 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 04:58:06,731 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 04:58:08,262 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 04:58:13,002 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 05:00:44,696 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 05:00:44,699 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 05:00:50,221 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 05:00:50,329 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 05:00:52,150 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 05:00:56,634 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 05:08:35,460 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 05:08:35,463 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 05:08:40,840 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 05:08:40,932 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 05:08:42,582 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 05:08:47,636 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 05:14:06,790 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 05:14:06,793 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 05:14:12,205 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 05:14:12,313 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 05:14:13,857 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 05:14:17,833 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 05:16:54,582 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 05:16:54,586 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 05:16:59,779 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 05:16:59,887 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 05:17:01,273 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 05:17:05,799 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 11:58:04,973 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 11:58:04,976 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 11:58:09,768 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 11:58:09,822 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 11:58:11,966 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 11:58:17,132 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:02:00,227 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:02:00,230 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:02:06,727 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:02:06,837 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:02:08,829 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:02:12,446 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:03:53,985 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:03:53,986 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:03:59,552 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:03:59,599 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:04:01,641 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:04:06,999 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:06:38,581 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:06:38,584 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:06:44,958 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:06:45,065 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:06:47,068 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:06:52,260 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:08:56,253 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:08:56,254 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:09:01,650 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:09:01,743 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:09:03,651 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:09:08,443 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:11:23,467 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:11:23,468 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:11:28,932 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:11:29,008 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:11:30,906 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:11:36,327 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:13:43,227 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:13:43,231 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:13:48,600 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:13:48,631 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:13:50,572 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:13:55,754 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:16:10,667 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:16:10,669 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:16:16,447 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:16:16,540 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:16:18,609 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:16:23,374 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:19:12,205 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:19:12,206 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:19:18,280 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:19:18,388 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:19:20,598 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:19:25,613 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:19:31,067 - __main__ - INFO - Action: 1, Reward: 28.879999016950528, Done: False
2024-07-20 12:27:39,426 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:27:39,430 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:33:35,055 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:33:35,059 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:34:19,718 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:34:19,721 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:34:25,740 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-20 12:34:25,848 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-20 12:34:28,373 - __main__ - WARNING - Checkpoint does not contain required keys
2024-07-20 12:34:33,612 - __main__ - INFO - Environment reset and takeoff completed.
2024-07-20 12:34:38,876 - __main__ - INFO - Action: 5, Reward: 29.203624111014342, Done: False
2024-07-20 12:34:39,063 - __main__ - INFO - Epoch 0, Iteration 0: Reward: 29.203624111014342, Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-07-20 12:34:43,369 - __main__ - INFO - Action: 4, Reward: 33.88315356619223, Done: False
2024-07-20 12:34:47,845 - __main__ - INFO - Action: 4, Reward: 36.36969348346586, Done: False
2024-07-20 12:34:52,769 - __main__ - INFO - Action: 8, Reward: 35.95043043482096, Done: False
2024-07-20 12:34:56,700 - __main__ - INFO - Action: 3, Reward: 36.68849700598478, Done: False
2024-07-20 12:35:00,962 - __main__ - INFO - Action: 0, Reward: 37.9182355065798, Done: False
2024-07-20 12:35:05,548 - __main__ - INFO - Action: 4, Reward: 38.991821734198936, Done: False
2024-07-20 12:35:09,467 - __main__ - INFO - Action: 0, Reward: 39.594471717317944, Done: False
2024-07-20 12:35:13,370 - __main__ - INFO - Action: 7, Reward: 39.51329085944227, Done: False
2024-07-20 12:35:14,982 - __main__ - ERROR - Action index type error: [ 1.2772597   0.9771557  -0.01504873  1.0032154 ]
2024-07-20 12:35:18,212 - __main__ - INFO - Action: [ 1.2772597   0.9771557  -0.01504873  1.0032154 ], Reward: 39.52990889549464, Done: False
2024-07-20 12:51:14,356 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:51:14,358 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:54:50,144 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 12:54:50,146 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 12:54:50,876 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 12:54:50,922 - __main__ - INFO - Using device: cpu
2024-07-20 12:54:55,620 - __main__ - INFO - PPOAgent initialized successfully.
2024-07-20 13:11:26,330 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 13:11:26,332 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 13:11:27,027 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 13:11:27,071 - __main__ - INFO - Using device: cpu
2024-07-20 13:11:31,875 - __main__ - INFO - PPOAgent initialized successfully.
2024-07-20 13:16:47,164 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 13:16:47,166 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 13:16:47,991 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 13:16:48,037 - __main__ - INFO - Using device: cpu
2024-07-20 13:16:52,443 - __main__ - INFO - PPOAgent initialized successfully.
2024-07-20 13:18:33,034 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 13:18:33,035 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 13:18:33,642 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 13:18:33,688 - __main__ - INFO - Using device: cpu
2024-07-20 13:18:38,209 - __main__ - INFO - PPOAgent initialized successfully.
2024-07-20 13:20:43,565 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 13:20:43,567 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 13:20:44,679 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 13:20:44,726 - __main__ - INFO - Using device: cpu
2024-07-20 13:20:49,378 - __main__ - INFO - PPOAgent initialized successfully.
2024-07-20 13:22:55,106 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 13:22:55,107 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 13:22:56,064 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 13:22:56,111 - __main__ - INFO - Using device: cpu
2024-07-20 13:23:00,875 - __main__ - INFO - PPOAgent initialized successfully.
2024-07-20 13:25:32,516 - __main__ - INFO - Initializing AirSimEnv
2024-07-20 13:25:32,517 - __main__ - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-20 13:25:33,501 - __main__ - INFO - PPOAgent initialization started.
2024-07-20 13:25:33,548 - __main__ - INFO - Using device: cpu
2024-07-20 13:25:38,614 - __main__ - INFO - PPOAgent initialized successfully.
