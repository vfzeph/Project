2024-06-03 17:10:54,471 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 17:10:56,061 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 17:10:56,261 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:10:56,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:10:56,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:10:56,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:10:57,399 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:11:03,945 - __main__ - INFO - Action: 6, Reward: tensor([-19.8890], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:04,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:04,709 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-19.8890], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:11:04,755 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:04,755 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:05,050 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:11:16,274 - __main__ - INFO - Action: 8, Reward: tensor([-22.7541], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:16,397 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:16,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:16,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:16,645 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:11:28,860 - __main__ - INFO - Action: 12, Reward: tensor([-74.5007], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:29,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:29,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:29,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:29,576 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:11:41,570 - __main__ - INFO - Action: 0, Reward: tensor([-24.6277], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:41,710 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:41,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:41,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:41,868 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:11:52,679 - __main__ - INFO - Action: 16, Reward: tensor([-24.7108], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:52,782 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:52,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:52,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:53,753 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:12:06,035 - __main__ - INFO - Action: 15, Reward: tensor([-24.8546], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:06,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:06,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:06,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:07,171 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:12:12,569 - __main__ - INFO - Action: 6, Reward: tensor([-24.9427], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:12,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:12,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:12,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:13,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:12:25,769 - __main__ - INFO - Action: 0, Reward: tensor([-25.0380], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:26,098 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:26,192 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:26,192 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:26,960 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:12:38,650 - __main__ - INFO - Action: 14, Reward: tensor([-25.1980], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:38,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:38,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:38,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:39,426 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 17:12:46,140 - __main__ - INFO - Action: 7, Reward: tensor([-75.2582], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:46,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:46,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:46,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:47,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:12:52,948 - __main__ - INFO - Action: 11, Reward: tensor([-24.3718], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:52,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:52,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:52,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:52,971 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-366.1457], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:12:53,196 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:13:05,026 - __main__ - INFO - Episode timed out.
2024-06-03 17:13:05,481 - __main__ - INFO - Action: 1, Reward: tensor([-22.8578], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:13:06,570 - Drone.source.models.ppo.ppo_agent - INFO - Model saved at e:\Project\models/checkpoints\ppo_agent_epoch_0.pt
2024-06-03 17:13:06,991 - __main__ - INFO - Checkpoint saved at epoch 0
2024-06-03 17:13:07,396 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:13:07,921 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:08,030 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:08,030 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:08,760 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:13:21,151 - __main__ - INFO - Action: 4, Reward: tensor([-22.5970], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:21,885 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-22.5970], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:13:21,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:21,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:21,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:22,858 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:13:29,289 - __main__ - INFO - Action: 10, Reward: tensor([-23.7592], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:29,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:29,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:29,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:30,084 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:13:42,356 - __main__ - INFO - Action: 9, Reward: tensor([-22.7700], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:42,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:42,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:42,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:43,737 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:13:56,205 - __main__ - INFO - Action: 4, Reward: tensor([-22.8452], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:56,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:56,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:56,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:57,508 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:14:04,117 - __main__ - INFO - Action: 6, Reward: tensor([-23.0838], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:04,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:04,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:04,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:05,315 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:14:18,153 - __main__ - INFO - Action: 15, Reward: tensor([-24.3568], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:18,464 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:18,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:18,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:19,435 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:14:32,036 - __main__ - INFO - Action: 15, Reward: tensor([-26.5830], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:32,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:32,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:32,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:33,106 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:14:45,868 - __main__ - INFO - Action: 1, Reward: tensor([-28.6251], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:46,148 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:46,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:46,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:47,066 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:14:59,661 - __main__ - INFO - Action: 12, Reward: tensor([-30.7287], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:59,974 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:00,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:00,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:00,892 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:15:07,475 - __main__ - INFO - Action: 6, Reward: tensor([-31.6827], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:07,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:07,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:07,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:08,482 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:15:20,112 - __main__ - INFO - Episode timed out.
2024-06-03 17:15:20,333 - __main__ - INFO - Action: 4, Reward: tensor([-34.4819], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:15:20,736 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-291.5134], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:15:21,000 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:15:21,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:21,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:21,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:21,729 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:15:28,307 - __main__ - INFO - Action: 10, Reward: tensor([-16.5137], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:29,093 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-16.5137], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:15:29,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:29,204 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:29,204 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:30,061 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:15:41,633 - __main__ - INFO - Action: 8, Reward: tensor([-20.5039], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:41,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:41,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:41,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:41,892 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:15:47,591 - __main__ - INFO - Action: 3, Reward: tensor([-21.5602], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:47,840 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:47,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:47,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:48,280 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:16:00,725 - __main__ - INFO - Action: 15, Reward: tensor([-23.4707], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:01,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:01,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:01,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:01,427 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:16:14,158 - __main__ - INFO - Action: 14, Reward: tensor([-74.1754], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:14,406 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:14,517 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:14,517 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:15,031 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:16:21,555 - __main__ - INFO - Action: 11, Reward: tensor([-23.5138], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:21,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:21,957 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:21,957 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:22,912 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:16:34,989 - __main__ - INFO - Action: 0, Reward: tensor([-22.3746], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:35,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:35,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:35,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:35,659 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:16:41,325 - __main__ - INFO - Action: 10, Reward: tensor([-23.2680], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:41,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:41,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:41,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:41,885 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:16:53,451 - __main__ - INFO - Action: 14, Reward: tensor([-24.6421], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:53,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:53,651 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:53,651 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:54,573 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:17:00,783 - __main__ - INFO - Action: 2, Reward: tensor([-24.9542], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:01,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:01,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:01,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:01,909 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:17:08,579 - __main__ - INFO - Action: 6, Reward: tensor([-75.0038], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:09,334 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-349.9803], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:17:09,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:09,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:09,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:09,631 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:17:21,439 - __main__ - INFO - Episode timed out.
2024-06-03 17:17:22,047 - __main__ - INFO - Action: 4, Reward: tensor([-25.1570], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:17:22,622 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:17:23,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:23,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:23,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:23,464 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:17:36,245 - __main__ - INFO - Action: 1, Reward: tensor([-17.0775], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:36,969 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-17.0775], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:17:36,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:37,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:37,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:37,823 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:17:44,451 - __main__ - INFO - Action: 10, Reward: tensor([-18.2853], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:44,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:44,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:44,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:45,020 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:17:57,046 - __main__ - INFO - Action: 0, Reward: tensor([-19.6392], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:57,372 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:57,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:57,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:58,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:18:10,682 - __main__ - INFO - Action: 16, Reward: tensor([-20.5892], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:10,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:10,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:10,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:11,914 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:18:24,595 - __main__ - INFO - Action: 8, Reward: tensor([-23.6994], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:24,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:25,919 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:18:32,604 - __main__ - INFO - Action: 6, Reward: tensor([-74.4044], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:32,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:33,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:33,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:34,097 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:18:46,469 - __main__ - INFO - Action: 15, Reward: tensor([-24.5843], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:46,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:46,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:46,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:46,905 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:18:52,773 - __main__ - INFO - Action: 3, Reward: tensor([-24.6401], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:53,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:53,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:53,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:53,659 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 17:19:05,926 - __main__ - INFO - Action: 4, Reward: tensor([-24.7601], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:06,174 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:06,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:06,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:07,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-03 17:19:13,474 - __main__ - INFO - Action: 3, Reward: tensor([-24.8369], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:13,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:13,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:13,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:14,623 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:19:20,867 - __main__ - INFO - Action: 3, Reward: tensor([-24.9209], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:21,613 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-297.4372], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:19:21,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:21,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:21,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:22,719 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:19:34,539 - __main__ - INFO - Episode timed out.
2024-06-03 17:19:34,617 - __main__ - INFO - Action: 12, Reward: tensor([-25.0386], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:19:35,084 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:19:35,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:35,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:35,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:36,151 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:19:48,684 - __main__ - INFO - Action: 0, Reward: tensor([-19.3565], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:49,540 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-19.3565], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:19:49,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:49,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:49,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:50,398 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:20:02,693 - __main__ - INFO - Action: 0, Reward: tensor([-19.7765], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:03,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:03,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:03,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:03,611 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:20:15,861 - __main__ - INFO - Action: 0, Reward: tensor([-20.8344], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:16,097 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:16,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:16,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:17,157 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:20:23,530 - __main__ - INFO - Action: 3, Reward: tensor([-20.8634], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:23,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:23,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:23,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:24,679 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:20:37,059 - __main__ - INFO - Action: 13, Reward: tensor([-20.6910], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:37,320 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:37,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:37,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:37,792 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:20:49,576 - __main__ - INFO - Action: 9, Reward: tensor([-18.4716], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:49,889 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:49,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:49,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:50,669 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:20:56,346 - __main__ - INFO - Action: 11, Reward: tensor([-17.3052], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:56,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:56,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:56,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:57,249 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:21:09,513 - __main__ - INFO - Action: 8, Reward: tensor([-18.1812], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:09,792 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:09,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:09,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:10,303 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:21:16,470 - __main__ - INFO - Action: 2, Reward: tensor([-18.7459], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:16,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:16,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:16,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:17,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:21:29,796 - __main__ - INFO - Action: 0, Reward: tensor([-20.3025], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:30,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:30,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:30,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:31,104 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:21:42,658 - __main__ - INFO - Episode timed out.
2024-06-03 17:21:42,736 - __main__ - INFO - Action: 14, Reward: tensor([-23.1933], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:21:43,330 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-217.7215], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:21:43,688 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:21:44,139 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:44,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:44,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:44,328 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:21:56,663 - __main__ - INFO - Action: 14, Reward: tensor([-16.4346], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:57,458 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-16.4346], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:21:57,473 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:57,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:57,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:58,615 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:22:11,232 - __main__ - INFO - Action: 9, Reward: tensor([-14.8396], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:11,499 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:11,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:11,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:12,326 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:22:18,713 - __main__ - INFO - Action: 11, Reward: tensor([-13.7488], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:18,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:19,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:19,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:19,259 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:22:31,364 - __main__ - INFO - Action: 4, Reward: tensor([-11.4678], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:31,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:31,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:31,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:32,345 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:22:37,930 - __main__ - INFO - Action: 11, Reward: tensor([-10.2464], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:38,083 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:38,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:38,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:38,894 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:22:51,275 - __main__ - INFO - Action: 12, Reward: tensor([-9.8550], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:51,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:51,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:51,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:52,162 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 17:23:04,415 - __main__ - INFO - Action: 1, Reward: tensor([-9.3453], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:04,667 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:04,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:04,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:05,478 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:23:17,883 - __main__ - INFO - Action: 4, Reward: tensor([-10.6159], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:18,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:18,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:18,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:19,288 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:23:30,513 - __main__ - INFO - Action: 15, Reward: tensor([-12.8370], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:30,710 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:30,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:30,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:31,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:23:36,863 - __main__ - INFO - Action: 2, Reward: tensor([-13.3480], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:37,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:37,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:37,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:38,242 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:23:50,169 - __main__ - INFO - Episode timed out.
2024-06-03 17:23:50,202 - __main__ - INFO - Action: 4, Reward: tensor([-16.1083], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:23:50,787 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-138.8469], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:23:51,020 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:23:51,610 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:51,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:51,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:51,938 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:24:04,482 - __main__ - INFO - Action: 1, Reward: tensor([-20.6464], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:05,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:05,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:05,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:05,137 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-20.6464], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:24:05,654 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:24:17,734 - __main__ - INFO - Action: 0, Reward: tensor([-21.0850], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:17,984 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:18,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:18,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:18,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:24:25,382 - __main__ - INFO - Action: 6, Reward: tensor([-21.3626], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:25,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:25,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:25,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:26,753 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:24:38,849 - __main__ - INFO - Action: 4, Reward: tensor([-22.2967], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:39,099 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:39,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:39,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:39,679 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:24:52,403 - __main__ - INFO - Action: 12, Reward: tensor([-23.6358], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:52,731 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:52,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:52,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:53,232 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:25:05,679 - __main__ - INFO - Action: 0, Reward: tensor([-25.0558], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:06,025 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:06,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:06,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:06,973 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:25:13,325 - __main__ - INFO - Action: 10, Reward: tensor([-26.3295], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:13,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:13,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:13,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:14,439 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:25:27,075 - __main__ - INFO - Action: 4, Reward: tensor([-77.3545], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:27,431 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:27,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:27,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:28,461 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:25:41,037 - __main__ - INFO - Action: 13, Reward: tensor([-77.4649], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:41,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:41,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:41,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:41,756 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:25:53,935 - __main__ - INFO - Episode timed out.
2024-06-03 17:25:54,510 - __main__ - INFO - Action: 0, Reward: tensor([-27.2230], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:25:55,199 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:25:55,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:55,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:55,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:56,854 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:26:09,642 - __main__ - INFO - Action: 12, Reward: tensor([-73.8171], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:10,451 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-73.8171], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:26:10,467 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:10,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:10,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:11,324 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:26:22,727 - __main__ - INFO - Action: 13, Reward: tensor([-73.9549], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:23,008 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:23,101 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:23,101 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:23,864 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:26:30,111 - __main__ - INFO - Action: 2, Reward: tensor([-74.0372], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:30,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:30,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:30,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:31,381 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:26:38,183 - __main__ - INFO - Action: 10, Reward: tensor([-24.1289], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:38,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:38,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:38,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:39,507 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:26:52,051 - __main__ - INFO - Action: 8, Reward: tensor([-24.2899], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:52,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:53,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:27:06,012 - __main__ - INFO - Action: 14, Reward: tensor([-24.4363], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:06,337 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:06,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:06,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:06,712 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-03 17:27:19,673 - __main__ - INFO - Action: 5, Reward: tensor([-24.5686], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:19,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:19,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:19,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:20,716 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:27:33,339 - __main__ - INFO - Action: 8, Reward: tensor([-24.6968], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:34,306 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:27:47,100 - __main__ - INFO - Action: 0, Reward: tensor([-24.8280], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:47,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:47,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:47,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:48,381 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:28:00,683 - __main__ - INFO - Episode timed out.
2024-06-03 17:28:01,090 - __main__ - INFO - Action: 14, Reward: tensor([-25.0019], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:28:01,822 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:28:02,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:02,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:02,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:02,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:28:09,314 - __main__ - INFO - Action: 10, Reward: tensor([-20.4511], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:10,194 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-20.4511], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:28:10,224 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:10,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:10,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:11,003 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:28:16,596 - __main__ - INFO - Action: 10, Reward: tensor([-22.1368], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:16,846 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:16,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:16,971 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:17,731 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:28:30,291 - __main__ - INFO - Action: 0, Reward: tensor([-74.0579], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:30,494 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:30,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:30,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:31,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 17:28:43,542 - __main__ - INFO - Action: 1, Reward: tensor([-24.1627], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:43,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:43,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:43,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:44,437 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:28:56,422 - __main__ - INFO - Action: 8, Reward: tensor([-24.3239], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:56,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:56,810 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:56,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:57,368 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:29:03,032 - __main__ - INFO - Action: 11, Reward: tensor([-23.9985], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:03,172 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:03,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:03,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:03,604 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:29:09,931 - __main__ - INFO - Action: 2, Reward: tensor([-24.1018], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:10,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:10,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:10,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:10,489 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 17:29:21,901 - __main__ - INFO - Action: 1, Reward: tensor([-23.9316], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:22,133 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:22,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:22,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:22,413 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:29:29,011 - __main__ - INFO - Action: 2, Reward: tensor([-24.0417], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:29,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:29,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:29,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:30,346 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:29:36,384 - __main__ - INFO - Action: 6, Reward: tensor([-24.1445], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:36,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:36,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:36,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:37,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:29:49,529 - __main__ - INFO - Action: 0, Reward: tensor([-24.2832], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:50,457 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-309.6338], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:29:50,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:50,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:50,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:51,250 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:30:03,073 - __main__ - INFO - Episode timed out.
2024-06-03 17:30:03,351 - __main__ - INFO - Action: 0, Reward: tensor([-24.9155], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:30:03,914 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:30:04,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:04,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:04,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:05,187 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:30:11,230 - __main__ - INFO - Action: 3, Reward: tensor([-20.6662], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:11,683 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-20.6662], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:30:11,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:11,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:11,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:12,210 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:30:18,008 - __main__ - INFO - Action: 3, Reward: tensor([-21.0325], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:18,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:18,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:18,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:19,058 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:30:30,822 - __main__ - INFO - Action: 8, Reward: tensor([-74.0322], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:31,102 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:31,195 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:31,195 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:31,551 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:30:37,936 - __main__ - INFO - Action: 2, Reward: tensor([-74.0917], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:38,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:38,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:38,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:39,340 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:30:46,003 - __main__ - INFO - Action: 2, Reward: tensor([-24.1971], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:46,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:46,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:46,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:47,064 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:30:59,284 - __main__ - INFO - Action: 14, Reward: tensor([-24.3629], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:59,390 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:59,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:59,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:00,324 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:31:07,088 - __main__ - INFO - Action: 6, Reward: tensor([-24.3841], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:07,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:07,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:07,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:08,258 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:31:15,092 - __main__ - INFO - Action: 11, Reward: tensor([-23.5534], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:15,436 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:15,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:15,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:16,467 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:31:29,171 - __main__ - INFO - Action: 9, Reward: tensor([-20.1364], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:29,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:29,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:29,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:30,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:31:42,964 - __main__ - INFO - Action: 1, Reward: tensor([-19.1860], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:43,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:43,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:43,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:44,168 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:31:50,956 - __main__ - INFO - Action: 3, Reward: tensor([-19.2350], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:51,645 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-344.8773], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:31:51,661 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:51,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:51,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:52,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:31:59,123 - __main__ - INFO - Action: 11, Reward: tensor([-18.3667], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:59,360 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:59,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:59,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:32:00,311 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:32:12,245 - __main__ - INFO - Episode timed out.
2024-06-03 17:32:12,713 - __main__ - INFO - Action: 9, Reward: tensor([-15.2723], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:32:14,027 - __main__ - INFO - Dropped rows with NA values.
2024-06-03 17:32:14,078 - __main__ - DEBUG - Transformed column values.
2024-06-03 17:34:02,146 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-03 17:34:28,748 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-03 17:34:47,149 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-03 17:35:04,709 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-03 17:40:43,811 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 17:40:46,705 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 17:40:46,782 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 17:40:49,451 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 17:40:49,466 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 17:40:49,512 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:40:49,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:40:50,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:40:50,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:40:50,807 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:40:57,646 - __main__ - INFO - Action: 2, Reward: tensor([-21.9299], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:40:58,548 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-21.9299], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:40:58,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:40:58,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:40:58,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:40:59,588 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:41:12,464 - __main__ - INFO - Action: 12, Reward: tensor([-22.6393], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:12,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:12,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:12,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:13,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:41:26,454 - __main__ - INFO - Action: 5, Reward: tensor([-23.0668], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:26,766 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:26,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:26,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:27,824 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:41:40,314 - __main__ - INFO - Action: 9, Reward: tensor([-21.1836], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:40,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:40,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:40,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:41,309 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:41:53,557 - __main__ - INFO - Action: 4, Reward: tensor([-20.4096], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:53,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:53,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:53,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:54,550 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 17:42:00,583 - __main__ - INFO - Action: 7, Reward: tensor([-20.2180], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:00,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:00,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:00,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:01,270 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:42:13,426 - __main__ - INFO - Action: 12, Reward: tensor([-21.0988], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:13,689 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:13,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:13,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:14,485 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:42:27,008 - __main__ - INFO - Action: 14, Reward: tensor([-22.0579], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:27,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:27,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:27,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:28,334 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:42:40,950 - __main__ - INFO - Action: 15, Reward: tensor([-22.2747], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:41,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:41,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:41,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:42,356 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:42:54,492 - __main__ - INFO - Episode timed out.
2024-06-03 17:42:54,869 - __main__ - INFO - Action: 16, Reward: tensor([-22.4385], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:42:55,573 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:42:56,008 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:56,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:56,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:56,439 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:43:08,834 - __main__ - INFO - Action: 8, Reward: tensor([-20.8966], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:09,595 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-20.8966], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:43:09,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:09,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:09,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:10,232 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:43:23,066 - __main__ - INFO - Action: 5, Reward: tensor([-22.6262], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:23,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:23,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:23,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:24,237 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:43:36,692 - __main__ - INFO - Action: 12, Reward: tensor([-23.4417], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:36,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:37,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:37,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:38,029 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:43:44,584 - __main__ - INFO - Action: 2, Reward: tensor([-23.6734], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:44,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:44,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:44,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:45,974 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:43:52,471 - __main__ - INFO - Action: 2, Reward: tensor([-23.9301], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:52,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:52,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:52,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:53,879 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:44:06,433 - __main__ - INFO - Action: 8, Reward: tensor([-74.7434], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:06,746 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:06,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:06,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:07,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:44:19,476 - __main__ - INFO - Action: 4, Reward: tensor([-24.8769], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:19,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:19,868 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:19,868 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:20,930 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:44:33,683 - __main__ - INFO - Action: 4, Reward: tensor([-25.0319], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:33,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:34,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:34,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:34,958 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:44:47,814 - __main__ - INFO - Action: 8, Reward: tensor([-25.1625], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:48,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:48,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:48,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:49,233 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:45:01,585 - __main__ - INFO - Episode timed out.
2024-06-03 17:45:02,151 - __main__ - INFO - Action: 0, Reward: tensor([-25.2963], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:45:02,918 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:45:03,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:03,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:03,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:04,463 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:45:16,979 - __main__ - INFO - Action: 1, Reward: tensor([-73.2076], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:17,728 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-73.2076], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:45:17,744 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:17,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:17,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:18,665 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:45:31,300 - __main__ - INFO - Action: 5, Reward: tensor([-23.7042], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:31,534 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:31,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:31,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:32,478 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:45:44,290 - __main__ - INFO - Action: 14, Reward: tensor([-24.6020], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:44,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:44,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:44,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:45,506 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:45:57,849 - __main__ - INFO - Action: 5, Reward: tensor([-25.8698], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:58,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:58,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:58,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:59,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 17:46:05,648 - __main__ - INFO - Action: 7, Reward: tensor([-26.5121], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:05,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:06,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:06,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:06,866 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:46:19,445 - __main__ - INFO - Action: 5, Reward: tensor([-27.9847], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:19,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:19,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:19,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:20,639 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:46:27,388 - __main__ - INFO - Action: 3, Reward: tensor([-28.3049], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:27,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:27,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:27,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:28,719 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:46:41,441 - __main__ - INFO - Action: 15, Reward: tensor([-28.1955], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:41,770 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:41,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:41,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:42,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:46:49,475 - __main__ - INFO - Action: 2, Reward: tensor([-28.3694], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:49,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:49,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:49,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:50,407 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:47:02,595 - __main__ - INFO - Action: 0, Reward: tensor([-28.1251], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:02,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:02,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:02,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:03,876 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:47:09,710 - __main__ - INFO - Episode timed out.
2024-06-03 17:47:10,117 - __main__ - INFO - Action: 11, Reward: tensor([-27.3610], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:47:10,879 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-342.2364], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:47:11,238 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:47:11,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:11,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:11,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:12,920 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:47:25,853 - __main__ - INFO - Action: 0, Reward: tensor([-73.4598], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:26,401 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:26,386 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-73.4598], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:47:26,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:26,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:27,398 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:47:40,159 - __main__ - INFO - Action: 14, Reward: tensor([-24.2943], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:40,418 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:40,527 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:40,527 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:41,386 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:47:53,891 - __main__ - INFO - Action: 13, Reward: tensor([-24.5271], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:54,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:54,332 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:54,332 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:55,203 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:48:07,958 - __main__ - INFO - Action: 13, Reward: tensor([-24.9323], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:08,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:08,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:08,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:09,350 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:48:22,232 - __main__ - INFO - Action: 14, Reward: tensor([-26.6796], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:22,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:22,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:22,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:23,735 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:48:36,314 - __main__ - INFO - Action: 5, Reward: tensor([-28.8332], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:36,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:36,627 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:36,627 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:37,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:48:50,319 - __main__ - INFO - Action: 12, Reward: tensor([-29.2838], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:50,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:50,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:50,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:51,741 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:48:58,417 - __main__ - INFO - Action: 2, Reward: tensor([-29.6062], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:58,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:58,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:58,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:59,774 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:49:06,545 - __main__ - INFO - Action: 6, Reward: tensor([-29.5435], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:06,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:07,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:07,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:07,881 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:49:20,117 - __main__ - INFO - Episode timed out.
2024-06-03 17:49:20,676 - __main__ - INFO - Action: 0, Reward: tensor([-30.3778], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:49:21,406 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:49:21,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:22,103 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:22,103 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:23,039 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:49:35,151 - __main__ - INFO - Action: 5, Reward: tensor([-73.8184], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:35,791 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-73.8184], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:49:35,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:35,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:35,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:36,270 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:49:42,992 - __main__ - INFO - Action: 11, Reward: tensor([-22.9771], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:43,307 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:43,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:43,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:44,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:49:56,862 - __main__ - INFO - Action: 0, Reward: tensor([-22.0366], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:57,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:57,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:57,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:58,297 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:50:05,120 - __main__ - INFO - Action: 2, Reward: tensor([-22.1583], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:05,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:05,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:05,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:06,381 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:50:18,998 - __main__ - INFO - Action: 0, Reward: tensor([-22.8756], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:19,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:19,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:19,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:20,342 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:50:33,065 - __main__ - INFO - Action: 1, Reward: tensor([-22.8173], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:33,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:33,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:33,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:34,293 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:50:39,848 - __main__ - INFO - Action: 2, Reward: tensor([-23.1356], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:40,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:40,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:40,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:41,062 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:50:53,695 - __main__ - INFO - Action: 1, Reward: tensor([-22.8391], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:53,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:54,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:54,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:54,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:51:01,466 - __main__ - INFO - Action: 3, Reward: tensor([-22.6859], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:01,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:01,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:01,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:02,764 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:51:15,027 - __main__ - INFO - Action: 15, Reward: tensor([-21.8917], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:15,272 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:15,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:15,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:16,401 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:51:22,625 - __main__ - INFO - Episode timed out.
2024-06-03 17:51:23,205 - __main__ - INFO - Action: 11, Reward: tensor([-20.9171], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:51:23,970 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-298.1526], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:51:24,500 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:51:25,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:25,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:25,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:26,189 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:51:38,843 - __main__ - INFO - Action: 14, Reward: tensor([-73.8977], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:39,642 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-73.8977], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:51:39,674 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:39,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:39,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:39,929 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:51:52,497 - __main__ - INFO - Action: 8, Reward: tensor([-24.0279], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:52,750 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:52,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:52,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:53,810 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:52:06,536 - __main__ - INFO - Action: 4, Reward: tensor([-24.1535], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:06,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:06,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:06,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:07,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:52:20,211 - __main__ - INFO - Action: 9, Reward: tensor([-23.1420], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:20,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:20,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:20,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:21,587 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:52:35,422 - __main__ - INFO - Action: 4, Reward: tensor([-22.3097], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:35,754 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:35,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:35,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:36,745 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:52:49,092 - __main__ - INFO - Action: 14, Reward: tensor([-21.7945], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:49,388 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:49,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:49,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:50,394 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:52:56,823 - __main__ - INFO - Action: 3, Reward: tensor([-21.4972], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:57,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:57,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:57,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:57,902 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:53:04,257 - __main__ - INFO - Action: 10, Reward: tensor([-22.4390], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:53:04,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:53:04,502 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:53:04,502 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:53:04,955 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:53:17,387 - __main__ - INFO - Action: 16, Reward: tensor([-23.1504], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:53:17,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:53:17,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:53:17,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:53:18,553 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:53:23,748 - __main__ - INFO - Action: 6, Reward: tensor([-23.7152], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:53:23,913 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:53:23,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:53:23,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:53:24,678 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:53:30,789 - __main__ - INFO - Episode timed out.
2024-06-03 17:53:31,289 - __main__ - INFO - Action: 3, Reward: tensor([-23.9484], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:53:32,178 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-304.0754], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:53:32,256 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-03 17:53:33,846 - __main__ - INFO - Dropped rows with NA values.
2024-06-03 17:53:33,874 - __main__ - DEBUG - Transformed column values.
2024-06-03 17:54:36,165 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-03 17:57:04,515 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-03 17:57:55,363 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-03 18:00:14,758 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-03 18:10:45,081 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 18:10:47,956 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 18:10:48,110 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 18:10:52,170 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 18:10:52,215 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 18:10:52,513 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:10:53,138 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:10:53,231 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:10:53,231 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:10:54,251 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:14:07,523 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 18:14:10,055 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 18:14:10,177 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 18:14:13,377 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 18:14:13,485 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 18:14:13,888 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:14:14,437 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:14,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:14,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:15,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:14:28,299 - __main__ - INFO - Action: 8, Reward: tensor([-73.8770], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:14:29,125 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-73.8770], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:14:29,141 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:29,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:29,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:30,127 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 18:14:36,812 - __main__ - INFO - Action: 3, Reward: tensor([-23.9703], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:14:37,059 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:37,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:37,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:37,859 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 18:14:50,583 - __main__ - INFO - Action: 9, Reward: tensor([-22.2869], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:14:50,897 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:50,928 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:50,928 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:51,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:15:03,721 - __main__ - INFO - Action: 5, Reward: tensor([-21.4110], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:04,034 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:04,160 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:04,160 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:04,664 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:15:17,567 - __main__ - INFO - Action: 15, Reward: tensor([-21.3748], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:17,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:18,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:18,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:19,033 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:15:31,764 - __main__ - INFO - Action: 8, Reward: tensor([-24.0044], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:32,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:32,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:32,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:33,216 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:15:45,706 - __main__ - INFO - Action: 14, Reward: tensor([-74.9845], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:45,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:46,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:46,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:46,680 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:15:59,427 - __main__ - INFO - Action: 12, Reward: tensor([-25.1309], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:59,737 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:59,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:59,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:00,299 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:16:12,552 - __main__ - INFO - Action: 0, Reward: tensor([-25.2769], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:16:12,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:16:12,834 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:12,834 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:13,739 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:16:25,440 - __main__ - INFO - Episode timed out.
2024-06-03 18:16:25,500 - __main__ - INFO - Action: 15, Reward: tensor([-25.3827], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:16:26,187 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:16:26,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:16:26,763 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:26,763 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:27,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:16:40,051 - __main__ - INFO - Action: 16, Reward: tensor([-23.3492], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:16:40,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
n=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:16:40,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:40,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:41,050 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:16:53,281 - __main__ - INFO - Action: 16, Reward: tensor([-23.4702], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:16:53,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:16:53,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:53,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:53,579 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:17:00,025 - __main__ - INFO - Action: 2, Reward: tensor([-23.6086], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:00,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:00,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:00,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:00,869 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:17:13,570 - __main__ - INFO - Action: 14, Reward: tensor([-23.7759], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:13,773 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:13,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:13,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:14,805 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:17:26,621 - __main__ - INFO - Action: 12, Reward: tensor([-24.7972], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:26,727 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:26,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:26,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:27,628 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:17:40,165 - __main__ - INFO - Action: 16, Reward: tensor([-25.3602], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:40,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:40,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:40,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:41,540 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:17:53,971 - __main__ - INFO - Action: 12, Reward: tensor([-26.8760], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:54,220 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:55,278 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 18:18:07,837 - __main__ - INFO - Action: 4, Reward: tensor([-28.2464], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:08,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:08,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:08,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:08,998 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:18:21,175 - __main__ - INFO - Action: 12, Reward: tensor([-30.6605], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:21,502 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:21,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:21,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:22,174 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:18:34,151 - __main__ - INFO - Episode timed out.
2024-06-03 18:18:34,638 - __main__ - INFO - Action: 0, Reward: tensor([-33.1924], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:18:35,356 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:18:35,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:36,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:36,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:36,765 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:18:43,326 - __main__ - INFO - Action: 11, Reward: tensor([-72.9635], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:43,779 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-72.9635], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:18:43,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:43,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:43,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:44,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:18:57,431 - __main__ - INFO - Action: 5, Reward: tensor([-21.7229], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:57,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:57,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:57,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:58,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:19:11,108 - __main__ - INFO - Action: 13, Reward: tensor([-22.0833], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:11,484 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:11,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:11,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:12,313 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:19:17,613 - __main__ - INFO - Action: 7, Reward: tensor([-22.4088], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:17,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:17,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:17,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:18,873 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 18:19:25,581 - __main__ - INFO - Action: 10, Reward: tensor([-23.5560], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:25,814 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:25,908 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:25,908 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:26,543 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:19:38,278 - __main__ - INFO - Action: 15, Reward: tensor([-24.6303], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:38,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:38,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:38,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:38,812 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:19:51,206 - __main__ - INFO - Action: 0, Reward: tensor([-24.9146], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:51,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:51,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:51,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:52,530 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 18:19:59,231 - __main__ - INFO - Action: 3, Reward: tensor([-25.3120], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:59,528 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:59,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:59,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:00,544 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 18:20:12,629 - __main__ - INFO - Action: 1, Reward: tensor([-75.7144], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:20:12,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:12,913 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:12,913 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:13,667 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 18:20:26,302 - __main__ - INFO - Action: 4, Reward: tensor([-25.8395], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:20:26,597 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:26,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:26,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:27,192 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:20:38,920 - __main__ - INFO - Episode timed out.
2024-06-03 18:20:38,937 - __main__ - INFO - Action: 16, Reward: tensor([-25.9471], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:20:39,513 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-365.0925], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:20:39,905 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:20:40,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:40,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:40,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:41,250 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:20:53,021 - __main__ - INFO - Action: 0, Reward: tensor([-73.5036], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:20:53,723 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-73.5036], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:20:53,739 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:53,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:53,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:54,358 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:21:06,853 - __main__ - INFO - Action: 14, Reward: tensor([-24.3959], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:07,118 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:07,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:07,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:07,756 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:21:13,781 - __main__ - INFO - Action: 7, Reward: tensor([-24.6870], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:13,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:14,051 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:14,051 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:14,880 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 18:21:21,376 - __main__ - INFO - Action: 6, Reward: tensor([-74.9528], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:21,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:21,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:21,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:22,339 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:21:35,069 - __main__ - INFO - Action: 8, Reward: tensor([-75.1211], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:35,307 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:35,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:35,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:36,392 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:21:43,065 - __main__ - INFO - Action: 11, Reward: tensor([-24.8045], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:43,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:43,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:43,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:44,551 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:21:57,361 - __main__ - INFO - Action: 14, Reward: tensor([-25.2265], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:57,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:57,731 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:57,731 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:58,639 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:22:05,530 - __main__ - INFO - Action: 7, Reward: tensor([-25.5168], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:05,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:05,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:05,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:06,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 18:22:12,541 - __main__ - INFO - Action: 10, Reward: tensor([-26.5632], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:12,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:12,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:12,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:13,515 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:22:25,903 - __main__ - INFO - Action: 14, Reward: tensor([-76.7314], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:26,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:26,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:26,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:26,591 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:22:38,734 - __main__ - INFO - Action: 13, Reward: tensor([-26.8631], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:38,966 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-478.3659], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:22:38,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:38,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:38,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:39,502 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:22:51,200 - __main__ - INFO - Episode timed out.
2024-06-03 18:22:51,635 - __main__ - INFO - Action: 5, Reward: tensor([-26.9949], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:22:52,338 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:22:52,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:53,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:53,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:53,692 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 18:23:06,600 - __main__ - INFO - Action: 1, Reward: tensor([-73.4694], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:07,329 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-73.4694], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:23:07,344 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:07,420 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:07,420 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:08,169 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:23:20,548 - __main__ - INFO - Action: 15, Reward: tensor([-74.1694], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:20,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:20,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:20,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:21,533 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:23:34,322 - __main__ - INFO - Action: 14, Reward: tensor([-23.9856], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:34,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:34,668 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:34,668 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:35,463 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:23:47,196 - __main__ - INFO - Action: 14, Reward: tensor([-23.8372], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:47,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:47,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:47,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:47,724 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:24:00,179 - __main__ - INFO - Action: 8, Reward: tensor([-74.5382], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:00,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:00,543 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:00,543 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:01,497 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 18:24:08,334 - __main__ - INFO - Action: 2, Reward: tensor([-24.6262], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:08,598 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:08,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:08,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:09,705 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 18:24:16,138 - __main__ - INFO - Action: 10, Reward: tensor([-24.7105], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:16,387 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:16,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:16,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:16,743 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:24:22,598 - __main__ - INFO - Action: 7, Reward: tensor([-24.7693], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:22,883 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:22,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:22,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:23,151 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:24:35,399 - __main__ - INFO - Action: 8, Reward: tensor([-24.9008], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:35,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:35,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:35,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:36,581 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:24:42,359 - __main__ - INFO - Action: 11, Reward: tensor([-24.6694], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:42,473 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:42,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:42,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:42,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:24:54,639 - __main__ - INFO - Episode timed out.
2024-06-03 18:24:54,688 - __main__ - INFO - Action: 5, Reward: tensor([-24.4835], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:24:55,188 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-418.1597], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:24:55,658 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:24:56,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:56,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:56,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:57,425 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:25:09,733 - __main__ - INFO - Action: 0, Reward: tensor([-73.8234], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:10,540 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-73.8234], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:25:10,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:10,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:10,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:10,607 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:25:22,503 - __main__ - INFO - Action: 12, Reward: tensor([-24.4939], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:22,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:22,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:22,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:23,357 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 18:25:35,722 - __main__ - INFO - Action: 0, Reward: tensor([-75.6006], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:36,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:36,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:36,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:36,891 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 18:25:48,268 - __main__ - INFO - Action: 4, Reward: tensor([-75.7290], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:48,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:48,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:48,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:48,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 18:25:54,644 - __main__ - INFO - Action: 3, Reward: tensor([-25.6895], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:54,957 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:55,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:55,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:55,954 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:26:08,148 - __main__ - INFO - Action: 0, Reward: tensor([-26.1804], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:08,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:08,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:08,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:09,051 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:26:14,672 - __main__ - INFO - Action: 2, Reward: tensor([-26.3731], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:14,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:15,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:15,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:15,906 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 18:26:27,527 - __main__ - INFO - Action: 1, Reward: tensor([-25.4603], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:27,667 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:27,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:27,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:28,521 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:26:40,209 - __main__ - INFO - Action: 14, Reward: tensor([-25.7706], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:40,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:40,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:40,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:41,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 18:26:53,527 - __main__ - INFO - Action: 1, Reward: tensor([-25.5965], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:53,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:53,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:53,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:54,726 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:27:00,668 - __main__ - INFO - Episode timed out.
2024-06-03 18:27:01,211 - __main__ - INFO - Action: 2, Reward: tensor([-26.1000], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:27:01,838 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-430.8173], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:27:02,229 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:27:02,776 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:02,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:02,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:03,771 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:27:15,300 - __main__ - INFO - Action: 16, Reward: tensor([-73.4724], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:15,889 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-73.4724], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:27:15,904 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:15,952 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:15,952 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:16,894 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 18:27:28,490 - __main__ - INFO - Action: 1, Reward: tensor([-74.0937], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:28,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:28,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:28,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:28,956 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:27:40,555 - __main__ - INFO - Action: 13, Reward: tensor([-24.2530], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:40,663 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:40,696 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:40,696 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:41,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 18:27:53,584 - __main__ - INFO - Action: 0, Reward: tensor([-24.1083], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:53,865 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:53,960 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:53,960 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:54,318 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:28:07,017 - __main__ - INFO - Action: 16, Reward: tensor([-23.9383], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:07,330 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:07,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:07,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:08,416 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 18:28:20,709 - __main__ - INFO - Action: 4, Reward: tensor([-23.8820], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:20,984 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:21,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:21,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:21,809 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:28:28,123 - __main__ - INFO - Action: 11, Reward: tensor([-23.0133], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:28,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:28,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:28,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:29,171 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:28:40,898 - __main__ - INFO - Action: 13, Reward: tensor([-22.3513], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:41,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:41,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:41,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:42,097 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 18:28:54,322 - __main__ - INFO - Action: 9, Reward: tensor([-19.7789], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:54,615 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:55,365 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:29:01,389 - __main__ - INFO - Action: 2, Reward: tensor([-19.1672], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:29:01,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:29:01,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:29:01,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:29:02,689 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 18:29:14,861 - __main__ - INFO - Episode timed out.
2024-06-03 18:29:15,330 - __main__ - INFO - Action: 0, Reward: tensor([-18.6951], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:29:16,204 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-346.7535], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:29:16,329 - __main__ - INFO - Early stopping triggered after 8 epochs.
2024-06-03 18:29:20,950 - __main__ - INFO - Dropped rows with NA values.
2024-06-03 18:29:20,982 - __main__ - DEBUG - Transformed column values.
2024-06-03 18:29:55,186 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-03 18:30:21,461 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-03 18:30:40,512 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-03 18:31:15,314 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-03 18:36:44,230 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 18:36:47,253 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 18:36:47,394 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 18:36:50,944 - __main__ - ERROR - Error loading checkpoint: Error(s) in loading state_dict for AdvancedPolicyNetwork:
	Missing key(s) in state_dict: "attention_layers.0.context_vector", "attention_layers.0.fc.weight", "attention_layers.0.fc.bias", "attention_layers.1.context_vector", "attention_layers.1.fc.weight", "attention_layers.1.fc.bias". 
2024-06-03 18:36:51,628 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:36:52,189 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:36:52,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:36:52,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:23:50,419 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 22:23:53,492 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 22:23:53,647 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 22:23:56,869 - __main__ - ERROR - Error loading checkpoint: Error(s) in loading state_dict for AdvancedPolicyNetwork:
	Missing key(s) in state_dict: "attention_layers.0.context_vector", "attention_layers.0.fc.weight", "attention_layers.0.fc.bias", "attention_layers.1.context_vector", "attention_layers.1.fc.weight", "attention_layers.1.fc.bias". 
2024-06-03 22:23:57,371 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:23:57,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:23:58,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:23:58,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:24,330 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 22:31:28,399 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 22:31:28,554 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 22:31:31,847 - __main__ - ERROR - Error loading checkpoint: Error(s) in loading state_dict for AdvancedPolicyNetwork:
	Missing key(s) in state_dict: "attention_layers.0.context_vector", "attention_layers.0.fc.weight", "attention_layers.0.fc.bias", "attention_layers.1.context_vector", "attention_layers.1.fc.weight", "attention_layers.1.fc.bias". 
2024-06-03 22:31:32,411 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:31:32,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:33,040 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:33,040 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:34,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 22:31:38,749 - __main__ - INFO - Action: 2, Reward: tensor([-124.8774], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:31:39,640 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-124.8774], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:31:39,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:39,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:39,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:40,780 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:31:49,303 - __main__ - INFO - Action: 9, Reward: tensor([-123.6867], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:31:49,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:49,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:49,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:50,975 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:31:59,207 - __main__ - INFO - Action: 9, Reward: tensor([-21.7931], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:31:59,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:59,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:59,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:00,425 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:32:08,775 - __main__ - INFO - Action: 12, Reward: tensor([-21.0015], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:09,133 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:09,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:09,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:10,447 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:32:18,464 - __main__ - INFO - Action: 0, Reward: tensor([-21.1119], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:18,587 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:18,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:18,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:19,306 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:32:23,535 - __main__ - INFO - Action: 6, Reward: tensor([-21.3886], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:23,876 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:23,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:23,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:24,407 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 22:32:32,868 - __main__ - INFO - Action: 5, Reward: tensor([-21.4900], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:33,136 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:33,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:33,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:33,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:32:43,000 - __main__ - INFO - Action: 16, Reward: tensor([-21.5154], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:43,330 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:43,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:43,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:44,750 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:32:53,791 - __main__ - INFO - Action: 14, Reward: tensor([-21.9326], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:54,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:54,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:54,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:55,355 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:33:04,434 - __main__ - INFO - Action: 16, Reward: tensor([-22.3983], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:04,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:05,296 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:33:13,751 - __main__ - INFO - Action: 14, Reward: tensor([-23.0391], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:14,412 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-444.2346], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:33:14,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:14,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:14,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:15,834 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:33:24,610 - __main__ - INFO - Action: 0, Reward: tensor([-24.0124], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:24,921 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:24,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:24,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:25,646 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:33:33,399 - __main__ - INFO - Episode timed out.
2024-06-03 22:33:34,009 - __main__ - INFO - Action: 14, Reward: tensor([-25.1596], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:33:34,943 - Drone.source.models.ppo.ppo_agent - INFO - Model saved at e:\Project\models/checkpoints\ppo_agent_epoch_0.pt
2024-06-03 22:33:35,742 - __main__ - INFO - Checkpoint saved at epoch 0
2024-06-03 22:33:36,151 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:33:36,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:36,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:36,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:37,936 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:33:47,110 - __main__ - INFO - Action: 9, Reward: tensor([-123.5851], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:47,986 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-123.5851], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:33:48,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:48,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:48,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:48,827 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 22:33:57,834 - __main__ - INFO - Action: 5, Reward: tensor([-23.2442], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:58,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:58,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:58,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:58,869 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:34:07,464 - __main__ - INFO - Action: 1, Reward: tensor([-23.3597], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:07,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:07,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:07,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:08,633 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:34:17,060 - __main__ - INFO - Action: 9, Reward: tensor([-22.1373], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:17,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:17,418 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:17,418 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:18,747 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:34:23,615 - __main__ - INFO - Action: 10, Reward: tensor([-22.5376], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:23,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:23,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:23,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:25,186 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:34:34,359 - __main__ - INFO - Action: 16, Reward: tensor([-23.3814], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:34,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:35,890 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:34:44,753 - __main__ - INFO - Action: 16, Reward: tensor([-24.0185], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:45,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:45,173 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:45,173 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:46,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:34:51,419 - __main__ - INFO - Action: 7, Reward: tensor([-24.3664], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:51,684 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:51,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:51,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:52,891 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:35:02,026 - __main__ - INFO - Action: 14, Reward: tensor([-24.8708], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:02,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:02,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:02,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:03,383 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 22:35:08,245 - __main__ - INFO - Action: 3, Reward: tensor([-25.1214], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:08,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:08,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:08,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:09,815 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:35:18,449 - __main__ - INFO - Action: 9, Reward: tensor([-24.1071], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:19,136 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-360.7294], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:35:19,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:19,247 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:19,247 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:20,467 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:35:28,549 - __main__ - INFO - Action: 4, Reward: tensor([-23.3953], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:28,690 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:28,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:28,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:30,142 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:35:34,283 - __main__ - INFO - Action: 10, Reward: tensor([-23.8912], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:34,312 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:34,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:34,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:35,614 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 22:35:44,269 - __main__ - INFO - Episode timed out.
2024-06-03 22:35:44,845 - __main__ - INFO - Action: 8, Reward: tensor([-26.1680], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:35:45,645 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:35:46,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:46,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:46,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:46,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 22:35:51,569 - __main__ - INFO - Action: 3, Reward: tensor([-23.1403], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:52,431 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-23.1403], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:35:52,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:52,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:52,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:53,104 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:36:01,793 - __main__ - INFO - Action: 12, Reward: tensor([-23.4525], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:02,123 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:02,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:02,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:03,421 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:36:12,299 - __main__ - INFO - Action: 1, Reward: tensor([-23.7282], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:12,437 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:12,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:12,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:13,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 22:36:18,105 - __main__ - INFO - Action: 3, Reward: tensor([-23.9074], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:18,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:18,465 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:18,465 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:19,684 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:36:24,617 - __main__ - INFO - Action: 7, Reward: tensor([-24.0427], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:24,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:25,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:25,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:26,260 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:36:35,317 - __main__ - INFO - Action: 14, Reward: tensor([-24.0669], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:35,643 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:35,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:35,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:36,915 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 22:36:46,081 - __main__ - INFO - Action: 13, Reward: tensor([-23.8853], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:46,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:46,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:46,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:47,833 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-03 22:36:56,768 - __main__ - INFO - Action: 5, Reward: tensor([-23.8923], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:57,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:57,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:57,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:58,130 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:37:07,092 - __main__ - INFO - Action: 0, Reward: tensor([-24.1345], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:07,469 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:07,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:07,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:08,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:37:17,049 - __main__ - INFO - Action: 14, Reward: tensor([-24.9217], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:17,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:18,423 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:37:27,055 - __main__ - INFO - Action: 14, Reward: tensor([-25.9729], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:27,711 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-265.1446], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:37:27,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:27,850 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:27,850 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:29,088 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:37:37,702 - __main__ - INFO - Action: 15, Reward: tensor([-26.0410], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:38,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:38,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:38,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:39,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:37:43,969 - __main__ - INFO - Action: 6, Reward: tensor([-26.1279], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:44,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:44,375 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:44,375 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:45,622 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 22:37:53,997 - __main__ - INFO - Episode timed out.
2024-06-03 22:37:54,545 - __main__ - INFO - Action: 13, Reward: tensor([-26.4828], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:37:55,297 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:37:55,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:56,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:56,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:57,244 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:38:05,995 - __main__ - INFO - Action: 0, Reward: tensor([-124.9624], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:06,665 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.9624], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:38:06,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:06,822 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:06,822 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:08,042 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:38:16,396 - __main__ - INFO - Action: 0, Reward: tensor([-25.2289], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:16,691 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:16,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:16,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:17,910 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:38:22,884 - __main__ - INFO - Action: 6, Reward: tensor([-25.3284], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:23,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:23,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:23,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:24,567 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 22:38:33,480 - __main__ - INFO - Action: 5, Reward: tensor([-25.4902], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:33,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:33,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:33,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:34,743 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:38:43,703 - __main__ - INFO - Action: 14, Reward: tensor([-125.7994], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:43,968 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:44,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:44,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:45,378 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:38:54,191 - __main__ - INFO - Action: 4, Reward: tensor([-125.8758], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:54,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:55,830 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:39:04,603 - __main__ - INFO - Action: 15, Reward: tensor([-26.0607], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:04,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:04,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:04,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:06,195 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:39:14,566 - __main__ - INFO - Action: 12, Reward: tensor([-26.3205], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:14,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:14,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:14,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:16,128 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:39:25,023 - __main__ - INFO - Action: 0, Reward: tensor([-26.8074], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:25,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:25,524 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:25,524 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:26,748 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:39:35,596 - __main__ - INFO - Action: 0, Reward: tensor([-27.3212], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:35,928 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:35,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:35,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:37,166 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:39:45,924 - __main__ - INFO - Action: 0, Reward: tensor([-128.1371], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:46,597 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-687.3320], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:39:46,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:46,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:46,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:47,916 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 22:39:52,491 - __main__ - INFO - Action: 2, Reward: tensor([-28.2860], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:52,675 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:52,770 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:52,770 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:53,885 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-03 22:39:57,961 - __main__ - INFO - Episode timed out.
2024-06-03 22:39:57,965 - __main__ - INFO - Action: 3, Reward: tensor([-28.2246], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:39:58,625 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:39:59,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:59,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:59,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:00,558 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:40:09,501 - __main__ - INFO - Action: 9, Reward: tensor([-123.5880], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:10,204 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-123.5880], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:40:10,218 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:10,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:10,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:11,507 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:40:16,001 - __main__ - INFO - Action: 6, Reward: tensor([-23.5145], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:16,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:16,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:16,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:16,826 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:40:26,018 - __main__ - INFO - Action: 1, Reward: tensor([-23.5565], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:26,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:26,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:26,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:27,619 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:40:32,295 - __main__ - INFO - Action: 10, Reward: tensor([-24.2382], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:32,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:32,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:32,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:34,011 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:40:39,043 - __main__ - INFO - Action: 10, Reward: tensor([-25.1888], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:39,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:39,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:39,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:40,680 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:40:49,024 - __main__ - INFO - Action: 12, Reward: tensor([-125.8320], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:49,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:49,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:49,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:49,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:40:58,200 - __main__ - INFO - Action: 0, Reward: tensor([-26.0123], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:58,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:58,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:58,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:59,856 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:41:08,773 - __main__ - INFO - Action: 14, Reward: tensor([-26.2500], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:09,087 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:09,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:09,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:10,122 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 22:41:18,761 - __main__ - INFO - Action: 8, Reward: tensor([-26.4088], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:18,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:19,025 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:19,025 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:20,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 22:41:25,114 - __main__ - INFO - Action: 11, Reward: tensor([-26.3039], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:25,442 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:25,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:25,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:26,662 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 22:41:31,015 - __main__ - INFO - Action: 11, Reward: tensor([-25.9270], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:31,796 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-476.8200], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:41:31,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:31,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:31,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:33,141 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:41:42,041 - __main__ - INFO - Action: 4, Reward: tensor([-25.1634], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:42,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:42,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:42,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:43,720 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:41:52,673 - __main__ - INFO - Action: 14, Reward: tensor([-24.7226], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:52,969 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:53,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:53,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:54,297 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:42:02,305 - __main__ - INFO - Episode timed out.
2024-06-03 22:42:02,916 - __main__ - INFO - Action: 16, Reward: tensor([-24.5570], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:42:03,666 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:42:04,232 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:04,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:04,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:05,669 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 22:42:14,128 - __main__ - INFO - Action: 4, Reward: tensor([-124.9585], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:14,843 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:14,826 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.9585], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:42:14,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:14,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:15,829 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:42:23,882 - __main__ - INFO - Action: 0, Reward: tensor([-25.1048], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:24,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:24,121 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:24,121 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:25,310 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:42:34,281 - __main__ - INFO - Action: 15, Reward: tensor([-25.3481], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:34,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:35,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-03 22:42:44,950 - __main__ - INFO - Action: 5, Reward: tensor([-125.5897], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:45,264 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:45,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:45,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:46,666 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:42:55,187 - __main__ - INFO - Action: 15, Reward: tensor([-25.8942], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:55,466 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:55,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:55,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:56,859 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:43:05,632 - __main__ - INFO - Action: 16, Reward: tensor([-26.0375], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:05,897 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:05,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:05,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:07,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 22:43:15,805 - __main__ - INFO - Action: 1, Reward: tensor([-26.2979], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:15,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:15,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:15,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:17,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:43:21,981 - __main__ - INFO - Action: 7, Reward: tensor([-26.3817], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:22,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:23,502 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:43:31,154 - __main__ - INFO - Action: 12, Reward: tensor([-26.5829], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:31,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:31,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:31,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:32,360 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:43:40,661 - __main__ - INFO - Action: 16, Reward: tensor([-26.7095], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:40,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:41,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:41,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:42,009 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:43:50,575 - __main__ - INFO - Action: 1, Reward: tensor([-26.9457], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:51,244 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-485.8506], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:43:51,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:51,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:51,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:52,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:43:57,166 - __main__ - INFO - Action: 7, Reward: tensor([-27.0968], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:57,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:57,617 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:57,617 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:58,765 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 22:44:03,437 - __main__ - INFO - Action: 2, Reward: tensor([-27.1910], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:03,672 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:03,782 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:03,782 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:05,096 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 22:44:09,407 - __main__ - INFO - Episode timed out.
2024-06-03 22:44:09,957 - __main__ - INFO - Action: 2, Reward: tensor([-27.3744], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:44:10,615 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:44:11,193 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:12,491 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:44:16,689 - __main__ - INFO - Action: 7, Reward: tensor([-124.8812], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:17,419 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8812], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:44:17,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:18,841 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:44:27,484 - __main__ - INFO - Action: 4, Reward: tensor([-25.0410], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:27,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:27,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:27,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:29,131 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:44:37,505 - __main__ - INFO - Action: 0, Reward: tensor([-25.2990], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:37,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:37,672 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:37,672 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:38,833 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:44:47,530 - __main__ - INFO - Action: 0, Reward: tensor([-25.5364], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:47,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:47,967 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:47,967 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:49,199 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:44:57,337 - __main__ - INFO - Action: 1, Reward: tensor([-25.6276], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:57,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:57,682 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:57,682 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:58,983 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 22:45:07,835 - __main__ - INFO - Action: 1, Reward: tensor([-25.9061], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:08,193 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:08,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:08,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:09,490 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:45:18,455 - __main__ - INFO - Action: 0, Reward: tensor([-26.0484], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:18,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:18,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:18,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:19,530 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:45:24,494 - __main__ - INFO - Action: 10, Reward: tensor([-26.2327], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:24,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:24,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:24,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:26,062 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:45:30,984 - __main__ - INFO - Action: 10, Reward: tensor([-26.4104], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:31,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:31,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:31,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:32,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:49:24,868 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 22:49:29,090 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 22:49:29,246 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 22:49:33,186 - __main__ - ERROR - Error loading checkpoint: 'icm_state_dict'
2024-06-03 22:49:35,122 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:01:09,309 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:29:14,094 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:42:51,186 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:45:34,435 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:45:39,541 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 23:45:39,696 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 23:45:43,792 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 23:45:45,038 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 23:45:45,448 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:45:45,964 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:45:46,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:45:46,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:45:47,275 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:45:51,972 - __main__ - INFO - Action: 11, Reward: tensor([-124.3182], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:47:58,009 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:48:03,573 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 23:48:03,714 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 23:48:07,189 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 23:48:08,686 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 23:48:09,092 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:48:09,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:09,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:09,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:11,121 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:48:19,801 - __main__ - INFO - Action: 9, Reward: tensor([-123.5375], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:20,488 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-123.5375], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:48:20,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:20,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:20,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:22,052 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:48:30,757 - __main__ - INFO - Action: 9, Reward: tensor([-21.6875], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:31,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:31,509 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:31,509 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:32,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 23:48:41,566 - __main__ - INFO - Action: 13, Reward: tensor([-20.9474], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:42,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:42,346 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:42,346 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:43,569 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:48:52,029 - __main__ - INFO - Action: 0, Reward: tensor([-20.6387], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:52,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:52,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:52,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:53,845 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:49:02,609 - __main__ - INFO - Action: 15, Reward: tensor([-20.9335], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:03,217 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:03,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:03,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:04,493 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:49:13,215 - __main__ - INFO - Action: 15, Reward: tensor([-21.0516], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:13,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:13,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:13,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:15,061 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:49:19,541 - __main__ - INFO - Action: 7, Reward: tensor([-21.0263], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:20,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:20,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:20,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:21,378 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 23:49:30,005 - __main__ - INFO - Action: 14, Reward: tensor([-20.8035], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:30,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:30,648 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:30,648 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:31,929 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:49:40,720 - __main__ - INFO - Action: 4, Reward: tensor([-20.7368], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:41,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:41,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:41,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:42,661 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:49:50,677 - __main__ - INFO - Action: 9, Reward: tensor([-19.4768], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:51,049 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:51,174 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:51,174 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:52,444 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:50:01,304 - __main__ - INFO - Action: 5, Reward: tensor([-19.0873], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:02,161 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-329.9269], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:50:02,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:02,616 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:02,616 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:03,764 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 23:50:12,290 - __main__ - INFO - Episode timed out.
2024-06-03 23:50:12,805 - __main__ - INFO - Action: 12, Reward: tensor([-19.0988], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:50:13,522 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:50:14,257 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:14,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:14,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:15,435 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 23:50:24,147 - __main__ - INFO - Action: 14, Reward: tensor([-124.8637], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:24,851 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8637], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:50:25,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:25,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:25,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:26,447 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 23:50:34,258 - __main__ - INFO - Action: 8, Reward: tensor([-125.1672], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:34,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:34,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:34,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:36,161 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:50:40,958 - __main__ - INFO - Action: 3, Reward: tensor([-25.2516], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:41,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:41,572 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:41,572 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:42,829 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:50:51,540 - __main__ - INFO - Action: 16, Reward: tensor([-25.4276], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:52,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:52,229 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:52,229 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:53,487 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:50:58,273 - __main__ - INFO - Action: 7, Reward: tensor([-25.6082], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:58,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:59,020 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:59,020 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:00,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:51:04,919 - __main__ - INFO - Action: 6, Reward: tensor([-25.6930], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:05,484 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:05,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:05,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:06,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:51:11,615 - __main__ - INFO - Action: 11, Reward: tensor([-25.4803], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:12,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:12,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:12,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:13,511 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:51:22,311 - __main__ - INFO - Action: 15, Reward: tensor([-25.0998], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:22,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:23,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:23,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:24,219 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:51:33,001 - __main__ - INFO - Action: 4, Reward: tensor([-25.2463], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:33,615 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:33,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:33,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:34,901 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:51:43,779 - __main__ - INFO - Action: 5, Reward: tensor([-25.6765], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:44,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:44,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:44,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:45,782 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:51:54,637 - __main__ - INFO - Action: 4, Reward: tensor([-26.0885], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:55,219 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-479.6028], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:51:55,500 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:55,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:55,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:56,787 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 23:52:01,661 - __main__ - INFO - Action: 10, Reward: tensor([-26.8418], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:02,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:02,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:02,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:03,493 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:52:08,427 - __main__ - INFO - Action: 3, Reward: tensor([-127.0811], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:08,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:08,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:08,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:09,708 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:52:17,836 - __main__ - INFO - Episode timed out.
2024-06-03 23:52:17,837 - __main__ - INFO - Action: 16, Reward: tensor([-27.2313], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:52:18,433 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:52:19,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:19,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:19,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:20,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 23:52:29,488 - __main__ - INFO - Action: 14, Reward: tensor([-124.8970], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:30,254 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.8970], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:52:30,458 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:30,519 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:30,519 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:31,689 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:52:40,548 - __main__ - INFO - Action: 4, Reward: tensor([-24.9968], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:41,172 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:41,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:41,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:42,219 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:52:50,711 - __main__ - INFO - Action: 16, Reward: tensor([-24.8432], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:51,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:51,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:51,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:52,256 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:52:56,735 - __main__ - INFO - Action: 7, Reward: tensor([-24.9844], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:57,158 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:57,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:57,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:58,416 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:53:07,074 - __main__ - INFO - Action: 0, Reward: tensor([-24.9892], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:07,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:07,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:07,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:08,346 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:53:16,857 - __main__ - INFO - Action: 0, Reward: tensor([-24.9755], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:17,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:17,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:17,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:18,537 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:53:22,937 - __main__ - INFO - Action: 11, Reward: tensor([-24.6555], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:23,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:23,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:23,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:24,795 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:53:33,704 - __main__ - INFO - Action: 5, Reward: tensor([-24.1067], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:34,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:34,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:34,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:35,629 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 23:53:44,369 - __main__ - INFO - Action: 1, Reward: tensor([-24.0210], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:44,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:45,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:45,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:46,139 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:53:55,121 - __main__ - INFO - Action: 4, Reward: tensor([-24.0400], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:55,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:55,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:55,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:56,640 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:54:04,960 - __main__ - INFO - Action: 4, Reward: tensor([-24.2773], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:05,769 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-370.7867], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:54:06,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:06,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:06,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:07,335 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:54:15,000 - __main__ - INFO - Action: 5, Reward: tensor([-24.3969], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:15,589 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:15,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:15,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:16,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:54:23,970 - __main__ - INFO - Episode timed out.
2024-06-03 23:54:24,024 - __main__ - INFO - Action: 15, Reward: tensor([-24.4285], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:54:24,516 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:54:25,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:25,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:25,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:26,724 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 23:54:35,785 - __main__ - INFO - Action: 13, Reward: tensor([-124.9566], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:36,538 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.9566], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:54:36,865 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:36,974 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:36,974 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:38,178 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:54:47,133 - __main__ - INFO - Action: 16, Reward: tensor([-25.1164], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:47,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:47,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:47,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:48,963 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:54:57,091 - __main__ - INFO - Action: 0, Reward: tensor([-25.3210], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:57,594 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:57,594 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:57,595 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:58,728 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:55:03,625 - __main__ - INFO - Action: 2, Reward: tensor([-25.5109], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:04,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:04,286 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:04,286 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:05,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:55:10,593 - __main__ - INFO - Action: 3, Reward: tensor([-125.5480], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:11,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:11,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:11,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:12,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:55:17,280 - __main__ - INFO - Action: 2, Reward: tensor([-25.6955], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:17,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:18,017 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:18,017 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:19,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:55:28,013 - __main__ - INFO - Action: 5, Reward: tensor([-25.7750], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:28,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:28,653 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:28,653 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:29,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:55:34,611 - __main__ - INFO - Action: 11, Reward: tensor([-25.4699], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:35,217 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:35,326 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:35,326 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:36,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:55:41,069 - __main__ - INFO - Action: 3, Reward: tensor([-25.3986], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:41,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:41,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:41,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:42,884 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:55:51,480 - __main__ - INFO - Action: 0, Reward: tensor([-25.2994], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:52,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:52,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:52,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:53,442 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-03 23:55:58,315 - __main__ - INFO - Action: 3, Reward: tensor([-25.4328], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:59,115 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-479.5242], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:55:59,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:59,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:59,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:00,743 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:56:05,584 - __main__ - INFO - Action: 6, Reward: tensor([-25.5151], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:06,051 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:06,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:06,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:07,374 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 23:56:12,165 - __main__ - INFO - Action: 10, Reward: tensor([-26.1468], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:12,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:12,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:12,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:13,979 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:56:18,792 - __main__ - INFO - Action: 2, Reward: tensor([-26.5669], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:19,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:19,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:19,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:20,701 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:56:24,956 - __main__ - INFO - Episode timed out.
2024-06-03 23:56:25,395 - __main__ - INFO - Action: 6, Reward: tensor([-26.9326], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:56:25,926 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:56:26,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:26,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:26,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:28,109 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:56:36,860 - __main__ - INFO - Action: 0, Reward: tensor([-124.9084], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:37,578 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.9084], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:56:37,908 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:38,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:38,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:39,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:56:47,911 - __main__ - INFO - Action: 4, Reward: tensor([-125.1381], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:48,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:48,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:48,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:49,889 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:56:54,454 - __main__ - INFO - Action: 2, Reward: tensor([-25.3028], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:55,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:55,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:55,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:56,387 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 23:57:05,266 - __main__ - INFO - Action: 8, Reward: tensor([-25.4947], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:05,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:06,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:06,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:07,222 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:57:15,996 - __main__ - INFO - Action: 9, Reward: tensor([-25.0073], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:16,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:16,623 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:16,623 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:17,690 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:57:26,430 - __main__ - INFO - Action: 15, Reward: tensor([-24.9869], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:26,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:27,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:27,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:28,299 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:57:33,273 - __main__ - INFO - Action: 11, Reward: tensor([-24.6470], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:33,697 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:33,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:33,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:35,015 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:57:43,817 - __main__ - INFO - Action: 4, Reward: tensor([-24.2960], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:44,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:44,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:44,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:45,604 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:57:54,377 - __main__ - INFO - Action: 0, Reward: tensor([-24.3938], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:54,972 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:55,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:55,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:56,285 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:58:01,113 - __main__ - INFO - Action: 2, Reward: tensor([-24.6113], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:01,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:01,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:01,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:02,863 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 23:58:07,566 - __main__ - INFO - Action: 10, Reward: tensor([-25.2377], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:08,405 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-474.0241], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:58:08,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:08,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:08,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:09,976 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:58:18,758 - __main__ - INFO - Action: 0, Reward: tensor([-26.5905], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:19,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:19,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:19,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:20,562 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:58:28,219 - __main__ - INFO - Episode timed out.
2024-06-03 23:58:28,812 - __main__ - INFO - Action: 0, Reward: tensor([-127.7448], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:58:29,487 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:58:30,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:30,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:30,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:31,814 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:58:36,460 - __main__ - INFO - Action: 3, Reward: tensor([-124.8358], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:37,228 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8358], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:58:37,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:37,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:37,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:38,832 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:58:47,428 - __main__ - INFO - Action: 15, Reward: tensor([-25.1353], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:47,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:48,052 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:48,052 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:49,373 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:58:58,092 - __main__ - INFO - Action: 0, Reward: tensor([-25.2151], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:58,595 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:58,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:58,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:59,895 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:59:08,537 - __main__ - INFO - Action: 9, Reward: tensor([-24.2709], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:09,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:09,164 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:09,164 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:10,400 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:59:19,266 - __main__ - INFO - Action: 5, Reward: tensor([-24.0182], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:19,863 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:19,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:19,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:21,086 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:59:29,722 - __main__ - INFO - Action: 16, Reward: tensor([-23.6766], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:30,272 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:30,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:30,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:31,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:59:36,398 - __main__ - INFO - Action: 7, Reward: tensor([-23.8591], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:36,823 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:36,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:36,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:38,021 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:59:42,863 - __main__ - INFO - Action: 6, Reward: tensor([-23.8959], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:43,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:43,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:43,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:44,850 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:59:49,647 - __main__ - INFO - Action: 7, Reward: tensor([-24.1227], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:50,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:50,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:50,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:51,559 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 00:00:00,189 - __main__ - INFO - Action: 0, Reward: tensor([-24.6508], grad_fn=<AddBackward0>), Done: False
2024-06-04 00:00:00,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 00:00:00,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 00:00:00,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 00:00:02,164 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 00:00:10,788 - __main__ - INFO - Action: 0, Reward: tensor([-25.4066], grad_fn=<AddBackward0>), Done: False
2024-06-04 00:00:11,522 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-369.0869], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 00:00:11,773 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 00:00:11,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 00:00:11,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 00:00:12,997 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 00:00:21,530 - __main__ - INFO - Action: 8, Reward: tensor([-127.3891], grad_fn=<AddBackward0>), Done: False
2024-06-04 00:00:22,170 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 00:00:22,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 00:00:22,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 00:00:23,504 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 00:00:31,787 - __main__ - INFO - Episode timed out.
2024-06-04 00:00:32,157 - __main__ - INFO - Action: 12, Reward: tensor([-27.6410], grad_fn=<AddBackward0>), Done: True
2024-06-04 00:00:32,545 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-04 00:00:51,775 - __main__ - INFO - Dropped rows with NA values. Shape changed from (424, 2) to (424, 2).
2024-06-04 00:00:51,801 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 03:54:49,587 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 03:55:10,594 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 03:55:27,000 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 03:55:54,017 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 03:55:59,276 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 03:55:59,417 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 03:56:03,064 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 03:56:03,376 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 03:56:03,749 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 03:56:04,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:06,114 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 03:56:15,040 - __main__ - INFO - Action: 4, Reward: tensor([-124.9441], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:15,848 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.9441], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 03:56:16,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:16,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:16,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:17,440 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 03:56:26,169 - __main__ - INFO - Action: 9, Reward: tensor([-23.7795], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:26,792 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:26,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:26,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:28,152 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 03:56:37,014 - __main__ - INFO - Action: 4, Reward: tensor([-23.5944], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:37,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:37,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:37,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:38,931 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 03:56:47,734 - __main__ - INFO - Action: 13, Reward: tensor([-23.6331], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:48,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:48,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:48,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:49,596 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 03:56:54,116 - __main__ - INFO - Action: 2, Reward: tensor([-23.7464], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:54,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:54,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:54,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:56,133 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 03:57:00,968 - __main__ - INFO - Action: 7, Reward: tensor([-23.9713], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:01,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:01,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:01,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:02,740 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 03:57:11,464 - __main__ - INFO - Action: 13, Reward: tensor([-24.4470], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:11,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:12,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:12,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:13,193 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 03:57:17,625 - __main__ - INFO - Action: 7, Reward: tensor([-24.6821], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:18,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:18,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:18,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:19,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 03:57:28,174 - __main__ - INFO - Action: 1, Reward: tensor([-25.2934], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:28,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:28,842 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:28,842 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:30,095 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 03:57:34,889 - __main__ - INFO - Action: 11, Reward: tensor([-25.0735], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:35,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:35,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:35,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:36,772 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 03:57:45,375 - __main__ - INFO - Action: 1, Reward: tensor([-25.0531], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:46,220 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-368.2179], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 03:57:46,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:46,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:46,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:47,266 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 03:57:55,979 - __main__ - INFO - Action: 14, Reward: tensor([-25.1631], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:56,617 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:56,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:56,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:57,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 03:58:02,605 - __main__ - INFO - Action: 6, Reward: tensor([-25.1670], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:03,265 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:03,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:03,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:04,562 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 03:58:12,580 - __main__ - INFO - Episode timed out.
2024-06-04 03:58:13,131 - __main__ - INFO - Action: 9, Reward: tensor([-23.9102], grad_fn=<AddBackward0>), Done: True
2024-06-04 03:58:13,807 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 03:58:14,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:14,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:14,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:15,800 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 03:58:24,366 - __main__ - INFO - Action: 15, Reward: tensor([-124.9716], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:25,101 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.9716], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 03:58:25,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:25,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:25,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:26,824 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 03:58:35,684 - __main__ - INFO - Action: 1, Reward: tensor([-125.2090], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:36,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:36,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:36,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:37,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 03:58:46,467 - __main__ - INFO - Action: 4, Reward: tensor([-25.4024], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:47,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:47,124 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:47,124 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:48,424 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 03:58:53,300 - __main__ - INFO - Action: 7, Reward: tensor([-25.4537], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:53,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:54,004 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:54,004 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:55,020 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 03:59:03,722 - __main__ - INFO - Action: 13, Reward: tensor([-25.7808], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:04,193 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:04,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:04,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:05,556 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 03:59:14,508 - __main__ - INFO - Action: 5, Reward: tensor([-26.0037], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:14,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:15,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:15,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:16,268 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 03:59:21,271 - __main__ - INFO - Action: 2, Reward: tensor([-26.0654], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:21,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:21,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:21,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:23,118 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 03:59:28,078 - __main__ - INFO - Action: 11, Reward: tensor([-25.8075], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:28,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:28,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:28,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:29,944 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 03:59:39,025 - __main__ - INFO - Action: 15, Reward: tensor([-25.3535], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:39,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:39,691 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:39,691 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:40,968 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 03:59:49,491 - __main__ - INFO - Action: 0, Reward: tensor([-25.2500], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:50,102 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:50,212 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:50,212 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:51,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:00:00,026 - __main__ - INFO - Action: 14, Reward: tensor([-25.4145], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:00,884 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-480.7121], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:00:01,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:01,373 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:01,373 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:02,595 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 04:00:11,305 - __main__ - INFO - Action: 4, Reward: tensor([-25.6069], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:11,870 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:11,996 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:11,996 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:13,215 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 04:00:17,589 - __main__ - INFO - Episode timed out.
2024-06-04 04:00:18,120 - __main__ - INFO - Action: 2, Reward: tensor([-25.8925], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:00:18,823 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:00:19,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:19,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:19,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:21,072 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 04:00:25,584 - __main__ - INFO - Action: 2, Reward: tensor([-124.8542], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:26,380 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.8542], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:00:26,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:26,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:26,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:27,964 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:00:36,785 - __main__ - INFO - Action: 5, Reward: tensor([-25.0853], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:37,363 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:37,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:37,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:38,448 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:00:47,073 - __main__ - INFO - Action: 12, Reward: tensor([-25.3158], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:47,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:47,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:47,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:49,059 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 04:00:53,978 - __main__ - INFO - Action: 7, Reward: tensor([-125.4170], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:54,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:54,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:54,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:55,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 04:01:04,625 - __main__ - INFO - Action: 1, Reward: tensor([-25.6124], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:05,189 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:05,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:05,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:06,461 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 04:01:15,008 - __main__ - INFO - Action: 13, Reward: tensor([-25.9223], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:15,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:15,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:15,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:16,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:01:25,711 - __main__ - INFO - Action: 15, Reward: tensor([-26.1256], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:26,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:26,461 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:26,461 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:27,681 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:01:32,500 - __main__ - INFO - Action: 10, Reward: tensor([-26.2323], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:33,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:33,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:33,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:34,315 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 04:01:42,916 - __main__ - INFO - Action: 8, Reward: tensor([-26.4599], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:43,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:43,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:43,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:44,797 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:01:49,641 - __main__ - INFO - Action: 10, Reward: tensor([-26.6114], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:50,171 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:50,297 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:50,297 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:51,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:01:59,900 - __main__ - INFO - Action: 5, Reward: tensor([-26.7613], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:00,435 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-484.3975], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:02:00,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:00,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:00,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:02,077 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 04:02:10,986 - __main__ - INFO - Action: 1, Reward: tensor([-26.9787], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:11,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:11,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:11,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:12,926 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:02:17,067 - __main__ - INFO - Action: 3, Reward: tensor([-27.1528], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:17,521 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:17,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:17,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:18,888 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:02:23,120 - __main__ - INFO - Episode timed out.
2024-06-04 04:02:23,621 - __main__ - INFO - Action: 11, Reward: tensor([-26.8503], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:02:24,351 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:02:25,150 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:25,276 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:25,276 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:26,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:02:34,950 - __main__ - INFO - Action: 15, Reward: tensor([-124.9780], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:35,602 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.9780], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:02:35,931 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:36,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:36,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:37,311 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 04:02:42,116 - __main__ - INFO - Action: 2, Reward: tensor([-25.0376], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:42,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:42,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:42,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:44,008 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:02:52,980 - __main__ - INFO - Action: 5, Reward: tensor([-24.9643], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:53,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:53,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:53,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:54,715 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:03:03,463 - __main__ - INFO - Action: 14, Reward: tensor([-25.2133], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:04,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:04,104 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:04,104 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:05,251 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:03:13,920 - __main__ - INFO - Action: 12, Reward: tensor([-25.5709], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:14,512 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:14,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:14,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:15,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:03:24,637 - __main__ - INFO - Action: 5, Reward: tensor([-26.1653], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:25,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:25,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:25,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:26,517 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:03:31,267 - __main__ - INFO - Action: 11, Reward: tensor([-25.9879], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:31,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:31,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:31,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:33,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 04:03:37,912 - __main__ - INFO - Action: 7, Reward: tensor([-26.0846], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:38,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:38,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:38,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:39,771 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:03:48,492 - __main__ - INFO - Action: 14, Reward: tensor([-26.8218], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:49,058 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:49,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:49,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:50,302 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:03:59,123 - __main__ - INFO - Action: 14, Reward: tensor([-28.1610], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:59,653 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:59,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:59,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:01,043 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 04:04:09,868 - __main__ - INFO - Action: 0, Reward: tensor([-29.3359], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:10,663 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-388.3207], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:04:10,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:11,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:11,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:12,265 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 04:04:21,067 - __main__ - INFO - Action: 8, Reward: tensor([-130.0104], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:21,646 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:21,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:21,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:22,962 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 04:04:31,401 - __main__ - INFO - Episode timed out.
2024-06-04 04:04:31,890 - __main__ - INFO - Action: 13, Reward: tensor([-30.2247], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:04:32,651 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:04:33,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:33,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:33,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:34,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 04:04:39,494 - __main__ - INFO - Action: 2, Reward: tensor([-124.8556], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:40,290 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.8556], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:04:40,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:40,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:40,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:42,052 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:04:46,960 - __main__ - INFO - Action: 3, Reward: tensor([-24.9511], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:47,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:47,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:47,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:48,862 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 04:04:53,493 - __main__ - INFO - Action: 6, Reward: tensor([-25.1365], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:53,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:54,117 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:54,117 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:55,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:05:00,106 - __main__ - INFO - Action: 3, Reward: tensor([-25.2912], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:00,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:00,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:00,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:02,075 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:05:07,032 - __main__ - INFO - Action: 10, Reward: tensor([-25.4351], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:07,594 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:07,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:07,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:08,998 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:05:13,690 - __main__ - INFO - Action: 11, Reward: tensor([-125.5134], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:14,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:14,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:14,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:15,547 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 04:05:20,537 - __main__ - INFO - Action: 3, Reward: tensor([-25.6224], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:21,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:21,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:21,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:22,415 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 04:05:31,384 - __main__ - INFO - Action: 5, Reward: tensor([-25.7305], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:31,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:32,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:32,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:33,243 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 04:05:41,630 - __main__ - INFO - Action: 4, Reward: tensor([-126.0842], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:41,955 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:41,955 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:41,955 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:43,205 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 04:05:52,188 - __main__ - INFO - Action: 4, Reward: tensor([-126.3581], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:52,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:52,846 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:52,846 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:53,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:05:58,593 - __main__ - INFO - Action: 3, Reward: tensor([-26.4798], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:59,215 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-681.4578], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:05:59,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:59,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:59,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:00,815 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:06:05,572 - __main__ - INFO - Action: 10, Reward: tensor([-26.6190], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:06,195 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:06,320 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:06,320 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:07,448 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:06:16,168 - __main__ - INFO - Action: 12, Reward: tensor([-26.8674], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:16,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:16,732 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:16,732 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:17,988 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:06:26,572 - __main__ - INFO - Action: 15, Reward: tensor([-27.0949], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:27,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:27,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:27,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:27,912 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 04:06:35,996 - __main__ - INFO - Episode timed out.
2024-06-04 04:06:36,559 - __main__ - INFO - Action: 5, Reward: tensor([-27.1210], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:06:37,040 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:06:37,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:37,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:37,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:39,212 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 04:06:47,988 - __main__ - INFO - Action: 4, Reward: tensor([-124.9249], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:48,613 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.9249], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:06:48,845 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:48,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:48,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:50,160 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 04:06:54,892 - __main__ - INFO - Action: 3, Reward: tensor([-125.0920], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:55,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:55,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:55,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:56,663 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:07:05,371 - __main__ - INFO - Action: 12, Reward: tensor([-125.3327], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:06,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:06,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:06,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:07,321 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:07:12,199 - __main__ - INFO - Action: 11, Reward: tensor([-25.0473], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:12,776 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:12,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:12,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:14,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:07:18,720 - __main__ - INFO - Action: 3, Reward: tensor([-24.9509], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:19,205 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:19,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:19,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:20,565 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 7 selected.
2024-06-04 04:07:25,235 - __main__ - INFO - Action: 7, Reward: tensor([-25.0307], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:25,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:25,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:25,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:27,125 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 04:07:35,953 - __main__ - INFO - Action: 13, Reward: tensor([-25.5300], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:36,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:36,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:36,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:36,706 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 04:07:45,235 - __main__ - INFO - Action: 0, Reward: tensor([-25.8734], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:45,827 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:45,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:45,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:47,082 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:07:55,516 - __main__ - INFO - Action: 14, Reward: tensor([-26.3254], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:56,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:56,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:56,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:57,515 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 04:08:06,630 - __main__ - INFO - Action: 9, Reward: tensor([-25.2726], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:08:07,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:08:07,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:08:07,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:08:08,521 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 04:08:17,213 - __main__ - INFO - Action: 8, Reward: tensor([-26.3476], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:08:18,072 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-579.7277], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:08:18,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:08:18,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:08:18,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:08:19,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 04:08:28,237 - __main__ - INFO - Action: 0, Reward: tensor([-27.1758], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:08:28,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:08:28,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:08:28,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:08:29,711 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:08:38,026 - __main__ - INFO - Episode timed out.
2024-06-04 04:08:38,432 - __main__ - INFO - Action: 15, Reward: tensor([-27.2904], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:08:38,730 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-04 04:08:59,654 - __main__ - INFO - Dropped rows with NA values. Shape changed from (506, 2) to (506, 2).
2024-06-04 04:08:59,657 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 07:11:20,524 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 07:11:36,311 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 07:11:55,430 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 07:12:13,576 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-04 07:13:29,905 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 07:13:35,495 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 07:13:35,665 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 07:13:39,533 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 07:13:40,632 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 07:13:40,979 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:13:41,848 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:13:41,958 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:13:41,958 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:13:43,150 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:13:51,700 - __main__ - INFO - Action: 15, Reward: tensor([-124.8712], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:13:52,572 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.8712], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:13:52,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:13:53,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:13:53,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:13:54,305 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 07:14:03,186 - __main__ - INFO - Action: 8, Reward: tensor([-25.0924], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:03,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:03,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:03,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:04,934 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:14:13,656 - __main__ - INFO - Action: 15, Reward: tensor([-25.3583], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:14,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:14,390 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:14,390 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:15,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:14:20,566 - __main__ - INFO - Action: 3, Reward: tensor([-25.4832], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:21,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:21,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:21,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:22,469 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:14:31,398 - __main__ - INFO - Action: 15, Reward: tensor([-25.7421], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:31,959 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:32,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:32,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:33,322 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:14:38,184 - __main__ - INFO - Action: 2, Reward: tensor([-25.7129], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:38,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:38,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:38,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:40,200 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:14:49,039 - __main__ - INFO - Action: 0, Reward: tensor([-25.8989], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:49,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:49,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:49,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:50,996 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 07:14:59,217 - __main__ - INFO - Action: 12, Reward: tensor([-26.2957], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:59,700 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:59,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:59,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:00,982 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:15:05,597 - __main__ - INFO - Action: 11, Reward: tensor([-26.0084], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:06,033 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:06,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:06,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:07,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 07:15:16,177 - __main__ - INFO - Action: 4, Reward: tensor([-25.8115], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:16,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:16,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:16,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:17,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:15:22,976 - __main__ - INFO - Action: 2, Reward: tensor([-26.0748], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:23,768 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-382.3494], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:15:24,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:24,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:24,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:25,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 07:15:34,121 - __main__ - INFO - Action: 9, Reward: tensor([-24.9760], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:34,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:34,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:34,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:35,933 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:15:44,255 - __main__ - INFO - Episode timed out.
2024-06-04 07:15:44,646 - __main__ - INFO - Action: 1, Reward: tensor([-24.2898], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:15:45,346 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:15:46,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:46,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:46,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:47,184 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:15:55,936 - __main__ - INFO - Action: 16, Reward: tensor([-124.5046], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:56,642 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.5046], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:15:56,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:57,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:57,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:58,226 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:16:06,768 - __main__ - INFO - Action: 0, Reward: tensor([-24.6631], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:07,330 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:07,439 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:07,439 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:08,628 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 07:16:17,550 - __main__ - INFO - Action: 8, Reward: tensor([-125.2922], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:18,113 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:18,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:18,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:19,518 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:16:28,456 - __main__ - INFO - Action: 15, Reward: tensor([-25.5285], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:28,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:29,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:29,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:30,346 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:16:39,176 - __main__ - INFO - Action: 1, Reward: tensor([-25.7651], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:39,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:39,914 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:39,914 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:40,757 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:16:45,340 - __main__ - INFO - Action: 11, Reward: tensor([-25.4647], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:45,765 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:45,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:45,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:47,019 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:16:56,028 - __main__ - INFO - Action: 14, Reward: tensor([-25.1145], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:56,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:56,764 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:56,764 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:57,801 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:17:06,660 - __main__ - INFO - Action: 1, Reward: tensor([-25.1229], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:07,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:07,410 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:07,410 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:08,676 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 07:17:17,452 - __main__ - INFO - Action: 12, Reward: tensor([-25.5076], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:17,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:17,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:17,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:19,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:17:23,844 - __main__ - INFO - Action: 11, Reward: tensor([-25.2505], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:24,405 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:24,512 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:24,512 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:25,638 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:17:30,160 - __main__ - INFO - Action: 11, Reward: tensor([-24.7565], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:30,911 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-476.9703], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:17:31,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:31,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:31,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:32,505 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:17:37,162 - __main__ - INFO - Action: 2, Reward: tensor([-24.6035], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:37,615 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:37,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:37,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:38,988 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:17:47,503 - __main__ - INFO - Episode timed out.
2024-06-04 07:17:48,048 - __main__ - INFO - Action: 0, Reward: tensor([-24.4970], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:17:48,689 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:17:49,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:49,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:49,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:50,957 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:17:59,904 - __main__ - INFO - Action: 13, Reward: tensor([-124.8738], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:00,670 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.8738], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:18:00,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:01,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:01,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:02,359 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:18:11,241 - __main__ - INFO - Action: 0, Reward: tensor([-25.0205], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:11,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:11,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:11,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:13,078 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:18:17,871 - __main__ - INFO - Action: 2, Reward: tensor([-25.2197], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:18,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:18,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:18,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:19,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 07:18:24,118 - __main__ - INFO - Action: 2, Reward: tensor([-125.3805], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:24,661 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:24,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:24,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:25,946 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:18:34,516 - __main__ - INFO - Action: 16, Reward: tensor([-25.5239], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:35,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:35,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:35,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:36,287 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 07:18:41,094 - __main__ - INFO - Action: 7, Reward: tensor([-25.6723], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:41,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:41,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:41,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:42,927 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:18:51,647 - __main__ - INFO - Action: 14, Reward: tensor([-25.8620], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:52,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:53,556 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:19:02,509 - __main__ - INFO - Action: 14, Reward: tensor([-26.2588], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:03,102 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:03,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:03,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:04,368 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:19:09,307 - __main__ - INFO - Action: 2, Reward: tensor([-26.4480], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:09,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:10,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:10,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:11,347 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:19:20,247 - __main__ - INFO - Action: 0, Reward: tensor([-26.9535], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:20,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:20,889 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:20,889 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:22,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:19:31,156 - __main__ - INFO - Action: 15, Reward: tensor([-27.1655], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:31,839 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-484.3785], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:19:32,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:32,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:32,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:33,403 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 07:19:42,125 - __main__ - INFO - Action: 5, Reward: tensor([-27.1543], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:42,700 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:42,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:42,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:44,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:19:48,634 - __main__ - INFO - Action: 3, Reward: tensor([-27.3107], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:49,150 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:49,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:49,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:50,539 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 07:19:54,935 - __main__ - INFO - Episode timed out.
2024-06-04 07:19:55,124 - __main__ - INFO - Action: 2, Reward: tensor([-27.5594], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:19:55,857 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:19:56,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:56,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:56,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:58,026 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:20:02,750 - __main__ - INFO - Action: 11, Reward: tensor([-124.3435], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:03,596 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.3435], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:20:03,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:03,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:03,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:05,162 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:20:10,146 - __main__ - INFO - Action: 3, Reward: tensor([-24.2980], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:10,645 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:10,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:10,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:12,021 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:20:20,976 - __main__ - INFO - Action: 0, Reward: tensor([-24.2763], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:21,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:21,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:21,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:22,947 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:20:31,646 - __main__ - INFO - Action: 5, Reward: tensor([-24.3023], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:32,017 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:32,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:32,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:33,185 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 07:20:41,851 - __main__ - INFO - Action: 12, Reward: tensor([-24.4531], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:42,447 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:42,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:42,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:43,471 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:20:48,198 - __main__ - INFO - Action: 10, Reward: tensor([-25.1221], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:48,697 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:48,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:48,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:49,966 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:20:54,527 - __main__ - INFO - Action: 3, Reward: tensor([-25.4018], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:55,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:55,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:55,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:56,517 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:21:01,334 - __main__ - INFO - Action: 3, Reward: tensor([-25.5778], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:01,867 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:01,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:01,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:03,163 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 07:21:07,867 - __main__ - INFO - Action: 6, Reward: tensor([-25.7038], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:08,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:08,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:08,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:09,761 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 07:21:18,807 - __main__ - INFO - Action: 4, Reward: tensor([-25.9649], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:19,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:19,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:19,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:20,852 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:21:29,631 - __main__ - INFO - Action: 0, Reward: tensor([-26.5366], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:30,362 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-375.9802], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:21:30,674 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:30,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:30,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:32,016 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:21:41,107 - __main__ - INFO - Action: 5, Reward: tensor([-126.9125], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:41,781 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:41,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:41,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:43,078 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:21:51,999 - __main__ - INFO - Action: 1, Reward: tensor([-27.1244], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:52,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:52,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:52,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:53,971 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:22:02,325 - __main__ - INFO - Episode timed out.
2024-06-04 07:22:02,808 - __main__ - INFO - Action: 1, Reward: tensor([-27.4002], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:22:03,436 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:22:04,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:04,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:04,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:05,725 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:22:10,780 - __main__ - INFO - Action: 2, Reward: tensor([-124.7547], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:11,622 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.7547], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:22:11,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:12,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:12,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:13,394 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:22:22,010 - __main__ - INFO - Action: 0, Reward: tensor([-125.0353], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:22,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:22,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:22,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:23,813 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:22:32,913 - __main__ - INFO - Action: 1, Reward: tensor([-25.1242], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:33,258 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:33,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:33,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:34,528 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:22:42,678 - __main__ - INFO - Action: 1, Reward: tensor([-25.4245], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:43,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:43,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:43,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:44,682 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:22:53,140 - __main__ - INFO - Action: 0, Reward: tensor([-25.5531], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:53,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:53,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:53,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:54,272 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:22:58,970 - __main__ - INFO - Action: 11, Reward: tensor([-25.3365], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:59,483 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:59,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:59,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:00,809 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 07:23:05,777 - __main__ - INFO - Action: 6, Reward: tensor([-25.3402], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:06,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:06,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:06,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:07,612 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 07:23:12,481 - __main__ - INFO - Action: 6, Reward: tensor([-25.5209], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:13,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:13,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:13,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:14,406 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 07:23:23,091 - __main__ - INFO - Action: 9, Reward: tensor([-24.2906], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:23,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:23,777 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:23,777 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:25,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:23:34,091 - __main__ - INFO - Action: 0, Reward: tensor([-23.5452], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:34,572 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:34,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:34,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:35,963 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:23:44,953 - __main__ - INFO - Action: 0, Reward: tensor([-23.4205], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:45,780 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-473.3457], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:23:46,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:46,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:46,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:47,208 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:23:51,765 - __main__ - INFO - Action: 10, Reward: tensor([-24.0467], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:52,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:52,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:52,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:53,734 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:24:02,451 - __main__ - INFO - Action: 0, Reward: tensor([-25.2198], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:03,058 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:03,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:03,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:04,406 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:24:08,741 - __main__ - INFO - Episode timed out.
2024-06-04 07:24:09,238 - __main__ - INFO - Action: 3, Reward: tensor([-25.4527], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:24:09,941 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:24:10,877 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:10,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:10,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:12,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:24:21,143 - __main__ - INFO - Action: 5, Reward: tensor([-124.8389], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:22,082 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8389], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:24:22,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:22,521 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:22,521 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:23,763 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:24:32,566 - __main__ - INFO - Action: 14, Reward: tensor([-25.1454], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:33,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:33,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:33,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:34,509 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:24:39,364 - __main__ - INFO - Action: 3, Reward: tensor([-25.1896], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:39,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:40,005 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:40,005 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:41,125 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:24:49,990 - __main__ - INFO - Action: 5, Reward: tensor([-25.4529], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:50,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:50,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:50,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:52,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:25:00,838 - __main__ - INFO - Action: 0, Reward: tensor([-25.6417], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:01,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:01,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:01,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:02,899 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 07:25:11,353 - __main__ - INFO - Action: 4, Reward: tensor([-25.8226], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:11,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:12,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:12,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:13,369 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 07:25:22,227 - __main__ - INFO - Action: 9, Reward: tensor([-24.7780], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:22,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:22,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:22,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:24,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:25:32,779 - __main__ - INFO - Action: 13, Reward: tensor([-24.1221], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:33,437 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:33,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:33,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:34,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:25:43,591 - __main__ - INFO - Action: 0, Reward: tensor([-24.2300], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:44,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:44,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:44,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:45,354 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:25:54,246 - __main__ - INFO - Action: 0, Reward: tensor([-24.6337], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:54,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:54,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:54,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:56,198 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:26:04,976 - __main__ - INFO - Action: 0, Reward: tensor([-25.1081], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:05,566 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-374.9631], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:26:05,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:06,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:06,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:07,257 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:26:15,707 - __main__ - INFO - Episode timed out.
2024-06-04 07:26:16,303 - __main__ - INFO - Action: 0, Reward: tensor([-25.8144], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:26:16,994 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:26:17,885 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:18,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:18,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:19,261 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:26:27,774 - __main__ - INFO - Action: 13, Reward: tensor([-124.8785], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:28,634 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-124.8785], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:26:28,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:29,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:29,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:30,215 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:26:39,029 - __main__ - INFO - Action: 0, Reward: tensor([-125.0350], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:39,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:39,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:39,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:40,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 07:26:49,481 - __main__ - INFO - Action: 8, Reward: tensor([-25.2803], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:50,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:50,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:50,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:51,383 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:27:00,156 - __main__ - INFO - Action: 0, Reward: tensor([-25.5095], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:00,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:00,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:00,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:02,114 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:27:11,027 - __main__ - INFO - Action: 0, Reward: tensor([-25.7587], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:11,689 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:11,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:11,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:13,118 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:27:21,982 - __main__ - INFO - Action: 0, Reward: tensor([-25.9958], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:22,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:22,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:22,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:23,965 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:27:32,915 - __main__ - INFO - Action: 16, Reward: tensor([-26.1081], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:33,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:33,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:33,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:34,848 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 07:27:39,603 - __main__ - INFO - Action: 2, Reward: tensor([-26.3073], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:40,230 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:40,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:40,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:41,602 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:27:46,478 - __main__ - INFO - Action: 10, Reward: tensor([-26.4289], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:47,058 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:47,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:47,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:48,482 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:27:56,978 - __main__ - INFO - Action: 0, Reward: tensor([-26.6657], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:57,602 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:57,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:57,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:58,883 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:28:03,818 - __main__ - INFO - Action: 11, Reward: tensor([-26.4889], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:04,492 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-484.4567], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:28:04,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:04,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:04,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:06,180 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:28:14,872 - __main__ - INFO - Action: 1, Reward: tensor([-26.2861], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:15,469 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:15,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:15,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:16,844 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:28:20,975 - __main__ - INFO - Episode timed out.
2024-06-04 07:28:21,553 - __main__ - INFO - Action: 10, Reward: tensor([-26.9247], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:28:22,100 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:28:22,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:22,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:22,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:24,149 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:28:29,207 - __main__ - INFO - Action: 3, Reward: tensor([-124.7613], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:29,969 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-124.7613], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:28:30,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:30,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:30,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:31,709 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:28:40,753 - __main__ - INFO - Action: 1, Reward: tensor([-25.0396], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:41,270 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:41,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:41,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:42,558 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:28:51,373 - __main__ - INFO - Action: 13, Reward: tensor([-25.2997], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:51,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:52,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:52,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:53,277 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:29:02,138 - __main__ - INFO - Action: 0, Reward: tensor([-25.3709], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:02,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:02,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:02,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:04,108 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:29:09,103 - __main__ - INFO - Action: 3, Reward: tensor([-25.5562], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:09,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:09,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:09,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:10,898 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:29:15,651 - __main__ - INFO - Action: 11, Reward: tensor([-25.6251], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:16,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:16,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:16,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:17,529 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:29:26,342 - __main__ - INFO - Action: 0, Reward: tensor([-25.1316], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:26,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:27,044 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:27,044 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:28,334 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 07:29:36,900 - __main__ - INFO - Action: 5, Reward: tensor([-24.8325], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:37,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:37,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:37,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:38,657 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:29:47,320 - __main__ - INFO - Action: 13, Reward: tensor([-24.7477], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:47,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:48,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:48,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:49,181 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:29:57,969 - __main__ - INFO - Action: 16, Reward: tensor([-24.7858], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:58,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:58,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:58,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:59,787 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 07:30:04,615 - __main__ - INFO - Action: 7, Reward: tensor([-25.1095], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:05,352 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-376.2601], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:30:05,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:05,744 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:05,744 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:07,046 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:30:12,197 - __main__ - INFO - Action: 10, Reward: tensor([-25.7538], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:12,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:12,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:12,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:13,989 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:30:18,884 - __main__ - INFO - Action: 3, Reward: tensor([-26.2105], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:19,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:19,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:19,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:20,875 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:30:29,082 - __main__ - INFO - Episode timed out.
2024-06-04 07:30:29,630 - __main__ - INFO - Action: 0, Reward: tensor([-26.8885], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:30:30,407 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:30:31,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:31,171 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:31,171 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:32,467 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:30:41,225 - __main__ - INFO - Action: 1, Reward: tensor([-124.6648], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:42,036 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-124.6648], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:30:42,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:42,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:42,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:43,710 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:30:52,245 - __main__ - INFO - Action: 0, Reward: tensor([-24.7938], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:52,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:52,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:52,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:53,983 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:31:02,782 - __main__ - INFO - Action: 14, Reward: tensor([-25.1477], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:03,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:03,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:03,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:04,799 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:31:09,677 - __main__ - INFO - Action: 3, Reward: tensor([-25.2546], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:10,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:10,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:10,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:11,527 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:31:19,777 - __main__ - INFO - Action: 1, Reward: tensor([-25.4562], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:20,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:20,356 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:20,356 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:21,558 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:31:30,323 - __main__ - INFO - Action: 0, Reward: tensor([-25.4746], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:30,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:30,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:30,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:32,059 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:31:36,509 - __main__ - INFO - Action: 3, Reward: tensor([-25.7007], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:36,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:37,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:37,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:38,280 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:31:43,032 - __main__ - INFO - Action: 3, Reward: tensor([-25.9149], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:43,643 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:43,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:43,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:44,886 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:31:53,734 - __main__ - INFO - Action: 0, Reward: tensor([-26.0710], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:54,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:55,704 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:32:04,768 - __main__ - INFO - Action: 16, Reward: tensor([-26.4160], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:32:05,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:32:05,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:32:05,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:32:06,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:32:15,318 - __main__ - INFO - Action: 0, Reward: tensor([-26.6571], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:32:16,082 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-381.5514], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:32:16,380 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:32:16,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:32:16,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:32:17,713 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 07:32:26,556 - __main__ - INFO - Action: 4, Reward: tensor([-26.8809], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:32:27,136 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:32:28,387 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:32:36,730 - __main__ - INFO - Episode timed out.
2024-06-04 07:32:37,274 - __main__ - INFO - Action: 0, Reward: tensor([-127.2464], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:33:00,236 - __main__ - INFO - Dropped rows with NA values. Shape changed from (626, 2) to (626, 2).
2024-06-04 07:33:00,240 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 08:01:43,073 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 08:02:14,060 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 08:02:30,931 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 08:02:51,732 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 08:02:56,583 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 08:02:56,737 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 08:03:00,833 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 08:03:02,084 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 08:03:02,427 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:03:03,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:03,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:03,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:04,673 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:03:13,627 - __main__ - INFO - Action: 1, Reward: tensor([-124.7726], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:14,442 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.7726], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:03:14,768 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:14,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:14,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:15,975 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:03:24,574 - __main__ - INFO - Action: 1, Reward: tensor([-24.7839], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:25,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:25,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:25,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:26,426 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:03:35,160 - __main__ - INFO - Action: 0, Reward: tensor([-25.0020], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:35,804 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:35,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:35,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:37,218 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:03:41,617 - __main__ - INFO - Action: 3, Reward: tensor([-25.4140], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:42,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:42,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:42,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:43,671 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:03:48,554 - __main__ - INFO - Action: 2, Reward: tensor([-25.5312], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:49,148 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:49,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:49,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:50,464 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:03:59,315 - __main__ - INFO - Action: 16, Reward: tensor([-125.8895], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:59,860 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:59,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:59,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:01,252 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:04:10,197 - __main__ - INFO - Action: 12, Reward: tensor([-26.2409], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:10,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:10,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:10,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:12,015 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:04:20,830 - __main__ - INFO - Action: 0, Reward: tensor([-26.4620], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:21,345 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:21,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:21,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:22,616 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:04:31,528 - __main__ - INFO - Action: 15, Reward: tensor([-26.6351], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:32,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:32,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:32,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:33,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:04:38,369 - __main__ - INFO - Action: 11, Reward: tensor([-26.3956], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:38,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:39,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:39,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:40,108 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:04:48,731 - __main__ - INFO - Action: 8, Reward: tensor([-126.8963], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:49,611 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-584.0230], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:04:49,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:50,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:50,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:51,270 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:04:59,913 - __main__ - INFO - Action: 4, Reward: tensor([-27.1702], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:00,428 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:00,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:00,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:01,793 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:05:06,042 - __main__ - INFO - Episode timed out.
2024-06-04 08:05:06,274 - __main__ - INFO - Action: 7, Reward: tensor([-27.2210], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:05:06,883 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:05:07,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:07,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:07,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:09,056 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:05:17,887 - __main__ - INFO - Action: 5, Reward: tensor([-124.8964], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:18,700 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8964], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:05:19,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:19,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:19,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:20,253 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:05:29,326 - __main__ - INFO - Action: 0, Reward: tensor([-25.1372], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:29,877 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:30,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:30,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:31,225 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:05:40,115 - __main__ - INFO - Action: 8, Reward: tensor([-125.4085], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:40,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:40,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:40,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:41,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:05:51,053 - __main__ - INFO - Action: 8, Reward: tensor([-25.6646], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:51,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:51,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:51,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:52,949 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:05:57,705 - __main__ - INFO - Action: 3, Reward: tensor([-25.7478], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:58,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:58,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:58,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:59,653 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:06:04,325 - __main__ - INFO - Action: 6, Reward: tensor([-25.8855], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:04,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:05,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:05,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:06,238 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:06:15,039 - __main__ - INFO - Action: 15, Reward: tensor([-26.1784], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:15,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:15,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:15,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:16,946 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:06:21,674 - __main__ - INFO - Action: 3, Reward: tensor([-26.2977], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:22,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:22,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:22,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:23,455 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:06:32,223 - __main__ - INFO - Action: 9, Reward: tensor([-25.1168], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:32,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:32,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:32,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:34,030 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:06:42,873 - __main__ - INFO - Action: 0, Reward: tensor([-24.7312], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:43,387 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:43,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:43,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:44,755 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:06:53,702 - __main__ - INFO - Action: 0, Reward: tensor([-24.4748], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:54,516 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-479.5388], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:06:54,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:54,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:54,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:56,030 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:07:05,027 - __main__ - INFO - Action: 14, Reward: tensor([-24.6660], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:05,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:05,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:05,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:06,901 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:07:15,380 - __main__ - INFO - Episode timed out.
2024-06-04 08:07:15,974 - __main__ - INFO - Action: 16, Reward: tensor([-25.0251], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:07:16,645 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:07:17,504 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:17,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:17,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:18,860 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:07:27,625 - __main__ - INFO - Action: 8, Reward: tensor([-124.9285], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:28,507 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.9285], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:07:28,787 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:28,881 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:28,881 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:30,187 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:07:35,120 - __main__ - INFO - Action: 6, Reward: tensor([-25.0895], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:35,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:35,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:35,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:36,968 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:07:41,625 - __main__ - INFO - Action: 2, Reward: tensor([-25.2279], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:42,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:42,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:42,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:43,415 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:07:52,311 - __main__ - INFO - Action: 1, Reward: tensor([-25.3855], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:52,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:53,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:53,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:54,189 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:07:59,067 - __main__ - INFO - Action: 2, Reward: tensor([-25.5349], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:59,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:59,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:59,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:00,931 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:08:05,786 - __main__ - INFO - Action: 6, Reward: tensor([-25.7087], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:06,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:06,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:06,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:07,845 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:08:16,906 - __main__ - INFO - Action: 4, Reward: tensor([-25.9675], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:17,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:17,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:17,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:18,959 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:08:23,878 - __main__ - INFO - Action: 3, Reward: tensor([-26.0569], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:24,460 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:24,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:24,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:25,736 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:08:34,664 - __main__ - INFO - Action: 1, Reward: tensor([-26.3180], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:35,231 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:35,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:35,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:36,613 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:08:45,423 - __main__ - INFO - Action: 4, Reward: tensor([-26.4911], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:45,964 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:46,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:46,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:47,294 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:08:55,930 - __main__ - INFO - Action: 15, Reward: tensor([-126.9078], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:56,662 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-483.6164], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:08:57,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:57,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:57,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:58,330 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:09:07,209 - __main__ - INFO - Action: 8, Reward: tensor([-127.0488], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:07,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:07,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:07,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:09,150 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:09:17,604 - __main__ - INFO - Episode timed out.
2024-06-04 08:09:18,070 - __main__ - INFO - Action: 1, Reward: tensor([-27.2697], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:09:18,661 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:09:19,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:19,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:19,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:20,820 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:09:29,497 - __main__ - INFO - Action: 0, Reward: tensor([-124.9252], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:30,266 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.9252], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:09:30,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:30,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:30,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:31,876 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:09:40,630 - __main__ - INFO - Action: 5, Reward: tensor([-25.1549], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:41,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:41,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:41,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:42,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:09:51,375 - __main__ - INFO - Action: 14, Reward: tensor([-125.4542], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:51,940 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:52,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:52,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:53,331 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:10:02,346 - __main__ - INFO - Action: 9, Reward: tensor([-24.2403], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:02,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:03,034 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:03,034 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:04,238 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:10:13,062 - __main__ - INFO - Action: 14, Reward: tensor([-23.8970], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:13,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:13,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:13,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:14,749 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:10:23,559 - __main__ - INFO - Action: 9, Reward: tensor([-22.4644], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:24,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:24,263 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:24,263 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:25,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:10:34,318 - __main__ - INFO - Action: 15, Reward: tensor([-21.4787], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:34,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:34,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:34,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:36,044 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:10:40,922 - __main__ - INFO - Action: 10, Reward: tensor([-21.9236], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:41,529 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:41,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:41,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:42,923 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:10:51,676 - __main__ - INFO - Action: 0, Reward: tensor([-22.5388], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:52,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:52,300 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:52,300 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:53,629 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:11:02,312 - __main__ - INFO - Action: 9, Reward: tensor([-21.6449], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:02,827 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:02,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:02,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:04,271 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:11:12,963 - __main__ - INFO - Action: 16, Reward: tensor([-21.1656], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:13,529 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-454.8874], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:11:13,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:13,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:13,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:15,098 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:11:23,233 - __main__ - INFO - Episode timed out.
2024-06-04 08:11:23,859 - __main__ - INFO - Action: 5, Reward: tensor([-21.3429], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:11:24,626 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:11:25,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:25,377 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:25,377 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:26,600 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:11:35,357 - __main__ - INFO - Action: 8, Reward: tensor([-124.9235], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:36,090 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.9235], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:11:36,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:36,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:36,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:37,684 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:11:46,399 - __main__ - INFO - Action: 16, Reward: tensor([-25.1027], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:46,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:46,963 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:46,963 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:48,091 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:11:53,094 - __main__ - INFO - Action: 11, Reward: tensor([-25.2527], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:53,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:53,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:53,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:55,142 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:11:59,896 - __main__ - INFO - Action: 11, Reward: tensor([-24.8341], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:00,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:00,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:00,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:01,740 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:12:10,429 - __main__ - INFO - Action: 0, Reward: tensor([-24.1993], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:10,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:10,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:10,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:12,243 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:12:17,212 - __main__ - INFO - Action: 7, Reward: tensor([-24.3273], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:17,696 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:17,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:17,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:19,038 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:12:27,948 - __main__ - INFO - Action: 0, Reward: tensor([-24.8111], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:28,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:28,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:28,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:29,665 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:12:38,756 - __main__ - INFO - Action: 9, Reward: tensor([-23.7807], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:39,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:39,400 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:39,400 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:40,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:12:45,175 - __main__ - INFO - Action: 10, Reward: tensor([-24.2294], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:45,625 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:45,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:45,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:46,818 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:12:55,564 - __main__ - INFO - Action: 5, Reward: tensor([-25.1165], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:56,095 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:56,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:56,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:57,361 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:13:02,206 - __main__ - INFO - Action: 2, Reward: tensor([-25.3699], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:03,037 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-371.9472], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:13:03,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:03,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:03,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:04,696 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:13:13,569 - __main__ - INFO - Action: 4, Reward: tensor([-25.4303], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:14,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:14,348 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:14,348 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:15,412 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:13:23,804 - __main__ - INFO - Action: 1, Reward: tensor([-25.5444], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:24,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:24,539 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:24,539 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:25,770 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:13:34,103 - __main__ - INFO - Episode timed out.
2024-06-04 08:13:34,559 - __main__ - INFO - Action: 5, Reward: tensor([-25.8346], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:13:35,183 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:13:36,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:36,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:36,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:37,503 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:13:46,311 - __main__ - INFO - Action: 0, Reward: tensor([-124.9316], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:47,134 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.9316], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:13:47,431 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:47,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:47,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:48,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:13:57,720 - __main__ - INFO - Action: 4, Reward: tensor([-125.1676], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:58,255 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:58,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:58,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:59,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:14:04,420 - __main__ - INFO - Action: 2, Reward: tensor([-25.3254], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:04,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:05,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:05,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:06,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:14:11,049 - __main__ - INFO - Action: 7, Reward: tensor([-25.4273], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:11,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:11,851 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:11,851 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:13,109 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 08:14:21,752 - __main__ - INFO - Action: 4, Reward: tensor([-25.5592], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:22,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:22,429 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:22,429 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:23,643 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:14:28,302 - __main__ - INFO - Action: 2, Reward: tensor([-125.8025], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:28,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:28,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:28,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:30,175 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:14:39,080 - __main__ - INFO - Action: 4, Reward: tensor([-126.0704], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:39,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:39,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:39,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:41,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:14:49,848 - __main__ - INFO - Action: 0, Reward: tensor([-126.3391], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:50,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:50,520 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:50,520 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:51,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 08:14:56,515 - __main__ - INFO - Action: 3, Reward: tensor([-126.3763], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:57,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:57,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:57,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:58,465 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:15:03,423 - __main__ - INFO - Action: 11, Reward: tensor([-26.1955], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:04,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:04,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:04,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:05,373 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:15:14,194 - __main__ - INFO - Action: 9, Reward: tensor([-24.4027], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:14,538 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-881.5978], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:15:14,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:14,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:14,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:16,127 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:15:20,931 - __main__ - INFO - Action: 2, Reward: tensor([-24.3208], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:21,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:21,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:21,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:22,840 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:15:31,559 - __main__ - INFO - Action: 0, Reward: tensor([-24.2964], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:32,190 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:32,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:32,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:33,488 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:15:41,845 - __main__ - INFO - Episode timed out.
2024-06-04 08:15:42,390 - __main__ - INFO - Action: 0, Reward: tensor([-24.5583], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:15:43,063 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:15:43,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:44,077 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:44,077 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:45,236 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:15:54,243 - __main__ - INFO - Action: 13, Reward: tensor([-124.9701], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:54,963 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-124.9701], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:15:55,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:55,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:55,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:56,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:16:01,239 - __main__ - INFO - Action: 3, Reward: tensor([-25.1321], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:01,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:01,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:01,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:03,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:16:11,677 - __main__ - INFO - Action: 5, Reward: tensor([-125.3219], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:12,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:12,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:12,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:13,622 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:16:22,498 - __main__ - INFO - Action: 0, Reward: tensor([-25.4794], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:23,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:23,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:23,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:23,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:16:28,715 - __main__ - INFO - Action: 2, Reward: tensor([-25.6590], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:29,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:29,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:29,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:30,589 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:16:35,385 - __main__ - INFO - Action: 10, Reward: tensor([-25.7803], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:35,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:36,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:36,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:37,277 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:16:46,118 - __main__ - INFO - Action: 14, Reward: tensor([-26.0461], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:46,619 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:46,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:46,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:47,967 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:16:52,889 - __main__ - INFO - Action: 2, Reward: tensor([-26.1713], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:53,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:53,590 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:53,590 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:54,791 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:17:03,570 - __main__ - INFO - Action: 5, Reward: tensor([-26.3815], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:04,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:04,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:04,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:05,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:17:14,423 - __main__ - INFO - Action: 0, Reward: tensor([-26.6057], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:14,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:15,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:15,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:16,332 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:17:25,240 - __main__ - INFO - Action: 16, Reward: tensor([-26.7335], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:25,946 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-484.2811], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:17:26,181 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:26,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:26,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:27,395 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:17:36,232 - __main__ - INFO - Action: 0, Reward: tensor([-126.9926], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:36,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:36,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:36,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:38,157 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:17:46,641 - __main__ - INFO - Episode timed out.
2024-06-04 08:17:47,280 - __main__ - INFO - Action: 16, Reward: tensor([-27.1253], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:17:47,966 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:17:48,795 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:48,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:48,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:50,054 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:17:54,672 - __main__ - INFO - Action: 11, Reward: tensor([-124.4319], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:55,358 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-124.4319], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:17:55,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:55,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:55,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:57,062 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:18:01,740 - __main__ - INFO - Action: 2, Reward: tensor([-24.4264], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:02,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:02,394 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:02,394 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:03,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:18:08,420 - __main__ - INFO - Action: 2, Reward: tensor([-24.5944], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:08,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:09,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:09,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:10,313 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:18:19,161 - __main__ - INFO - Action: 9, Reward: tensor([-23.3342], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:19,643 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:19,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:19,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:20,819 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:18:29,340 - __main__ - INFO - Action: 0, Reward: tensor([-22.9494], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:29,827 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:29,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:29,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:31,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:18:40,176 - __main__ - INFO - Action: 15, Reward: tensor([-22.7957], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:40,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:40,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:40,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:42,076 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:18:50,800 - __main__ - INFO - Action: 1, Reward: tensor([-22.7977], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:51,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:51,538 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:51,538 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:52,702 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:19:01,422 - __main__ - INFO - Action: 4, Reward: tensor([-23.0879], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:02,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:02,110 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:02,110 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:03,406 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:19:07,909 - __main__ - INFO - Action: 7, Reward: tensor([-23.1693], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:08,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:08,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:08,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:09,556 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:19:18,318 - __main__ - INFO - Action: 1, Reward: tensor([-23.1953], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:18,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:18,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:18,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:20,050 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:19:28,980 - __main__ - INFO - Action: 14, Reward: tensor([-23.0656], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:29,796 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-357.8477], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:19:30,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:30,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:30,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:31,375 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:19:40,182 - __main__ - INFO - Action: 12, Reward: tensor([-23.1386], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:40,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:40,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:40,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:42,216 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:19:50,473 - __main__ - INFO - Episode timed out.
2024-06-04 08:19:50,925 - __main__ - INFO - Action: 5, Reward: tensor([-23.3401], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:19:51,562 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:19:52,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:52,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:52,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:53,684 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:20:02,372 - __main__ - INFO - Action: 16, Reward: tensor([-124.8412], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:03,198 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-124.8412], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:20:03,464 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:03,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:03,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:04,857 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:20:13,441 - __main__ - INFO - Action: 1, Reward: tensor([-25.1536], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:14,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:14,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:14,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:15,231 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:20:20,024 - __main__ - INFO - Action: 6, Reward: tensor([-125.2921], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:20,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:20,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:20,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:21,859 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:20:30,586 - __main__ - INFO - Action: 1, Reward: tensor([-25.3909], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:31,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:31,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:31,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:32,383 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:20:40,974 - __main__ - INFO - Action: 9, Reward: tensor([-24.1427], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:41,536 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:41,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:41,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:42,787 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:20:51,623 - __main__ - INFO - Action: 4, Reward: tensor([-23.8153], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:52,233 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:52,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:52,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:53,634 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:21:02,174 - __main__ - INFO - Action: 0, Reward: tensor([-23.3849], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:02,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:02,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:02,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:04,072 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:21:12,863 - __main__ - INFO - Action: 9, Reward: tensor([-21.9169], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:13,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:13,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:13,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:14,922 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:21:23,939 - __main__ - INFO - Action: 4, Reward: tensor([-21.7916], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:24,552 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:24,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:24,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:25,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:21:30,540 - __main__ - INFO - Action: 10, Reward: tensor([-22.4334], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:31,087 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:31,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:31,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:32,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:21:41,332 - __main__ - INFO - Action: 14, Reward: tensor([-23.0471], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:42,100 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-461.2096], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:21:42,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:43,791 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:21:48,549 - __main__ - INFO - Action: 10, Reward: tensor([-23.8211], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:49,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:49,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:49,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:50,514 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:21:58,936 - __main__ - INFO - Episode timed out.
2024-06-04 08:21:59,326 - __main__ - INFO - Action: 12, Reward: tensor([-25.2289], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:22:23,869 - __main__ - INFO - Dropped rows with NA values. Shape changed from (744, 2) to (744, 2).
2024-06-04 08:22:23,874 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 08:30:16,532 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 08:30:33,034 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 08:30:49,219 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 08:31:55,244 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 08:32:00,584 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 08:32:00,770 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 08:32:04,674 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 08:32:05,783 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 08:32:06,130 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:32:06,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:07,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:07,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:08,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:32:12,819 - __main__ - INFO - Action: 10, Reward: tensor([-124.7415], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:13,572 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.7415], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:32:13,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:13,948 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:13,948 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:15,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:32:19,947 - __main__ - INFO - Action: 7, Reward: tensor([-24.8849], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:20,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:20,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:20,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:21,576 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:32:26,590 - __main__ - INFO - Action: 10, Reward: tensor([-25.0397], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:27,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:28,392 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:32:37,139 - __main__ - INFO - Action: 0, Reward: tensor([-25.2380], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:37,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:37,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:37,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:39,067 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:32:47,909 - __main__ - INFO - Action: 12, Reward: tensor([-25.5074], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:48,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:48,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:48,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:49,813 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:32:54,430 - __main__ - INFO - Action: 7, Reward: tensor([-25.5467], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:54,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:55,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:55,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:56,371 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:33:00,886 - __main__ - INFO - Action: 11, Reward: tensor([-25.6067], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:01,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:01,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:01,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:02,589 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:33:11,119 - __main__ - INFO - Action: 9, Reward: tensor([-23.7648], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:11,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:11,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:11,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:12,908 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:33:17,588 - __main__ - INFO - Action: 2, Reward: tensor([-23.5046], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:18,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:18,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:18,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:18,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:33:27,935 - __main__ - INFO - Action: 1, Reward: tensor([-22.8873], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:28,573 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:28,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:28,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:29,873 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:33:38,457 - __main__ - INFO - Action: 15, Reward: tensor([-22.9180], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:39,211 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-369.6396], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:33:39,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:39,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:39,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:40,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:33:49,406 - __main__ - INFO - Action: 9, Reward: tensor([-21.6332], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:49,923 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:49,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:49,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:51,156 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:33:56,043 - __main__ - INFO - Action: 7, Reward: tensor([-21.5716], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:56,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:56,781 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:56,781 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:58,065 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:34:06,437 - __main__ - INFO - Episode timed out.
2024-06-04 08:34:06,888 - __main__ - INFO - Action: 12, Reward: tensor([-21.6569], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:34:07,450 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:34:08,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:08,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:08,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:09,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:34:18,667 - __main__ - INFO - Action: 13, Reward: tensor([-124.8710], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:19,558 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8710], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:34:19,936 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:20,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:20,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:21,298 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:34:29,922 - __main__ - INFO - Action: 15, Reward: tensor([-125.1174], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:30,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:30,547 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:30,547 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:31,609 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:34:36,428 - __main__ - INFO - Action: 10, Reward: tensor([-125.2883], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:37,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:37,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:37,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:38,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:34:47,512 - __main__ - INFO - Action: 1, Reward: tensor([-25.5265], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:48,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:48,107 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:48,107 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:49,295 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 08:34:58,225 - __main__ - INFO - Action: 4, Reward: tensor([-25.7076], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:58,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:58,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:58,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:59,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:35:08,860 - __main__ - INFO - Action: 0, Reward: tensor([-25.8894], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:09,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:09,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:09,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:10,896 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:35:19,751 - __main__ - INFO - Action: 15, Reward: tensor([-26.1573], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:20,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:20,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:20,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:21,724 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:35:26,514 - __main__ - INFO - Action: 11, Reward: tensor([-25.9076], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:27,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:27,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:27,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:28,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:35:37,278 - __main__ - INFO - Action: 5, Reward: tensor([-25.2528], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:37,959 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:38,054 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:38,054 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:39,295 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:35:48,151 - __main__ - INFO - Action: 1, Reward: tensor([-25.1895], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:48,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:48,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:48,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:49,957 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:35:58,978 - __main__ - INFO - Action: 0, Reward: tensor([-25.1544], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:59,832 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-580.0618], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:36:00,146 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:00,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:00,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:01,438 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:36:05,807 - __main__ - INFO - Action: 7, Reward: tensor([-25.3248], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:06,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:06,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:06,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:07,651 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:36:15,741 - __main__ - INFO - Episode timed out.
2024-06-04 08:36:16,238 - __main__ - INFO - Action: 0, Reward: tensor([-25.0791], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:36:16,958 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:36:17,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:17,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:17,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:18,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:36:23,503 - __main__ - INFO - Action: 7, Reward: tensor([-124.7340], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:24,301 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.7340], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:36:24,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:24,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:24,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:25,910 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:36:34,879 - __main__ - INFO - Action: 1, Reward: tensor([-24.9703], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:35,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:35,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:35,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:36,813 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:36:45,709 - __main__ - INFO - Action: 0, Reward: tensor([-25.1345], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:46,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:46,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:46,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:47,634 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:36:52,182 - __main__ - INFO - Action: 3, Reward: tensor([-25.2822], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:52,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:52,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:52,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:54,024 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:36:58,765 - __main__ - INFO - Action: 10, Reward: tensor([-25.4360], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:59,405 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:59,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:59,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:00,668 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:37:09,452 - __main__ - INFO - Action: 1, Reward: tensor([-25.6803], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:10,030 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:10,141 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:10,141 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:11,294 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:37:19,919 - __main__ - INFO - Action: 13, Reward: tensor([-25.9415], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:20,369 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:20,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:20,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:21,670 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:37:30,281 - __main__ - INFO - Action: 15, Reward: tensor([-26.1578], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:30,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:30,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:30,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:32,124 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:37:36,903 - __main__ - INFO - Action: 3, Reward: tensor([-26.2894], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:37,462 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:37,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:37,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:38,753 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:37:47,520 - __main__ - INFO - Action: 1, Reward: tensor([-26.5117], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:48,209 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:48,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:48,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:49,632 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:37:54,437 - __main__ - INFO - Action: 7, Reward: tensor([-26.5772], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:55,157 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-382.7147], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:37:55,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:55,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:55,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:56,762 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:38:01,638 - __main__ - INFO - Action: 11, Reward: tensor([-126.2683], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:02,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:02,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:02,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:03,595 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:38:12,215 - __main__ - INFO - Action: 1, Reward: tensor([-25.7693], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:12,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:12,873 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:12,873 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:14,121 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:38:22,632 - __main__ - INFO - Episode timed out.
2024-06-04 08:38:23,192 - __main__ - INFO - Action: 14, Reward: tensor([-25.6465], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:38:23,752 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:38:24,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:24,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:24,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:26,004 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:38:34,737 - __main__ - INFO - Action: 14, Reward: tensor([-124.6853], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:35,470 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.6853], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:38:35,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:35,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:35,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:37,258 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:38:41,713 - __main__ - INFO - Action: 10, Reward: tensor([-124.9942], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:42,262 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:42,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:42,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:43,675 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:38:52,604 - __main__ - INFO - Action: 1, Reward: tensor([-25.1799], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:53,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:53,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:53,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:54,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:38:59,118 - __main__ - INFO - Action: 2, Reward: tensor([-25.2848], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:59,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:59,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:59,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:00,951 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 08:39:05,709 - __main__ - INFO - Action: 3, Reward: tensor([-25.4197], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:06,316 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:06,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:06,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:07,657 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:39:16,696 - __main__ - INFO - Action: 16, Reward: tensor([-25.5933], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:17,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:17,397 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:17,397 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:18,714 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:39:27,653 - __main__ - INFO - Action: 0, Reward: tensor([-25.8778], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:28,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:28,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:28,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:29,410 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:39:34,330 - __main__ - INFO - Action: 2, Reward: tensor([-26.0661], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:34,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:35,038 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:35,038 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:36,308 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 08:39:41,219 - __main__ - INFO - Action: 3, Reward: tensor([-26.1037], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:41,764 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:41,858 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:41,858 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:43,053 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:39:51,679 - __main__ - INFO - Action: 14, Reward: tensor([-26.3748], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:52,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:52,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:52,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:53,384 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:40:01,826 - __main__ - INFO - Action: 12, Reward: tensor([-26.6292], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:02,558 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-482.2089], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:40:02,918 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:03,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:03,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:04,315 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:40:13,259 - __main__ - INFO - Action: 13, Reward: tensor([-26.7769], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:13,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:13,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:13,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:15,164 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:40:20,088 - __main__ - INFO - Action: 10, Reward: tensor([-127.1543], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:20,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:20,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:20,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:22,042 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:40:30,404 - __main__ - INFO - Episode timed out.
2024-06-04 08:40:30,886 - __main__ - INFO - Action: 1, Reward: tensor([-27.3958], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:40:31,496 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:40:32,329 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:32,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:32,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:33,683 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:40:42,403 - __main__ - INFO - Action: 0, Reward: tensor([-124.8052], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:43,199 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.8052], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:40:43,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:43,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:43,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:44,887 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:40:53,681 - __main__ - INFO - Action: 9, Reward: tensor([-23.6599], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:54,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:54,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:54,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:55,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:41:00,252 - __main__ - INFO - Action: 3, Reward: tensor([-23.6893], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:00,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:00,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:00,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:02,067 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:41:07,027 - __main__ - INFO - Action: 2, Reward: tensor([-23.7998], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:07,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:07,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:07,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:08,883 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:41:13,867 - __main__ - INFO - Action: 11, Reward: tensor([-23.5111], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:14,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:14,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:14,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:15,836 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:41:24,508 - __main__ - INFO - Action: 15, Reward: tensor([-23.0928], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:25,008 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:25,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:25,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:26,405 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:41:31,213 - __main__ - INFO - Action: 2, Reward: tensor([-23.0979], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:31,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:31,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:31,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:33,102 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:41:41,882 - __main__ - INFO - Action: 4, Reward: tensor([-23.4894], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:42,429 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:43,553 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:41:48,081 - __main__ - INFO - Action: 2, Reward: tensor([-23.7372], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:48,599 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:48,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:48,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:49,988 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:41:58,723 - __main__ - INFO - Action: 0, Reward: tensor([-24.1373], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:59,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:59,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:59,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:00,682 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:42:09,644 - __main__ - INFO - Action: 0, Reward: tensor([-24.9593], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:10,505 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-361.9790], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:42:10,834 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:10,944 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:10,944 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:11,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:42:20,629 - __main__ - INFO - Action: 12, Reward: tensor([-25.9861], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:21,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:21,208 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:21,208 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:22,367 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:42:31,344 - __main__ - INFO - Action: 4, Reward: tensor([-26.8723], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:31,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:32,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:32,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:33,252 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:42:41,488 - __main__ - INFO - Episode timed out.
2024-06-04 08:42:42,049 - __main__ - INFO - Action: 9, Reward: tensor([-26.1789], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:42:42,679 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:42:43,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:43,746 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:43,746 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:44,870 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:42:53,753 - __main__ - INFO - Action: 0, Reward: tensor([-124.8253], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:54,491 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8253], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:42:54,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:54,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:54,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:55,887 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:43:04,910 - __main__ - INFO - Action: 12, Reward: tensor([-125.1189], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:05,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:05,567 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:05,567 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:06,725 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:43:15,609 - __main__ - INFO - Action: 4, Reward: tensor([-25.3284], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:16,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:16,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:16,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:17,422 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:43:26,141 - __main__ - INFO - Action: 5, Reward: tensor([-25.4185], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:26,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:26,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:26,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:28,053 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:43:36,834 - __main__ - INFO - Action: 0, Reward: tensor([-25.5781], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:37,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:37,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:37,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:38,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:43:47,586 - __main__ - INFO - Action: 16, Reward: tensor([-25.6779], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:48,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:48,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:48,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:49,482 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:43:58,278 - __main__ - INFO - Action: 15, Reward: tensor([-26.1144], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:58,792 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:58,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:58,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:00,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:44:08,891 - __main__ - INFO - Action: 5, Reward: tensor([-126.2877], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:09,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:09,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:09,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:10,781 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:44:19,612 - __main__ - INFO - Action: 9, Reward: tensor([-25.1436], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:20,173 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:20,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:20,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:21,615 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:44:30,424 - __main__ - INFO - Action: 0, Reward: tensor([-24.7825], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:30,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:31,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:31,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:32,376 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:44:41,306 - __main__ - INFO - Action: 15, Reward: tensor([-24.8744], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:42,091 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-579.1499], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:44:42,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:42,587 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:42,587 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:43,670 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:44:51,988 - __main__ - INFO - Episode timed out.
2024-06-04 08:44:52,253 - __main__ - INFO - Action: 5, Reward: tensor([-24.8878], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:44:52,505 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-04 08:45:18,068 - __main__ - INFO - Dropped rows with NA values. Shape changed from (825, 2) to (825, 2).
2024-06-04 08:45:18,073 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 09:46:24,567 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 09:46:47,813 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 09:47:00,031 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 09:47:14,429 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-04 09:51:00,806 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 09:51:06,099 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 09:51:06,254 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 09:51:09,971 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 09:51:11,110 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 09:51:11,486 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:51:12,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:12,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:12,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:13,675 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 09:51:18,655 - __main__ - INFO - Action: 2, Reward: tensor([-124.8188], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:19,572 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.8188], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:51:19,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:20,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:20,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:21,279 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 09:51:30,214 - __main__ - INFO - Action: 14, Reward: tensor([-25.0060], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:30,651 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:30,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:30,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:32,015 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:51:40,951 - __main__ - INFO - Action: 0, Reward: tensor([-25.4946], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:41,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:41,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:41,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:42,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:51:51,625 - __main__ - INFO - Action: 1, Reward: tensor([-25.6425], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:52,160 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:52,268 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:52,268 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:53,520 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:52:02,378 - __main__ - INFO - Action: 8, Reward: tensor([-126.0157], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:03,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:03,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:03,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:04,351 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 09:52:08,959 - __main__ - INFO - Action: 3, Reward: tensor([-26.1706], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:09,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:09,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:09,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:10,430 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 09:52:19,348 - __main__ - INFO - Action: 4, Reward: tensor([-26.3751], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:19,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:20,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:20,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:21,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:52:30,159 - __main__ - INFO - Action: 1, Reward: tensor([-26.6168], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:30,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:30,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:30,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:32,128 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:52:41,108 - __main__ - INFO - Action: 0, Reward: tensor([-26.7511], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:41,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:41,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:41,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:42,936 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:52:51,882 - __main__ - INFO - Action: 0, Reward: tensor([-27.0249], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:52,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:52,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:52,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:53,709 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:53:02,518 - __main__ - INFO - Action: 16, Reward: tensor([-27.1731], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:03,282 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-487.0893], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:53:03,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:03,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:03,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:04,906 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:53:13,007 - __main__ - INFO - Episode timed out.
2024-06-04 09:53:13,539 - __main__ - INFO - Action: 0, Reward: tensor([-27.4654], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:53:14,378 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:53:15,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:15,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:15,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:16,580 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:53:25,239 - __main__ - INFO - Action: 5, Reward: tensor([-124.8914], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:26,046 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8914], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:53:26,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:26,494 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:26,494 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:27,750 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 09:53:32,514 - __main__ - INFO - Action: 2, Reward: tensor([-25.0479], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:32,967 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:33,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:33,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:34,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:53:42,953 - __main__ - INFO - Action: 0, Reward: tensor([-25.2254], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:43,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:43,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:43,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:44,796 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 09:53:53,769 - __main__ - INFO - Action: 4, Reward: tensor([-25.4586], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:54,262 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:54,372 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:54,372 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:55,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 09:54:00,468 - __main__ - INFO - Action: 6, Reward: tensor([-25.7124], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:01,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:01,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:01,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:01,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:54:10,972 - __main__ - INFO - Action: 8, Reward: tensor([-125.9801], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:11,505 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:11,598 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:11,598 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:12,752 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 09:54:17,830 - __main__ - INFO - Action: 3, Reward: tensor([-26.0892], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:18,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:18,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:18,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:19,645 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:54:28,243 - __main__ - INFO - Action: 16, Reward: tensor([-26.2571], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:28,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:28,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:28,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:29,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 09:54:39,023 - __main__ - INFO - Action: 4, Reward: tensor([-26.5204], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:39,527 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:39,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:39,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:40,953 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:54:49,671 - __main__ - INFO - Action: 5, Reward: tensor([-26.6591], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:50,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:50,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:50,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:51,592 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 09:54:56,541 - __main__ - INFO - Action: 10, Reward: tensor([-26.8637], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:57,338 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-484.7054], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:54:57,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:57,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:57,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:59,091 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:55:03,875 - __main__ - INFO - Action: 11, Reward: tensor([-26.8042], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:04,469 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:04,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:04,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:05,811 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 09:55:14,668 - __main__ - INFO - Action: 15, Reward: tensor([-27.0465], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:15,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:15,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:15,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:16,621 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:55:24,825 - __main__ - INFO - Episode timed out.
2024-06-04 09:55:25,374 - __main__ - INFO - Action: 5, Reward: tensor([-27.2232], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:55:26,125 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:55:27,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:27,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:27,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:28,444 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:55:33,156 - __main__ - INFO - Action: 11, Reward: tensor([-124.4017], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:33,514 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.4017], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:55:33,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:33,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:33,948 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:35,236 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 09:55:44,118 - __main__ - INFO - Action: 15, Reward: tensor([-24.3176], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:44,660 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:44,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:44,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:45,916 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 09:55:50,523 - __main__ - INFO - Action: 7, Reward: tensor([-24.3623], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:51,129 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:51,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:51,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:52,224 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:56:00,806 - __main__ - INFO - Action: 5, Reward: tensor([-24.9164], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:01,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:01,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:01,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:02,619 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:56:11,119 - __main__ - INFO - Action: 16, Reward: tensor([-25.2901], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:11,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:11,774 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:11,774 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:12,880 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:56:21,607 - __main__ - INFO - Action: 1, Reward: tensor([-25.8781], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:22,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:23,593 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:56:28,626 - __main__ - INFO - Action: 11, Reward: tensor([-25.6740], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:29,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:29,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:29,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:30,630 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 09:56:35,379 - __main__ - INFO - Action: 6, Reward: tensor([-25.5907], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:36,022 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:36,132 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:36,132 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:37,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:56:45,837 - __main__ - INFO - Action: 16, Reward: tensor([-25.5433], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:46,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:46,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:46,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:47,778 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:56:56,609 - __main__ - INFO - Action: 5, Reward: tensor([-25.7135], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:57,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:57,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:57,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:57,480 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 09:57:05,902 - __main__ - INFO - Action: 13, Reward: tensor([-25.7741], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:06,543 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-377.4617], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:57:06,745 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:06,855 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:06,855 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:08,040 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:57:17,043 - __main__ - INFO - Action: 8, Reward: tensor([-127.3534], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:17,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:17,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:17,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:18,909 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 09:57:27,156 - __main__ - INFO - Episode timed out.
2024-06-04 09:57:27,546 - __main__ - INFO - Action: 1, Reward: tensor([-127.5957], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:57:28,382 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:57:29,398 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:29,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:29,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:30,757 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:57:35,730 - __main__ - INFO - Action: 11, Reward: tensor([-124.4065], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:36,530 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.4065], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:57:36,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:36,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:36,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:37,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:57:46,645 - __main__ - INFO - Action: 8, Reward: tensor([-124.9980], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:47,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:47,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:47,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:48,508 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 09:57:57,182 - __main__ - INFO - Action: 4, Reward: tensor([-25.2576], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:57,772 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:57,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:57,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:59,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:58:07,994 - __main__ - INFO - Action: 0, Reward: tensor([-25.4825], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:08,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:08,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:08,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:09,738 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:58:18,704 - __main__ - INFO - Action: 8, Reward: tensor([-25.6952], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:19,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:19,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:19,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:20,574 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:58:29,577 - __main__ - INFO - Action: 16, Reward: tensor([-25.8510], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:30,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:30,295 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:30,295 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:31,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:58:40,017 - __main__ - INFO - Action: 5, Reward: tensor([-26.1134], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:40,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:40,703 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:40,703 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:41,202 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:58:50,288 - __main__ - INFO - Action: 5, Reward: tensor([-26.3639], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:50,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:50,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:50,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:52,224 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:59:00,364 - __main__ - INFO - Action: 0, Reward: tensor([-26.5610], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:00,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:00,727 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:00,727 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:00,987 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 09:59:09,326 - __main__ - INFO - Action: 1, Reward: tensor([-26.6582], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:09,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:09,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:09,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:11,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 09:59:19,816 - __main__ - INFO - Action: 9, Reward: tensor([-25.5355], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:20,506 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-482.9229], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:59:20,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:20,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:20,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:22,145 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:59:30,352 - __main__ - INFO - Episode timed out.
2024-06-04 09:59:30,819 - __main__ - INFO - Action: 0, Reward: tensor([-25.1440], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:59:31,553 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:59:32,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:32,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:32,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:33,659 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 09:59:42,058 - __main__ - INFO - Action: 9, Reward: tensor([-123.5264], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:42,731 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-123.5264], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:59:42,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:42,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:42,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:44,292 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:59:52,982 - __main__ - INFO - Action: 1, Reward: tensor([-23.1318], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:53,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:53,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:53,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:54,728 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 09:59:59,629 - __main__ - INFO - Action: 6, Reward: tensor([-23.1878], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:00,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:00,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:00,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:01,595 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:00:06,457 - __main__ - INFO - Action: 3, Reward: tensor([-23.2988], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:06,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:07,069 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:07,069 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:08,332 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:00:12,951 - __main__ - INFO - Action: 10, Reward: tensor([-23.9015], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:13,532 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:13,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:13,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:14,812 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:00:23,500 - __main__ - INFO - Action: 15, Reward: tensor([-24.9879], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:24,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:24,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:24,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:25,084 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 10:00:29,886 - __main__ - INFO - Action: 6, Reward: tensor([-25.3956], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:30,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:30,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:30,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:31,773 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:00:36,025 - __main__ - INFO - Action: 11, Reward: tensor([-25.1314], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:36,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:36,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:36,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:37,935 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:00:46,760 - __main__ - INFO - Action: 0, Reward: tensor([-24.6850], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:47,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:47,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:47,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:48,691 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:00:53,716 - __main__ - INFO - Action: 3, Reward: tensor([-24.7853], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:54,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:54,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:54,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:55,391 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:01:03,896 - __main__ - INFO - Action: 0, Reward: tensor([-24.9169], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:04,689 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-366.9485], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:01:04,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:04,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:04,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:05,972 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:01:14,693 - __main__ - INFO - Action: 0, Reward: tensor([-25.0035], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:15,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:15,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:15,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:16,500 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:01:21,004 - __main__ - INFO - Action: 3, Reward: tensor([-25.0616], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:21,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:21,690 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:21,690 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:22,938 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 10:01:27,551 - __main__ - INFO - Action: 6, Reward: tensor([-25.3797], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:28,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:28,235 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:28,235 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:29,013 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:01:37,021 - __main__ - INFO - Episode timed out.
2024-06-04 10:01:37,580 - __main__ - INFO - Action: 15, Reward: tensor([-25.9383], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:01:38,408 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:01:39,106 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:39,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:39,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:40,454 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 10:01:49,337 - __main__ - INFO - Action: 9, Reward: tensor([-123.5322], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:49,963 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-123.5322], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:01:50,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:50,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:50,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:51,520 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:02:00,484 - __main__ - INFO - Action: 16, Reward: tensor([-23.1886], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:00,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:00,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:00,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:01,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 10:02:10,365 - __main__ - INFO - Action: 13, Reward: tensor([-23.4014], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:10,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:11,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:11,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:12,290 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:02:21,150 - __main__ - INFO - Action: 0, Reward: tensor([-23.5623], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:21,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:21,804 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:21,804 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:23,060 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:02:28,017 - __main__ - INFO - Action: 3, Reward: tensor([-23.7401], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:28,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:28,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:28,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:29,686 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:02:37,607 - __main__ - INFO - Action: 0, Reward: tensor([-23.9704], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:38,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:38,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:38,282 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:38,879 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:02:47,470 - __main__ - INFO - Action: 16, Reward: tensor([-24.3627], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:48,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:48,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:48,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:49,480 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:02:58,243 - __main__ - INFO - Action: 0, Reward: tensor([-25.0507], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:58,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:58,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:58,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:00,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 10:03:08,646 - __main__ - INFO - Action: 4, Reward: tensor([-25.3271], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:09,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:09,398 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:09,398 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:10,337 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 10:03:14,697 - __main__ - INFO - Action: 2, Reward: tensor([-25.5329], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:14,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:14,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:14,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:16,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:03:20,938 - __main__ - INFO - Action: 10, Reward: tensor([-26.1656], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:21,641 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-367.8339], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:03:21,668 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:21,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:21,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:22,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:03:31,979 - __main__ - INFO - Action: 5, Reward: tensor([-127.1256], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:32,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:32,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:32,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:33,003 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:03:40,859 - __main__ - INFO - Episode timed out.
2024-06-04 10:03:41,159 - __main__ - INFO - Action: 0, Reward: tensor([-27.3431], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:03:41,809 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:03:42,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:42,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:42,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:43,966 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:03:48,788 - __main__ - INFO - Action: 7, Reward: tensor([-124.8132], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:49,709 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-124.8132], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:03:50,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:50,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:50,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:51,215 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:04:00,228 - __main__ - INFO - Action: 0, Reward: tensor([-25.0567], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:00,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:00,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:00,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:02,158 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 10:04:06,967 - __main__ - INFO - Action: 3, Reward: tensor([-25.1361], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:07,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:07,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:07,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:08,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 10:04:17,732 - __main__ - INFO - Action: 8, Reward: tensor([-25.4061], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:18,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:18,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:18,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:19,481 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:04:27,851 - __main__ - INFO - Action: 15, Reward: tensor([-25.6570], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:28,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:28,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:28,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:29,041 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:04:36,959 - __main__ - INFO - Action: 0, Reward: tensor([-25.7725], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:37,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:37,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:37,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:38,756 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:04:48,045 - __main__ - INFO - Action: 0, Reward: tensor([-26.0229], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:48,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:48,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:48,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:49,157 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:04:53,041 - __main__ - INFO - Action: 7, Reward: tensor([-26.1312], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:53,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:53,561 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:53,561 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:54,857 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 10:05:03,764 - __main__ - INFO - Action: 13, Reward: tensor([-26.3704], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:04,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:04,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:04,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:05,373 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 10:05:09,976 - __main__ - INFO - Action: 6, Reward: tensor([-26.3811], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:10,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:10,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:10,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:11,963 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:05:20,584 - __main__ - INFO - Action: 0, Reward: tensor([-26.6444], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:21,411 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-383.3917], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:05:21,597 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:21,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:21,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:22,770 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:05:31,591 - __main__ - INFO - Action: 0, Reward: tensor([-26.8626], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:32,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:32,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:32,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:33,514 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:05:38,624 - __main__ - INFO - Action: 11, Reward: tensor([-26.5662], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:39,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:39,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:39,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:40,547 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:05:49,092 - __main__ - INFO - Episode timed out.
2024-06-04 10:05:49,652 - __main__ - INFO - Action: 15, Reward: tensor([-26.0751], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:05:50,372 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:05:51,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:52,557 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:06:01,517 - __main__ - INFO - Action: 5, Reward: tensor([-124.8956], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:01,809 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-124.8956], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:06:01,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:01,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:01,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:02,765 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 10:06:07,626 - __main__ - INFO - Action: 2, Reward: tensor([-25.0332], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:08,223 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:08,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:08,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:09,571 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:06:18,772 - __main__ - INFO - Action: 5, Reward: tensor([-125.2036], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:19,405 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:19,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:19,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:20,741 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:06:29,751 - __main__ - INFO - Action: 0, Reward: tensor([-125.5371], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:30,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:30,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:30,270 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:31,545 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 10:06:40,631 - __main__ - INFO - Action: 4, Reward: tensor([-125.7091], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:41,208 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:41,317 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:41,317 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:42,465 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 10:06:46,906 - __main__ - INFO - Action: 2, Reward: tensor([-125.9245], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:47,435 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:47,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:47,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:48,697 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:06:53,227 - __main__ - INFO - Action: 11, Reward: tensor([-25.6616], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:53,849 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:53,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:53,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:55,144 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 10:07:04,032 - __main__ - INFO - Action: 9, Reward: tensor([-23.8762], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:04,515 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:05,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:07:10,676 - __main__ - INFO - Action: 7, Reward: tensor([-23.6739], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:11,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:11,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:11,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:12,438 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 10:07:20,846 - __main__ - INFO - Action: 9, Reward: tensor([-21.9662], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:21,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:21,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:21,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:22,780 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 10:07:31,762 - __main__ - INFO - Action: 4, Reward: tensor([-21.1734], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:32,701 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-768.6545], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:07:33,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:33,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:33,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:34,207 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 10:07:42,368 - __main__ - INFO - Action: 13, Reward: tensor([-20.7968], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:42,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:42,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:42,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:43,969 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:07:52,503 - __main__ - INFO - Episode timed out.
2024-06-04 10:07:52,974 - __main__ - INFO - Action: 15, Reward: tensor([-20.8198], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:07:53,642 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:07:54,593 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:54,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:54,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:55,440 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:08:00,130 - __main__ - INFO - Action: 10, Reward: tensor([-124.7957], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:00,973 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-124.7957], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:08:01,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:01,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:01,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:02,742 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:08:11,871 - __main__ - INFO - Action: 16, Reward: tensor([-25.0093], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:12,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:12,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:12,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:13,117 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:08:17,807 - __main__ - INFO - Action: 7, Reward: tensor([-25.1510], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:18,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:18,585 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:18,585 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:19,671 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:08:24,029 - __main__ - INFO - Action: 11, Reward: tensor([-24.9538], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:24,387 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:24,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:24,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:25,002 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:08:32,995 - __main__ - INFO - Action: 0, Reward: tensor([-24.7641], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:33,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:33,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:33,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:34,677 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:08:39,208 - __main__ - INFO - Action: 10, Reward: tensor([-25.3829], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:39,754 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:39,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:39,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:41,097 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 10:08:49,854 - __main__ - INFO - Action: 4, Reward: tensor([-125.8392], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:50,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:50,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:50,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:51,625 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:09:00,614 - __main__ - INFO - Action: 5, Reward: tensor([-26.0104], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:01,129 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:01,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:01,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:01,513 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:09:10,402 - __main__ - INFO - Action: 0, Reward: tensor([-26.2560], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:11,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:11,104 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:11,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:12,341 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:09:20,775 - __main__ - INFO - Action: 0, Reward: tensor([-26.5176], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:21,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:21,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:21,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:22,772 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:09:31,840 - __main__ - INFO - Action: 0, Reward: tensor([-26.7487], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:32,637 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-481.4288], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:09:33,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:34,350 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:09:43,070 - __main__ - INFO - Action: 0, Reward: tensor([-26.9916], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:43,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:43,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:43,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:44,832 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 10:09:53,770 - __main__ - INFO - Action: 14, Reward: tensor([-27.2356], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:54,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:54,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:54,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:55,768 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:10:04,133 - __main__ - INFO - Episode timed out.
2024-06-04 10:10:04,433 - __main__ - INFO - Action: 16, Reward: tensor([-27.3492], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:10:32,396 - __main__ - INFO - Dropped rows with NA values. Shape changed from (945, 2) to (945, 2).
2024-06-04 10:10:32,472 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 10:11:55,488 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 10:12:32,569 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 10:12:54,688 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 10:32:58,403 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:40:09,843 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:40:17,302 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 13:40:17,443 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 13:40:17,880 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 13:40:17,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 13:40:17,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 13:40:18,990 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 13:40:27,750 - __main__ - INFO - Action: 9, Reward: tensor([-123.2824], grad_fn=<AddBackward0>), Done: False
2024-06-04 13:45:52,873 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:50:28,636 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:53:49,356 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:53:57,405 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 13:53:57,937 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 13:53:58,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 13:53:58,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 13:53:58,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 13:53:59,778 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 13:54:08,427 - __main__ - INFO - Action: 0, Reward: tensor([-124.7604], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:37:35,995 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 17:37:43,449 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 17:37:43,790 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 17:37:44,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:37:44,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:37:44,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:37:45,654 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:37:54,530 - __main__ - INFO - Action: 12, Reward: tensor([-124.7073], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:37:55,307 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-124.7073], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:37:55,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:37:55,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:37:55,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:37:56,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:38:01,362 - __main__ - INFO - Action: 6, Reward: tensor([-24.9723], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:01,596 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:01,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:01,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:02,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:38:11,226 - __main__ - INFO - Action: 13, Reward: tensor([-125.1077], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:11,584 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:11,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:11,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:12,849 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:38:21,556 - __main__ - INFO - Action: 13, Reward: tensor([-25.4075], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:21,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:22,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:22,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:23,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:38:30,955 - __main__ - INFO - Action: 1, Reward: tensor([-25.6059], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:31,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:31,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:31,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:32,168 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:38:36,750 - __main__ - INFO - Action: 2, Reward: tensor([-25.5505], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:36,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:37,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:37,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:37,867 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:38:45,450 - __main__ - INFO - Action: 13, Reward: tensor([-25.8587], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:45,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:45,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:45,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:46,174 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:38:54,713 - __main__ - INFO - Action: 8, Reward: tensor([-26.0122], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:54,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:55,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:55,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:55,836 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:39:04,158 - __main__ - INFO - Action: 8, Reward: tensor([-26.2379], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:04,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:04,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:04,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:05,727 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:39:13,771 - __main__ - INFO - Action: 4, Reward: tensor([-26.4053], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:13,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:14,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:14,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:14,331 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:39:22,640 - __main__ - INFO - Action: 1, Reward: tensor([-26.5824], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:23,325 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:23,310 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-482.4476], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:39:23,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:23,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:24,120 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:39:32,217 - __main__ - INFO - Action: 14, Reward: tensor([-26.7355], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:32,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:32,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:32,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:33,549 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:39:41,935 - __main__ - INFO - Action: 4, Reward: tensor([-26.9058], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:42,215 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:42,308 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:42,308 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:42,712 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:39:47,059 - __main__ - INFO - Action: 7, Reward: tensor([-27.0098], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:47,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:47,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:47,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:48,435 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:39:52,863 - __main__ - INFO - Action: 2, Reward: tensor([-27.1685], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:53,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:53,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:53,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:53,862 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:40:01,685 - __main__ - INFO - Action: 1, Reward: tensor([-27.1195], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:01,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:02,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:02,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:02,710 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:40:07,185 - __main__ - INFO - Action: 7, Reward: tensor([-27.2852], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:07,467 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:07,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:07,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:08,653 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:40:16,536 - __main__ - INFO - Action: 0, Reward: tensor([-27.4662], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:16,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:16,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:16,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:17,626 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:40:21,814 - __main__ - INFO - Action: 6, Reward: tensor([-27.6572], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:22,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:22,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:22,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:22,573 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:40:30,780 - __main__ - INFO - Action: 16, Reward: tensor([-27.8626], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:31,091 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:31,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:31,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:31,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:40:39,760 - __main__ - INFO - Action: 9, Reward: tensor([-26.7809], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:40,364 - __main__ - INFO - Epoch 0, Iteration 20: Reward: tensor([-754.4388], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:40:40,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:40,534 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:40,534 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:40,676 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:40:48,159 - __main__ - INFO - Action: 14, Reward: tensor([-26.4016], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:48,236 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:48,236 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:48,236 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:48,873 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:40:56,207 - __main__ - INFO - Action: 15, Reward: tensor([-25.8747], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:56,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:56,515 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:56,515 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:56,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:41:04,890 - __main__ - INFO - Action: 4, Reward: tensor([-25.7439], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:05,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:05,188 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:05,188 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:05,588 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:41:09,167 - __main__ - INFO - Action: 6, Reward: tensor([-25.9603], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:09,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:09,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:09,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:09,744 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:41:13,093 - __main__ - INFO - Action: 10, Reward: tensor([-26.5257], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:13,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:13,532 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:13,532 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:13,967 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:41:21,305 - __main__ - INFO - Action: 13, Reward: tensor([-27.2891], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:21,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:21,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:21,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:21,939 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:41:29,455 - __main__ - INFO - Action: 0, Reward: tensor([-27.8095], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:29,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:29,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:29,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:30,091 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 17:41:33,502 - __main__ - INFO - Action: 3, Reward: tensor([-27.9721], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:33,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:33,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:33,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:34,121 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:41:42,017 - __main__ - INFO - Action: 8, Reward: tensor([-129.4059], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:42,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:42,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:42,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:42,812 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:41:50,588 - __main__ - INFO - Action: 8, Reward: tensor([-29.6310], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:50,821 - __main__ - INFO - Epoch 0, Iteration 30: Reward: tensor([-1127.0525], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:41:50,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:50,883 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:50,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:51,099 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:41:58,432 - __main__ - INFO - Action: 12, Reward: tensor([-29.7879], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:58,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:58,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:58,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:58,868 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:42:06,042 - __main__ - INFO - Action: 15, Reward: tensor([-29.9539], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:06,258 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:06,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:06,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:06,614 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:42:13,534 - __main__ - INFO - Action: 4, Reward: tensor([-30.1211], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:13,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:13,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:13,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:14,093 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:42:17,282 - __main__ - INFO - Action: 11, Reward: tensor([-29.7845], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:17,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:18,087 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:42:25,276 - __main__ - INFO - Action: 4, Reward: tensor([-29.5044], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:25,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:25,445 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:25,445 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:25,913 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:42:29,538 - __main__ - INFO - Action: 7, Reward: tensor([-29.5906], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:29,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:29,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:29,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:30,436 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:42:37,331 - __main__ - INFO - Action: 8, Reward: tensor([-130.6261], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:37,363 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:37,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:37,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:37,670 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 17:42:44,826 - __main__ - INFO - Action: 0, Reward: tensor([-30.7951], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:44,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:44,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:44,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:45,259 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:42:48,387 - __main__ - INFO - Action: 11, Reward: tensor([-30.6109], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:48,590 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:48,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:48,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:49,026 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:42:52,306 - __main__ - INFO - Action: 6, Reward: tensor([-30.6481], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:52,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:52,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:52,712 - __main__ - INFO - Epoch 0, Iteration 40: Reward: tensor([-1528.4753], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:42:52,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:53,009 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:43:00,433 - __main__ - INFO - Action: 16, Reward: tensor([-30.7394], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:00,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:00,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:00,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:00,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:43:04,264 - __main__ - INFO - Action: 2, Reward: tensor([-30.9092], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:04,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:04,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:04,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:04,830 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 17:43:07,971 - __main__ - INFO - Action: 3, Reward: tensor([-30.9363], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:08,204 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:08,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:08,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:08,606 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:43:12,271 - __main__ - INFO - Action: 11, Reward: tensor([-30.6545], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:12,597 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:12,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:12,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:12,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:43:20,072 - __main__ - INFO - Action: 15, Reward: tensor([-30.1745], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:20,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:20,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:20,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:20,679 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:43:27,823 - __main__ - INFO - Action: 15, Reward: tensor([-30.3310], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:27,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:28,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:28,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:28,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:43:35,920 - __main__ - INFO - Action: 16, Reward: tensor([-30.4803], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:36,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:36,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:36,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:36,820 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:43:40,804 - __main__ - INFO - Action: 7, Reward: tensor([-30.5946], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:41,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:41,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:41,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:41,814 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:43:45,371 - __main__ - INFO - Action: 10, Reward: tensor([-31.2410], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:45,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:45,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:45,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:46,314 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:43:53,935 - __main__ - INFO - Action: 14, Reward: tensor([-31.8477], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:54,587 - __main__ - INFO - Epoch 0, Iteration 50: Reward: tensor([-1836.3838], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:43:54,619 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:54,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:54,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:55,443 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:44:02,876 - __main__ - INFO - Action: 0, Reward: tensor([-31.9627], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:03,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:03,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:03,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:03,500 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:44:10,961 - __main__ - INFO - Action: 9, Reward: tensor([-30.7549], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:11,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:11,210 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:11,210 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:11,535 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 17:44:18,871 - __main__ - INFO - Action: 5, Reward: tensor([-29.9219], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:19,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:19,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:19,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:20,148 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:44:23,833 - __main__ - INFO - Action: 11, Reward: tensor([-29.4261], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:24,003 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:24,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:24,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:24,887 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:44:32,401 - __main__ - INFO - Action: 0, Reward: tensor([-28.7715], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:32,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:32,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:32,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:33,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:44:40,530 - __main__ - INFO - Action: 15, Reward: tensor([-28.5258], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:40,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:40,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:40,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:40,947 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:44:47,828 - __main__ - INFO - Action: 9, Reward: tensor([-27.1675], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:47,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:47,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:47,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:48,462 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:44:51,728 - __main__ - INFO - Action: 2, Reward: tensor([-27.0586], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:51,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:51,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:51,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:52,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:45:00,004 - __main__ - INFO - Action: 16, Reward: tensor([-26.6445], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:00,050 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:00,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:00,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:00,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:45:07,769 - __main__ - INFO - Action: 9, Reward: tensor([-25.2714], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:07,997 - __main__ - INFO - Epoch 0, Iteration 60: Reward: tensor([-2121.8887], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:45:08,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:08,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:08,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:08,320 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 17:45:16,498 - __main__ - INFO - Action: 4, Reward: tensor([-24.8661], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:16,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:16,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:16,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:17,245 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:45:20,966 - __main__ - INFO - Action: 10, Reward: tensor([-25.4389], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:21,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:21,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:21,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:22,002 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:45:25,822 - __main__ - INFO - Action: 2, Reward: tensor([-25.7962], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:26,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:26,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:26,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:26,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:45:34,483 - __main__ - INFO - Action: 8, Reward: tensor([-27.7089], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:34,606 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:34,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:34,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:34,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:45:43,232 - __main__ - INFO - Action: 13, Reward: tensor([-28.3464], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:43,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:43,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:43,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:44,348 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:45:52,404 - __main__ - INFO - Action: 1, Reward: tensor([-28.6805], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:52,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:52,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:52,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:53,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 17:46:01,911 - __main__ - INFO - Action: 5, Reward: tensor([-28.6740], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:02,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:02,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:02,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:03,339 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 17:46:11,452 - __main__ - INFO - Action: 5, Reward: tensor([-28.8288], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:11,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:11,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:11,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:12,910 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 17:46:20,844 - __main__ - INFO - Action: 0, Reward: tensor([-29.1719], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:21,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:21,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:21,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:21,850 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:46:30,607 - __main__ - INFO - Action: 1, Reward: tensor([-29.5338], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:31,198 - __main__ - INFO - Epoch 0, Iteration 70: Reward: tensor([-2398.9343], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:46:31,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:31,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:31,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:32,297 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:46:40,330 - __main__ - INFO - Action: 14, Reward: tensor([-29.9624], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:40,689 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:40,768 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:40,768 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:41,267 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:46:45,290 - __main__ - INFO - Action: 11, Reward: tensor([-29.6637], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:45,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:45,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:45,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:46,195 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:46:53,814 - __main__ - INFO - Action: 15, Reward: tensor([-29.1012], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:54,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:54,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:54,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:54,869 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:46:58,781 - __main__ - INFO - Action: 6, Reward: tensor([-29.2162], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:58,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:59,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:59,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:59,499 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:47:07,259 - __main__ - INFO - Action: 14, Reward: tensor([-29.5441], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:07,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:07,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:07,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:08,023 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:47:12,174 - __main__ - INFO - Action: 7, Reward: tensor([-29.8548], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:12,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:12,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:12,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:13,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:47:17,145 - __main__ - INFO - Action: 2, Reward: tensor([-30.1551], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:17,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:18,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:47:25,929 - __main__ - INFO - Action: 1, Reward: tensor([-30.2600], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:26,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:26,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:26,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:26,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:47:29,957 - __main__ - INFO - Action: 2, Reward: tensor([-30.4675], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:30,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:30,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:30,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:30,528 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:47:34,465 - __main__ - INFO - Action: 2, Reward: tensor([-30.6903], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:34,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:34,526 - __main__ - INFO - Epoch 0, Iteration 80: Reward: tensor([-2697.8499], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:47:34,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:34,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:34,828 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:47:42,702 - __main__ - INFO - Action: 1, Reward: tensor([-31.0019], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:42,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:43,118 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:43,118 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:43,693 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:47:52,038 - __main__ - INFO - Action: 15, Reward: tensor([-31.1251], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:52,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:52,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:52,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:53,242 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:47:57,393 - __main__ - INFO - Action: 2, Reward: tensor([-31.1954], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:57,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:57,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:57,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:58,533 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:48:06,003 - __main__ - INFO - Action: 8, Reward: tensor([-32.9407], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:06,093 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:06,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:06,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:07,090 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:48:15,140 - __main__ - INFO - Action: 15, Reward: tensor([-33.9088], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:15,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:15,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:15,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:16,337 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:48:23,722 - __main__ - INFO - Action: 14, Reward: tensor([-34.4533], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:23,893 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:24,000 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:24,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:24,190 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 7 selected.
2024-06-04 17:48:28,494 - __main__ - INFO - Action: 7, Reward: tensor([-34.7802], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:28,822 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:28,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:28,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:30,042 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:48:38,249 - __main__ - INFO - Action: 12, Reward: tensor([-35.0514], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:38,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:38,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:38,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:39,752 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:48:48,604 - __main__ - INFO - Action: 0, Reward: tensor([-35.6482], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:48,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:48,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:48,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:49,735 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:48:58,690 - __main__ - INFO - Action: 0, Reward: tensor([-36.3071], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:59,536 - __main__ - INFO - Epoch 0, Iteration 90: Reward: tensor([-3034.2622], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:48:59,552 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:59,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:59,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:00,704 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:49:09,044 - __main__ - INFO - Action: 0, Reward: tensor([-37.3258], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:09,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:09,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:09,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:10,500 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:49:18,937 - __main__ - INFO - Action: 4, Reward: tensor([-37.8156], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:19,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:19,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:19,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:20,365 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:49:24,436 - __main__ - INFO - Action: 7, Reward: tensor([-38.0586], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:24,795 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:24,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:24,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:25,929 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:49:34,888 - __main__ - INFO - Action: 0, Reward: tensor([-39.0500], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:35,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:35,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:35,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:36,335 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:49:44,578 - __main__ - INFO - Action: 16, Reward: tensor([-39.6343], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:44,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:44,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:44,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:45,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:49:50,907 - __main__ - INFO - Action: 2, Reward: tensor([-40.0855], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:51,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:52,522 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:49:57,655 - __main__ - INFO - Action: 2, Reward: tensor([-40.5093], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:57,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:58,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:58,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:59,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:50:04,030 - __main__ - INFO - Action: 10, Reward: tensor([-41.1383], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:04,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:04,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:04,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:05,664 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:50:14,479 - __main__ - INFO - Action: 14, Reward: tensor([-142.3421], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:14,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:14,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:14,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:15,364 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:50:23,954 - __main__ - INFO - Action: 0, Reward: tensor([-42.5317], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:24,870 - __main__ - INFO - Epoch 0, Iteration 100: Reward: tensor([-3532.7532], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:50:24,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:24,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:24,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:26,114 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:50:34,837 - __main__ - INFO - Action: 1, Reward: tensor([-42.5943], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:35,147 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:35,255 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:35,255 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:36,545 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:50:41,338 - __main__ - INFO - Action: 11, Reward: tensor([-42.4510], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:41,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:41,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:41,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:42,981 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:50:48,027 - __main__ - INFO - Action: 3, Reward: tensor([-42.3248], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:48,337 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:48,462 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:48,462 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:49,734 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:50:58,507 - __main__ - INFO - Action: 1, Reward: tensor([-41.7030], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:58,772 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:58,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:58,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:59,826 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:51:04,541 - __main__ - INFO - Action: 11, Reward: tensor([-41.2091], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:04,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:04,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:04,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:05,562 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:51:10,060 - __main__ - INFO - Action: 2, Reward: tensor([-41.2218], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:10,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:10,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:10,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:11,477 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:51:16,428 - __main__ - INFO - Action: 3, Reward: tensor([-41.0562], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:16,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:16,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:16,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:17,844 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:51:26,141 - __main__ - INFO - Action: 0, Reward: tensor([-41.6600], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:26,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:26,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:26,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:27,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:51:32,317 - __main__ - INFO - Action: 2, Reward: tensor([-42.1099], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:32,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:32,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:32,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:33,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:51:42,739 - __main__ - INFO - Action: 13, Reward: tensor([-42.1483], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:43,517 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:43,502 - __main__ - INFO - Epoch 0, Iteration 110: Reward: tensor([-3951.2312], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:51:43,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:43,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:44,818 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:51:53,261 - __main__ - INFO - Action: 0, Reward: tensor([-42.7145], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:53,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:53,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:53,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:54,614 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:52:03,321 - __main__ - INFO - Action: 14, Reward: tensor([-43.7600], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:03,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:04,725 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:52:09,843 - __main__ - INFO - Action: 3, Reward: tensor([-43.8498], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:10,110 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:10,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:10,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:11,498 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:52:16,386 - __main__ - INFO - Action: 6, Reward: tensor([-43.9645], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:16,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:16,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:16,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:17,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:52:22,656 - __main__ - INFO - Action: 2, Reward: tensor([-44.2905], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:23,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:23,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:23,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:24,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:52:29,209 - __main__ - INFO - Action: 2, Reward: tensor([-44.6609], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:29,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:29,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:29,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:30,811 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:52:35,805 - __main__ - INFO - Action: 6, Reward: tensor([-44.7752], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:36,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:36,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:36,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:37,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:52:42,027 - __main__ - INFO - Action: 3, Reward: tensor([-44.6943], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:42,259 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:42,369 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:42,369 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:43,412 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:52:52,622 - __main__ - INFO - Action: 1, Reward: tensor([-44.2991], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:52,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:52,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:52,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:54,272 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:52:59,155 - __main__ - INFO - Action: 2, Reward: tensor([-44.4466], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:00,029 - __main__ - INFO - Epoch 0, Iteration 120: Reward: tensor([-4392.6865], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:53:00,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:00,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:00,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:01,277 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:53:06,200 - __main__ - INFO - Action: 2, Reward: tensor([-44.8662], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:06,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:06,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:06,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:07,581 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:53:16,690 - __main__ - INFO - Action: 0, Reward: tensor([-46.0379], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:17,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:17,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:17,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:18,332 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:53:27,337 - __main__ - INFO - Action: 1, Reward: tensor([-46.1244], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:27,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:27,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:27,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:28,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:53:37,625 - __main__ - INFO - Action: 4, Reward: tensor([-46.2451], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:37,936 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:38,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:38,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:39,198 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:53:48,198 - __main__ - INFO - Action: 0, Reward: tensor([-47.0115], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:48,524 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:48,633 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:48,633 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:49,876 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:53:58,602 - __main__ - INFO - Action: 1, Reward: tensor([-47.0206], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:58,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:58,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:58,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:59,917 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:54:08,793 - __main__ - INFO - Action: 12, Reward: tensor([-47.6410], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:54:09,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:54:09,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:54:09,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:54:10,330 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:54:19,016 - __main__ - INFO - Action: 9, Reward: tensor([-46.8375], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:54:19,264 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:54:19,312 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:54:19,312 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:54:20,604 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:54:28,897 - __main__ - INFO - Episode timed out.
2024-06-04 17:54:29,395 - __main__ - INFO - Action: 4, Reward: tensor([-46.5437], grad_fn=<AddBackward0>), Done: True
2024-06-05 17:11:43,829 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:16:10,189 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:20:08,414 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:23:22,104 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:23:31,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:23:31,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:23:31,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:23:32,513 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:27:33,952 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:27:43,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:44,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:44,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:45,098 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:27:48,966 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:49,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:49,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:50,093 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:27:53,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:53,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:53,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:53,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:27:56,716 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:56,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:56,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:57,167 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:00,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:00,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:00,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:01,252 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:28:04,453 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:05,696 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:08,843 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:08,922 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:08,922 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:10,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:13,076 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:13,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:13,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:14,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:17,453 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:17,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:17,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:18,657 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:28:21,843 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:21,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:21,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:23,013 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:26,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:26,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:26,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:27,201 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:28:30,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:30,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:30,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:32,038 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:28:34,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:34,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:34,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:34,941 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:37,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:37,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:37,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:38,866 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:41,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:41,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:41,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:42,902 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:45,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:46,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:46,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:46,944 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:50,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:50,305 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:50,305 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:51,207 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:28:54,210 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:54,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:54,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:55,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:57,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:58,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:58,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:58,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:01,789 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:01,867 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:01,867 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:02,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:05,364 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:05,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:05,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:06,597 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:29:10,230 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:10,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:10,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:11,018 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:29:13,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:13,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:13,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:14,605 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:29:17,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:17,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:17,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:18,668 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:21,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:21,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:21,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:22,971 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:29:25,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:26,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:26,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:27,207 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:30,188 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:30,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:30,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:31,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:33,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:33,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:34,900 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:29:38,099 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:38,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:38,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:38,998 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:29:42,115 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:42,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:42,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:43,300 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:46,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:46,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:46,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:47,185 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:50,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:50,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:50,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:51,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:29:54,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:54,765 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:54,765 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:55,698 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:29:58,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:58,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:58,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:59,884 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:02,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:02,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:02,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:03,877 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:30:06,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:06,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:06,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:07,984 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:11,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:12,223 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:30:15,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:15,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:15,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:16,548 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:19,504 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:19,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:19,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:20,713 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:30:23,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:23,823 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:23,823 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:24,930 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:27,830 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:27,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:27,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:28,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:30:32,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:32,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:32,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:33,580 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:30:36,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:36,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:36,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:37,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:40,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:40,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:40,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:42,089 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:44,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:44,956 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:44,956 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:46,048 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:30:49,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:49,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:49,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:50,382 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:53,432 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:53,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:53,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:54,602 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:30:57,496 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:57,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:57,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:58,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:31:01,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:01,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:01,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:02,807 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:31:05,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:05,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:05,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:07,061 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:09,918 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:10,042 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:10,042 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:10,742 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:31:14,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:14,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:14,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:15,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:31:18,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:18,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:18,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:19,669 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:31:22,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:22,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:22,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:23,742 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:31:26,865 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:26,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:26,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:28,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:30,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:30,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:30,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:31,794 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:34,766 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:34,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:34,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:35,990 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:39,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:39,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:39,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:40,104 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:42,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:43,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:43,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:43,938 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:31:46,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:46,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:46,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:48,047 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:51,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:51,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:51,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:52,274 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:55,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:55,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:55,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:56,822 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:59,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:59,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:59,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:00,857 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:03,882 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:04,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:04,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:05,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:07,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:07,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:07,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:08,884 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:12,033 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:12,158 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:12,158 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:13,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:16,265 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:16,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:16,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:17,526 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:20,665 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:20,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:20,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:21,965 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:25,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:25,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:25,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:26,172 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:29,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:29,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:29,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:30,421 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:32:33,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:33,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:33,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:34,584 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:37,697 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:37,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:37,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:38,538 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:41,684 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:41,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:41,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:42,839 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:32:45,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:45,939 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:45,939 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:47,122 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:50,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:50,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:50,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:51,302 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:32:54,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:54,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:54,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:55,415 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:32:58,661 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:58,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:58,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:59,911 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:03,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:03,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:03,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:04,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:33:07,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:07,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:07,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:08,411 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:33:11,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:11,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:11,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:12,717 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:33:15,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:15,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:15,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:16,937 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:33:20,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:20,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:20,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:21,734 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:33:24,844 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:25,950 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:33:29,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:29,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:29,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:30,202 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:33:33,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:33,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:33,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:34,411 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:33:37,288 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:37,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:37,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:38,441 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:33:41,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:41,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:41,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:42,731 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:33:45,766 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:45,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:45,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:46,996 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:49,984 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:50,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:50,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:51,075 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:54,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:54,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:54,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:55,382 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:58,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:58,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:58,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:59,745 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:34:03,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:04,709 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:34:07,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:07,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:07,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:08,536 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:34:11,519 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:11,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:11,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:12,703 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:34:15,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:15,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:15,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:16,797 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:34:19,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:19,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:19,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:20,745 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:34:23,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:23,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:23,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:24,972 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:34:28,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:28,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:28,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:29,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:34:32,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:32,466 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:32,466 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:33,631 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:34:36,925 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:37,050 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:37,050 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:38,173 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:34:41,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:41,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:41,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:42,138 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:34:45,787 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:45,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:45,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:46,996 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:34:49,922 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:50,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:50,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:51,168 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:34:54,292 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:54,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:54,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:55,524 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:34:58,739 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:58,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:58,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:59,950 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:35:03,020 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:03,036 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:03,036 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:03,908 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:35:07,042 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:07,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:07,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:08,166 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:35:11,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:11,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:11,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:12,484 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:15,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:15,816 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:15,816 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:16,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:20,148 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:20,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:20,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:21,503 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:35:24,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:24,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:24,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:25,602 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:35:29,124 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:29,202 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:29,202 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:30,382 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:35:33,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:33,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:33,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:34,583 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:35:37,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:37,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:37,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:38,423 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:35:41,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:41,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:41,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:42,631 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:35:45,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:45,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:45,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:47,017 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:35:50,132 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:50,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:50,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:51,320 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:54,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:54,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:54,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:55,615 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:58,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:58,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:58,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:59,773 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:36:02,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:02,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:02,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:03,846 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:36:06,814 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:06,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:06,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:07,965 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:36:11,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:11,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:11,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:12,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:36:15,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:15,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:15,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:17,023 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:36:20,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:20,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:20,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:21,156 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:36:24,473 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:24,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:24,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:25,737 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:28,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:29,039 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:29,039 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:30,034 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:36:32,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:34,149 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:37,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:37,316 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:37,316 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:38,341 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:41,326 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:41,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:41,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:42,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:45,600 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:45,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:45,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:46,815 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:36:49,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:50,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:50,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:51,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:36:55,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:55,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:55,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:56,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:36:59,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:59,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:59,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:00,312 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:03,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:03,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:03,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:04,525 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:37:07,648 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:07,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:07,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:08,711 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:11,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:12,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:12,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:12,510 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:15,170 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:15,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:15,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:16,359 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:37:19,458 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:19,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:19,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:20,658 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:37:23,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:23,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:23,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:24,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:37:27,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:28,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:28,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:29,040 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:37:32,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:32,360 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:32,360 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:33,137 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:37:36,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:37,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:37,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:38,110 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:41,136 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:41,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:41,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:42,353 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:37:45,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:45,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:45,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:46,396 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:49,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:49,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:49,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:50,526 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:37:53,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:53,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:53,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:54,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:37:57,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:57,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:57,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:58,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:38:01,969 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:02,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:02,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:02,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:38:05,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:05,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:05,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:06,989 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:38:10,000 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:10,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:10,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:11,129 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:38:14,101 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:14,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:14,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:15,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:38:19,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:19,232 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:19,232 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:20,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:38:23,432 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:23,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:23,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:24,613 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:38:27,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:27,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:27,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:28,619 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:38:31,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:31,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:31,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:32,784 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:38:35,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:35,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:35,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:36,376 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:38:39,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:39,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:39,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:40,262 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:38:43,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:43,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:43,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:44,504 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:38:47,456 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:47,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:47,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:48,560 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:38:51,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:51,797 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:51,797 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:52,795 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:38:56,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:56,091 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:56,091 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:57,123 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:00,918 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:01,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:01,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:02,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:39:05,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:05,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:05,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:06,367 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:39:09,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:09,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:09,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:10,592 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:39:13,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:13,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:13,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:14,532 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:17,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:17,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:17,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:18,567 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:39:21,800 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:21,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:21,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:23,019 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:39:25,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:26,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:26,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:27,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:30,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:30,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:30,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:31,250 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:34,528 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:34,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:34,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:35,721 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:39:38,855 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:38,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:38,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:40,054 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:39:43,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:44,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:44,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:45,001 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:47,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:48,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:48,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:49,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:39:52,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:52,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:52,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:53,510 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:39:56,582 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:56,660 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:56,660 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:57,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:00,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:00,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:00,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:01,976 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:40:04,688 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:04,767 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:04,767 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:05,888 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:40:09,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:09,205 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:09,205 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:10,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:40:13,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:13,573 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:13,573 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:14,567 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:40:17,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:17,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:17,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:18,677 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:21,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:21,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:21,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:22,861 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:40:26,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:26,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:26,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:27,772 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:30,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:30,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:30,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:31,719 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:40:34,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:34,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:34,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:35,913 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:39,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:39,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:39,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:40,240 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:43,317 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:43,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:43,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:44,599 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:40:47,750 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:47,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:47,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:48,937 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:40:52,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:52,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:52,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:53,139 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:40:56,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:56,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:56,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:57,443 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:41:00,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:00,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:00,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:01,557 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:41:04,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:04,625 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:04,625 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:05,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:41:09,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:09,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:09,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:10,282 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:13,265 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:13,345 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:13,345 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:14,278 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:41:17,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:17,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:17,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:18,687 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:41:21,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:21,923 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:21,923 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:23,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:41:26,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:26,190 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:26,190 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:27,299 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:41:30,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:30,606 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:30,606 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:31,702 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:34,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:34,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:34,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:35,981 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:39,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:39,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:39,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:40,234 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:41:43,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:43,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:43,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:44,336 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:47,076 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:47,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:47,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:48,290 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:41:51,797 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:51,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:51,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:53,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:55,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:56,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:56,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:57,247 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:00,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:00,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:00,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:01,447 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:04,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:04,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:04,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:05,477 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:42:08,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:08,599 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:08,599 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:09,630 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:42:12,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:12,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:12,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:13,977 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:42:17,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:17,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:17,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:18,197 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:21,223 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:21,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:21,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:22,410 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:42:25,337 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:25,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:25,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:26,566 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:42:29,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:29,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:29,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:30,830 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:34,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:34,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:34,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:35,546 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:42:37,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:37,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:37,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:38,882 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:42,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:42,166 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:42,166 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:43,212 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:46,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:46,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:46,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:47,568 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:42:50,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:50,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:50,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:51,985 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:42:55,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:55,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:55,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:56,271 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:42:59,342 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:59,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:59,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:00,296 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:43:03,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:03,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:03,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:04,561 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:43:07,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:07,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:07,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:08,349 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:11,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:11,496 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:11,496 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:12,429 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:16,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:16,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:16,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:17,272 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:43:20,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:20,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:20,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:21,335 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:43:24,431 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:24,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:24,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:25,650 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:43:28,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:28,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:28,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:29,606 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:32,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:32,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:32,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:33,839 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:36,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:36,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:36,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:38,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:43:41,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:41,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:41,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:42,293 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:45,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:45,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:45,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:46,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:49,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:49,745 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:49,745 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:50,858 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:43:54,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:54,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:54,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:55,292 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:58,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:58,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:58,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:59,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:44:02,871 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:02,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:02,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:04,048 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:44:06,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:07,093 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:07,093 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:08,247 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:44:11,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:11,449 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:11,449 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:12,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:44:15,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:15,472 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:15,472 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:16,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:44:19,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:19,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:19,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:20,848 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:44:23,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:24,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:24,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:25,117 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:52:29,739 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-09 00:52:38,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:38,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:38,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:39,687 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:52:42,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:42,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:42,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:43,452 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:52:45,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:45,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:45,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:46,240 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:52:48,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:48,593 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:48,593 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:49,041 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:52:51,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:51,486 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:51,486 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:51,937 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:52:54,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:54,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:54,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:55,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:52:58,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:58,146 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:58,146 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:59,084 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:01,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:01,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:01,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:02,419 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:05,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:05,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:05,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:06,133 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:08,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:08,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:08,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:09,712 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:12,113 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:12,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:12,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:13,030 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:53:15,505 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:15,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:15,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:16,440 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:19,220 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:19,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:19,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:20,021 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:22,483 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:22,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:22,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:23,232 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:53:25,213 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:25,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:25,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:26,257 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:28,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:28,800 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:28,800 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:29,376 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:31,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:32,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:32,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:32,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:35,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:35,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:35,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:36,261 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:39,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:39,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:39,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:40,181 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:42,858 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:42,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:42,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:43,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:46,172 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:46,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:46,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:47,124 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:53:50,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:50,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:50,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:50,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:52,325 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:52,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:52,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:52,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:54,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:54,708 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:54,708 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:55,191 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:57,261 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:57,277 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:57,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:57,650 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:59,297 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:59,344 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:59,344 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:59,748 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:54:02,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:02,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:02,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:03,198 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:04,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:04,949 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:04,949 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:05,261 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:06,966 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:06,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:06,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:07,513 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:54:09,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:09,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:09,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:10,201 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:54:12,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:12,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:12,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:13,053 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:15,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:15,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:15,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:15,856 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:17,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:17,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:17,509 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:17,897 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:20,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:20,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:20,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:20,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:54:23,049 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:23,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:23,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:23,871 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:25,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:25,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:25,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:26,676 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:28,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:28,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:28,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:29,790 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:32,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:32,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:32,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:33,671 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:54:36,305 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:36,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:36,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:37,007 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:54:39,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:40,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:40,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:40,953 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:43,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:43,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:43,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:44,757 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:47,754 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:47,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:47,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:48,717 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:51,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:51,602 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:51,602 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:52,536 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:55,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:55,095 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:55,095 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:56,088 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:54:58,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:58,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:58,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:59,244 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-09 00:55:01,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:01,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:01,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:02,487 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:05,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:05,059 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:05,059 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:05,760 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:55:08,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:08,435 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:08,435 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:09,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:55:11,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:11,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:11,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:12,336 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:55:14,268 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:14,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:14,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:15,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:55:17,863 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:17,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:17,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:18,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-09 00:55:21,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:22,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:22,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:22,476 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:55:25,233 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:25,342 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:25,342 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:26,231 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:55:28,944 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:28,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:28,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:29,816 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:32,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:32,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:32,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:33,436 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:55:35,939 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:36,064 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:36,064 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:36,983 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:55:39,490 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:39,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:39,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:40,691 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:55:42,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:43,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:43,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:44,140 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:46,595 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:46,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:46,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:47,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:49,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:49,758 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:49,758 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:50,515 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:55:52,795 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:52,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:52,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:53,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:55:56,461 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:56,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:56,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:57,456 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:00,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:00,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:00,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:00,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:56:03,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:03,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:03,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:04,122 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:56:06,583 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:06,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:06,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:07,474 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:09,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:09,447 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:09,447 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:10,292 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:12,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:12,282 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:12,282 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:13,060 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:16,033 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:16,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:16,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:17,115 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-09 00:56:19,499 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:19,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:19,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:20,037 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:56:21,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:21,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:21,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:22,633 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:25,223 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:25,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:25,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:25,974 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:28,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:28,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:28,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:29,809 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:56:32,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:32,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:32,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:32,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:35,426 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:35,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:35,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:36,305 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:38,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:38,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:38,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:39,689 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:56:41,732 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:41,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:41,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:42,790 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:45,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:45,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:45,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:46,262 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:48,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:48,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:48,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:49,005 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-09 00:56:50,995 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:51,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:51,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:51,386 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:53,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:53,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:53,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:54,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 01:03:15,133 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-09 01:03:24,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 01:03:24,339 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 01:03:24,339 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 01:03:25,307 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-09 01:10:49,985 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-09 01:10:58,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 01:10:58,393 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 01:10:58,393 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 01:10:59,444 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-07-20 15:38:16,248 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-20 15:38:16,251 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:31:15,687 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:31:15,706 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:31:15,706 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:31:16,742 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:31:16,742 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:31:22,242 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:31:22,304 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:31:23,540 - AirSimEnvLogger - ERROR - Error loading checkpoint: 'policy_state_dict'
2024-07-21 06:31:25,316 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:31:29,083 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:31:29,675 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:31:35,887 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:31:35,919 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 06:34:42,958 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:34:42,975 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:34:42,975 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:34:43,966 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:34:43,967 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:34:49,846 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:34:49,907 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:34:51,162 - AirSimEnvLogger - ERROR - Error loading checkpoint: 'policy_state_dict'
2024-07-21 06:34:52,487 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:34:56,834 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:34:57,481 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:35:03,906 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:35:04,000 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 06:50:56,343 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:50:56,363 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:50:56,363 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:50:57,559 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:50:57,560 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:51:03,922 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:51:03,951 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:51:05,319 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 06:51:07,430 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:51:11,742 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:51:12,275 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:51:19,043 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:51:19,105 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 06:54:37,293 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:54:37,320 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:54:37,320 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:54:38,757 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:54:38,758 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:55:04,232 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:55:04,322 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:55:05,745 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 06:55:07,496 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:55:12,282 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:55:12,860 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:55:19,110 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:55:19,172 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:03:23,586 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:03:23,607 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:03:23,608 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:03:24,774 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:03:24,774 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:03:31,289 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:03:31,352 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:03:33,041 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:03:35,281 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:03:40,609 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:03:41,217 - AirSimEnvLogger - ERROR - Error in training loop: float() argument must be a string or a number, not 'dict'
2024-07-21 07:03:41,217 - AirSimEnvLogger - ERROR - An error occurred: local variable 'observation' referenced before assignment
2024-07-21 07:03:47,683 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:03:47,778 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:07:50,211 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:07:50,232 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:07:50,233 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:07:51,402 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:07:51,403 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:07:58,207 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:07:58,302 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:07:59,831 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:08:01,840 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:08:06,771 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:08:07,352 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:08:07,352 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:08:07,352 - AirSimEnvLogger - ERROR - Error in training loop: float() argument must be a string or a number, not 'dict'
2024-07-21 07:08:07,352 - AirSimEnvLogger - ERROR - Current epoch: 1
2024-07-21 07:08:07,353 - AirSimEnvLogger - ERROR - An error occurred: local variable 'episode_steps' referenced before assignment
2024-07-21 07:08:14,106 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:08:14,169 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:10:27,692 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:10:27,710 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:10:27,711 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:10:28,741 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:10:28,742 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:10:35,298 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:10:35,392 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:10:36,948 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:10:39,363 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:10:44,614 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:10:45,255 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:10:45,255 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:10:45,255 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:10:51,454 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:10:51,517 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:21:12,654 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:21:12,689 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:21:12,689 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:21:13,790 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:21:13,790 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:21:19,878 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:21:19,957 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:21:21,435 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:21:23,888 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:21:28,274 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:21:28,918 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:21:28,918 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:21:28,918 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:21:35,195 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:21:35,259 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:28:09,103 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:28:09,121 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:28:09,122 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:28:10,422 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:28:10,423 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:28:17,156 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:28:17,259 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:28:18,947 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:28:21,436 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:28:25,943 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:28:26,474 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:28:26,474 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:28:26,475 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:28:32,895 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:28:33,022 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:32:03,725 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:32:03,743 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:32:03,744 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:32:04,843 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:32:04,844 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:32:11,243 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:32:11,244 - AirSimEnvLogger - ERROR - An error occurred: train_agents() takes 4 positional arguments but 5 were given
2024-07-21 07:32:12,043 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:32:12,152 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:37:34,633 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:37:34,650 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:37:34,650 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:37:35,830 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:37:35,832 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:37:42,131 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:37:47,319 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:37:47,902 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:37:47,902 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:37:47,903 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:37:53,603 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:37:53,667 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:40:37,945 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:40:37,967 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:40:37,967 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:40:39,150 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:40:39,152 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:40:45,517 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:40:50,175 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:40:50,740 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:40:50,740 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:40:50,741 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:40:56,388 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:40:56,482 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:45:41,291 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:45:41,310 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:45:41,311 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:45:42,522 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:45:42,524 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:45:49,137 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:45:54,620 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:45:55,234 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:45:55,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:45:55,234 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:46:00,896 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:46:00,958 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:49:26,101 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:49:26,122 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:49:26,123 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:49:27,149 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:49:27,151 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:49:34,092 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:49:34,092 - AirSimEnvLogger - ERROR - Error in reset function: 'AirSimEnv' object has no attribute '_reset_env'
2024-07-21 07:49:34,092 - AirSimEnvLogger - ERROR - An error occurred: 'AirSimEnv' object has no attribute '_reset_env'
2024-07-21 07:49:34,921 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:49:34,983 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 12:08:47,964 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 12:08:47,981 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 12:08:47,982 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 12:08:49,466 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 12:08:49,467 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 12:08:54,925 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 12:09:00,593 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 12:09:01,033 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 12:09:01,033 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 12:09:01,096 - AirSimEnvLogger - ERROR - Error during training: 'training'
2024-07-21 12:09:01,096 - AirSimEnvLogger - ERROR - An error occurred in main: 'training'
2024-07-21 12:09:06,772 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 12:09:06,868 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 19:18:38,383 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:18:38,399 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:18:38,400 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:18:39,887 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:18:39,888 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:18:46,083 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:18:50,956 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:18:51,522 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:18:51,522 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:18:53,156 - AirSimEnvLogger - ERROR - Error during training: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-21 19:18:53,156 - AirSimEnvLogger - ERROR - An error occurred in main: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-21 19:18:58,848 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:18:58,879 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 19:30:14,959 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:30:14,977 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:30:14,977 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:30:16,447 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:30:16,447 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:30:23,189 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:30:28,312 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:30:28,938 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:30:28,938 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:30:30,145 - AirSimEnvLogger - ERROR - Error during training: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 3, 144] to have 3 channels, but got 256 channels instead
2024-07-21 19:30:30,145 - AirSimEnvLogger - ERROR - An error occurred in main: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 3, 144] to have 3 channels, but got 256 channels instead
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 207, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 93, in forward
    visual_features = self.cnn(visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 35, in forward
    x = self.initial_relu(self.initial_conv(x))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 3, 144] to have 3 channels, but got 256 channels instead
2024-07-21 19:30:36,000 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:30:36,095 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:33:49,411 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:33:49,427 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:33:49,428 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:33:50,950 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:33:50,952 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:33:57,254 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:34:02,107 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:34:02,704 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:34:02,704 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:34:02,847 - AirSimEnvLogger - INFO - Visual tensor shape after permute: torch.Size([1, 256, 3, 144])
2024-07-21 19:34:04,027 - AirSimEnvLogger - ERROR - Error during training: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:34:04,027 - AirSimEnvLogger - ERROR - An error occurred in main: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 328, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 211, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 93, in forward
    visual_features = self.cnn(visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 36, in forward
    x = self.initial_relu(self.initial_conv(x))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:34:09,549 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:34:09,675 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:36:28,801 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:36:28,817 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:36:28,818 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:36:30,189 - AirSimEnvLogger - ERROR - Error initializing policy network: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:36:30,190 - AirSimEnvLogger - ERROR - An error occurred in main: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 107, in main
    env = AirSimEnv(
  File "e:\Project\Drone\source\envs\airsim_env.py", line 133, in __init__
    self._initialize_policy_network(config)
  File "e:\Project\Drone\source\envs\airsim_env.py", line 200, in _initialize_policy_network
    self.policy_network = AdvancedPolicyNetwork(
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 60, in __init__
    cnn_out = self.cnn(dummy_input)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 31, in forward
    x = self.initial_relu(self.initial_conv(x))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:36:30,200 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:41:49,192 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:41:49,207 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:41:49,207 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:41:50,797 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:41:50,799 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:41:56,361 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:42:00,928 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:42:01,553 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:42:01,553 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:42:01,649 - AirSimEnvLogger - INFO - Visual tensor shape after permute: torch.Size([1, 3, 144, 256])
2024-07-21 19:42:03,034 - AirSimEnvLogger - ERROR - Error during training: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:42:03,034 - AirSimEnvLogger - ERROR - An error occurred in main: Tensors must have same number of dimensions: got 3 and 2
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 328, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 211, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 96, in forward
    combined = torch.cat([state_features, visual_features], dim=1)
RuntimeError: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:42:08,614 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:42:08,645 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:44:53,807 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:44:53,822 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:44:53,823 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:44:55,443 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:44:55,445 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:45:01,317 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:45:06,042 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:45:06,576 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:45:06,576 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:45:06,686 - AirSimEnvLogger - INFO - Visual tensor shape after permute: torch.Size([1, 3, 144, 256])
2024-07-21 19:45:08,095 - AirSimEnvLogger - ERROR - Error during training: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:45:08,095 - AirSimEnvLogger - ERROR - An error occurred in main: Tensors must have same number of dimensions: got 3 and 2
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 328, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 211, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 186, in forward
    combined = torch.cat([state_features, visual_features], dim=1)
RuntimeError: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:45:13,809 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:45:13,934 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:56:07,460 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:56:07,476 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:56:07,477 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:56:09,053 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:56:09,053 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:56:15,398 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:56:20,265 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:56:20,894 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:56:20,894 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:56:22,494 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:56:22,494 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 115, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:56:28,190 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:56:28,253 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:57:58,227 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:57:58,248 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:57:58,248 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:57:59,875 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:57:59,876 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:58:06,430 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:58:11,889 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:58:12,487 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:58:12,487 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:58:14,479 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:58:14,479 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 136, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:58:20,514 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:58:20,561 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:00:07,506 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:00:07,523 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:00:07,523 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:00:09,128 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:00:09,128 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:00:15,127 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:00:20,471 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:00:21,110 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:00:21,110 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:00:22,995 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:00:22,995 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 144, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:00:28,668 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 20:00:28,716 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:01:58,690 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:01:58,706 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:01:58,707 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:02:00,352 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:02:00,353 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:02:07,365 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:02:12,764 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:02:13,362 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:02:13,362 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:02:15,272 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:02:15,272 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 136, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:02:20,873 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 20:02:20,951 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:06:04,162 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:06:04,178 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:06:04,178 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:06:05,718 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:06:05,719 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:06:12,628 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:06:17,880 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:06:18,460 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:18,460 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:23,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -8.537314387274387, Velocity: -0.7330309600353913, Collision: 0, Height: -1.0, Movement: -0.052879256010055546, Smoothness: -0.0, Curiosity: 2.823000192642212, Exploration: 0, Total: -7.997617696571982
2024-07-21 20:06:23,792 - AirSimEnvLogger - INFO - Action: [-0.26439628 -0.26439628 -0.26439628 -0.26439628], Velocity: (-0.2643962800502777, -0.2643962800502777, -0.2643962800502777), Duration: 0.5, Reward: -7.997617696571982, Done: False
2024-07-21 20:06:23,896 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:23,896 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:27,318 - AirSimEnvLogger - INFO - Predictive model loss: 0.1573164314031601
2024-07-21 20:06:32,535 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.138746752489784, Velocity: -0.4632007639958223, Collision: 0, Height: -1.0, Movement: -0.019336587190628054, Smoothness: -0.0, Curiosity: 0.9239609837532043, Exploration: 0.18498252261020992, Total: -9.373303796808113
2024-07-21 20:06:32,661 - AirSimEnvLogger - INFO - Action: [0.09668294 0.09668294 0.09668294 0.09668294], Velocity: (0.09668293595314026, 0.09668293595314026, 0.09668293595314026), Duration: 0.5, Reward: -9.373303796808113, Done: False
2024-07-21 20:06:32,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:32,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:35,558 - AirSimEnvLogger - INFO - Predictive model loss: 0.047452572733163834
2024-07-21 20:06:40,362 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.263902943094497, Velocity: -0.13462194965032698, Collision: 0, Height: -1.0, Movement: -0.08582771122455597, Smoothness: -0.0, Curiosity: 1.2977185249328613, Exploration: 0.10438579047764592, Total: -9.170060268480155
2024-07-21 20:06:40,487 - AirSimEnvLogger - INFO - Action: [0.42913856 0.42913856 0.42913856 0.42913856], Velocity: (0.42913855612277985, 0.42913855612277985, 0.42913855612277985), Duration: 0.5, Reward: -9.170060268480155, Done: False
2024-07-21 20:06:40,550 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:40,550 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:43,329 - AirSimEnvLogger - INFO - Predictive model loss: 0.024116137996315956
2024-07-21 20:06:48,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.342956418604631, Velocity: -0.18901729958519944, Collision: 0, Height: -1.0, Movement: -0.11907327324151994, Smoothness: -0.0, Curiosity: 1.9479552507400513, Exploration: 0.058389168184958845, Total: -8.963716936714366
2024-07-21 20:06:48,501 - AirSimEnvLogger - INFO - Action: [0.59536637 0.59536637 0.59536637 0.59536637], Velocity: (0.5953663662075996, 0.5953663662075996, 0.5953663662075996), Duration: 0.5, Reward: -8.963716936714366, Done: False
2024-07-21 20:06:48,596 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:48,596 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:51,538 - AirSimEnvLogger - INFO - Predictive model loss: 0.020372124388813972
2024-07-21 20:06:55,921 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.593620887214255, Velocity: -0.08994530635256104, Collision: 0, Height: -1.0, Movement: -0.047808886319398884, Smoothness: -0.0, Curiosity: 0.43681299686431885, Exploration: 0.033507838106208, Total: -9.918266362969074
2024-07-21 20:06:56,046 - AirSimEnvLogger - INFO - Action: [0.23904443 0.23904443 0.23904443 0.23904443], Velocity: (0.2390444315969944, 0.2390444315969944, 0.2390444315969944), Duration: 0.5, Reward: -9.918266362969074, Done: False
2024-07-21 20:06:56,091 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:56,091 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:59,016 - AirSimEnvLogger - INFO - Predictive model loss: 0.006978273391723633
2024-07-21 20:07:04,028 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.85169826440182, Velocity: -0.018013674955812506, Collision: 0, Height: -1.0, Movement: -0.0063237067312002185, Smoothness: -0.0, Curiosity: 0.04456177353858948, Exploration: 0.05763924644480213, Total: -10.327528736494592
2024-07-21 20:07:04,153 - AirSimEnvLogger - INFO - Action: [-0.03161853 -0.03161853 -0.03161853 -0.03161853], Velocity: (-0.03161853365600109, -0.03161853365600109, -0.03161853365600109), Duration: 0.5, Reward: -10.327528736494592, Done: False
2024-07-21 20:07:04,217 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:04,217 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:07,198 - AirSimEnvLogger - INFO - Predictive model loss: 0.008090468123555183
2024-07-21 20:07:12,180 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.13676828917405, Velocity: -0.12224443177266792, Collision: 0, Height: -1.0, Movement: -0.06689289789646864, Smoothness: -0.0, Curiosity: 0.503887951374054, Exploration: 0.06249215160866182, Total: -10.44013738884127
2024-07-21 20:07:12,275 - AirSimEnvLogger - INFO - Action: [-0.33446449 -0.33446449 -0.33446449 -0.33446449], Velocity: (-0.3344644894823432, -0.3344644894823432, -0.3344644894823432), Duration: 0.5, Reward: -10.44013738884127, Done: False
2024-07-21 20:07:12,322 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:12,322 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:15,105 - AirSimEnvLogger - INFO - Predictive model loss: 0.022139564156532288
2024-07-21 20:07:19,726 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.161139918823583, Velocity: -0.06345651334579028, Collision: 0, Height: -1.0, Movement: -0.03629631316289306, Smoothness: -0.0, Curiosity: 0.19075673818588257, Exploration: 0.033050901826732565, Total: -10.594509257354481
2024-07-21 20:07:19,805 - AirSimEnvLogger - INFO - Action: [-0.18148157 -0.18148157 -0.18148157 -0.18148157], Velocity: (-0.18148156581446528, -0.18148156581446528, -0.18148156581446528), Duration: 0.5, Reward: -10.594509257354481, Done: False
2024-07-21 20:07:19,868 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:19,868 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:22,792 - AirSimEnvLogger - INFO - Predictive model loss: 0.020095307379961014
2024-07-21 20:07:27,763 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.076307145557221, Velocity: -0.007550631577839004, Collision: 0, Height: -1.0, Movement: -0.0028453450184315445, Smoothness: -0.0, Curiosity: 0.03463318198919296, Exploration: 0.015854131799084174, Total: -10.55987957849357
2024-07-21 20:07:27,921 - AirSimEnvLogger - INFO - Action: [-0.01422673 -0.01422673 -0.01422673 -0.01422673], Velocity: (-0.014226725092157722, -0.014226725092157722, -0.014226725092157722), Duration: 0.5, Reward: -10.55987957849357, Done: False
2024-07-21 20:07:28,015 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:28,015 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:30,893 - AirSimEnvLogger - INFO - Predictive model loss: 0.011675531975924969
2024-07-21 20:07:35,891 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.235270210211043, Velocity: -0.11141033527169202, Collision: 0, Height: -1.0, Movement: -0.06721836233045907, Smoothness: -0.0, Curiosity: 0.5587345361709595, Exploration: 0.01319519047892635, Total: -10.515690907898671
2024-07-21 20:07:36,016 - AirSimEnvLogger - INFO - Action: [-0.33609181 -0.33609181 -0.33609181 -0.33609181], Velocity: (-0.3360918116522953, -0.3360918116522953, -0.3360918116522953), Duration: 0.5, Reward: -10.515690907898671, Done: False
2024-07-21 20:07:36,079 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:36,079 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:39,011 - AirSimEnvLogger - INFO - Predictive model loss: 0.020203847438097
2024-07-21 20:07:43,855 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.427661354344618, Velocity: -0.20764629656550257, Collision: 0, Height: -1.0, Movement: -0.10976859879447148, Smoothness: -0.0, Curiosity: 1.521005630493164, Exploration: 0.04910039929700762, Total: -10.272138467400833
2024-07-21 20:07:43,963 - AirSimEnvLogger - INFO - Action: [-0.54884299 -0.54884299 -0.54884299 -0.54884299], Velocity: (-0.5488429939723574, -0.5488429939723574, -0.5488429939723574), Duration: 0.5, Reward: -10.272138467400833, Done: False
2024-07-21 20:07:44,025 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:44,025 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:46,930 - AirSimEnvLogger - INFO - Predictive model loss: 0.035912878811359406
2024-07-21 20:07:51,877 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.110416689522078, Velocity: -0.04088284619712446, Collision: 0, Height: -1.0, Movement: -0.02127511823200621, Smoothness: -0.0, Curiosity: 0.08999551832675934, Exploration: 0.021996356756950948, Total: -10.58358859392907
2024-07-21 20:07:52,018 - AirSimEnvLogger - INFO - Action: [0.10637559 0.10637559 0.10637559 0.10637559], Velocity: (0.10637559116003104, 0.10637559116003104, 0.10637559116003104), Duration: 0.5, Reward: -10.58358859392907, Done: False
2024-07-21 20:07:52,080 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:52,080 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:55,003 - AirSimEnvLogger - INFO - Predictive model loss: 0.006613536272197962
2024-07-21 20:08:00,046 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.192742839306845, Velocity: -0.07799967031893497, Collision: 0, Height: -1.0, Movement: -0.04679782825696748, Smoothness: -0.0, Curiosity: 0.309664785861969, Exploration: 0.028181478822430963, Total: -10.57595376859654
2024-07-21 20:08:00,205 - AirSimEnvLogger - INFO - Action: [-0.23398914 -0.23398914 -0.23398914 -0.23398914], Velocity: (-0.2339891412848374, -0.2339891412848374, -0.2339891412848374), Duration: 0.5, Reward: -10.57595376859654, Done: False
2024-07-21 20:08:00,299 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:00,299 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:03,205 - AirSimEnvLogger - INFO - Predictive model loss: 0.010729952715337276
2024-07-21 20:08:08,035 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.04895934672994, Velocity: -0.022373053732730415, Collision: 0, Height: -1.0, Movement: -0.010521877616702114, Smoothness: -0.0, Curiosity: 0.03870673477649689, Exploration: 0.007523994124849115, Total: -10.54033989514476
2024-07-21 20:08:08,113 - AirSimEnvLogger - INFO - Action: [0.05260939 0.05260939 0.05260939 0.05260939], Velocity: (0.052609388083510567, 0.052609388083510567, 0.052609388083510567), Duration: 0.5, Reward: -10.54033989514476, Done: False
2024-07-21 20:08:08,159 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:08,159 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:10,812 - AirSimEnvLogger - INFO - Predictive model loss: 0.002762722549960017
2024-07-21 20:08:15,824 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.121515681664402, Velocity: -0.057341753136278005, Collision: 0, Height: -1.0, Movement: -0.03251039047536324, Smoothness: -0.0, Curiosity: 0.16320854425430298, Exploration: 0.007525199103813907, Total: -10.570328285332161
2024-07-21 20:08:15,840 - AirSimEnvLogger - INFO - Action: [-0.16255195 -0.16255195 -0.16255195 -0.16255195], Velocity: (-0.1625519523768162, -0.1625519523768162, -0.1625519523768162), Duration: 0.5, Reward: -10.570328285332161, Done: False
2024-07-21 20:08:15,903 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:15,903 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:18,783 - AirSimEnvLogger - INFO - Predictive model loss: 0.005474381148815155
2024-07-21 20:08:23,932 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.137703405730269, Velocity: -0.04624782810507101, Collision: 0, Height: -1.0, Movement: -0.028796855904147378, Smoothness: -0.0, Curiosity: 0.14771482348442078, Exploration: 0.014900672771422303, Total: -10.586869459076723
2024-07-21 20:08:24,121 - AirSimEnvLogger - INFO - Action: [-0.14398428 -0.14398428 -0.14398428 -0.14398428], Velocity: (-0.1439842795207369, -0.1439842795207369, -0.1439842795207369), Duration: 0.5, Reward: -10.586869459076723, Done: False
2024-07-21 20:08:24,183 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:24,183 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:27,185 - AirSimEnvLogger - INFO - Predictive model loss: 0.005294164642691612
2024-07-21 20:08:32,094 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.322962974252313, Velocity: -0.14574575869552195, Collision: 0, Height: -1.0, Movement: -0.08745696758105624, Smoothness: -0.0, Curiosity: 1.003562569618225, Exploration: 0.02714718069176819, Total: -10.397370829410713
2024-07-21 20:08:32,219 - AirSimEnvLogger - INFO - Action: [-0.43728484 -0.43728484 -0.43728484 -0.43728484], Velocity: (-0.43728483790528117, -0.43728483790528117, -0.43728483790528117), Duration: 0.5, Reward: -10.397370829410713, Done: False
2024-07-21 20:08:32,345 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:32,345 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:35,289 - AirSimEnvLogger - INFO - Predictive model loss: 0.017199214547872543
2024-07-21 20:08:40,340 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.03519470477193, Velocity: -0.05305891114288633, Collision: 0, Height: -1.0, Movement: -0.03243093383871383, Smoothness: -0.0, Curiosity: 0.13918259739875793, Exploration: 0.01590590192586549, Total: -10.492194774642693
2024-07-21 20:08:40,479 - AirSimEnvLogger - INFO - Action: [0.16215467 0.16215467 0.16215467 0.16215467], Velocity: (0.16215466919356913, 0.16215466919356913, 0.16215466919356913), Duration: 0.5, Reward: -10.492194774642693, Done: False
2024-07-21 20:08:40,541 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:40,541 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:43,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.0011661252938210964
2024-07-21 20:08:48,653 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.17387330122136, Velocity: -0.10402691856872726, Collision: 0, Height: -1.0, Movement: -0.05047677212673989, Smoothness: -0.0, Curiosity: 0.3742421269416809, Exploration: 0.01641164932399651, Total: -10.540531044382758
2024-07-21 20:08:48,810 - AirSimEnvLogger - INFO - Action: [-0.25238386 -0.25238386 -0.25238386 -0.25238386], Velocity: (-0.25238386063369944, -0.25238386063369944, -0.25238386063369944), Duration: 0.5, Reward: -10.540531044382758, Done: False
2024-07-21 20:08:48,903 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:48,903 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:51,879 - AirSimEnvLogger - INFO - Predictive model loss: 0.007580197881907225
2024-07-21 20:08:56,829 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.936615692049871, Velocity: -0.07982571116182884, Collision: 0, Height: -1.0, Movement: -0.045333888859954645, Smoothness: -0.0, Curiosity: 0.23741808533668518, Exploration: 0.010344801492369032, Total: -10.360283933549965
2024-07-21 20:08:56,955 - AirSimEnvLogger - INFO - Action: [0.22666944 0.22666944 0.22666944 0.22666944], Velocity: (0.2266694442997732, 0.2266694442997732, 0.2266694442997732), Duration: 0.5, Reward: -10.360283933549965, Done: False
2024-07-21 20:08:57,003 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:57,003 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:00,027 - AirSimEnvLogger - INFO - Predictive model loss: 0.00047663264558650553
2024-07-21 20:09:05,059 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.75693772518964, Velocity: -0.1152243483962964, Collision: 0, Height: -1.0, Movement: -0.07558094398747245, Smoothness: -0.0, Curiosity: 0.6528724431991577, Exploration: 0.05430436177575783, Total: -9.984810899831807
2024-07-21 20:09:05,202 - AirSimEnvLogger - INFO - Action: [0.37790472 0.37790472 0.37790472 0.37790472], Velocity: (0.3779047199373622, 0.3779047199373622, 0.3779047199373622), Duration: 0.5, Reward: -9.984810899831807, Done: False
2024-07-21 20:09:05,264 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:05,264 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:08,080 - AirSimEnvLogger - INFO - Predictive model loss: 0.0035763317719101906
2024-07-21 20:09:12,371 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.870647094891332, Velocity: -0.018541081063435924, Collision: 0, Height: -1.0, Movement: -0.012108386241249037, Smoothness: -0.0, Curiosity: 0.0430259145796299, Exploration: 0.009408474699336875, Total: -10.357733821817494
2024-07-21 20:09:12,495 - AirSimEnvLogger - INFO - Action: [0.06054193 0.06054193 0.06054193 0.06054193], Velocity: (0.06054193120624518, 0.06054193120624518, 0.06054193120624518), Duration: 0.5, Reward: -10.357733821817494, Done: False
2024-07-21 20:09:12,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:12,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:15,471 - AirSimEnvLogger - INFO - Predictive model loss: 0.0008268192177638412
2024-07-21 20:09:20,326 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.706846201996552, Velocity: -0.12639366523603007, Collision: 0, Height: -1.0, Movement: -0.08221361074986647, Smoothness: -0.0, Curiosity: 0.7955585718154907, Exploration: 0.009018043110380595, Total: -9.878681501159733
2024-07-21 20:09:20,436 - AirSimEnvLogger - INFO - Action: [0.41106805 0.41106805 0.41106805 0.41106805], Velocity: (0.4110680537493323, 0.4110680537493323, 0.4110680537493323), Duration: 0.5, Reward: -9.878681501159733, Done: False
2024-07-21 20:09:20,500 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:20,500 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:23,478 - AirSimEnvLogger - INFO - Predictive model loss: 0.006337056402117014
2024-07-21 20:09:28,605 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.812811876067942, Velocity: -0.037371853016888515, Collision: 0, Height: -1.0, Movement: -0.024520853331399906, Smoothness: -0.0, Curiosity: 0.10540065914392471, Exploration: 0.010494075428823826, Total: -10.2791507432518
2024-07-21 20:09:28,714 - AirSimEnvLogger - INFO - Action: [0.12260427 0.12260427 0.12260427 0.12260427], Velocity: (0.12260426665699953, 0.12260426665699953, 0.12260426665699953), Duration: 0.5, Reward: -10.2791507432518, Done: False
2024-07-21 20:09:28,808 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:28,808 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:31,673 - AirSimEnvLogger - INFO - Predictive model loss: 0.002043942455202341
2024-07-21 20:09:36,313 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.820585561846329, Velocity: -0.05688574063079837, Collision: 0, Height: -1.0, Movement: -0.03559161135093021, Smoothness: -0.0, Curiosity: 0.17695245146751404, Exploration: 0.013938800446397821, Total: -10.261323607473784
2024-07-21 20:09:36,437 - AirSimEnvLogger - INFO - Action: [0.17795806 0.17795806 0.17795806 0.17795806], Velocity: (0.17795805675465104, 0.17795805675465104, 0.17795805675465104), Duration: 0.5, Reward: -10.261323607473784, Done: False
2024-07-21 20:09:36,530 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:36,530 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:39,549 - AirSimEnvLogger - INFO - Predictive model loss: 0.002339982194826007
2024-07-21 20:09:44,609 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.654983770096472, Velocity: -0.13697055018309698, Collision: 0, Height: -1.0, Movement: -0.09395522330470706, Smoothness: -0.0, Curiosity: 1.0762226581573486, Exploration: 0.026359410192032157, Total: -9.689481356401409
2024-07-21 20:09:44,718 - AirSimEnvLogger - INFO - Action: [0.46977612 0.46977612 0.46977612 0.46977612], Velocity: (0.46977611652353524, 0.46977611652353524, 0.46977611652353524), Duration: 0.5, Reward: -9.689481356401409, Done: False
2024-07-21 20:09:44,779 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:44,779 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:47,761 - AirSimEnvLogger - INFO - Predictive model loss: 0.010871022939682007
2024-07-21 20:09:52,363 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.87125544117941, Velocity: -0.012601288137176552, Collision: 0, Height: -1.0, Movement: -0.0024559385862557904, Smoothness: -0.0, Curiosity: 0.03256514295935631, Exploration: 0.008419612672315184, Total: -10.359835185092482
2024-07-21 20:09:52,472 - AirSimEnvLogger - INFO - Action: [-0.01227969 -0.01227969 -0.01227969 -0.01227969], Velocity: (-0.012279692931278952, -0.012279692931278952, -0.012279692931278952), Duration: 0.5, Reward: -10.359835185092482, Done: False
2024-07-21 20:09:52,567 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:52,567 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:55,487 - AirSimEnvLogger - INFO - Predictive model loss: 0.001450380776077509
2024-07-21 20:10:00,690 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.077916448749, Velocity: -0.07401189265829702, Collision: 0, Height: -1.0, Movement: -0.043867013220320585, Smoothness: -0.0, Curiosity: 0.2244262844324112, Exploration: 0.05340637030443451, Total: -10.496414680123088
2024-07-21 20:10:00,815 - AirSimEnvLogger - INFO - Action: [-0.21933507 -0.21933507 -0.21933507 -0.21933507], Velocity: (-0.21933506610160292, -0.21933506610160292, -0.21933506610160292), Duration: 0.5, Reward: -10.496414680123088, Done: False
2024-07-21 20:10:00,924 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:00,924 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:03,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.00197173235937953
2024-07-21 20:10:07,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.325555908262503, Velocity: -0.18497227716760323, Collision: 0, Height: -1.0, Movement: -0.09809292423940225, Smoothness: -0.0, Curiosity: 1.1639407873153687, Exploration: 0.05412164251623195, Total: -10.335056617109313
2024-07-21 20:10:08,100 - AirSimEnvLogger - INFO - Action: [-0.49046462 -0.49046462 -0.49046462 -0.49046462], Velocity: (-0.4904646211970112, -0.4904646211970112, -0.4904646211970112), Duration: 0.5, Reward: -10.335056617109313, Done: False
2024-07-21 20:10:08,162 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:08,163 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:10,447 - AirSimEnvLogger - INFO - Predictive model loss: 0.013175143860280514
2024-07-21 20:10:15,409 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.050836749993959, Velocity: -0.05801876976822896, Collision: 0, Height: -1.0, Movement: -0.027112955509540827, Smoothness: -0.0, Curiosity: 0.10841643810272217, Exploration: 0.01557023015487841, Total: -10.52523516534669
2024-07-21 20:10:15,502 - AirSimEnvLogger - INFO - Action: [0.13556478 0.13556478 0.13556478 0.13556478], Velocity: (0.13556477754770413, 0.13556477754770413, 0.13556477754770413), Duration: 0.5, Reward: -10.52523516534669, Done: False
2024-07-21 20:10:15,581 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:15,581 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:18,504 - AirSimEnvLogger - INFO - Predictive model loss: 0.0012343639973551035
2024-07-21 20:10:22,964 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.051195935506616, Velocity: -0.015405424003432377, Collision: 0, Height: -1.0, Movement: -0.008186325103582248, Smoothness: -0.0, Curiosity: 0.03148689121007919, Exploration: 0.03574698115764957, Total: -10.536824438182121
2024-07-21 20:10:23,041 - AirSimEnvLogger - INFO - Action: [-0.04093163 -0.04093163 -0.04093163 -0.04093163], Velocity: (-0.04093162551791124, -0.04093162551791124, -0.04093162551791124), Duration: 0.5, Reward: -10.536824438182121, Done: False
2024-07-21 20:10:23,119 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:23,119 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:26,023 - AirSimEnvLogger - INFO - Predictive model loss: 0.000905893393792212
2024-07-21 20:10:30,980 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.81209494332104, Velocity: -0.11308257310379843, Collision: 0, Height: -1.0, Movement: -0.07206625507745083, Smoothness: -0.0, Curiosity: 0.6050204634666443, Exploration: 0.026789909028163267, Total: -10.06797464184173
2024-07-21 20:10:31,135 - AirSimEnvLogger - INFO - Action: [0.36033128 0.36033128 0.36033128 0.36033128], Velocity: (0.3603312753872541, 0.3603312753872541, 0.3603312753872541), Duration: 0.5, Reward: -10.06797464184173, Done: False
2024-07-21 20:10:31,197 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:31,197 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:34,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.003806555410847068
2024-07-21 20:10:39,235 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.603098258845964, Velocity: -0.16834931568663722, Collision: 0, Height: -1.0, Movement: -0.11219254516796735, Smoothness: -0.0, Curiosity: 1.533277153968811, Exploration: 0.057143269449527856, Total: -9.420424940331769
2024-07-21 20:10:39,377 - AirSimEnvLogger - INFO - Action: [0.56096273 0.56096273 0.56096273 0.56096273], Velocity: (0.5609627258398368, 0.5609627258398368, 0.5609627258398368), Duration: 0.5, Reward: -9.420424940331769, Done: False
2024-07-21 20:10:39,440 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:39,440 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:42,448 - AirSimEnvLogger - INFO - Predictive model loss: 0.014840662479400635
2024-07-21 20:10:47,421 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.876252912961696, Velocity: -0.024749882508368713, Collision: 0, Height: -1.0, Movement: -0.011179066781021808, Smoothness: -0.0, Curiosity: 0.046928297728300095, Exploration: 0.016875258957369476, Total: -10.36290656023836
2024-07-21 20:10:47,545 - AirSimEnvLogger - INFO - Action: [-0.05589533 -0.05589533 -0.05589533 -0.05589533], Velocity: (-0.055895333905109035, -0.055895333905109035, -0.055895333905109035), Duration: 0.5, Reward: -10.36290656023836, Done: False
2024-07-21 20:10:47,607 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:47,607 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:50,604 - AirSimEnvLogger - INFO - Predictive model loss: 0.0012741959653794765
2024-07-21 20:10:55,615 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.742665508643187, Velocity: -0.11054951401511699, Collision: 0, Height: -1.0, Movement: -0.07056988423873105, Smoothness: -0.0, Curiosity: 0.6529353857040405, Exploration: 0.015518176209379031, Total: -9.975425925980721
2024-07-21 20:10:55,740 - AirSimEnvLogger - INFO - Action: [0.35284942 0.35284942 0.35284942 0.35284942], Velocity: (0.3528494211936552, 0.3528494211936552, 0.3528494211936552), Duration: 0.5, Reward: -9.975425925980721, Done: False
2024-07-21 20:10:55,834 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:55,834 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:58,748 - AirSimEnvLogger - INFO - Predictive model loss: 0.007610654458403587
2024-07-21 20:11:03,528 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.579947796083099, Velocity: -0.1646349071836375, Collision: 0, Height: -1.0, Movement: -0.11056218716407379, Smoothness: -0.0, Curiosity: 1.5818194150924683, Exploration: 0.04625992776621783, Total: -9.373159775291846
2024-07-21 20:11:03,654 - AirSimEnvLogger - INFO - Action: [0.55281094 0.55281094 0.55281094 0.55281094], Velocity: (0.5528109358203689, 0.5528109358203689, 0.5528109358203689), Duration: 0.5, Reward: -9.373159775291846, Done: False
2024-07-21 20:11:03,670 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:03,670 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:06,516 - AirSimEnvLogger - INFO - Predictive model loss: 0.019102804362773895
2024-07-21 20:11:11,430 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.893659588325779, Velocity: -0.040865662350957836, Collision: 0, Height: -1.0, Movement: -0.02087832404720505, Smoothness: -0.0, Curiosity: 0.09950833022594452, Exploration: 0.02337617293020826, Total: -10.361750852206963
2024-07-21 20:11:11,556 - AirSimEnvLogger - INFO - Action: [-0.10439162 -0.10439162 -0.10439162 -0.10439162], Velocity: (-0.10439162023602522, -0.10439162023602522, -0.10439162023602522), Duration: 0.5, Reward: -10.361750852206963, Done: False
2024-07-21 20:11:11,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:11,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:14,571 - AirSimEnvLogger - INFO - Predictive model loss: 0.0022179498337209225
2024-07-21 20:11:19,475 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.814724449187755, Velocity: -0.08470758805561489, Collision: 0, Height: -1.0, Movement: -0.04825679802125832, Smoothness: -0.0, Curiosity: 0.3820393681526184, Exploration: 0.026264885839294386, Total: -10.165631261773521
2024-07-21 20:11:19,479 - AirSimEnvLogger - INFO - Action: [0.24128399 0.24128399 0.24128399 0.24128399], Velocity: (0.2412839901062916, 0.2412839901062916, 0.2412839901062916), Duration: 0.5, Reward: -10.165631261773521, Done: False
2024-07-21 20:11:19,554 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:19,554 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:22,606 - AirSimEnvLogger - INFO - Predictive model loss: 0.006475016474723816
2024-07-21 20:11:27,339 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.738447940911119, Velocity: -0.10156185692976163, Collision: 0, Height: -1.0, Movement: -0.06598722409817646, Smoothness: -0.0, Curiosity: 0.6714843511581421, Exploration: 0.029762302094994517, Total: -9.954132955787747
2024-07-21 20:11:27,464 - AirSimEnvLogger - INFO - Action: [0.32993612 0.32993612 0.32993612 0.32993612], Velocity: (0.3299361204908823, 0.3299361204908823, 0.3299361204908823), Duration: 0.5, Reward: -9.954132955787747, Done: False
2024-07-21 20:11:27,527 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:27,527 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:30,609 - AirSimEnvLogger - INFO - Predictive model loss: 0.010447682812809944
2024-07-21 20:11:35,092 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.585865115491822, Velocity: -0.17252473717466008, Collision: 0, Height: -1.0, Movement: -0.10915302967833017, Smoothness: -0.0, Curiosity: 1.6628444194793701, Exploration: 0.03211217327321446, Total: -9.345198142652656
2024-07-21 20:11:35,171 - AirSimEnvLogger - INFO - Action: [0.54576515 0.54576515 0.54576515 0.54576515], Velocity: (0.5457651483916508, 0.5457651483916508, 0.5457651483916508), Duration: 0.5, Reward: -9.345198142652656, Done: False
2024-07-21 20:11:35,251 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:35,251 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:37,810 - AirSimEnvLogger - INFO - Predictive model loss: 0.02272721566259861
2024-07-21 20:11:42,809 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.899046214893819, Velocity: -0.042202524202264974, Collision: 0, Height: -1.0, Movement: -0.021582902790076866, Smoothness: -0.0, Curiosity: 0.16947093605995178, Exploration: 0.02306291456963665, Total: -10.332957716330055
2024-07-21 20:11:42,918 - AirSimEnvLogger - INFO - Action: [-0.10791451 -0.10791451 -0.10791451 -0.10791451], Velocity: (-0.10791451395038432, -0.10791451395038432, -0.10791451395038432), Duration: 0.5, Reward: -10.332957716330055, Done: False
2024-07-21 20:11:42,981 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:42,981 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:46,012 - AirSimEnvLogger - INFO - Predictive model loss: 0.004509173799306154
2024-07-21 20:11:51,153 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.80524767350589, Velocity: -0.08533424360890268, Collision: 0, Height: -1.0, Movement: -0.05383301907729739, Smoothness: -0.0, Curiosity: 0.5657804012298584, Exploration: 0.0243293672472143, Total: -10.065542023153698
2024-07-21 20:11:51,264 - AirSimEnvLogger - INFO - Action: [0.2691651 0.2691651 0.2691651 0.2691651], Velocity: (0.2691650953864869, 0.2691650953864869, 0.2691650953864869), Duration: 0.5, Reward: -10.065542023153698, Done: False
2024-07-21 20:11:51,328 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:51,328 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:54,365 - AirSimEnvLogger - INFO - Predictive model loss: 0.010945504531264305
2024-07-21 20:11:59,531 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.800064169335698, Velocity: -0.06938514377290593, Collision: 0, Height: -1.0, Movement: -0.044412886768339245, Smoothness: -0.0, Curiosity: 0.4855053722858429, Exploration: 0.02265336509812051, Total: -10.091914670736438
2024-07-21 20:11:59,672 - AirSimEnvLogger - INFO - Action: [0.22206443 0.22206443 0.22206443 0.22206443], Velocity: (0.22206443384169622, 0.22206443384169622, 0.22206443384169622), Duration: 0.5, Reward: -10.091914670736438, Done: False
2024-07-21 20:11:59,735 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:59,735 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:02,581 - AirSimEnvLogger - INFO - Predictive model loss: 0.01083226315677166
2024-07-21 20:12:07,585 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.062507079533432, Velocity: -0.08388599707999303, Collision: 0, Height: -1.0, Movement: -0.04756557361793609, Smoothness: -0.0, Curiosity: 0.3509995639324188, Exploration: 0.035468251069486055, Total: -10.426613203255116
2024-07-21 20:12:07,644 - AirSimEnvLogger - INFO - Action: [-0.23782787 -0.23782787 -0.23782787 -0.23782787], Velocity: (-0.23782786808968043, -0.23782786808968043, -0.23782786808968043), Duration: 0.5, Reward: -10.426613203255116, Done: False
2024-07-21 20:12:07,692 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:07,692 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:10,669 - AirSimEnvLogger - INFO - Predictive model loss: 0.004855751059949398
2024-07-21 20:12:15,269 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.027585946137888, Velocity: -0.0037514184604571644, Collision: 0, Height: -1.0, Movement: -0.0005100160017009026, Smoothness: -0.0, Curiosity: 0.16903644800186157, Exploration: 0.025936444033818225, Total: -10.439807144160593
2024-07-21 20:12:15,379 - AirSimEnvLogger - INFO - Action: [0.00255008 0.00255008 0.00255008 0.00255008], Velocity: (0.0025500800085045128, 0.0025500800085045128, 0.0025500800085045128), Duration: 0.5, Reward: -10.439807144160593, Done: False
2024-07-21 20:12:15,442 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:15,442 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:18,356 - AirSimEnvLogger - INFO - Predictive model loss: 0.005023459438234568
2024-07-21 20:12:22,669 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.814077763482898, Velocity: -0.11283345588447247, Collision: 0, Height: -1.0, Movement: -0.07154866005849816, Smoothness: -0.0, Curiosity: 0.8584502339363098, Exploration: 0.03993931835385811, Total: -9.940436376792057
2024-07-21 20:12:22,690 - AirSimEnvLogger - INFO - Action: [0.3577433 0.3577433 0.3577433 0.3577433], Velocity: (0.3577433002924908, 0.3577433002924908, 0.3577433002924908), Duration: 0.5, Reward: -9.940436376792057, Done: False
2024-07-21 20:12:22,750 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:22,750 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:25,692 - AirSimEnvLogger - INFO - Predictive model loss: 0.01053526159375906
2024-07-21 20:12:30,860 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.731093623415655, Velocity: -0.10667315268035014, Collision: 0, Height: -1.0, Movement: -0.0711176611278056, Smoothness: -0.0, Curiosity: 0.9425386190414429, Exploration: 0.03781097507471129, Total: -9.812710461332946
2024-07-21 20:12:30,986 - AirSimEnvLogger - INFO - Action: [0.35558831 0.35558831 0.35558831 0.35558831], Velocity: (0.355588305639028, 0.355588305639028, 0.355588305639028), Duration: 0.5, Reward: -9.812710461332946, Done: False
2024-07-21 20:12:31,049 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:31,049 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:33,997 - AirSimEnvLogger - INFO - Predictive model loss: 0.013047143816947937
2024-07-21 20:12:38,896 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.017552819749577, Velocity: -0.09343034264293022, Collision: 0, Height: -1.0, Movement: -0.04060058706533914, Smoothness: -0.0, Curiosity: 0.34993207454681396, Exploration: 0.0328691635448152, Total: -10.386788179795206
2024-07-21 20:12:39,051 - AirSimEnvLogger - INFO - Action: [-0.20300294 -0.20300294 -0.20300294 -0.20300294], Velocity: (-0.20300293532669567, -0.20300293532669567, -0.20300293532669567), Duration: 0.5, Reward: -10.386788179795206, Done: False
2024-07-21 20:12:39,114 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:39,114 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:42,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.004695258568972349
2024-07-21 20:12:47,232 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.29075315536479, Velocity: -0.1922525109765769, Collision: 0, Height: -1.0, Movement: -0.09645971116191152, Smoothness: -0.0, Curiosity: 1.146602749824524, Exploration: 0.07333395545931484, Total: -10.308557215965145
2024-07-21 20:12:47,389 - AirSimEnvLogger - INFO - Action: [-0.48229856 -0.48229856 -0.48229856 -0.48229856], Velocity: (-0.48229855580955755, -0.48229855580955755, -0.48229855580955755), Duration: 0.5, Reward: -10.308557215965145, Done: False
2024-07-21 20:12:47,496 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:47,496 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:50,438 - AirSimEnvLogger - INFO - Predictive model loss: 0.013835412450134754
2024-07-21 20:12:55,345 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.414753749862886, Velocity: -0.1685142992293708, Collision: 0, Height: -1.0, Movement: -0.09982370166364314, Smoothness: -0.0, Curiosity: 1.2768919467926025, Exploration: 0.04660065095751434, Total: -10.361227166056132
2024-07-21 20:12:55,378 - AirSimEnvLogger - INFO - Action: [-0.49911851 -0.49911851 -0.49911851 -0.49911851], Velocity: (-0.4991185083182157, -0.4991185083182157, -0.4991185083182157), Duration: 0.5, Reward: -10.361227166056132, Done: False
2024-07-21 20:12:55,422 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:55,422 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:58,255 - AirSimEnvLogger - INFO - Predictive model loss: 0.020202213898301125
2024-07-21 20:13:02,950 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.132624181165228, Velocity: -0.031488513688158935, Collision: 0, Height: -1.0, Movement: -0.014760577537700527, Smoothness: -0.0, Curiosity: 0.208810493350029, Exploration: 0.03079660284372764, Total: -10.539279928519319
2024-07-21 20:13:03,123 - AirSimEnvLogger - INFO - Action: [0.07380289 0.07380289 0.07380289 0.07380289], Velocity: (0.07380288768850263, 0.07380288768850263, 0.07380288768850263), Duration: 0.5, Reward: -10.539279928519319, Done: False
2024-07-21 20:13:03,171 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:03,171 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:06,332 - AirSimEnvLogger - INFO - Predictive model loss: 0.003570795990526676
2024-07-21 20:13:11,317 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.230406842709485, Velocity: -0.09408308299567913, Collision: 0, Height: -1.0, Movement: -0.057101011346735064, Smoothness: -0.0, Curiosity: 0.46850448846817017, Exploration: 0.020527454293153603, Total: -10.544800750249282
2024-07-21 20:13:11,505 - AirSimEnvLogger - INFO - Action: [-0.28550506 -0.28550506 -0.28550506 -0.28550506], Velocity: (-0.2855050567336753, -0.2855050567336753, -0.2855050567336753), Duration: 0.5, Reward: -10.544800750249282, Done: False
2024-07-21 20:13:11,568 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:11,568 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:14,386 - AirSimEnvLogger - INFO - Predictive model loss: 0.007740159519016743
2024-07-21 20:13:19,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.407691430566185, Velocity: -0.1888384203430133, Collision: 0, Height: -1.0, Movement: -0.10470992330260948, Smoothness: -0.0, Curiosity: 1.3544540405273438, Exploration: 0.046386839820478275, Total: -10.326077244840185
2024-07-21 20:13:19,633 - AirSimEnvLogger - INFO - Action: [-0.52354962 -0.52354962 -0.52354962 -0.52354962], Velocity: (-0.5235496165130473, -0.5235496165130473, -0.5235496165130473), Duration: 0.5, Reward: -10.326077244840185, Done: False
2024-07-21 20:13:19,694 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:19,694 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:22,357 - AirSimEnvLogger - INFO - Predictive model loss: 0.01973523385822773
2024-07-21 20:13:27,162 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.23907110579186, Velocity: -0.03691320286883909, Collision: 0, Height: -1.0, Movement: -0.023732650297621262, Smoothness: -0.0, Curiosity: 0.1919792890548706, Exploration: 0.006819826776861712, Total: -10.662547362373234
2024-07-21 20:13:27,272 - AirSimEnvLogger - INFO - Action: [-0.11866325 -0.11866325 -0.11866325 -0.11866325], Velocity: (-0.1186632514881063, -0.1186632514881063, -0.1186632514881063), Duration: 0.5, Reward: -10.662547362373234, Done: False
2024-07-21 20:13:27,334 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:27,334 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:30,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.00579258194193244
2024-07-21 20:13:35,097 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.348710978372932, Velocity: -0.1660717384557503, Collision: 0, Height: -1.0, Movement: -0.0828778207390206, Smoothness: -0.0, Curiosity: 0.8812240362167358, Exploration: 0.007216297509041252, Total: -10.497979352064531
2024-07-21 20:13:35,175 - AirSimEnvLogger - INFO - Action: [-0.4143891 -0.4143891 -0.4143891 -0.4143891], Velocity: (-0.41438910369510296, -0.41438910369510296, -0.41438910369510296), Duration: 0.5, Reward: -10.497979352064531, Done: False
2024-07-21 20:13:35,253 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:35,253 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:38,262 - AirSimEnvLogger - INFO - Predictive model loss: 0.01390377152711153
2024-07-21 20:13:43,325 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.252877601997307, Velocity: -0.05812044221228966, Collision: 0, Height: -1.0, Movement: -0.03657860905372584, Smoothness: -0.0, Curiosity: 0.24855385720729828, Exploration: 0.010062892581119287, Total: -10.65930617688895
2024-07-21 20:13:43,451 - AirSimEnvLogger - INFO - Action: [-0.18289305 -0.18289305 -0.18289305 -0.18289305], Velocity: (-0.1828930452686292, -0.1828930452686292, -0.1828930452686292), Duration: 0.5, Reward: -10.65930617688895, Done: False
2024-07-21 20:13:43,528 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:43,528 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:46,408 - AirSimEnvLogger - INFO - Predictive model loss: 0.0060777077451348305
2024-07-21 20:13:51,535 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.938959207588656, Velocity: -0.09370324722385645, Collision: 0, Height: -1.0, Movement: -0.05787011310237903, Smoothness: -0.0, Curiosity: 0.4472541809082031, Exploration: 0.05453490642644248, Total: -10.257063770771433
2024-07-21 20:13:51,709 - AirSimEnvLogger - INFO - Action: [0.28935057 0.28935057 0.28935057 0.28935057], Velocity: (0.28935056551189514, 0.28935056551189514, 0.28935056551189514), Duration: 0.5, Reward: -10.257063770771433, Done: False
2024-07-21 20:13:51,740 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:51,740 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:54,635 - AirSimEnvLogger - INFO - Predictive model loss: 0.0016257950337603688
2024-07-21 20:13:59,789 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.72592684754172, Velocity: -0.1388269182293812, Collision: 0, Height: -1.0, Movement: -0.08925614319631586, Smoothness: -0.0, Curiosity: 1.048414945602417, Exploration: 0.06355031341717164, Total: -9.767348385491399
2024-07-21 20:13:59,915 - AirSimEnvLogger - INFO - Action: [0.44628072 0.44628072 0.44628072 0.44628072], Velocity: (0.44628071598157926, 0.44628071598157926, 0.44628071598157926), Duration: 0.5, Reward: -9.767348385491399, Done: False
2024-07-21 20:14:00,010 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:00,010 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:02,717 - AirSimEnvLogger - INFO - Predictive model loss: 0.009101694449782372
2024-07-21 20:14:07,608 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.790683068979964, Velocity: -0.054572055242688655, Collision: 0, Height: -1.0, Movement: -0.03594282027278905, Smoothness: -0.0, Curiosity: 0.2921631336212158, Exploration: 0.01624362891888706, Total: -10.172233086034202
2024-07-21 20:14:07,735 - AirSimEnvLogger - INFO - Action: [0.1797141 0.1797141 0.1797141 0.1797141], Velocity: (0.17971410136394522, 0.17971410136394522, 0.17971410136394522), Duration: 0.5, Reward: -10.172233086034202, Done: False
2024-07-21 20:14:07,797 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:07,797 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:10,751 - AirSimEnvLogger - INFO - Predictive model loss: 0.004895500838756561
2024-07-21 20:14:15,695 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.088152489207863, Velocity: -0.09581772513933673, Collision: 0, Height: -1.0, Movement: -0.058188007492847416, Smoothness: -0.0, Curiosity: 0.3976612687110901, Exploration: 0.05005988562706856, Total: -10.433037541045858
2024-07-21 20:14:15,869 - AirSimEnvLogger - INFO - Action: [-0.29094004 -0.29094004 -0.29094004 -0.29094004], Velocity: (-0.2909400374642371, -0.2909400374642371, -0.2909400374642371), Duration: 0.5, Reward: -10.433037541045858, Done: False
2024-07-21 20:14:15,947 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:15,947 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:18,905 - AirSimEnvLogger - INFO - Predictive model loss: 0.0011387659469619393
2024-07-21 20:14:23,985 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.273332841501192, Velocity: -0.18976470361453268, Collision: 0, Height: -1.0, Movement: -0.07937796623459814, Smoothness: -0.0, Curiosity: 0.7878140211105347, Exploration: 0.059312111510038894, Total: -10.470383557074644
2024-07-21 20:14:24,111 - AirSimEnvLogger - INFO - Action: [-0.39688983 -0.39688983 -0.39688983 -0.39688983], Velocity: (-0.3968898311729907, -0.3968898311729907, -0.3968898311729907), Duration: 0.5, Reward: -10.470383557074644, Done: False
2024-07-21 20:14:24,190 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:24,190 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:26,985 - AirSimEnvLogger - INFO - Predictive model loss: 0.0068174125626683235
2024-07-21 20:14:31,475 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.404363247315628, Velocity: -0.16104495415744366, Collision: 0, Height: -1.0, Movement: -0.0985057140934038, Smoothness: -0.0, Curiosity: 1.2064435482025146, Exploration: 0.036832470368469795, Total: -10.384148027628738
2024-07-21 20:14:31,615 - AirSimEnvLogger - INFO - Action: [-0.49252857 -0.49252857 -0.49252857 -0.49252857], Velocity: (-0.49252857046701903, -0.49252857046701903, -0.49252857046701903), Duration: 0.5, Reward: -10.384148027628738, Done: False
2024-07-21 20:14:31,677 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:31,677 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:34,598 - AirSimEnvLogger - INFO - Predictive model loss: 0.013640815392136574
2024-07-21 20:14:39,470 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.108095225917783, Velocity: -0.038731602294108956, Collision: 0, Height: -1.0, Movement: -0.020142792386434086, Smoothness: -0.0, Curiosity: 0.11456402391195297, Exploration: 0.02949409347408437, Total: -10.566294475652688
2024-07-21 20:14:39,611 - AirSimEnvLogger - INFO - Action: [0.10071396 0.10071396 0.10071396 0.10071396], Velocity: (0.10071396193217041, 0.10071396193217041, 0.10071396193217041), Duration: 0.5, Reward: -10.566294475652688, Done: False
2024-07-21 20:14:39,673 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:39,673 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:42,597 - AirSimEnvLogger - INFO - Predictive model loss: 0.0009038071730174124
2024-07-21 20:14:47,419 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.09597103875037, Velocity: -0.03048989889334099, Collision: 0, Height: -1.0, Movement: -0.0151959932234776, Smoothness: -0.0, Curiosity: 0.0716477632522583, Exploration: 0.03654637802209319, Total: -10.56960243028884
2024-07-21 20:14:47,466 - AirSimEnvLogger - INFO - Action: [-0.07597997 -0.07597997 -0.07597997 -0.07597997], Velocity: (-0.075979966117388, -0.075979966117388, -0.075979966117388), Duration: 0.5, Reward: -10.56960243028884, Done: False
2024-07-21 20:14:47,567 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:47,568 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:50,226 - AirSimEnvLogger - INFO - Predictive model loss: 0.0006627020193263888
2024-07-21 20:14:55,114 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.858707825206658, Velocity: -0.09906628735629884, Collision: 0, Height: -1.0, Movement: -0.06316814241871775, Smoothness: -0.0, Curiosity: 0.5171423554420471, Exploration: 0.027648064720544097, Total: -10.150456992461546
2024-07-21 20:14:55,208 - AirSimEnvLogger - INFO - Action: [0.31584071 0.31584071 0.31584071 0.31584071], Velocity: (0.3158407120935887, 0.3158407120935887, 0.3158407120935887), Duration: 0.5, Reward: -10.150456992461546, Done: False
2024-07-21 20:14:55,255 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:55,255 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:58,219 - AirSimEnvLogger - INFO - Predictive model loss: 0.0037013832479715347
2024-07-21 20:15:02,422 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.811699270361245, Velocity: -0.06988038661132631, Collision: 0, Height: -1.0, Movement: -0.04702461718552083, Smoothness: -0.0, Curiosity: 0.350069522857666, Exploration: 0.034725815180874246, Total: -10.169362000920453
2024-07-21 20:15:02,562 - AirSimEnvLogger - INFO - Action: [0.23512309 0.23512309 0.23512309 0.23512309], Velocity: (0.23512308592760414, 0.23512308592760414, 0.23512308592760414), Duration: 0.5, Reward: -10.169362000920453, Done: False
2024-07-21 20:15:02,623 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:02,624 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:05,571 - AirSimEnvLogger - INFO - Predictive model loss: 0.004610566888004541
2024-07-21 20:15:10,679 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.081454801899675, Velocity: -0.08601610705843864, Collision: 0, Height: -1.0, Movement: -0.05264710903648154, Smoothness: -0.0, Curiosity: 0.3197546899318695, Exploration: 0.034787031148920906, Total: -10.462892815136822
2024-07-21 20:15:10,866 - AirSimEnvLogger - INFO - Action: [-0.26323555 -0.26323555 -0.26323555 -0.26323555], Velocity: (-0.26323554518240766, -0.26323554518240766, -0.26323554518240766), Duration: 0.5, Reward: -10.462892815136822, Done: False
2024-07-21 20:15:10,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:10,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:13,854 - AirSimEnvLogger - INFO - Predictive model loss: 0.0007712848600931466
2024-07-21 20:15:18,518 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.33695478160766, Velocity: -0.17621925905719987, Collision: 0, Height: -1.0, Movement: -0.1024829721474827, Smoothness: -0.0, Curiosity: 1.2386080026626587, Exploration: 0.0671908920249944, Total: -10.30257052861468
2024-07-21 20:15:18,612 - AirSimEnvLogger - INFO - Action: [-0.51241486 -0.51241486 -0.51241486 -0.51241486], Velocity: (-0.5124148607374135, -0.5124148607374135, -0.5124148607374135), Duration: 0.5, Reward: -10.30257052861468, Done: False
2024-07-21 20:15:18,675 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:18,675 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:21,344 - AirSimEnvLogger - INFO - Predictive model loss: 0.009925324469804764
2024-07-21 20:15:25,957 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.510387163283124, Velocity: -0.24261163478954126, Collision: 0, Height: -1.0, Movement: -0.12457068443418447, Smoothness: -0.0, Curiosity: 1.9470527172088623, Exploration: 0.05170587739638417, Total: -10.160282515037604
2024-07-21 20:15:26,034 - AirSimEnvLogger - INFO - Action: [-0.62285342 -0.62285342 -0.62285342 -0.62285342], Velocity: (-0.6228534221709223, -0.6228534221709223, -0.6228534221709223), Duration: 0.5, Reward: -10.160282515037604, Done: False
2024-07-21 20:15:26,128 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:26,128 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:29,062 - AirSimEnvLogger - INFO - Predictive model loss: 0.022782111540436745
2024-07-21 20:15:34,259 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.378816751214423, Velocity: -0.09296574812362185, Collision: 0, Height: -1.0, Movement: -0.057244149372582154, Smoothness: -0.0, Curiosity: 0.5312837958335876, Exploration: 0.010518102614437159, Total: -10.66327852177381
2024-07-21 20:15:34,353 - AirSimEnvLogger - INFO - Action: [-0.28622075 -0.28622075 -0.28622075 -0.28622075], Velocity: (-0.28622074686291077, -0.28622074686291077, -0.28622074686291077), Duration: 0.5, Reward: -10.66327852177381, Done: False
2024-07-21 20:15:34,429 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:34,429 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:37,308 - AirSimEnvLogger - INFO - Predictive model loss: 0.01052812859416008
2024-07-21 20:15:42,063 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.017469434545092, Velocity: -0.08033926582183082, Collision: 0, Height: -1.0, Movement: -0.047537342942950866, Smoothness: -0.0, Curiosity: 0.26976361870765686, Exploration: 0.06490494458485856, Total: -10.414530003479502
2024-07-21 20:15:42,172 - AirSimEnvLogger - INFO - Action: [0.23768671 0.23768671 0.23768671 0.23768671], Velocity: (0.23768671471475433, 0.23768671471475433, 0.23768671471475433), Duration: 0.5, Reward: -10.414530003479502, Done: False
2024-07-21 20:15:42,234 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:42,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:45,209 - AirSimEnvLogger - INFO - Predictive model loss: 0.00110615196172148
2024-07-21 20:15:50,279 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.978096902958839, Velocity: -0.020911055733398003, Collision: 0, Height: -1.0, Movement: -0.013095407802461457, Smoothness: -0.0, Curiosity: 0.04088519513607025, Exploration: 0.04418768601797687, Total: -10.460581836834153
2024-07-21 20:15:50,402 - AirSimEnvLogger - INFO - Action: [0.06547704 0.06547704 0.06547704 0.06547704], Velocity: (0.06547703901230728, 0.06547703901230728, 0.06547703901230728), Duration: 0.5, Reward: -10.460581836834153, Done: False
2024-07-21 20:15:50,494 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:50,494 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:53,495 - AirSimEnvLogger - INFO - Predictive model loss: 0.0010883454233407974
2024-07-21 20:15:58,338 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.88084003637211, Velocity: -0.06949539990237831, Collision: 0, Height: -1.0, Movement: -0.038411727109848894, Smoothness: -0.0, Curiosity: 0.19501608610153198, Exploration: 0.013917138424175338, Total: -10.319137438298684
2024-07-21 20:15:58,482 - AirSimEnvLogger - INFO - Action: [0.19205864 0.19205864 0.19205864 0.19205864], Velocity: (0.19205863554924446, 0.19205863554924446, 0.19205863554924446), Duration: 0.5, Reward: -10.319137438298684, Done: False
2024-07-21 20:15:58,543 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:58,543 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:01,516 - AirSimEnvLogger - INFO - Predictive model loss: 0.0030191922560334206
2024-07-21 20:16:06,484 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.67742934103931, Velocity: -0.14688598621048243, Collision: 0, Height: -1.0, Movement: -0.0953652811841664, Smoothness: -0.0, Curiosity: 1.113237977027893, Exploration: 0.038797296214094376, Total: -9.696030414506204
2024-07-21 20:16:06,594 - AirSimEnvLogger - INFO - Action: [0.47682641 0.47682641 0.47682641 0.47682641], Velocity: (0.476826405920832, 0.476826405920832, 0.476826405920832), Duration: 0.5, Reward: -9.696030414506204, Done: False
2024-07-21 20:16:06,657 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:06,657 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:09,637 - AirSimEnvLogger - INFO - Predictive model loss: 0.013091162778437138
2024-07-21 20:16:14,453 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.957382358773692, Velocity: -0.050307517848132616, Collision: 0, Height: -1.0, Movement: -0.028476777037158753, Smoothness: -0.0, Curiosity: 0.10315553843975067, Exploration: 0.015790638196500107, Total: -10.430647898542299
2024-07-21 20:16:14,562 - AirSimEnvLogger - INFO - Action: [-0.14238389 -0.14238389 -0.14238389 -0.14238389], Velocity: (-0.14238388518579376, -0.14238388518579376, -0.14238388518579376), Duration: 0.5, Reward: -10.430647898542299, Done: False
2024-07-21 20:16:14,624 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:14,624 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:17,629 - AirSimEnvLogger - INFO - Predictive model loss: 0.0009124440257437527
2024-07-21 20:16:22,804 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.116709779185388, Velocity: -0.07650848284816801, Collision: 0, Height: -1.0, Movement: -0.04563928959775723, Smoothness: -0.0, Curiosity: 0.2530922591686249, Exploration: 0.05612818712911619, Total: -10.521756182559113
2024-07-21 20:16:22,964 - AirSimEnvLogger - INFO - Action: [-0.22819645 -0.22819645 -0.22819645 -0.22819645], Velocity: (-0.22819644798878616, -0.22819644798878616, -0.22819644798878616), Duration: 0.5, Reward: -10.521756182559113, Done: False
2024-07-21 20:16:23,041 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:23,041 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:25,823 - AirSimEnvLogger - INFO - Predictive model loss: 0.0010052763391286135
2024-07-21 20:16:30,931 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.131642233574086, Velocity: -0.047702472687897755, Collision: 0, Height: -1.0, Movement: -0.028011647452954237, Smoothness: -0.0, Curiosity: 0.12661707401275635, Exploration: 0.017952692075049296, Total: -10.591395559241942
2024-07-21 20:16:31,041 - AirSimEnvLogger - INFO - Action: [-0.14005824 -0.14005824 -0.14005824 -0.14005824], Velocity: (-0.14005823726477118, -0.14005823726477118, -0.14005823726477118), Duration: 0.5, Reward: -10.591395559241942, Done: False
2024-07-21 20:16:31,087 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:31,087 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:33,948 - AirSimEnvLogger - INFO - Predictive model loss: 0.0009290826274082065
2024-07-21 20:16:39,289 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.880349291994225, Velocity: -0.09969524709101799, Collision: 0, Height: -1.0, Movement: -0.05908853829401544, Smoothness: -0.0, Curiosity: 0.4265177845954895, Exploration: 0.03517797151212549, Total: -10.215811282768966
2024-07-21 20:16:39,414 - AirSimEnvLogger - INFO - Action: [0.29544269 0.29544269 0.29544269 0.29544269], Velocity: (0.2954426914700772, 0.2954426914700772, 0.2954426914700772), Duration: 0.5, Reward: -10.215811282768966, Done: False
2024-07-21 20:16:39,507 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:39,507 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:42,452 - AirSimEnvLogger - INFO - Predictive model loss: 0.0033293862361460924
2024-07-21 20:16:47,437 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.758258601677941, Velocity: -0.10557119745729486, Collision: 0, Height: -1.0, Movement: -0.06822068304070737, Smoothness: -0.0, Curiosity: 0.6055042743682861, Exploration: 0.046673225124557016, Total: -10.005779486501606
2024-07-21 20:16:47,514 - AirSimEnvLogger - INFO - Action: [0.34110342 0.34110342 0.34110342 0.34110342], Velocity: (0.3411034152035368, 0.3411034152035368, 0.3411034152035368), Duration: 0.5, Reward: -10.005779486501606, Done: False
2024-07-21 20:16:47,576 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:47,576 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:50,611 - AirSimEnvLogger - INFO - Predictive model loss: 0.007443482521921396
2024-07-21 20:16:55,288 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.867379781543836, Velocity: -0.020009376915016402, Collision: 0, Height: -1.0, Movement: -0.013140208573385603, Smoothness: -0.0, Curiosity: 0.07673931121826172, Exploration: 0.008780283528245695, Total: -10.338572778543902
2024-07-21 20:16:55,459 - AirSimEnvLogger - INFO - Action: [0.06570104 0.06570104 0.06570104 0.06570104], Velocity: (0.06570104286692802, 0.06570104286692802, 0.06570104286692802), Duration: 0.5, Reward: -10.338572778543902, Done: False
2024-07-21 20:16:55,536 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:55,536 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:58,568 - AirSimEnvLogger - INFO - Predictive model loss: 0.0022619422525167465
2024-07-21 20:17:03,656 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.153527724785619, Velocity: -0.11428161483503563, Collision: 0, Height: -1.0, Movement: -0.06958931334254914, Smoothness: -0.0, Curiosity: 0.5468392968177795, Exploration: 0.052011793412354845, Total: -10.43380545644603
2024-07-21 20:17:03,781 - AirSimEnvLogger - INFO - Action: [-0.34794657 -0.34794657 -0.34794657 -0.34794657], Velocity: (-0.3479465667127457, -0.3479465667127457, -0.3479465667127457), Duration: 0.5, Reward: -10.43380545644603, Done: False
2024-07-21 20:17:03,876 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:03,876 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:06,850 - AirSimEnvLogger - INFO - Predictive model loss: 0.002133336616680026
2024-07-21 20:17:12,129 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.392540948895261, Velocity: -0.18981698310247086, Collision: 0, Height: -1.0, Movement: -0.11095407430051651, Smoothness: -0.0, Curiosity: 1.482046127319336, Exploration: 0.06425169439463593, Total: -10.244671445337953
2024-07-21 20:17:12,223 - AirSimEnvLogger - INFO - Action: [-0.55477037 -0.55477037 -0.55477037 -0.55477037], Velocity: (-0.5547703715025826, -0.5547703715025826, -0.5547703715025826), Duration: 0.5, Reward: -10.244671445337953, Done: False
2024-07-21 20:17:12,286 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:12,286 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:15,093 - AirSimEnvLogger - INFO - Predictive model loss: 0.013366161845624447
2024-07-21 20:17:20,086 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.456330090532626, Velocity: -0.20375850913022558, Collision: 0, Height: -1.0, Movement: -0.09908411117255135, Smoothness: -0.0, Curiosity: 1.3063528537750244, Exploration: 0.035544896458250846, Total: -10.40783235003583
2024-07-21 20:17:20,209 - AirSimEnvLogger - INFO - Action: [-0.49542056 -0.49542056 -0.49542056 -0.49542056], Velocity: (-0.49542055586275674, -0.49542055586275674, -0.49542055586275674), Duration: 0.5, Reward: -10.40783235003583, Done: False
2024-07-21 20:17:20,288 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:20,288 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:23,213 - AirSimEnvLogger - INFO - Predictive model loss: 0.01774054579436779
2024-07-21 20:17:28,256 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.463909787943065, Velocity: -0.15184991781490403, Collision: 0, Height: -1.0, Movement: -0.09189256236880741, Smoothness: -0.0, Curiosity: 1.1740951538085938, Exploration: 0.011691518615445783, Total: -10.459638122460012
2024-07-21 20:17:28,366 - AirSimEnvLogger - INFO - Action: [-0.45946281 -0.45946281 -0.45946281 -0.45946281], Velocity: (-0.45946281184403703, -0.45946281184403703, -0.45946281184403703), Duration: 0.5, Reward: -10.459638122460012, Done: False
2024-07-21 20:17:28,429 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:28,429 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:31,335 - AirSimEnvLogger - INFO - Predictive model loss: 0.018301978707313538
2024-07-21 20:17:36,398 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.337885326620084, Velocity: -0.09094163102881062, Collision: 0, Height: -1.0, Movement: -0.048999284920244024, Smoothness: -0.0, Curiosity: 0.43329542875289917, Exploration: 0.016705528487608617, Total: -10.668267250552542
2024-07-21 20:17:36,541 - AirSimEnvLogger - INFO - Action: [-0.24499642 -0.24499642 -0.24499642 -0.24499642], Velocity: (-0.24499642460122012, -0.24499642460122012, -0.24499642460122012), Duration: 0.5, Reward: -10.668267250552542, Done: False
2024-07-21 20:17:36,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:36,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:39,459 - AirSimEnvLogger - INFO - Predictive model loss: 0.00994583498686552
2024-07-21 20:17:44,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.99358569256857, Velocity: -0.07965255140281083, Collision: 0, Height: -1.0, Movement: -0.04984029570363532, Smoothness: -0.0, Curiosity: 0.2793174088001251, Exploration: 0.05733713110421447, Total: -10.387269867219436
2024-07-21 20:17:44,758 - AirSimEnvLogger - INFO - Action: [0.24920148 0.24920148 0.24920148 0.24920148], Velocity: (0.2492014785181766, 0.2492014785181766, 0.2492014785181766), Duration: 0.5, Reward: -10.387269867219436, Done: False
2024-07-21 20:17:44,837 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:44,837 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:47,850 - AirSimEnvLogger - INFO - Predictive model loss: 0.0011586399050429463
2024-07-21 20:17:53,030 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.131243462468065, Velocity: -0.07184079570352866, Collision: 0, Height: -1.0, Movement: -0.04152967910397764, Smoothness: -0.0, Curiosity: 0.2421128749847412, Exploration: 0.020859184101442774, Total: -10.546088553917569
2024-07-21 20:17:53,139 - AirSimEnvLogger - INFO - Action: [-0.2076484 -0.2076484 -0.2076484 -0.2076484], Velocity: (-0.20764839551988817, -0.20764839551988817, -0.20764839551988817), Duration: 0.5, Reward: -10.546088553917569, Done: False
2024-07-21 20:17:53,234 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:53,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:56,095 - AirSimEnvLogger - INFO - Predictive model loss: 0.00356260035187006
2024-07-21 20:18:00,911 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.963904337997002, Velocity: -0.050790979935119565, Collision: 0, Height: -1.0, Movement: -0.030516839124616415, Smoothness: -0.0, Curiosity: 0.11566314101219177, Exploration: 0.005687861473144289, Total: -10.433382369076298
2024-07-21 20:18:01,052 - AirSimEnvLogger - INFO - Action: [0.1525842 0.1525842 0.1525842 0.1525842], Velocity: (0.15258419562308206, 0.15258419562308206, 0.15258419562308206), Duration: 0.5, Reward: -10.433382369076298, Done: False
2024-07-21 20:18:01,099 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:01,099 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:03,928 - AirSimEnvLogger - INFO - Predictive model loss: 0.0014825102407485247
2024-07-21 20:18:08,946 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.721152439000548, Velocity: -0.1384790712350591, Collision: 0, Height: -1.0, Movement: -0.09141783719155017, Smoothness: -0.0, Curiosity: 0.9579166173934937, Exploration: 0.0541437419609897, Total: -9.809746701248288
2024-07-21 20:18:09,087 - AirSimEnvLogger - INFO - Action: [0.45708919 0.45708919 0.45708919 0.45708919], Velocity: (0.4570891859577508, 0.4570891859577508, 0.4570891859577508), Duration: 0.5, Reward: -9.809746701248288, Done: False
2024-07-21 20:18:09,150 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:09,150 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:12,125 - AirSimEnvLogger - INFO - Predictive model loss: 0.009101825766265392
2024-07-21 20:18:17,340 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.563201804910689, Velocity: -0.1890573575016247, Collision: 0, Height: -1.0, Movement: -0.11263444426105829, Smoothness: -0.0, Curiosity: 1.5442308187484741, Exploration: 0.04874991128904528, Total: -9.387128536455561
2024-07-21 20:18:17,495 - AirSimEnvLogger - INFO - Action: [0.56317222 0.56317222 0.56317222 0.56317222], Velocity: (0.5631722213052914, 0.5631722213052914, 0.5631722213052914), Duration: 0.5, Reward: -9.387128536455561, Done: False
2024-07-21 20:18:17,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:17,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:20,567 - AirSimEnvLogger - INFO - Predictive model loss: 0.019727477803826332
2024-07-21 20:18:25,504 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.61879635513197, Velocity: -0.10921792008006623, Collision: 0, Height: -1.0, Movement: -0.0711284220340173, Smoothness: -0.0, Curiosity: 0.7098733186721802, Exploration: 0.013647435092578578, Total: -9.8228520110208
2024-07-21 20:18:25,629 - AirSimEnvLogger - INFO - Action: [0.35564211 0.35564211 0.35564211 0.35564211], Velocity: (0.3556421101700865, 0.3556421101700865, 0.3556421101700865), Duration: 0.5, Reward: -9.8228520110208, Done: False
2024-07-21 20:18:25,691 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:25,691 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:27,804 - AirSimEnvLogger - INFO - Predictive model loss: 0.014121280051767826
2024-07-21 20:18:32,829 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.845984762052229, Velocity: -0.01146717528775093, Collision: 0, Height: -1.0, Movement: -0.0008918017209728846, Smoothness: -0.0, Curiosity: 0.044603731483221054, Exploration: 0.03616249907034738, Total: -10.322273164312522
2024-07-21 20:18:32,922 - AirSimEnvLogger - INFO - Action: [0.00445901 0.00445901 0.00445901 0.00445901], Velocity: (0.004459008604864423, 0.004459008604864423, 0.004459008604864423), Duration: 0.5, Reward: -10.322273164312522, Done: False
2024-07-21 20:18:33,032 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:33,032 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:35,866 - AirSimEnvLogger - INFO - Predictive model loss: 0.00261079054325819
2024-07-21 20:18:40,935 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.161510902901666, Velocity: -0.1345283249624009, Collision: 0, Height: -1.0, Movement: -0.0757135167687555, Smoothness: -0.0, Curiosity: 0.6441807150840759, Exploration: 0.06539771220085781, Total: -10.40117651707753
2024-07-21 20:18:41,045 - AirSimEnvLogger - INFO - Action: [-0.37856758 -0.37856758 -0.37856758 -0.37856758], Velocity: (-0.3785675838437775, -0.3785675838437775, -0.3785675838437775), Duration: 0.5, Reward: -10.40117651707753, Done: False
2024-07-21 20:18:41,109 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:41,109 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:44,058 - AirSimEnvLogger - INFO - Predictive model loss: 0.0023780714254826307
2024-07-21 20:18:49,244 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.15227510938061, Velocity: -0.049760955866471154, Collision: 0, Height: -1.0, Movement: -0.030306865273387225, Smoothness: -0.0, Curiosity: 0.1473618447780609, Exploration: 0.0328105230230474, Total: -10.599943246847545
2024-07-21 20:18:49,384 - AirSimEnvLogger - INFO - Action: [-0.15153433 -0.15153433 -0.15153433 -0.15153433], Velocity: (-0.15153432636693612, -0.15153432636693612, -0.15153432636693612), Duration: 0.5, Reward: -10.599943246847545, Done: False
2024-07-21 20:18:49,446 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:49,446 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:52,363 - AirSimEnvLogger - INFO - Predictive model loss: 0.0013134970795363188
2024-07-21 20:18:57,127 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.338134434125987, Velocity: -0.15376275201822853, Collision: 0, Height: -1.0, Movement: -0.09131285026593557, Smoothness: -0.0, Curiosity: 1.0341286659240723, Exploration: 0.02029047176573647, Total: -10.40302466784651
2024-07-21 20:18:57,175 - AirSimEnvLogger - INFO - Action: [-0.45656425 -0.45656425 -0.45656425 -0.45656425], Velocity: (-0.4565642513296778, -0.4565642513296778, -0.4565642513296778), Duration: 0.5, Reward: -10.40302466784651, Done: False
2024-07-21 20:18:57,238 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:57,238 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:59,986 - AirSimEnvLogger - INFO - Predictive model loss: 0.009672059677541256
2024-07-21 20:19:04,873 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.048400436963634, Velocity: -0.05534019912735057, Collision: 0, Height: -1.0, Movement: -0.030185991153500714, Smoothness: -0.0, Curiosity: 0.12760573625564575, Exploration: 0.01607277705595247, Total: -10.512071712103646
2024-07-21 20:19:05,001 - AirSimEnvLogger - INFO - Action: [0.15092996 0.15092996 0.15092996 0.15092996], Velocity: (0.15092995576750357, 0.15092995576750357, 0.15092995576750357), Duration: 0.5, Reward: -10.512071712103646, Done: False
2024-07-21 20:19:05,047 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:05,047 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:07,846 - AirSimEnvLogger - INFO - Predictive model loss: 0.0005613325047306716
2024-07-21 20:19:13,077 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.784235546721082, Velocity: -0.1275335207505235, Collision: 0, Height: -1.0, Movement: -0.08282991927136096, Smoothness: -0.0, Curiosity: 0.8024149537086487, Exploration: 0.07143864633411087, Total: -9.940790092902333
2024-07-21 20:19:13,203 - AirSimEnvLogger - INFO - Action: [0.4141496 0.4141496 0.4141496 0.4141496], Velocity: (0.41414959635680476, 0.41414959635680476, 0.41414959635680476), Duration: 0.5, Reward: -9.940790092902333, Done: False
2024-07-21 20:19:13,266 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:13,266 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:16,210 - AirSimEnvLogger - INFO - Predictive model loss: 0.0055831740610301495
2024-07-21 20:19:21,082 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.846912600242357, Velocity: -0.039600769566189936, Collision: 0, Height: -1.0, Movement: -0.024526891019290255, Smoothness: -0.0, Curiosity: 0.11084242910146713, Exploration: 0.02114823727574103, Total: -10.309514812121499
2024-07-21 20:19:21,098 - AirSimEnvLogger - INFO - Action: [0.12263446 0.12263446 0.12263446 0.12263446], Velocity: (0.12263445509645127, 0.12263445509645127, 0.12263445509645127), Duration: 0.5, Reward: -10.309514812121499, Done: False
2024-07-21 20:19:21,176 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:21,176 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:24,076 - AirSimEnvLogger - INFO - Predictive model loss: 0.002137690782546997
2024-07-21 20:19:28,470 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.129438585930894, Velocity: -0.10735396132206114, Collision: 0, Height: -1.0, Movement: -0.06389597211959683, Smoothness: -0.0, Curiosity: 0.46254757046699524, Exploration: 0.04900872570989266, Total: -10.44842963342841
2024-07-21 20:19:28,596 - AirSimEnvLogger - INFO - Action: [-0.31947986 -0.31947986 -0.31947986 -0.31947986], Velocity: (-0.3194798605979841, -0.3194798605979841, -0.3194798605979841), Duration: 0.5, Reward: -10.44842963342841, Done: False
2024-07-21 20:19:28,659 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:28,659 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:31,558 - AirSimEnvLogger - INFO - Predictive model loss: 0.0019668240565806627
2024-07-21 20:19:36,298 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.072977620171299, Velocity: -0.012671055313769901, Collision: 0, Height: -1.0, Movement: -0.0075905464822189185, Smoothness: -0.0, Curiosity: 0.037927690893411636, Exploration: 0.024798018596988006, Total: -10.556148753310303
2024-07-21 20:19:36,422 - AirSimEnvLogger - INFO - Action: [-0.03795273 -0.03795273 -0.03795273 -0.03795273], Velocity: (-0.03795273241109459, -0.03795273241109459, -0.03795273241109459), Duration: 0.5, Reward: -10.556148753310303, Done: False
2024-07-21 20:19:36,469 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:36,469 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:39,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.00041622252319939435
2024-07-21 20:19:44,430 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.202192986468376, Velocity: -0.09447259726239532, Collision: 0, Height: -1.0, Movement: -0.05679711216361008, Smoothness: -0.0, Curiosity: 0.40982672572135925, Exploration: 0.007923444305814785, Total: -10.548610944594092
2024-07-21 20:19:44,524 - AirSimEnvLogger - INFO - Action: [-0.28398556 -0.28398556 -0.28398556 -0.28398556], Velocity: (-0.2839855608180504, -0.2839855608180504, -0.2839855608180504), Duration: 0.5, Reward: -10.548610944594092, Done: False
2024-07-21 20:19:44,635 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:44,635 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:47,681 - AirSimEnvLogger - INFO - Predictive model loss: 0.0036293601151555777
2024-07-21 20:19:52,542 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.269741574268446, Velocity: -0.1111132010219444, Collision: 0, Height: -1.0, Movement: -0.062155122129473495, Smoothness: -0.0, Curiosity: 0.5224670171737671, Exploration: 0.027287647203361704, Total: -10.564822648964808
2024-07-21 20:19:52,667 - AirSimEnvLogger - INFO - Action: [-0.31077561 -0.31077561 -0.31077561 -0.31077561], Velocity: (-0.3107756106473675, -0.3107756106473675, -0.3107756106473675), Duration: 0.5, Reward: -10.564822648964808, Done: False
2024-07-21 20:19:52,729 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:52,729 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:55,680 - AirSimEnvLogger - INFO - Predictive model loss: 0.006274333223700523
2024-07-21 20:20:00,876 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.435065548162044, Velocity: -0.17310114578337474, Collision: 0, Height: -1.0, Movement: -0.10723697869397869, Smoothness: -0.0, Curiosity: 1.4823604822158813, Exploration: 0.030662653212365665, Total: -10.285027047172715
2024-07-21 20:20:01,002 - AirSimEnvLogger - INFO - Action: [-0.53618489 -0.53618489 -0.53618489 -0.53618489], Velocity: (-0.5361848934698934, -0.5361848934698934, -0.5361848934698934), Duration: 0.5, Reward: -10.285027047172715, Done: False
2024-07-21 20:20:01,065 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:01,065 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:04,130 - AirSimEnvLogger - INFO - Predictive model loss: 0.017612280324101448
2024-07-21 20:20:09,210 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.541137443944704, Velocity: -0.23723986983566173, Collision: 0, Height: -1.0, Movement: -0.11968726976920004, Smoothness: -0.0, Curiosity: 1.9564580917358398, Exploration: 0.03769421539693403, Total: -10.18595821689215
2024-07-21 20:20:09,319 - AirSimEnvLogger - INFO - Action: [-0.59843635 -0.59843635 -0.59843635 -0.59843635], Velocity: (-0.5984363488460002, -0.5984363488460002, -0.5984363488460002), Duration: 0.5, Reward: -10.18595821689215, Done: False
2024-07-21 20:20:09,381 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:09,381 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:12,391 - AirSimEnvLogger - INFO - Predictive model loss: 0.02828074060380459
2024-07-21 20:20:17,411 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.637699211055038, Velocity: -0.29121758396990866, Collision: 0, Height: -1.0, Movement: -0.13600305251384195, Smoothness: -0.0, Curiosity: 2.5987114906311035, Exploration: 0.030109363024261072, Total: -9.991530690370972
2024-07-21 20:20:17,520 - AirSimEnvLogger - INFO - Action: [-0.68001526 -0.68001526 -0.68001526 -0.68001526], Velocity: (-0.6800152625692097, -0.6800152625692097, -0.6800152625692097), Duration: 0.5, Reward: -9.991530690370972, Done: False
2024-07-21 20:20:17,613 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:17,613 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:20,597 - AirSimEnvLogger - INFO - Predictive model loss: 0.040716417133808136
2024-07-21 20:20:25,660 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.384625475199886, Velocity: -0.06650873402190167, Collision: 0, Height: -1.0, Movement: -0.04083133008134572, Smoothness: -0.0, Curiosity: 0.4674920439720154, Exploration: 0.023964039214433002, Total: -10.683424145390077
2024-07-21 20:20:25,800 - AirSimEnvLogger - INFO - Action: [-0.20415665 -0.20415665 -0.20415665 -0.20415665], Velocity: (-0.2041566504067286, -0.2041566504067286, -0.2041566504067286), Duration: 0.5, Reward: -10.683424145390077, Done: False
2024-07-21 20:20:25,910 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:25,910 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:28,874 - AirSimEnvLogger - INFO - Predictive model loss: 0.014597387984395027
2024-07-21 20:20:33,688 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.459832665053542, Velocity: -0.16964844230277737, Collision: 0, Height: -1.0, Movement: -0.09657508266991481, Smoothness: -0.0, Curiosity: 1.4414914846420288, Exploration: 0.021948182544736974, Total: -10.32917901564196
2024-07-21 20:20:33,703 - AirSimEnvLogger - INFO - Action: [-0.48287541 -0.48287541 -0.48287541 -0.48287541], Velocity: (-0.482875413349574, -0.482875413349574, -0.482875413349574), Duration: 0.5, Reward: -10.32917901564196, Done: False
2024-07-21 20:20:33,750 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:33,750 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:36,514 - AirSimEnvLogger - INFO - Predictive model loss: 0.025052305310964584
2024-07-21 20:20:41,499 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.37357585584817, Velocity: -0.09737235716573035, Collision: 0, Height: -1.0, Movement: -0.05994389719280653, Smoothness: -0.0, Curiosity: 0.7506651878356934, Exploration: 0.013357319327388649, Total: -10.550252366366992
2024-07-21 20:20:41,576 - AirSimEnvLogger - INFO - Action: [-0.29971949 -0.29971949 -0.29971949 -0.29971949], Velocity: (-0.29971948596403264, -0.29971948596403264, -0.29971948596403264), Duration: 0.5, Reward: -10.550252366366992, Done: False
2024-07-21 20:20:41,637 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:41,637 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:44,585 - AirSimEnvLogger - INFO - Predictive model loss: 0.017990095540881157
2024-07-21 20:20:49,420 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.408160332513114, Velocity: -0.13510953300321216, Collision: 0, Height: -1.0, Movement: -0.08106583091871467, Smoothness: -0.0, Curiosity: 1.1730797290802002, Exploration: 0.009391190346361105, Total: -10.39540357949722
2024-07-21 20:20:49,546 - AirSimEnvLogger - INFO - Action: [-0.40532915 -0.40532915 -0.40532915 -0.40532915], Velocity: (-0.4053291545935733, -0.4053291545935733, -0.4053291545935733), Duration: 0.5, Reward: -10.39540357949722, Done: False
2024-07-21 20:20:49,610 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:49,610 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:52,575 - AirSimEnvLogger - INFO - Predictive model loss: 0.023080313578248024
2024-07-21 20:20:57,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.528708517608404, Velocity: -0.19437158025930606, Collision: 0, Height: -1.0, Movement: -0.11510104404517277, Smoothness: -0.0, Curiosity: 2.1231608390808105, Exploration: 0.028054996738995964, Total: -10.07021299325437
2024-07-21 20:20:57,753 - AirSimEnvLogger - INFO - Action: [-0.57550522 -0.57550522 -0.57550522 -0.57550522], Velocity: (-0.5755052202258638, -0.5755052202258638, -0.5755052202258638), Duration: 0.5, Reward: -10.07021299325437, Done: False
2024-07-21 20:20:57,815 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:57,815 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:00,827 - AirSimEnvLogger - INFO - Predictive model loss: 0.03503039851784706
2024-07-21 20:21:05,766 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.204631113789857, Velocity: -0.024164184817859827, Collision: 0, Height: -1.0, Movement: -0.004765728778500056, Smoothness: -0.0, Curiosity: 0.29962894320487976, Exploration: 0.0262293396196719, Total: -10.562129439550263
2024-07-21 20:21:05,846 - AirSimEnvLogger - INFO - Action: [0.02382864 0.02382864 0.02382864 0.02382864], Velocity: (0.02382864389250028, 0.02382864389250028, 0.02382864389250028), Duration: 0.5, Reward: -10.562129439550263, Done: False
2024-07-21 20:21:05,941 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:05,941 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:08,982 - AirSimEnvLogger - INFO - Predictive model loss: 0.012651202268898487
2024-07-21 20:21:14,275 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.123823615604943, Velocity: -0.01989408968556024, Collision: 0, Height: -1.0, Movement: -0.008622941043546451, Smoothness: -0.0, Curiosity: 0.2954197824001312, Exploration: 0.0470227640800287, Total: -10.477518510536006
2024-07-21 20:21:14,385 - AirSimEnvLogger - INFO - Action: [-0.04311471 -0.04311471 -0.04311471 -0.04311471], Velocity: (-0.04311470521773225, -0.04311470521773225, -0.04311470521773225), Duration: 0.5, Reward: -10.477518510536006, Done: False
2024-07-21 20:21:14,446 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:14,446 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:17,371 - AirSimEnvLogger - INFO - Predictive model loss: 0.013291693292558193
2024-07-21 20:21:22,323 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.257297861733445, Velocity: -0.10464109782875262, Collision: 0, Height: -1.0, Movement: -0.06337109638580216, Smoothness: -0.0, Curiosity: 0.8941020965576172, Exploration: 0.02014505659557794, Total: -10.364875460688477
2024-07-21 20:21:22,446 - AirSimEnvLogger - INFO - Action: [-0.31685548 -0.31685548 -0.31685548 -0.31685548], Velocity: (-0.31685548192901075, -0.31685548192901075, -0.31685548192901075), Duration: 0.5, Reward: -10.364875460688477, Done: False
2024-07-21 20:21:22,461 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:22,461 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:25,377 - AirSimEnvLogger - INFO - Predictive model loss: 0.018190039321780205
2024-07-21 20:21:30,403 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.976271129481734, Velocity: -0.07151400896595453, Collision: 0, Height: -1.0, Movement: -0.04447386943634088, Smoothness: -0.0, Curiosity: 0.4070177674293518, Exploration: 0.018535094365012623, Total: -10.309259618320667
2024-07-21 20:21:30,543 - AirSimEnvLogger - INFO - Action: [0.22236935 0.22236935 0.22236935 0.22236935], Velocity: (0.22236934718170437, 0.22236934718170437, 0.22236934718170437), Duration: 0.5, Reward: -10.309259618320667, Done: False
2024-07-21 20:21:30,607 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:30,607 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:33,509 - AirSimEnvLogger - INFO - Predictive model loss: 0.014189639128744602
2024-07-21 20:21:38,618 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.054926507502321, Velocity: -0.026539275483561317, Collision: 0, Height: -1.0, Movement: -0.015988719049415314, Smoothness: -0.0, Curiosity: 0.33004704117774963, Exploration: 0.02338073424862838, Total: -10.400095349710442
2024-07-21 20:21:38,681 - AirSimEnvLogger - INFO - Action: [-0.0799436 -0.0799436 -0.0799436 -0.0799436], Velocity: (-0.07994359524707656, -0.07994359524707656, -0.07994359524707656), Duration: 0.5, Reward: -10.400095349710442, Done: False
2024-07-21 20:21:38,744 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:38,744 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:41,743 - AirSimEnvLogger - INFO - Predictive model loss: 0.013569937087595463
2024-07-21 20:21:47,152 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.82407838548129, Velocity: -0.10790434051094905, Collision: 0, Height: -1.0, Movement: -0.0681650581045343, Smoothness: -0.0, Curiosity: 0.6505451202392578, Exploration: 0.016809420595085812, Total: -10.05621261730857
2024-07-21 20:21:47,184 - AirSimEnvLogger - INFO - Action: [0.34082529 0.34082529 0.34082529 0.34082529], Velocity: (0.34082529052267146, 0.34082529052267146, 0.34082529052267146), Duration: 0.5, Reward: -10.05621261730857, Done: False
2024-07-21 20:21:47,262 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:47,262 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:50,339 - AirSimEnvLogger - INFO - Predictive model loss: 0.017568470910191536
2024-07-21 20:21:55,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.609780085002116, Velocity: -0.17312259554200235, Collision: 0, Height: -1.0, Movement: -0.11024194668150909, Smoothness: -0.0, Curiosity: 1.4560407400131226, Exploration: 0.05661730234680809, Total: -9.468021746965345
2024-07-21 20:21:55,647 - AirSimEnvLogger - INFO - Action: [0.55120973 0.55120973 0.55120973 0.55120973], Velocity: (0.5512097334075454, 0.5512097334075454, 0.5512097334075454), Duration: 0.5, Reward: -9.468021746965345, Done: False
2024-07-21 20:21:55,678 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:55,678 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:58,691 - AirSimEnvLogger - INFO - Predictive model loss: 0.028474153950810432
2024-07-21 20:22:03,559 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.460818473875337, Velocity: -0.19702858759192302, Collision: 0, Height: -1.0, Movement: -0.1312803909699965, Smoothness: -0.0, Curiosity: 2.0641744136810303, Exploration: 0.04374538733206102, Total: -9.031624522461371
2024-07-21 20:22:03,668 - AirSimEnvLogger - INFO - Action: [0.65640195 0.65640195 0.65640195 0.65640195], Velocity: (0.6564019548499824, 0.6564019548499824, 0.6564019548499824), Duration: 0.5, Reward: -9.031624522461371, Done: False
2024-07-21 20:22:03,747 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:03,747 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:06,539 - AirSimEnvLogger - INFO - Predictive model loss: 0.036971431225538254
2024-07-21 20:22:11,552 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.563924386897689, Velocity: -0.12379770763781728, Collision: 0, Height: -1.0, Movement: -0.07557720693552734, Smoothness: -0.0, Curiosity: 0.7978980541229248, Exploration: 0.00994086558877581, Total: -9.732443761230932
2024-07-21 20:22:11,675 - AirSimEnvLogger - INFO - Action: [0.37788603 0.37788603 0.37788603 0.37788603], Velocity: (0.3778860346776367, 0.3778860346776367, 0.3778860346776367), Duration: 0.5, Reward: -9.732443761230932, Done: False
2024-07-21 20:22:11,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:11,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:14,673 - AirSimEnvLogger - INFO - Predictive model loss: 0.02179531566798687
2024-07-21 20:22:20,043 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.488377379655052, Velocity: -0.1703552012083209, Collision: 0, Height: -1.0, Movement: -0.11394802109700562, Smoothness: -0.0, Curiosity: 1.5843346118927002, Exploration: 0.009728007778694045, Total: -9.290836874866823
2024-07-21 20:22:20,207 - AirSimEnvLogger - INFO - Action: [0.56974011 0.56974011 0.56974011 0.56974011], Velocity: (0.5697401054850281, 0.5697401054850281, 0.5697401054850281), Duration: 0.5, Reward: -9.290836874866823, Done: False
2024-07-21 20:22:20,295 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:20,295 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:23,004 - AirSimEnvLogger - INFO - Predictive model loss: 0.025026114657521248
2024-07-21 20:22:27,939 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.854386725604218, Velocity: -0.0410461217815881, Collision: 0, Height: -1.0, Movement: -0.01918540708073915, Smoothness: -0.0, Curiosity: 0.12679435312747955, Exploration: 0.034413531794548174, Total: -10.306548444280436
2024-07-21 20:22:28,064 - AirSimEnvLogger - INFO - Action: [-0.09592704 -0.09592704 -0.09592704 -0.09592704], Velocity: (-0.09592703540369574, -0.09592703540369574, -0.09592703540369574), Duration: 0.5, Reward: -10.306548444280436, Done: False
2024-07-21 20:22:28,142 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:28,142 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:31,413 - AirSimEnvLogger - INFO - Predictive model loss: 0.003344245022162795
2024-07-21 20:22:35,987 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.196521101751612, Velocity: -0.14701715208379268, Collision: 0, Height: -1.0, Movement: -0.08575212116961152, Smoothness: -0.0, Curiosity: 0.9767941236495972, Exploration: 0.08541984715587703, Total: -10.273123858654497
2024-07-21 20:22:36,112 - AirSimEnvLogger - INFO - Action: [-0.42876061 -0.42876061 -0.42876061 -0.42876061], Velocity: (-0.4287606058480576, -0.4287606058480576, -0.4287606058480576), Duration: 0.5, Reward: -10.273123858654497, Done: False
2024-07-21 20:22:36,175 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:36,175 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:39,305 - AirSimEnvLogger - INFO - Predictive model loss: 0.006593162193894386
2024-07-21 20:22:44,016 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.423970248993374, Velocity: -0.18776982088631902, Collision: 0, Height: -1.0, Movement: -0.11372742233636216, Smoothness: -0.0, Curiosity: 1.8082501888275146, Exploration: 0.0640113071190364, Total: -10.112300545832605
2024-07-21 20:22:44,125 - AirSimEnvLogger - INFO - Action: [-0.56863711 -0.56863711 -0.56863711 -0.56863711], Velocity: (-0.5686371116818107, -0.5686371116818107, -0.5686371116818107), Duration: 0.5, Reward: -10.112300545832605, Done: False
2024-07-21 20:22:44,218 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:44,218 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:47,169 - AirSimEnvLogger - INFO - Predictive model loss: 0.02062726579606533
2024-07-21 20:22:52,341 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.364973729640067, Velocity: -0.10069826045240839, Collision: 0, Height: -1.0, Movement: -0.06289063817740596, Smoothness: -0.0, Curiosity: 0.7899945378303528, Exploration: 0.01863855823070684, Total: -10.522886943122694
2024-07-21 20:22:52,468 - AirSimEnvLogger - INFO - Action: [-0.31445319 -0.31445319 -0.31445319 -0.31445319], Velocity: (-0.31445319088702983, -0.31445319088702983, -0.31445319088702983), Duration: 0.5, Reward: -10.522886943122694, Done: False
2024-07-21 20:22:52,532 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:52,532 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:55,378 - AirSimEnvLogger - INFO - Predictive model loss: 0.015339385718107224
2024-07-21 20:23:00,407 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.306157518135626, Velocity: -0.07905343081283094, Collision: 0, Height: -1.0, Movement: -0.049601101013427566, Smoothness: -0.0, Curiosity: 0.606966495513916, Exploration: 0.016911252371566488, Total: -10.543778845412112
2024-07-21 20:23:00,564 - AirSimEnvLogger - INFO - Action: [-0.24800551 -0.24800551 -0.24800551 -0.24800551], Velocity: (-0.2480055050671378, -0.2480055050671378, -0.2480055050671378), Duration: 0.5, Reward: -10.543778845412112, Done: False
2024-07-21 20:23:00,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:00,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:03,700 - AirSimEnvLogger - INFO - Predictive model loss: 0.013469203375279903
2024-07-21 20:23:08,972 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.9956016495641, Velocity: -0.07476331064152808, Collision: 0, Height: -1.0, Movement: -0.044646347000000086, Smoothness: -0.0, Curiosity: 0.32009363174438477, Exploration: 0.043798621122427536, Total: -10.368641399488185
2024-07-21 20:23:09,099 - AirSimEnvLogger - INFO - Action: [0.22323174 0.22323174 0.22323174 0.22323174], Velocity: (0.22323173500000043, 0.22323173500000043, 0.22323173500000043), Duration: 0.5, Reward: -10.368641399488185, Done: False
2024-07-21 20:23:09,193 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:09,193 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:12,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.002505091018974781
2024-07-21 20:23:17,148 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.176265120266471, Velocity: -0.09163294052154212, Collision: 0, Height: -1.0, Movement: -0.0538362441292419, Smoothness: -0.0, Curiosity: 0.590944766998291, Exploration: 0.01324955517604522, Total: -10.429342920405812
2024-07-21 20:23:17,195 - AirSimEnvLogger - INFO - Action: [-0.26918122 -0.26918122 -0.26918122 -0.26918122], Velocity: (-0.2691812206462095, -0.2691812206462095, -0.2691812206462095), Duration: 0.5, Reward: -10.429342920405812, Done: False
2024-07-21 20:23:17,241 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:17,241 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:20,138 - AirSimEnvLogger - INFO - Predictive model loss: 0.009886587969958782
2024-07-21 20:23:25,351 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.191037008029964, Velocity: -0.07037453497847102, Collision: 0, Height: -1.0, Movement: -0.03744709480956266, Smoothness: -0.0, Curiosity: 0.4410618543624878, Exploration: 0.03004489454595563, Total: -10.50342907890972
2024-07-21 20:23:25,446 - AirSimEnvLogger - INFO - Action: [-0.18723547 -0.18723547 -0.18723547 -0.18723547], Velocity: (-0.1872354740478133, -0.1872354740478133, -0.1872354740478133), Duration: 0.5, Reward: -10.50342907890972, Done: False
2024-07-21 20:23:25,509 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:25,509 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:28,481 - AirSimEnvLogger - INFO - Predictive model loss: 0.009487166069447994
2024-07-21 20:23:33,575 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.378645020527564, Velocity: -0.16142854495677897, Collision: 0, Height: -1.0, Movement: -0.09488296503402327, Smoothness: -0.0, Curiosity: 1.501624584197998, Exploration: 0.02556626395383634, Total: -10.212922044619589
2024-07-21 20:23:33,731 - AirSimEnvLogger - INFO - Action: [-0.47441483 -0.47441483 -0.47441483 -0.47441483], Velocity: (-0.47441482517011635, -0.47441482517011635, -0.47441482517011635), Duration: 0.5, Reward: -10.212922044619589, Done: False
2024-07-21 20:23:33,793 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:33,794 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:36,696 - AirSimEnvLogger - INFO - Predictive model loss: 0.023401660844683647
2024-07-21 20:23:41,594 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.468614893125887, Velocity: -0.17482832696239223, Collision: 0, Height: -1.0, Movement: -0.09964577552963066, Smoothness: -0.0, Curiosity: 1.7604092359542847, Exploration: 0.03612757591639488, Total: -10.178563500999624
2024-07-21 20:23:41,719 - AirSimEnvLogger - INFO - Action: [-0.49822888 -0.49822888 -0.49822888 -0.49822888], Velocity: (-0.49822887764815327, -0.49822887764815327, -0.49822887764815327), Duration: 0.5, Reward: -10.178563500999624, Done: False
2024-07-21 20:23:41,796 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:41,796 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:44,791 - AirSimEnvLogger - INFO - Predictive model loss: 0.03063318319618702
2024-07-21 20:23:49,798 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.592621993852587, Velocity: -0.20620351793339753, Collision: 0, Height: -1.0, Movement: -0.12598230539405728, Smoothness: -0.0, Curiosity: 2.6854257583618164, Exploration: 0.02816284936663895, Total: -9.859976534304455
2024-07-21 20:23:49,939 - AirSimEnvLogger - INFO - Action: [-0.62991153 -0.62991153 -0.62991153 -0.62991153], Velocity: (-0.6299115269702864, -0.6299115269702864, -0.6299115269702864), Duration: 0.5, Reward: -9.859976534304455, Done: False
2024-07-21 20:23:50,001 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:50,001 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:52,982 - AirSimEnvLogger - INFO - Predictive model loss: 0.04572155699133873
2024-07-21 20:23:57,969 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.207927812148672, Velocity: -0.03561498720677489, Collision: 0, Height: -1.0, Movement: -0.013168264932213304, Smoothness: -0.0, Curiosity: 0.43982043862342834, Exploration: 0.034774182722158674, Total: -10.500187076389134
2024-07-21 20:23:58,064 - AirSimEnvLogger - INFO - Action: [0.06584132 0.06584132 0.06584132 0.06584132], Velocity: (0.06584132466106651, 0.06584132466106651, 0.06584132466106651), Duration: 0.5, Reward: -10.500187076389134, Done: False
2024-07-21 20:23:58,126 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:58,126 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:00,909 - AirSimEnvLogger - INFO - Predictive model loss: 0.011141650378704071
2024-07-21 20:24:05,928 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.27492453343763, Velocity: -0.09745601803498315, Collision: 0, Height: -1.0, Movement: -0.05424769445917106, Smoothness: -0.0, Curiosity: 0.9335887432098389, Exploration: 0.034653573084970815, Total: -10.355352225679125
2024-07-21 20:24:06,053 - AirSimEnvLogger - INFO - Action: [-0.27123847 -0.27123847 -0.27123847 -0.27123847], Velocity: (-0.2712384722958553, -0.2712384722958553, -0.2712384722958553), Duration: 0.5, Reward: -10.355352225679125, Done: False
2024-07-21 20:24:06,085 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:06,086 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:08,806 - AirSimEnvLogger - INFO - Predictive model loss: 0.017199626192450523
2024-07-21 20:24:13,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.44885603873483, Velocity: -0.17079900304514617, Collision: 0, Height: -1.0, Movement: -0.10328326485882747, Smoothness: -0.0, Curiosity: 2.077057361602783, Exploration: 0.041536415019658976, Total: -9.997747902937963
2024-07-21 20:24:14,123 - AirSimEnvLogger - INFO - Action: [-0.51641632 -0.51641632 -0.51641632 -0.51641632], Velocity: (-0.5164163242941373, -0.5164163242941373, -0.5164163242941373), Duration: 0.5, Reward: -9.997747902937963, Done: False
2024-07-21 20:24:14,249 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:14,249 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:17,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.03014729544520378
2024-07-21 20:24:22,327 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.356528680012547, Velocity: -0.08702726406776798, Collision: 0, Height: -1.0, Movement: -0.052055706502028044, Smoothness: -0.0, Curiosity: 1.0926954746246338, Exploration: 0.01162467947300826, Total: -10.356575209489714
2024-07-21 20:24:22,422 - AirSimEnvLogger - INFO - Action: [-0.26027853 -0.26027853 -0.26027853 -0.26027853], Velocity: (-0.2602785325101402, -0.2602785325101402, -0.2602785325101402), Duration: 0.5, Reward: -10.356575209489714, Done: False
2024-07-21 20:24:22,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:22,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:25,420 - AirSimEnvLogger - INFO - Predictive model loss: 0.018636712804436684
2024-07-21 20:24:30,435 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.485437317447117, Velocity: -0.1674093457032207, Collision: 0, Height: -1.0, Movement: -0.10218727088025598, Smoothness: -0.0, Curiosity: 2.250913381576538, Exploration: 0.0097355668643726, Total: -9.951956913225608
2024-07-21 20:24:30,528 - AirSimEnvLogger - INFO - Action: [-0.51093635 -0.51093635 -0.51093635 -0.51093635], Velocity: (-0.5109363544012798, -0.5109363544012798, -0.5109363544012798), Duration: 0.5, Reward: -9.951956913225608, Done: False
2024-07-21 20:24:30,575 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:30,575 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:33,711 - AirSimEnvLogger - INFO - Predictive model loss: 0.028729159384965897
2024-07-21 20:24:38,791 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.195284212248186, Velocity: -0.020542336682349182, Collision: 0, Height: -1.0, Movement: -0.005591341376049397, Smoothness: -0.0, Curiosity: 0.680039644241333, Exploration: 0.01919795107869957, Total: -10.36225510239056
2024-07-21 20:24:38,853 - AirSimEnvLogger - INFO - Action: [0.02795671 0.02795671 0.02795671 0.02795671], Velocity: (0.027956706880246984, 0.027956706880246984, 0.027956706880246984), Duration: 0.5, Reward: -10.36225510239056, Done: False
2024-07-21 20:24:38,916 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:38,916 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:41,881 - AirSimEnvLogger - INFO - Predictive model loss: 0.007631292566657066
2024-07-21 20:24:46,884 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.896416283851632, Velocity: -0.10573157755684781, Collision: 0, Height: -1.0, Movement: -0.06724989241348857, Smoothness: -0.0, Curiosity: 0.9191684722900391, Exploration: 0.07083175228696086, Total: -9.982256475268994
2024-07-21 20:24:47,009 - AirSimEnvLogger - INFO - Action: [0.33624946 0.33624946 0.33624946 0.33624946], Velocity: (0.33624946206744283, 0.33624946206744283, 0.33624946206744283), Duration: 0.5, Reward: -9.982256475268994, Done: False
2024-07-21 20:24:47,073 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:47,073 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:50,118 - AirSimEnvLogger - INFO - Predictive model loss: 0.006637121085077524
2024-07-21 20:24:54,925 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.740760933363505, Velocity: -0.13303389984052777, Collision: 0, Height: -1.0, Movement: -0.07923360976570425, Smoothness: -0.0, Curiosity: 1.0783147811889648, Exploration: 0.04989704568201597, Total: -9.766064444529455
2024-07-21 20:24:55,068 - AirSimEnvLogger - INFO - Action: [0.39616805 0.39616805 0.39616805 0.39616805], Velocity: (0.3961680488285212, 0.3961680488285212, 0.3961680488285212), Duration: 0.5, Reward: -9.766064444529455, Done: False
2024-07-21 20:24:55,177 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:55,178 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:58,107 - AirSimEnvLogger - INFO - Predictive model loss: 0.013336333446204662
2024-07-21 20:25:02,573 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.997993048383153, Velocity: -0.04810105463346957, Collision: 0, Height: -1.0, Movement: -0.026059532597692916, Smoothness: -0.0, Curiosity: 0.6473649144172668, Exploration: 0.021237827029773722, Total: -10.196719506345069
2024-07-21 20:25:02,667 - AirSimEnvLogger - INFO - Action: [-0.13029766 -0.13029766 -0.13029766 -0.13029766], Velocity: (-0.13029766298846457, -0.13029766298846457, -0.13029766298846457), Duration: 0.5, Reward: -10.196719506345069, Done: False
2024-07-21 20:25:02,730 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:25:02,730 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:25:05,758 - AirSimEnvLogger - INFO - Predictive model loss: 0.0037898668088018894
2024-07-21 20:25:10,806 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.876611358303563, Velocity: -0.06871770929729708, Collision: 0, Height: -1.0, Movement: -0.043720572970791606, Smoothness: -0.0, Curiosity: 0.6063529849052429, Exploration: 0.015191278453043223, Total: -10.10912752210606
2024-07-21 20:25:10,839 - AirSimEnvLogger - INFO - Action: [0.21860286 0.21860286 0.21860286 0.21860286], Velocity: (0.21860286485395802, 0.21860286485395802, 0.21860286485395802), Duration: 0.5, Reward: -10.10912752210606, Done: False
2024-07-21 20:25:10,901 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:25:10,901 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:25:13,879 - AirSimEnvLogger - INFO - Predictive model loss: 0.009429915808141232
2024-07-21 20:25:37,633 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:25:37,654 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:25:37,655 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:25:39,244 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:25:39,245 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:25:43,648 - AirSimEnvLogger - ERROR - An error occurred in main: __init__() takes 2 positional arguments but 4 were given
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 129, in main
    ppo_agent = PPOAgent(config, logger=logger, drone_controller=env.envs[0].drone_controller)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 77, in __init__
    self.setup_training_components()
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 162, in setup_training_components
    self.setup_curriculum_learning()
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 181, in setup_curriculum_learning
    self.curriculum = CurriculumLearning(
TypeError: __init__() takes 2 positional arguments but 4 were given
2024-07-21 20:25:44,386 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 20:25:44,450 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:28:52,831 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:28:52,858 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:28:52,858 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:28:54,388 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:28:54,388 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:29:00,044 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:29:05,030 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:29:05,656 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:05,656 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:11,671 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.614561102824306, Velocity: -0.17204010711519885, Movement: 0.43252655286410635, Collision: 0, Height: -1.0, Movement Penalty: -0.09988772869110107, Smoothness: -0.0, Curiosity: 1.3084137439727783, Exploration: 0, Total: -8.812741516507623
2024-07-21 20:29:11,813 - AirSimEnvLogger - INFO - Action: [-0.49943864 -0.49943864 -0.49943864 -0.49943864], Velocity: (-0.49943864345550537, -0.49943864345550537, -0.49943864345550537), Duration: 1.0, Reward: -8.812741516507623, Done: False
2024-07-21 20:29:11,874 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:11,874 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:15,361 - AirSimEnvLogger - INFO - Predictive model loss: 0.025348786264657974
2024-07-21 20:29:20,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.09595598087787, Velocity: -0.031859382348853646, Movement: 0.172019898283861, Collision: 0, Height: -1.0, Movement Penalty: -0.03972629383206368, Smoothness: -0.0, Curiosity: 0.35525965690612793, Exploration: 0.8214963628684021, Total: -10.050718459903187
2024-07-21 20:29:21,151 - AirSimEnvLogger - INFO - Action: [-0.19863147 -0.19863147 -0.19863147 -0.19863147], Velocity: (-0.19863146916031837, -0.19863146916031837, -0.19863146916031837), Duration: 1.0, Reward: -10.050718459903187, Done: False
2024-07-21 20:29:21,230 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:21,230 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:24,071 - AirSimEnvLogger - INFO - Predictive model loss: 0.0093222726136446
2024-07-21 20:29:29,864 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.543088378539114, Velocity: -0.04395662180476603, Movement: 0.5055195494825352, Collision: 0, Height: -1.0, Movement Penalty: -0.116744739189744, Smoothness: -0.0, Curiosity: 2.5467934608459473, Exploration: 0.31949596881469305, Total: -8.41480613971358
2024-07-21 20:29:29,880 - AirSimEnvLogger - INFO - Action: [-0.5837237 -0.5837237 -0.5837237 -0.5837237], Velocity: (-0.58372369594872, -0.58372369594872, -0.58372369594872), Duration: 1.0, Reward: -8.41480613971358, Done: False
2024-07-21 20:29:29,958 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:29,958 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:32,802 - AirSimEnvLogger - INFO - Predictive model loss: 0.06253871321678162
2024-07-21 20:29:38,620 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.445492714002768, Velocity: -0.056614282887171095, Movement: 0.31914719788897994, Collision: 0, Height: -1.0, Movement Penalty: -0.07370388824492694, Smoothness: -0.0, Curiosity: 1.7457008361816406, Exploration: 0.20451702925259277, Total: -9.150585725799754
2024-07-21 20:29:38,715 - AirSimEnvLogger - INFO - Action: [-0.36851944 -0.36851944 -0.36851944 -0.36851944], Velocity: (-0.36851944122463465, -0.36851944122463465, -0.36851944122463465), Duration: 1.0, Reward: -9.150585725799754, Done: False
2024-07-21 20:29:38,824 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:38,824 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:41,454 - AirSimEnvLogger - INFO - Predictive model loss: 0.06001318246126175
2024-07-21 20:29:47,760 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.965104107547987, Velocity: -0.023583893375421803, Movement: 0.13292933551547936, Collision: 0, Height: -1.0, Movement Penalty: -0.030698715057224036, Smoothness: -0.0, Curiosity: 0.49454012513160706, Exploration: 0.1996930112599288, Total: -9.923732229720583
2024-07-21 20:29:47,914 - AirSimEnvLogger - INFO - Action: [0.15349358 0.15349358 0.15349358 0.15349358], Velocity: (0.15349357528612018, 0.15349357528612018, 0.15349357528612018), Duration: 1.0, Reward: -9.923732229720583, Done: False
2024-07-21 20:29:48,008 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:48,008 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:50,932 - AirSimEnvLogger - INFO - Predictive model loss: 0.015042601153254509
2024-07-21 20:29:56,777 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.734351846264337, Velocity: -0.039197440587441054, Movement: 0.2816095123579682, Collision: 0, Height: -1.0, Movement Penalty: -0.06503493110649287, Smoothness: -0.0, Curiosity: 0.6387184858322144, Exploration: 0.2868347350637472, Total: -9.526673249249335
2024-07-21 20:29:56,871 - AirSimEnvLogger - INFO - Action: [0.32517466 0.32517466 0.32517466 0.32517466], Velocity: (0.3251746555324644, 0.3251746555324644, 0.3251746555324644), Duration: 1.0, Reward: -9.526673249249335, Done: False
2024-07-21 20:29:56,933 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:56,933 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:59,753 - AirSimEnvLogger - INFO - Predictive model loss: 0.008507788181304932
2024-07-21 20:30:05,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.41734830386744, Velocity: -0.08504731025089685, Movement: 0.5722110963510545, Collision: 0, Height: -1.0, Movement Penalty: -0.1321464922046289, Smoothness: -0.0, Curiosity: 2.168203115463257, Exploration: 0.2863198516194808, Total: -7.6777600461699596
2024-07-21 20:30:06,054 - AirSimEnvLogger - INFO - Action: [0.66073246 0.66073246 0.66073246 0.66073246], Velocity: (0.6607324610231444, 0.6607324610231444, 0.6607324610231444), Duration: 1.0, Reward: -7.6777600461699596, Done: False
2024-07-21 20:30:06,132 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:06,132 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:08,998 - AirSimEnvLogger - INFO - Predictive model loss: 0.0352761335670948
2024-07-21 20:30:14,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.633585845248902, Velocity: -0.05070123260554181, Movement: 0.28256172830165366, Collision: 0, Height: -1.0, Movement Penalty: -0.06525483595905826, Smoothness: -0.0, Curiosity: 0.6191284656524658, Exploration: 0.15202173206676414, Total: -9.477631280498931
2024-07-21 20:30:15,101 - AirSimEnvLogger - INFO - Action: [0.32627418 0.32627418 0.32627418 0.32627418], Velocity: (0.3262741797952913, 0.3262741797952913, 0.3262741797952913), Duration: 1.0, Reward: -9.477631280498931, Done: False
2024-07-21 20:30:15,164 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:15,164 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:18,240 - AirSimEnvLogger - INFO - Predictive model loss: 0.019899848848581314
2024-07-21 20:30:24,039 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.230179915803276, Velocity: -0.028412664816001222, Movement: 0.2898189099692195, Collision: 0, Height: -1.0, Movement Penalty: -0.06693081027478912, Smoothness: -0.0, Curiosity: 0.6889470219612122, Exploration: 0.3037390916654122, Total: -9.967104926982325
2024-07-21 20:30:24,133 - AirSimEnvLogger - INFO - Action: [-0.33465405 -0.33465405 -0.33465405 -0.33465405], Velocity: (-0.33465405137394555, -0.33465405137394555, -0.33465405137394555), Duration: 1.0, Reward: -9.967104926982325, Done: False
2024-07-21 20:30:24,197 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:24,197 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:26,922 - AirSimEnvLogger - INFO - Predictive model loss: 0.006392229348421097
2024-07-21 20:30:32,915 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.427636607264208, Velocity: -0.06653714220041684, Movement: 0.3827301991316984, Collision: 0, Height: -1.0, Movement Penalty: -0.08838775339827408, Smoothness: -0.0, Curiosity: 1.4803942441940308, Exploration: 0.35054334085511396, Total: -9.36388275771424
2024-07-21 20:30:33,055 - AirSimEnvLogger - INFO - Action: [-0.44193877 -0.44193877 -0.44193877 -0.44193877], Velocity: (-0.44193876699137036, -0.44193876699137036, -0.44193876699137036), Duration: 1.0, Reward: -9.36388275771424, Done: False
2024-07-21 20:30:33,118 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:33,118 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:36,104 - AirSimEnvLogger - INFO - Predictive model loss: 0.026096640154719353
2024-07-21 20:30:41,974 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.718495184924288, Velocity: -0.051860624311785376, Movement: 0.6018784651678085, Collision: 0, Height: -1.0, Movement Penalty: -0.13899787755362922, Smoothness: -0.0, Curiosity: 3.928715229034424, Exploration: 0.283468990541664, Total: -7.215004104149782
2024-07-21 20:30:42,099 - AirSimEnvLogger - INFO - Action: [-0.69498939 -0.69498939 -0.69498939 -0.69498939], Velocity: (-0.6949893877681461, -0.6949893877681461, -0.6949893877681461), Duration: 1.0, Reward: -7.215004104149782, Done: False
2024-07-21 20:30:42,178 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:42,178 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:45,035 - AirSimEnvLogger - INFO - Predictive model loss: 0.09066463261842728
2024-07-21 20:30:51,088 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.591789590965142, Velocity: -0.07682168785255002, Movement: 0.40504462976746264, Collision: 0, Height: -1.0, Movement Penalty: -0.09354105041202275, Smoothness: -0.0, Curiosity: 2.9765429496765137, Exploration: 0.19580740697178026, Total: -8.06795738350894
2024-07-21 20:30:51,167 - AirSimEnvLogger - INFO - Action: [-0.46770525 -0.46770525 -0.46770525 -0.46770525], Velocity: (-0.4677052520601137, -0.4677052520601137, -0.4677052520601137), Duration: 1.0, Reward: -8.06795738350894, Done: False
2024-07-21 20:30:51,230 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:51,230 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:54,277 - AirSimEnvLogger - INFO - Predictive model loss: 0.08942160755395889
2024-07-21 20:31:00,131 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.359171462692844, Velocity: -0.0333073886754857, Movement: 0.19166784607429332, Collision: 0, Height: -1.0, Movement Penalty: -0.044263793010395604, Smoothness: -0.0, Curiosity: 2.013113021850586, Exploration: 0.1159656159085534, Total: -8.818015486105635
2024-07-21 20:31:00,240 - AirSimEnvLogger - INFO - Action: [-0.22131897 -0.22131897 -0.22131897 -0.22131897], Velocity: (-0.221318965051978, -0.221318965051978, -0.221318965051978), Duration: 1.0, Reward: -8.818015486105635, Done: False
2024-07-21 20:31:00,302 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:00,302 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:03,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.06683352589607239
2024-07-21 20:31:08,937 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.718165560861038, Velocity: -0.10198687916427925, Movement: 0.5288375399613315, Collision: 0, Height: -1.0, Movement Penalty: -0.12212979842170171, Smoothness: -0.0, Curiosity: 5.364080429077148, Exploration: 0.16083729353692058, Total: -5.814318885803092
2024-07-21 20:31:09,014 - AirSimEnvLogger - INFO - Action: [-0.61064899 -0.61064899 -0.61064899 -0.61064899], Velocity: (-0.6106489921085085, -0.6106489921085085, -0.6106489921085085), Duration: 1.0, Reward: -5.814318885803092, Done: False
2024-07-21 20:31:09,077 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:09,077 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:11,835 - AirSimEnvLogger - INFO - Predictive model loss: 0.1640063375234604
2024-07-21 20:31:17,939 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.133665583643483, Velocity: -0.011597582462700698, Movement: 0.06741513329434626, Collision: 0, Height: -1.0, Movement Penalty: -0.015568858141978127, Smoothness: -0.0, Curiosity: 2.0217602252960205, Exploration: 0.08065090177106472, Total: -8.592861719596169
2024-07-21 20:31:18,049 - AirSimEnvLogger - INFO - Action: [0.07784429 0.07784429 0.07784429 0.07784429], Velocity: (0.07784429070989063, 0.07784429070989063, 0.07784429070989063), Duration: 1.0, Reward: -8.592861719596169, Done: False
2024-07-21 20:31:18,112 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:18,112 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:20,965 - AirSimEnvLogger - INFO - Predictive model loss: 0.05900371074676514
2024-07-21 20:31:27,134 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.628781140390766, Velocity: -0.07014283172988799, Movement: 0.4433862766205411, Collision: 0, Height: -1.0, Movement Penalty: -0.10239567446474213, Smoothness: -0.0, Curiosity: 2.0288023948669434, Exploration: 0.420730734986058, Total: -7.999348718155512
2024-07-21 20:31:27,230 - AirSimEnvLogger - INFO - Action: [0.51197837 0.51197837 0.51197837 0.51197837], Velocity: (0.5119783723237106, 0.5119783723237106, 0.5119783723237106), Duration: 1.0, Reward: -7.999348718155512, Done: False
2024-07-21 20:31:27,291 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:27,291 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:30,188 - AirSimEnvLogger - INFO - Predictive model loss: 0.01884705200791359
2024-07-21 20:31:36,101 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.233294417545078, Velocity: -0.03481325820519606, Movement: 0.20491151235037353, Collision: 0, Height: -1.0, Movement Penalty: -0.04732228672621659, Smoothness: -0.0, Curiosity: 2.351763963699341, Exploration: 0.037213424860465485, Total: -8.371426223582406
2024-07-21 20:31:36,211 - AirSimEnvLogger - INFO - Action: [-0.23661143 -0.23661143 -0.23661143 -0.23661143], Velocity: (-0.23661143363108295, -0.23661143363108295, -0.23661143363108295), Duration: 1.0, Reward: -8.371426223582406, Done: False
2024-07-21 20:31:36,304 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:36,304 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:39,109 - AirSimEnvLogger - INFO - Predictive model loss: 0.04590431600809097
2024-07-21 20:31:45,048 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.471190853209238, Velocity: -0.06139418065103781, Movement: 0.3210448876623169, Collision: 0, Height: -1.0, Movement Penalty: -0.07414214092551674, Smoothness: -0.0, Curiosity: 3.6751229763031006, Exploration: 0.3405934902000527, Total: -7.215995578415264
2024-07-21 20:31:45,157 - AirSimEnvLogger - INFO - Action: [-0.3707107 -0.3707107 -0.3707107 -0.3707107], Velocity: (-0.3707107046275837, -0.3707107046275837, -0.3707107046275837), Duration: 1.0, Reward: -7.215995578415264, Done: False
2024-07-21 20:31:45,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:45,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:48,115 - AirSimEnvLogger - INFO - Predictive model loss: 0.07253023982048035
2024-07-21 20:31:53,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.236677462290691, Velocity: -0.009131293795834964, Movement: 0.05578784163490582, Collision: 0, Height: -1.0, Movement Penalty: -0.012883650154168437, Smoothness: -0.0, Curiosity: 2.523552179336548, Exploration: 0.03992770019046863, Total: -8.203486568538118
2024-07-21 20:31:54,084 - AirSimEnvLogger - INFO - Action: [-0.06441825 -0.06441825 -0.06441825 -0.06441825], Velocity: (-0.06441825077084218, -0.06441825077084218, -0.06441825077084218), Duration: 1.0, Reward: -8.203486568538118, Done: False
2024-07-21 20:31:54,148 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:54,148 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:57,057 - AirSimEnvLogger - INFO - Predictive model loss: 0.03453578054904938
2024-07-21 20:32:02,837 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.696362511390001, Velocity: -0.07142676156463436, Movement: 0.399646636408143, Collision: 0, Height: -1.0, Movement Penalty: -0.09229443724438796, Smoothness: -0.0, Curiosity: 2.048673629760742, Exploration: 0.3002641364503965, Total: -8.075967495927186
2024-07-21 20:32:02,917 - AirSimEnvLogger - INFO - Action: [0.46147219 0.46147219 0.46147219 0.46147219], Velocity: (0.46147218622193975, 0.46147218622193975, 0.46147218622193975), Duration: 1.0, Reward: -8.075967495927186, Done: False
2024-07-21 20:32:02,964 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:02,964 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:05,834 - AirSimEnvLogger - INFO - Predictive model loss: 0.013038602657616138
2024-07-21 20:32:11,879 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.3572623042127, Velocity: -0.06857666790060668, Movement: 0.6328034226115539, Collision: 0, Height: -1.0, Movement Penalty: -0.14613969055555884, Smoothness: -0.0, Curiosity: 2.8574273586273193, Exploration: 0.39599839843905177, Total: -6.900090346846621
2024-07-21 20:32:11,989 - AirSimEnvLogger - INFO - Action: [0.73069845 0.73069845 0.73069845 0.73069845], Velocity: (0.7306984527777942, 0.7306984527777942, 0.7306984527777942), Duration: 1.0, Reward: -6.900090346846621, Done: False
2024-07-21 20:32:12,084 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:12,084 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:14,969 - AirSimEnvLogger - INFO - Predictive model loss: 0.0657591000199318
2024-07-21 20:32:20,715 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.21083897288663, Velocity: -0.07486757994109092, Movement: 0.7191677671781368, Collision: 0, Height: -1.0, Movement Penalty: -0.16608468158911976, Smoothness: -0.0, Curiosity: 3.4153294563293457, Exploration: 0.31972586399105996, Total: -6.211818179223069
2024-07-21 20:32:20,840 - AirSimEnvLogger - INFO - Action: [0.83042341 0.83042341 0.83042341 0.83042341], Velocity: (0.8304234079455988, 0.8304234079455988, 0.8304234079455988), Duration: 1.0, Reward: -6.211818179223069, Done: False
2024-07-21 20:32:20,949 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:20,949 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:23,953 - AirSimEnvLogger - INFO - Predictive model loss: 0.14046615362167358
2024-07-21 20:32:30,158 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.347697681168865, Velocity: -0.08510850537275663, Movement: 0.5481472602464594, Collision: 0, Height: -1.0, Movement Penalty: -0.12658918730353969, Smoothness: -0.0, Curiosity: 2.091301679611206, Exploration: 0.20082053315539847, Total: -7.70527210668646
2024-07-21 20:32:30,220 - AirSimEnvLogger - INFO - Action: [0.63294594 0.63294594 0.63294594 0.63294594], Velocity: (0.6329459365176984, 0.6329459365176984, 0.6329459365176984), Duration: 1.0, Reward: -7.70527210668646, Done: False
2024-07-21 20:32:30,283 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:30,283 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:33,231 - AirSimEnvLogger - INFO - Predictive model loss: 0.15162122249603271
2024-07-21 20:32:39,336 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.246746485250018, Velocity: -0.1080877422443835, Movement: 0.7070319254455159, Collision: 0, Height: -1.0, Movement Penalty: -0.1632820289926512, Smoothness: -0.0, Curiosity: 3.7551214694976807, Exploration: 0.21313260465209402, Total: -5.936069835840747
2024-07-21 20:32:39,462 - AirSimEnvLogger - INFO - Action: [0.81641014 0.81641014 0.81641014 0.81641014], Velocity: (0.8164101449632559, 0.8164101449632559, 0.8164101449632559), Duration: 1.0, Reward: -5.936069835840747, Done: False
2024-07-21 20:32:39,493 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:39,493 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:42,301 - AirSimEnvLogger - INFO - Predictive model loss: 0.23846954107284546
2024-07-21 20:32:48,555 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.241814842157757, Velocity: -0.11856956523468636, Movement: 0.7332415330867411, Collision: 0, Height: -1.0, Movement Penalty: -0.16933487860345756, Smoothness: -0.0, Curiosity: 4.795995712280273, Exploration: 0.2843400134651814, Total: -4.874291507973545
2024-07-21 20:32:48,679 - AirSimEnvLogger - INFO - Action: [0.84667439 0.84667439 0.84667439 0.84667439], Velocity: (0.8466743930172878, 0.8466743930172878, 0.8466743930172878), Duration: 1.0, Reward: -4.874291507973545, Done: False
2024-07-21 20:32:48,741 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:48,741 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:51,594 - AirSimEnvLogger - INFO - Predictive model loss: 0.31259992718696594
2024-07-21 20:32:57,681 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.753334140564107, Velocity: -0.028753110906492886, Movement: 0.15516288329579186, Collision: 0, Height: -1.0, Movement Penalty: -0.03583333297562557, Smoothness: -0.0, Curiosity: 0.8935874104499817, Exploration: 0.15984342244949196, Total: -9.322051916867792
2024-07-21 20:32:57,759 - AirSimEnvLogger - INFO - Action: [0.17916666 0.17916666 0.17916666 0.17916666], Velocity: (0.17916666487812782, 0.17916666487812782, 0.17916666487812782), Duration: 1.0, Reward: -9.322051916867792, Done: False
2024-07-21 20:32:57,822 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:57,822 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:00,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.1797233670949936
2024-07-21 20:33:06,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.69983481558925, Velocity: -0.061708460826306946, Movement: 0.3394714406949829, Collision: 0, Height: -1.0, Movement Penalty: -0.07839757106697538, Smoothness: -0.0, Curiosity: 2.0683298110961914, Exploration: 0.14002807121445054, Total: -8.097142178032556
2024-07-21 20:33:06,774 - AirSimEnvLogger - INFO - Action: [0.39198786 0.39198786 0.39198786 0.39198786], Velocity: (0.3919878553348769, 0.3919878553348769, 0.3919878553348769), Duration: 1.0, Reward: -8.097142178032556, Done: False
2024-07-21 20:33:06,789 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:06,789 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:09,667 - AirSimEnvLogger - INFO - Predictive model loss: 0.22192926704883575
2024-07-21 20:33:15,615 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.593621670856056, Velocity: -0.08207795415018806, Movement: 0.49557447386426406, Collision: 0, Height: -1.0, Movement Penalty: -0.114448022355616, Smoothness: -0.0, Curiosity: 3.933405876159668, Exploration: 0.22617683209409475, Total: -6.104246503566576
2024-07-21 20:33:15,709 - AirSimEnvLogger - INFO - Action: [0.57224011 0.57224011 0.57224011 0.57224011], Velocity: (0.57224011177808, 0.57224011177808, 0.57224011177808), Duration: 1.0, Reward: -6.104246503566576, Done: False
2024-07-21 20:33:15,756 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:15,756 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:18,652 - AirSimEnvLogger - INFO - Predictive model loss: 0.2923852205276489
2024-07-21 20:33:24,431 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.619200099135835, Velocity: -0.031369626035921985, Movement: 0.486882147846044, Collision: 0, Height: -1.0, Movement Penalty: -0.11244061564901467, Smoothness: -0.0, Curiosity: 4.807024002075195, Exploration: 0.2050646943878461, Total: -5.256204987081088
2024-07-21 20:33:24,509 - AirSimEnvLogger - INFO - Action: [0.56220308 0.56220308 0.56220308 0.56220308], Velocity: (0.5622030782450733, 0.5622030782450733, 0.5622030782450733), Duration: 1.0, Reward: -5.256204987081088, Done: False
2024-07-21 20:33:24,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:24,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:27,503 - AirSimEnvLogger - INFO - Predictive model loss: 0.3368881940841675
2024-07-21 20:33:33,407 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.632057592610794, Velocity: -0.08885463270755607, Movement: 0.5499820906659039, Collision: 0, Height: -1.0, Movement Penalty: -0.12701292323817312, Smoothness: -0.0, Curiosity: 6.632537841796875, Exploration: 0.19017192757035073, Total: -3.451174639669354
2024-07-21 20:33:33,517 - AirSimEnvLogger - INFO - Action: [0.63506462 0.63506462 0.63506462 0.63506462], Velocity: (0.6350646161908655, 0.6350646161908655, 0.6350646161908655), Duration: 1.0, Reward: -3.451174639669354, Done: False
2024-07-21 20:33:33,548 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:33,548 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:36,490 - AirSimEnvLogger - INFO - Predictive model loss: 0.41913866996765137
2024-07-21 20:33:42,358 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.607485959773204, Velocity: -0.06385151530338198, Movement: 0.7006926191994957, Collision: 0, Height: -1.0, Movement Penalty: -0.16181802892560512, Smoothness: -0.0, Curiosity: 10.20175552368164, Exploration: 0.2623909597631615, Total: 0.1654222853487528
2024-07-21 20:33:42,466 - AirSimEnvLogger - INFO - Action: [0.80909014 0.80909014 0.80909014 0.80909014], Velocity: (0.8090901446280255, 0.8090901446280255, 0.8090901446280255), Duration: 1.0, Reward: 0.1654222853487528, Done: False
2024-07-21 20:33:42,591 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:42,591 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:45,471 - AirSimEnvLogger - INFO - Predictive model loss: 0.5714067816734314
2024-07-21 20:33:51,391 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.668484883932384, Velocity: -0.06178392762268092, Movement: 0.7591360606128643, Collision: 0, Height: -1.0, Movement Penalty: -0.17531496357855572, Smoothness: -0.0, Curiosity: 13.45190715789795, Exploration: 0.2954262440801932, Total: 3.363813484038266
2024-07-21 20:33:51,530 - AirSimEnvLogger - INFO - Action: [0.87657482 0.87657482 0.87657482 0.87657482], Velocity: (0.8765748178927786, 0.8765748178927786, 0.8765748178927786), Duration: 1.0, Reward: 3.363813484038266, Done: False
2024-07-21 20:33:51,594 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:51,594 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:54,379 - AirSimEnvLogger - INFO - Predictive model loss: 0.7347251772880554
2024-07-21 20:34:00,167 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.244450335714474, Velocity: -0.009071811706331506, Movement: 0.04949332992046609, Collision: 0, Height: -1.0, Movement Penalty: -0.011429994941068822, Smoothness: -0.0, Curiosity: 6.510580062866211, Exploration: 0.21277721310505415, Total: -4.184624626286403
2024-07-21 20:34:00,277 - AirSimEnvLogger - INFO - Action: [-0.05714997 -0.05714997 -0.05714997 -0.05714997], Velocity: (-0.05714997470534411, -0.05714997470534411, -0.05714997470534411), Duration: 1.0, Reward: -4.184624626286403, Done: False
2024-07-21 20:34:00,340 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:00,340 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:03,298 - AirSimEnvLogger - INFO - Predictive model loss: 0.45934391021728516
2024-07-21 20:34:09,108 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.612120937114641, Velocity: -0.07453689279416627, Movement: 0.4500172706556454, Collision: 0, Height: -1.0, Movement Penalty: -0.10392703694120703, Smoothness: -0.0, Curiosity: 5.270089149475098, Exploration: 0.5111645486588634, Total: -5.720878731306237
2024-07-21 20:34:09,217 - AirSimEnvLogger - INFO - Action: [-0.51963518 -0.51963518 -0.51963518 -0.51963518], Velocity: (-0.5196351847060351, -0.5196351847060351, -0.5196351847060351), Duration: 1.0, Reward: -5.720878731306237, Done: False
2024-07-21 20:34:09,311 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:09,311 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:12,147 - AirSimEnvLogger - INFO - Predictive model loss: 0.29111745953559875
2024-07-21 20:34:17,816 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.327188594178757, Velocity: -0.0054144294899325035, Movement: 0.027864507135563585, Collision: 0, Height: -1.0, Movement Penalty: -0.006435032278221554, Smoothness: -0.0, Curiosity: 5.548956394195557, Exploration: 0.0698178697966083, Total: -5.262032018019107
2024-07-21 20:34:17,910 - AirSimEnvLogger - INFO - Action: [0.03217516 0.03217516 0.03217516 0.03217516], Velocity: (0.032175161391107765, 0.032175161391107765, 0.032175161391107765), Duration: 1.0, Reward: -5.262032018019107, Done: False
2024-07-21 20:34:17,988 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:17,988 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:20,820 - AirSimEnvLogger - INFO - Predictive model loss: 0.36896443367004395
2024-07-21 20:34:26,707 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.023732397395483, Velocity: -0.05929601894161146, Movement: 0.4392709190947472, Collision: 0, Height: -1.0, Movement Penalty: -0.10144527335461065, Smoothness: -0.0, Curiosity: 9.56267261505127, Exploration: 0.3710533397877774, Total: -0.8708718240912268
2024-07-21 20:34:26,816 - AirSimEnvLogger - INFO - Action: [0.50722637 0.50722637 0.50722637 0.50722637], Velocity: (0.5072263667730532, 0.5072263667730532, 0.5072263667730532), Duration: 1.0, Reward: -0.8708718240912268, Done: False
2024-07-21 20:34:26,831 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:26,831 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:29,569 - AirSimEnvLogger - INFO - Predictive model loss: 0.5497702956199646
2024-07-21 20:34:35,406 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.228354097405301, Velocity: -0.009123207836564243, Movement: 0.06538471667746071, Collision: 0, Height: -1.0, Movement Penalty: -0.015099953509847741, Smoothness: -0.0, Curiosity: 7.2196879386901855, Exploration: 0.11191087243571692, Total: -3.4822350952586363
2024-07-21 20:34:35,500 - AirSimEnvLogger - INFO - Action: [0.07549977 0.07549977 0.07549977 0.07549977], Velocity: (0.0754997675492387, 0.0754997675492387, 0.0754997675492387), Duration: 1.0, Reward: -3.4822350952586363, Done: False
2024-07-21 20:34:35,594 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:35,594 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:38,468 - AirSimEnvLogger - INFO - Predictive model loss: 0.45757463574409485
2024-07-21 20:34:44,411 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.559011302018716, Velocity: -0.058535092611282466, Movement: 0.369160632436605, Collision: 0, Height: -1.0, Movement Penalty: -0.08525399620459456, Smoothness: -0.0, Curiosity: 5.49835205078125, Exploration: 0.3189610237149578, Total: -5.4840962346053255
2024-07-21 20:34:44,551 - AirSimEnvLogger - INFO - Action: [-0.42626998 -0.42626998 -0.42626998 -0.42626998], Velocity: (-0.42626998102297275, -0.42626998102297275, -0.42626998102297275), Duration: 1.0, Reward: -5.4840962346053255, Done: False
2024-07-21 20:34:44,582 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:44,582 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:47,468 - AirSimEnvLogger - INFO - Predictive model loss: 0.29908254742622375
2024-07-21 20:34:53,336 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.729780587894858, Velocity: -0.09299630109364256, Movement: 0.5630749508423306, Collision: 0, Height: -1.0, Movement Penalty: -0.13003658977710195, Smoothness: -0.0, Curiosity: 4.8486151695251465, Exploration: 0.3781669953094913, Total: -6.289674441328517
2024-07-21 20:34:53,478 - AirSimEnvLogger - INFO - Action: [-0.65018295 -0.65018295 -0.65018295 -0.65018295], Velocity: (-0.6501829488855098, -0.6501829488855098, -0.6501829488855098), Duration: 1.0, Reward: -6.289674441328517, Done: False
2024-07-21 20:34:53,557 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:53,557 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:56,596 - AirSimEnvLogger - INFO - Predictive model loss: 0.19447672367095947
2024-07-21 20:35:02,594 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.219571352940504, Velocity: -0.02079307160405111, Movement: 0.14868193432486662, Collision: 0, Height: -1.0, Movement Penalty: -0.03433662192243841, Smoothness: -0.0, Curiosity: 5.256308555603027, Exploration: 0.12202802365672506, Total: -5.433628499212574
2024-07-21 20:35:02,736 - AirSimEnvLogger - INFO - Action: [0.17168311 0.17168311 0.17168311 0.17168311], Velocity: (0.17168310961219202, 0.17168310961219202, 0.17168310961219202), Duration: 1.0, Reward: -5.433628499212574, Done: False
2024-07-21 20:35:02,799 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:02,799 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:05,808 - AirSimEnvLogger - INFO - Predictive model loss: 0.3167564570903778
2024-07-21 20:35:11,883 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.934085468094928, Velocity: -0.06690096526670297, Movement: 0.4861979014603195, Collision: 0, Height: -1.0, Movement Penalty: -0.11228259571501864, Smoothness: -0.0, Curiosity: 9.184707641601562, Exploration: 0.46750477678111185, Total: -1.1366154160134438
2024-07-21 20:35:12,010 - AirSimEnvLogger - INFO - Action: [0.56141298 0.56141298 0.56141298 0.56141298], Velocity: (0.5614129785750932, 0.5614129785750932, 0.5614129785750932), Duration: 1.0, Reward: -1.1366154160134438, Done: False
2024-07-21 20:35:12,074 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:12,074 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:15,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.48864027857780457
2024-07-21 20:35:21,569 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.334060565791283, Velocity: -0.0282678085561455, Movement: 0.164224429661156, Collision: 0, Height: -1.0, Movement Penalty: -0.03792600746895247, Smoothness: -0.0, Curiosity: 5.214900970458984, Exploration: 0.04306878290499731, Total: -5.608052139701678
2024-07-21 20:35:21,616 - AirSimEnvLogger - INFO - Action: [-0.18963004 -0.18963004 -0.18963004 -0.18963004], Velocity: (-0.18963003734476236, -0.18963003734476236, -0.18963003734476236), Duration: 1.0, Reward: -5.608052139701678, Done: False
2024-07-21 20:35:21,679 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:21,679 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:24,633 - AirSimEnvLogger - INFO - Predictive model loss: 0.3057638108730316
2024-07-21 20:35:30,466 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.273928937616327, Velocity: -0.005316084805818667, Movement: 0.03276863839532664, Collision: 0, Height: -1.0, Movement Penalty: -0.0075675928794077365, Smoothness: -0.0, Curiosity: 5.776591777801514, Exploration: 0.17832638458545216, Total: -4.9560498869051
2024-07-21 20:35:30,559 - AirSimEnvLogger - INFO - Action: [0.03783796 0.03783796 0.03783796 0.03783796], Velocity: (0.03783796439703868, 0.03783796439703868, 0.03783796439703868), Duration: 1.0, Reward: -4.9560498869051, Done: False
2024-07-21 20:35:30,621 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:30,621 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:33,572 - AirSimEnvLogger - INFO - Predictive model loss: 0.3461875021457672
2024-07-21 20:35:39,498 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.484858129563948, Velocity: -0.05381864989488847, Movement: 0.31104618782361193, Collision: 0, Height: -1.0, Movement Penalty: -0.0718330401081477, Smoothness: -0.0, Curiosity: 4.512038707733154, Exploration: 0.10476846830244262, Total: -6.446474592869489
2024-07-21 20:35:39,620 - AirSimEnvLogger - INFO - Action: [-0.3591652 -0.3591652 -0.3591652 -0.3591652], Velocity: (-0.35916520054073847, -0.35916520054073847, -0.35916520054073847), Duration: 1.0, Reward: -6.446474592869489, Done: False
2024-07-21 20:35:39,636 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:39,636 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:42,502 - AirSimEnvLogger - INFO - Predictive model loss: 0.23288856446743011
2024-07-21 20:35:48,581 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.236403124998064, Velocity: -0.014197940735342241, Movement: 0.09259130616177112, Collision: 0, Height: -1.0, Movement Penalty: -0.021383046214847046, Smoothness: -0.0, Curiosity: 5.395857334136963, Exploration: 0.0277356777153388, Total: -5.333315119304884
2024-07-21 20:35:48,676 - AirSimEnvLogger - INFO - Action: [0.10691523 0.10691523 0.10691523 0.10691523], Velocity: (0.10691523107423523, 0.10691523107423523, 0.10691523107423523), Duration: 1.0, Reward: -5.333315119304884, Done: False
2024-07-21 20:35:48,706 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:48,706 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:51,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.3128349184989929
2024-07-21 20:35:57,748 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.141984523649805, Velocity: -0.029710146041511284, Movement: 0.17328141321121446, Collision: 0, Height: -1.0, Movement Penalty: -0.040017628225221376, Smoothness: -0.0, Curiosity: 6.295618057250977, Exploration: 0.2072576787412071, Total: -4.297417631155542
2024-07-21 20:35:57,839 - AirSimEnvLogger - INFO - Action: [0.20008814 0.20008814 0.20008814 0.20008814], Velocity: (0.20008814112610687, 0.20008814112610687, 0.20008814112610687), Duration: 1.0, Reward: -4.297417631155542, Done: False
2024-07-21 20:35:57,871 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:57,871 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:00,822 - AirSimEnvLogger - INFO - Predictive model loss: 0.35609742999076843
2024-07-21 20:36:06,391 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.94718331827469, Velocity: -0.031160929583369332, Movement: 0.5196454333412281, Collision: 0, Height: -1.0, Movement Penalty: -0.12000697232908714, Smoothness: -0.0, Curiosity: 10.712013244628906, Exploration: 0.2543556877469361, Total: 0.33296251608090477
2024-07-21 20:36:06,517 - AirSimEnvLogger - INFO - Action: [0.60003486 0.60003486 0.60003486 0.60003486], Velocity: (0.6000348616454356, 0.6000348616454356, 0.6000348616454356), Duration: 1.0, Reward: 0.33296251608090477, Done: False
2024-07-21 20:36:06,566 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:06,566 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:09,560 - AirSimEnvLogger - INFO - Predictive model loss: 0.5408284068107605
2024-07-21 20:36:15,295 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.99825533483957, Velocity: -0.08450328698574917, Movement: 0.49139968748822666, Collision: 0, Height: -1.0, Movement Penalty: -0.11348389674041026, Smoothness: -0.0, Curiosity: 12.248441696166992, Exploration: 0.2656616957368196, Total: 1.8148922305264623
2024-07-21 20:36:15,374 - AirSimEnvLogger - INFO - Action: [0.56741948 0.56741948 0.56741948 0.56741948], Velocity: (0.5674194837020513, 0.5674194837020513, 0.5674194837020513), Duration: 1.0, Reward: 1.8148922305264623, Done: False
2024-07-21 20:36:15,421 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:15,421 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:18,354 - AirSimEnvLogger - INFO - Predictive model loss: 0.6340818405151367
2024-07-21 20:36:24,185 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.193710660980406, Velocity: -0.043961899150728165, Movement: 0.2658523805146375, Collision: 0, Height: -1.0, Movement Penalty: -0.06139597738193151, Smoothness: -0.0, Curiosity: 11.177961349487305, Exploration: 0.09676431590424357, Total: 0.508631635830886
2024-07-21 20:36:24,294 - AirSimEnvLogger - INFO - Action: [0.30697989 0.30697989 0.30697989 0.30697989], Velocity: (0.30697988690965755, 0.30697988690965755, 0.30697988690965755), Duration: 1.0, Reward: 0.508631635830886, Done: False
2024-07-21 20:36:24,324 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:24,324 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:27,223 - AirSimEnvLogger - INFO - Predictive model loss: 0.6135286688804626
2024-07-21 20:36:33,183 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.57625825778699, Velocity: -0.041816943613042265, Movement: 0.27931806823173816, Collision: 0, Height: -1.0, Movement Penalty: -0.06450574475324812, Smoothness: -0.0, Curiosity: 7.924701690673828, Exploration: 0.28557685560168694, Total: -3.083203927510258
2024-07-21 20:36:33,307 - AirSimEnvLogger - INFO - Action: [-0.32252872 -0.32252872 -0.32252872 -0.32252872], Velocity: (-0.3225287237662406, -0.3225287237662406, -0.3225287237662406), Duration: 1.0, Reward: -3.083203927510258, Done: False
2024-07-21 20:36:33,401 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:33,401 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:36,343 - AirSimEnvLogger - INFO - Predictive model loss: 0.408012330532074
2024-07-21 20:36:42,395 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.804228738086106, Velocity: -0.10714034801272092, Movement: 0.5660311405663147, Collision: 0, Height: -1.0, Movement Penalty: -0.13071929255026907, Smoothness: -0.0, Curiosity: 6.808797836303711, Exploration: 0.43521058130916396, Total: -4.392161789478408
2024-07-21 20:36:42,471 - AirSimEnvLogger - INFO - Action: [-0.65359646 -0.65359646 -0.65359646 -0.65359646], Velocity: (-0.6535964627513453, -0.6535964627513453, -0.6535964627513453), Duration: 1.0, Reward: -4.392161789478408, Done: False
2024-07-21 20:36:42,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:42,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:45,444 - AirSimEnvLogger - INFO - Predictive model loss: 0.26166319847106934
2024-07-21 20:36:51,193 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.727325992197185, Velocity: -0.07186379640741575, Movement: 0.45487791083125995, Collision: 0, Height: -1.0, Movement Penalty: -0.10504955370673702, Smoothness: -0.0, Curiosity: 5.325860023498535, Exploration: 0.24789773900626735, Total: -5.840477737704942
2024-07-21 20:36:51,333 - AirSimEnvLogger - INFO - Action: [-0.52524777 -0.52524777 -0.52524777 -0.52524777], Velocity: (-0.525247768533685, -0.525247768533685, -0.525247768533685), Duration: 1.0, Reward: -5.840477737704942, Done: False
2024-07-21 20:36:51,396 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:51,396 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:54,364 - AirSimEnvLogger - INFO - Predictive model loss: 0.20472322404384613
2024-07-21 20:37:00,095 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.795719978665607, Velocity: -0.0654335074852026, Movement: 0.6251206811943345, Collision: 0, Height: -1.0, Movement Penalty: -0.1443654374254205, Smoothness: -0.0, Curiosity: 4.737236022949219, Exploration: 0.2176811950381485, Total: -6.499626454951638
2024-07-21 20:37:00,143 - AirSimEnvLogger - INFO - Action: [-0.72182719 -0.72182719 -0.72182719 -0.72182719], Velocity: (-0.7218271871271025, -0.7218271871271025, -0.7218271871271025), Duration: 1.0, Reward: -6.499626454951638, Done: False
2024-07-21 20:37:00,236 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:00,237 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:03,229 - AirSimEnvLogger - INFO - Predictive model loss: 0.11787058413028717
2024-07-21 20:37:09,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.472580480997737, Velocity: -0.009847223410284848, Movement: 0.2285450403860728, Collision: 0, Height: -1.0, Movement Penalty: -0.052780216235541216, Smoothness: -0.0, Curiosity: 3.004380702972412, Exploration: 0.12687962659289523, Total: -7.934395988713706
2024-07-21 20:37:09,663 - AirSimEnvLogger - INFO - Action: [-0.26390108 -0.26390108 -0.26390108 -0.26390108], Velocity: (-0.26390108117770605, -0.26390108117770605, -0.26390108117770605), Duration: 1.0, Reward: -7.934395988713706, Done: False
2024-07-21 20:37:09,726 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:09,726 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:12,703 - AirSimEnvLogger - INFO - Predictive model loss: 0.10989519953727722
2024-07-21 20:37:18,880 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.129973418088763, Velocity: -0.022872749465423923, Movement: 0.1303600916909289, Collision: 0, Height: -1.0, Movement Penalty: -0.030105373611736843, Smoothness: -0.0, Curiosity: 3.851043701171875, Exploration: 0.27191679894241677, Total: -6.715478402075813
2024-07-21 20:37:18,988 - AirSimEnvLogger - INFO - Action: [0.15052687 0.15052687 0.15052687 0.15052687], Velocity: (0.1505268680586842, 0.1505268680586842, 0.1505268680586842), Duration: 1.0, Reward: -6.715478402075813, Done: False
2024-07-21 20:37:19,050 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:19,050 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:21,728 - AirSimEnvLogger - INFO - Predictive model loss: 0.15106631815433502
2024-07-21 20:37:27,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.844957215504504, Velocity: -0.07188397972423234, Movement: 0.4880999418715466, Collision: 0, Height: -1.0, Movement Penalty: -0.11272185313239125, Smoothness: -0.0, Curiosity: 7.556431293487549, Exploration: 0.35726398304827595, Total: -2.7015705385924544
2024-07-21 20:37:28,102 - AirSimEnvLogger - INFO - Action: [0.56360927 0.56360927 0.56360927 0.56360927], Velocity: (0.5636092656619562, 0.5636092656619562, 0.5636092656619562), Duration: 1.0, Reward: -2.7015705385924544, Done: False
2024-07-21 20:37:28,133 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:28,133 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:31,057 - AirSimEnvLogger - INFO - Predictive model loss: 0.26826566457748413
2024-07-21 20:37:37,242 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.746708396872053, Velocity: -0.0928120239054558, Movement: 0.6768146686863947, Collision: 0, Height: -1.0, Movement Penalty: -0.15630365246303096, Smoothness: -0.0, Curiosity: 11.760554313659668, Exploration: 0.3685677276949683, Total: 1.6059375191514917
2024-07-21 20:37:37,369 - AirSimEnvLogger - INFO - Action: [0.78151826 0.78151826 0.78151826 0.78151826], Velocity: (0.7815182623151548, 0.7815182623151548, 0.7815182623151548), Duration: 1.0, Reward: 1.6059375191514917, Done: False
2024-07-21 20:37:37,432 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:37,432 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:40,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.4064730405807495
2024-07-21 20:37:46,128 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.786842463819559, Velocity: -0.06298281456736445, Movement: 0.7710243497441234, Collision: 0, Height: -1.0, Movement Penalty: -0.17806044635061036, Smoothness: -0.0, Curiosity: 15.893390655517578, Exploration: 0.3218872272888035, Total: 5.69319715830658
2024-07-21 20:37:46,223 - AirSimEnvLogger - INFO - Action: [0.89030223 0.89030223 0.89030223 0.89030223], Velocity: (0.8903022317530516, 0.8903022317530516, 0.8903022317530516), Duration: 1.0, Reward: 5.69319715830658, Done: False
2024-07-21 20:37:46,302 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:46,302 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:49,237 - AirSimEnvLogger - INFO - Predictive model loss: 0.5511857271194458
2024-07-21 20:37:55,265 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.929591990481132, Velocity: -0.12408319516400437, Movement: 0.7581166788711797, Collision: 0, Height: -1.0, Movement Penalty: -0.17507954744936827, Smoothness: -0.0, Curiosity: 18.839519500732422, Exploration: 0.27643177073695746, Total: 8.47969505932167
2024-07-21 20:37:55,358 - AirSimEnvLogger - INFO - Action: [0.87539774 0.87539774 0.87539774 0.87539774], Velocity: (0.8753977372468413, 0.8753977372468413, 0.8753977372468413), Duration: 1.0, Reward: 8.47969505932167, Done: False
2024-07-21 20:37:55,419 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:55,419 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:58,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.6697343587875366
2024-07-21 20:38:04,265 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.263634375397082, Velocity: -0.04706222143116578, Movement: 0.3436661914190566, Collision: 0, Height: -1.0, Movement Penalty: -0.07936630725086631, Smoothness: -0.0, Curiosity: 15.329765319824219, Exploration: 0.15520467254911302, Total: 4.605551910143551
2024-07-21 20:38:04,312 - AirSimEnvLogger - INFO - Action: [0.39683154 0.39683154 0.39683154 0.39683154], Velocity: (0.39683153625433154, 0.39683153625433154, 0.39683153625433154), Duration: 1.0, Reward: 4.605551910143551, Done: False
2024-07-21 20:38:04,374 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:04,374 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:07,307 - AirSimEnvLogger - INFO - Predictive model loss: 0.5655396580696106
2024-07-21 20:38:13,484 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.33296159851296, Velocity: -0.06937604397084343, Movement: 0.523187446523115, Collision: 0, Height: -1.0, Movement Penalty: -0.12082496523470135, Smoothness: -0.0, Curiosity: 19.604522705078125, Exploration: 0.1375474253172013, Total: 8.809093169886923
2024-07-21 20:38:13,595 - AirSimEnvLogger - INFO - Action: [0.60412483 0.60412483 0.60412483 0.60412483], Velocity: (0.6041248261735067, 0.6041248261735067, 0.6041248261735067), Duration: 1.0, Reward: 8.809093169886923, Done: False
2024-07-21 20:38:13,656 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:13,656 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:16,435 - AirSimEnvLogger - INFO - Predictive model loss: 0.672691285610199
2024-07-21 20:38:22,791 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.39211654039433, Velocity: -0.1140687740663304, Movement: 0.6879068032293478, Collision: 0, Height: -1.0, Movement Penalty: -0.15886527120873556, Smoothness: -0.0, Curiosity: 25.361896514892578, Exploration: 0.28762792171149937, Total: 14.541401835933
2024-07-21 20:38:22,823 - AirSimEnvLogger - INFO - Action: [0.79432636 0.79432636 0.79432636 0.79432636], Velocity: (0.7943263560436777, 0.7943263560436777, 0.7943263560436777), Duration: 1.0, Reward: 14.541401835933, Done: False
2024-07-21 20:38:22,871 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:22,871 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:25,757 - AirSimEnvLogger - INFO - Predictive model loss: 0.8138813376426697
2024-07-21 20:38:31,673 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.548297300577307, Velocity: -0.08558847229069358, Movement: 0.6905872598186416, Collision: 0, Height: -1.0, Movement Penalty: -0.1594842961420875, Smoothness: -0.0, Curiosity: 29.304359436035156, Exploration: 0.27416499449400933, Total: 18.32750130526597
2024-07-21 20:38:31,783 - AirSimEnvLogger - INFO - Action: [0.79742148 0.79742148 0.79742148 0.79742148], Velocity: (0.7974214807104375, 0.7974214807104375, 0.7974214807104375), Duration: 1.0, Reward: 18.32750130526597, Done: False
2024-07-21 20:38:31,846 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:31,846 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:34,543 - AirSimEnvLogger - INFO - Predictive model loss: 0.8832322359085083
2024-07-21 20:38:40,798 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.717868990196664, Velocity: -0.10713322142737704, Movement: 0.774999480865727, Collision: 0, Height: -1.0, Movement Penalty: -0.17897846355985908, Smoothness: -0.0, Curiosity: 35.26026153564453, Exploration: 0.2650524687817851, Total: 24.111651986666214
2024-07-21 20:38:40,939 - AirSimEnvLogger - INFO - Action: [0.89489232 0.89489232 0.89489232 0.89489232], Velocity: (0.8948923177992953, 0.8948923177992953, 0.8948923177992953), Duration: 1.0, Reward: 24.111651986666214, Done: False
2024-07-21 20:38:40,987 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:40,987 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:43,959 - AirSimEnvLogger - INFO - Predictive model loss: 0.9865942001342773
2024-07-21 20:38:49,912 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.938298121809877, Velocity: -0.12641537302044817, Movement: 0.8183541946064129, Collision: 0, Height: -1.0, Movement Penalty: -0.18899080581938876, Smoothness: -0.0, Curiosity: 41.48979568481445, Exploration: 0.2959780429849216, Total: 30.127005159924266
2024-07-21 20:38:50,022 - AirSimEnvLogger - INFO - Action: [0.94495403 0.94495403 0.94495403 0.94495403], Velocity: (0.9449540290969438, 0.9449540290969438, 0.9449540290969438), Duration: 1.0, Reward: 30.127005159924266, Done: False
2024-07-21 20:38:50,100 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:50,100 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:52,993 - AirSimEnvLogger - INFO - Predictive model loss: 1.0623544454574585
2024-07-21 20:38:58,967 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.189283326745809, Velocity: -0.10468069653816107, Movement: 0.8359223584918374, Collision: 0, Height: -1.0, Movement Penalty: -0.19304799947875567, Smoothness: -0.0, Curiosity: 47.709228515625, Exploration: 0.2918542569775246, Total: 36.09710872877969
2024-07-21 20:38:59,046 - AirSimEnvLogger - INFO - Action: [0.96524 0.96524 0.96524 0.96524], Velocity: (0.9652399973937783, 0.9652399973937783, 0.9652399973937783), Duration: 1.0, Reward: 36.09710872877969, Done: False
2024-07-21 20:38:59,078 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:59,078 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:01,943 - AirSimEnvLogger - INFO - Predictive model loss: 1.0951104164123535
2024-07-21 20:39:07,865 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.466020478113315, Velocity: -0.11306729348107648, Movement: 0.7319357904159625, Collision: 0, Height: -1.0, Movement Penalty: -0.16903333025047096, Smoothness: -0.0, Curiosity: 51.25620651245117, Exploration: 0.2456349425028487, Total: 39.353329687298405
2024-07-21 20:39:07,990 - AirSimEnvLogger - INFO - Action: [0.84516665 0.84516665 0.84516665 0.84516665], Velocity: (0.8451666512523548, 0.8451666512523548, 0.8451666512523548), Duration: 1.0, Reward: 39.353329687298405, Done: False
2024-07-21 20:39:08,069 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:08,069 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:11,122 - AirSimEnvLogger - INFO - Predictive model loss: 1.0189988613128662
2024-07-21 20:39:16,380 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.773712479271538, Velocity: -0.008150325580048795, Movement: 0.04141967172432112, Collision: 0, Height: -1.0, Movement Penalty: -0.009565463447913092, Smoothness: -0.0, Curiosity: 41.93588638305664, Exploration: 0.23938381186902424, Total: 29.717433170274422
2024-07-21 20:39:16,504 - AirSimEnvLogger - INFO - Action: [0.04782732 0.04782732 0.04782732 0.04782732], Velocity: (0.04782731723956546, 0.04782731723956546, 0.04782731723956546), Duration: 1.0, Reward: 29.717433170274422, Done: False
2024-07-21 20:39:16,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:16,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:19,517 - AirSimEnvLogger - INFO - Predictive model loss: 0.6103212833404541
2024-07-21 20:39:25,681 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.909801881406544, Velocity: -0.045425299134504125, Movement: 0.21791487633683515, Collision: 0, Height: -1.0, Movement Penalty: -0.05032528500539832, Smoothness: -0.0, Curiosity: 38.34402084350586, Exploration: 0.3836525992773081, Total: 26.0232619702041
2024-07-21 20:39:25,805 - AirSimEnvLogger - INFO - Action: [-0.25162643 -0.25162643 -0.25162643 -0.25162643], Velocity: (-0.25162642502699156, -0.25162642502699156, -0.25162642502699156), Duration: 1.0, Reward: 26.0232619702041, Done: False
2024-07-21 20:39:25,821 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:25,821 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:28,375 - AirSimEnvLogger - INFO - Predictive model loss: 0.36582469940185547
2024-07-21 20:39:34,463 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.937564731579188, Velocity: -0.08797284179776198, Movement: 0.4851570859810311, Collision: 0, Height: -1.0, Movement Penalty: -0.11204222967616109, Smoothness: -0.0, Curiosity: 33.68349838256836, Exploration: 0.31489089389603003, Total: 21.32146214960475
2024-07-21 20:39:34,540 - AirSimEnvLogger - INFO - Action: [-0.56021115 -0.56021115 -0.56021115 -0.56021115], Velocity: (-0.5602111483808054, -0.5602111483808054, -0.5602111483808054), Duration: 1.0, Reward: 21.32146214960475, Done: False
2024-07-21 20:39:34,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:34,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:37,628 - AirSimEnvLogger - INFO - Predictive model loss: 0.20378659665584564
2024-07-21 20:39:43,603 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.748593363936234, Velocity: -0.004647930958857676, Movement: 0.0419951759026203, Collision: 0, Height: -1.0, Movement Penalty: -0.009698370444817406, Smoothness: -0.0, Curiosity: 36.81371307373047, Exploration: 0.05262960986593102, Total: 24.577789866371475
2024-07-21 20:39:43,742 - AirSimEnvLogger - INFO - Action: [0.04849185 0.04849185 0.04849185 0.04849185], Velocity: (0.04849185222408703, 0.04849185222408703, 0.04849185222408703), Duration: 1.0, Reward: 24.577789866371475, Done: False
2024-07-21 20:39:43,805 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:43,805 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:46,762 - AirSimEnvLogger - INFO - Predictive model loss: 0.12009178847074509
2024-07-21 20:39:52,806 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.689436057960004, Velocity: -0.014942918402538204, Movement: 0.08018152187785667, Collision: 0, Height: -1.0, Movement Penalty: -0.018517129296085767, Smoothness: -0.0, Curiosity: 37.74980545043945, Exploration: 0.22374076435905824, Total: 25.612302324856635
2024-07-21 20:39:52,899 - AirSimEnvLogger - INFO - Action: [0.09258565 0.09258565 0.09258565 0.09258565], Velocity: (0.09258564648042883, 0.09258564648042883, 0.09258564648042883), Duration: 1.0, Reward: 25.612302324856635, Done: False
2024-07-21 20:39:52,946 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:52,946 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:55,960 - AirSimEnvLogger - INFO - Predictive model loss: 0.13534444570541382
2024-07-21 20:40:01,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.678525053843462, Velocity: -0.04270806933882148, Movement: 0.24629905542715796, Collision: 0, Height: -1.0, Movement Penalty: -0.056880330380808086, Smoothness: -0.0, Curiosity: 41.33144760131836, Exploration: 0.1340744390974007, Total: 29.185530564102816
2024-07-21 20:40:02,167 - AirSimEnvLogger - INFO - Action: [0.28440165 0.28440165 0.28440165 0.28440165], Velocity: (0.28440165190404043, 0.28440165190404043, 0.28440165190404043), Duration: 1.0, Reward: 29.185530564102816, Done: False
2024-07-21 20:40:02,198 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:02,198 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:05,240 - AirSimEnvLogger - INFO - Predictive model loss: 0.17071551084518433
2024-07-21 20:40:11,161 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.71359810000335, Velocity: -0.06422001719428408, Movement: 0.5518150982109108, Collision: 0, Height: -1.0, Movement Penalty: -0.12743623819798766, Smoothness: -0.0, Curiosity: 49.84965515136719, Exploration: 0.27960286985124827, Total: 37.70747970436073
2024-07-21 20:40:11,254 - AirSimEnvLogger - INFO - Action: [0.63718119 0.63718119 0.63718119 0.63718119], Velocity: (0.6371811909899383, 0.6371811909899383, 0.6371811909899383), Duration: 1.0, Reward: 37.70747970436073, Done: False
2024-07-21 20:40:11,333 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:11,333 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:14,343 - AirSimEnvLogger - INFO - Predictive model loss: 0.13559414446353912
2024-07-21 20:40:20,374 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.856851349352352, Velocity: -0.11207911069748834, Movement: 0.6907318610875743, Collision: 0, Height: -1.0, Movement Penalty: -0.1595176903747049, Smoothness: -0.0, Curiosity: 57.88017272949219, Exploration: 0.3376379737323958, Total: 45.606713818393445
2024-07-21 20:40:20,516 - AirSimEnvLogger - INFO - Action: [0.79758845 0.79758845 0.79758845 0.79758845], Velocity: (0.7975884518735245, 0.7975884518735245, 0.7975884518735245), Duration: 1.0, Reward: 45.606713818393445, Done: False
2024-07-21 20:40:20,595 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:20,595 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:23,753 - AirSimEnvLogger - INFO - Predictive model loss: 0.07738036662340164
2024-07-21 20:40:29,847 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.0887010759276, Velocity: -0.11927994301593854, Movement: 0.7678337977910219, Collision: 0, Height: -1.0, Movement Penalty: -0.1773236199390157, Smoothness: -0.0, Curiosity: 66.11954498291016, Exploration: 0.3031180236937841, Total: 53.60746797985177
2024-07-21 20:40:29,864 - AirSimEnvLogger - INFO - Action: [0.8866181 0.8866181 0.8866181 0.8866181], Velocity: (0.8866180996950783, 0.8866180996950783, 0.8866180996950783), Duration: 1.0, Reward: 53.60746797985177, Done: False
2024-07-21 20:40:29,941 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:29,941 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:33,004 - AirSimEnvLogger - INFO - Predictive model loss: 0.02450638823211193
2024-07-21 20:40:39,094 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.341269559671538, Velocity: -0.010754695196674161, Movement: 0.04710929214537045, Collision: 0, Height: -1.0, Movement Penalty: -0.010879425000584942, Smoothness: -0.0, Curiosity: 53.80894088745117, Exploration: 0.2074042164769075, Total: 41.015454416449714
2024-07-21 20:40:39,251 - AirSimEnvLogger - INFO - Action: [-0.05439713 -0.05439713 -0.05439713 -0.05439713], Velocity: (-0.05439712500292471, -0.05439712500292471, -0.05439712500292471), Duration: 1.0, Reward: 41.015454416449714, Done: False
2024-07-21 20:40:39,330 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:39,330 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:42,193 - AirSimEnvLogger - INFO - Predictive model loss: 0.0649547278881073
2024-07-21 20:40:48,132 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.424485895194827, Velocity: -0.03311352461983217, Movement: 0.36900751955166244, Collision: 0, Height: -1.0, Movement Penalty: -0.08521863629845937, Smoothness: -0.0, Curiosity: 62.09170913696289, Exploration: 0.15173310370422025, Total: 49.207862237746056
2024-07-21 20:40:48,241 - AirSimEnvLogger - INFO - Action: [0.42609318 0.42609318 0.42609318 0.42609318], Velocity: (0.4260931814922968, 0.4260931814922968, 0.4260931814922968), Duration: 1.0, Reward: 49.207862237746056, Done: False
2024-07-21 20:40:48,304 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:48,305 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:51,223 - AirSimEnvLogger - INFO - Predictive model loss: 0.010988228023052216
2024-07-21 20:40:57,156 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.520101658078964, Velocity: -0.03774630999970765, Movement: 0.270657318194963, Collision: 0, Height: -1.0, Movement Penalty: -0.06250563020720164, Smoothness: -0.0, Curiosity: 63.294742584228516, Exploration: 0.19289470089830482, Total: 50.32187128579989
2024-07-21 20:40:57,298 - AirSimEnvLogger - INFO - Action: [0.31252815 0.31252815 0.31252815 0.31252815], Velocity: (0.3125281510360082, 0.3125281510360082, 0.3125281510360082), Duration: 1.0, Reward: 50.32187128579989, Done: False
2024-07-21 20:40:57,378 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:57,378 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:00,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.008154264651238918
2024-07-21 20:41:06,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.651181081381475, Velocity: -0.08023716949642344, Movement: 0.5658584289020746, Collision: 0, Height: -1.0, Movement Penalty: -0.1306794064999326, Smoothness: -0.0, Curiosity: 73.27193450927734, Exploration: 0.19656177211876086, Total: 60.17181939532986
2024-07-21 20:41:06,301 - AirSimEnvLogger - INFO - Action: [0.65339703 0.65339703 0.65339703 0.65339703], Velocity: (0.653397032499663, 0.653397032499663, 0.653397032499663), Duration: 1.0, Reward: 60.17181939532986, Done: False
2024-07-21 20:41:06,348 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:06,348 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:09,321 - AirSimEnvLogger - INFO - Predictive model loss: 0.04230218753218651
2024-07-21 20:41:15,306 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.85173464964636, Velocity: -0.11243871105453804, Movement: 0.6770106345025905, Collision: 0, Height: -1.0, Movement Penalty: -0.156348908829724, Smoothness: -0.0, Curiosity: 82.16191864013672, Exploration: 0.3193466519808606, Total: 68.88899688375601
2024-07-21 20:41:15,414 - AirSimEnvLogger - INFO - Action: [0.78174454 0.78174454 0.78174454 0.78174454], Velocity: (0.7817445441486199, 0.7817445441486199, 0.7817445441486199), Duration: 1.0, Reward: 68.88899688375601, Done: False
2024-07-21 20:41:15,491 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:15,491 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:18,399 - AirSimEnvLogger - INFO - Predictive model loss: 0.10877823829650879
2024-07-21 20:41:24,139 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.009798732315442, Velocity: -0.012316445408699883, Movement: 0.057972959977671934, Collision: 0, Height: -1.0, Movement Penalty: -0.013388281619531317, Smoothness: -0.0, Curiosity: 70.23609924316406, Exploration: 0.16587328541471053, Total: 56.76464179554024
2024-07-21 20:41:24,280 - AirSimEnvLogger - INFO - Action: [-0.06694141 -0.06694141 -0.06694141 -0.06694141], Velocity: (-0.06694140809765659, -0.06694140809765659, -0.06694140809765659), Duration: 1.0, Reward: 56.76464179554024, Done: False
2024-07-21 20:41:24,342 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:24,342 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:27,147 - AirSimEnvLogger - INFO - Predictive model loss: 0.04557746276259422
2024-07-21 20:41:32,922 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.076822052943537, Velocity: -0.004668529996036015, Movement: 0.03531760125974579, Collision: 0, Height: -1.0, Movement Penalty: -0.00815625063777844, Smoothness: -0.0, Curiosity: 71.61782836914062, Exploration: 0.2503430868701845, Total: 58.098984712035104
2024-07-21 20:41:33,061 - AirSimEnvLogger - INFO - Action: [0.04078125 0.04078125 0.04078125 0.04078125], Velocity: (0.0407812531888922, 0.0407812531888922, 0.0407812531888922), Duration: 1.0, Reward: 58.098984712035104, Done: False
2024-07-21 20:41:33,077 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:33,077 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:36,074 - AirSimEnvLogger - INFO - Predictive model loss: 0.03434443846344948
2024-07-21 20:41:41,846 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.14794083562725, Velocity: -0.06596999276638357, Movement: 0.4367599645524541, Collision: 0, Height: -1.0, Movement Penalty: -0.1008653932422443, Smoothness: -0.0, Curiosity: 81.97900390625, Exploration: 0.21616104798239175, Total: 68.38489681079828
2024-07-21 20:41:41,955 - AirSimEnvLogger - INFO - Action: [0.50432697 0.50432697 0.50432697 0.50432697], Velocity: (0.5043269662112215, 0.5043269662112215, 0.5043269662112215), Duration: 1.0, Reward: 68.38489681079828, Done: False
2024-07-21 20:41:42,032 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:42,032 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:44,966 - AirSimEnvLogger - INFO - Predictive model loss: 0.04493115469813347
2024-07-21 20:41:50,970 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.31525059206794, Velocity: -0.059911214295022115, Movement: 0.6512316840815188, Collision: 0, Height: -1.0, Movement Penalty: -0.15039551524371128, Smoothness: -0.0, Curiosity: 93.00689697265625, Exploration: 0.36797658652000004, Total: 79.28626455220797
2024-07-21 20:41:51,111 - AirSimEnvLogger - INFO - Action: [0.75197758 0.75197758 0.75197758 0.75197758], Velocity: (0.7519775762185563, 0.7519775762185563, 0.7519775762185563), Duration: 1.0, Reward: 79.28626455220797, Done: False
2024-07-21 20:41:51,190 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:51,190 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:54,042 - AirSimEnvLogger - INFO - Predictive model loss: 0.06448247283697128
2024-07-21 20:41:59,976 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.422577546016003, Velocity: -0.009643593911179416, Movement: 0.07585668988856765, Collision: 0, Height: -1.0, Movement Penalty: -0.017518352130799397, Smoothness: -0.0, Curiosity: 80.68440246582031, Exploration: 0.12114434966070033, Total: 66.79058452135415
2024-07-21 20:42:00,008 - AirSimEnvLogger - INFO - Action: [-0.08759176 -0.08759176 -0.08759176 -0.08759176], Velocity: (-0.08759176065399699, -0.08759176065399699, -0.08759176065399699), Duration: 1.0, Reward: 66.79058452135415, Done: False
2024-07-21 20:42:00,070 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:00,070 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:03,025 - AirSimEnvLogger - INFO - Predictive model loss: 0.011695997789502144
2024-07-21 20:42:08,986 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.427624432376888, Velocity: -0.06439831047118943, Movement: 0.4051333582114853, Collision: 0, Height: -1.0, Movement Penalty: -0.09356154136843925, Smoothness: -0.0, Curiosity: 73.27825164794922, Exploration: 0.4445055504011453, Total: 59.45636156058286
2024-07-21 20:42:09,097 - AirSimEnvLogger - INFO - Action: [-0.46780771 -0.46780771 -0.46780771 -0.46780771], Velocity: (-0.46780770684219625, -0.46780770684219625, -0.46780770684219625), Duration: 1.0, Reward: 59.45636156058286, Done: False
2024-07-21 20:42:09,128 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:09,128 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:12,082 - AirSimEnvLogger - INFO - Predictive model loss: 0.060604456812143326
2024-07-21 20:42:17,740 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.32052854922621, Velocity: -0.08781680397194759, Movement: 0.481084492516909, Collision: 0, Height: -1.0, Movement Penalty: -0.11110170450303676, Smoothness: -0.0, Curiosity: 67.86429595947266, Exploration: 0.30276210435097034, Total: 54.116421990550826
2024-07-21 20:42:17,882 - AirSimEnvLogger - INFO - Action: [-0.55550852 -0.55550852 -0.55550852 -0.55550852], Velocity: (-0.5555085225151838, -0.5555085225151838, -0.5555085225151838), Duration: 1.0, Reward: 54.116421990550826, Done: False
2024-07-21 20:42:17,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:17,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:20,759 - AirSimEnvLogger - INFO - Predictive model loss: 0.132494255900383
2024-07-21 20:42:26,433 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.155420044457802, Velocity: -0.06380889684697126, Movement: 0.6529779859345771, Collision: 0, Height: -1.0, Movement Penalty: -0.1507988063816911, Smoothness: -0.0, Curiosity: 60.752830505371094, Exploration: 0.2853149729788383, Total: 47.17266952985372
2024-07-21 20:42:26,544 - AirSimEnvLogger - INFO - Action: [-0.75399403 -0.75399403 -0.75399403 -0.75399403], Velocity: (-0.7539940319084555, -0.7539940319084555, -0.7539940319084555), Duration: 1.0, Reward: 47.17266952985372, Done: False
2024-07-21 20:42:26,589 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:26,589 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:29,292 - AirSimEnvLogger - INFO - Predictive model loss: 0.2118525356054306
2024-07-21 20:42:34,954 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.978778338154914, Velocity: -0.01978017333979383, Movement: 0.10554400352260856, Collision: 0, Height: -1.0, Movement Penalty: -0.024374343538051547, Smoothness: -0.0, Curiosity: 67.76917266845703, Exploration: 0.16020960297806014, Total: 54.32785351037723
2024-07-21 20:42:35,080 - AirSimEnvLogger - INFO - Action: [0.12187172 0.12187172 0.12187172 0.12187172], Velocity: (0.12187171769025773, 0.12187171769025773, 0.12187171769025773), Duration: 1.0, Reward: 54.32785351037723, Done: False
2024-07-21 20:42:35,143 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:35,143 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:37,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.09063160419464111
2024-07-21 20:42:43,290 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.916405680042391, Velocity: -0.0034281781096229887, Movement: 0.02289889726282821, Collision: 0, Height: -1.0, Movement Penalty: -0.005288273799535781, Smoothness: -0.0, Curiosity: 66.00225067138672, Exploration: 0.2610973062503205, Total: 52.64611626268797
2024-07-21 20:42:43,399 - AirSimEnvLogger - INFO - Action: [-0.02644137 -0.02644137 -0.02644137 -0.02644137], Velocity: (-0.026441368997678905, -0.026441368997678905, -0.026441368997678905), Duration: 1.0, Reward: 52.64611626268797, Done: False
2024-07-21 20:42:43,431 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:43,431 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:46,463 - AirSimEnvLogger - INFO - Predictive model loss: 0.04738466441631317
2024-07-21 20:42:51,770 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.952572902119352, Velocity: -0.0380258337733952, Movement: 0.40389128700240967, Collision: 0, Height: -1.0, Movement Penalty: -0.09327469730967425, Smoothness: -0.0, Curiosity: 75.87943267822266, Exploration: 0.17555122164374024, Total: 62.473341405229526
2024-07-21 20:42:51,881 - AirSimEnvLogger - INFO - Action: [0.46637349 0.46637349 0.46637349 0.46637349], Velocity: (0.46637348654837124, 0.46637348654837124, 0.46637348654837124), Duration: 1.0, Reward: 62.473341405229526, Done: False
2024-07-21 20:42:51,944 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:51,944 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:54,622 - AirSimEnvLogger - INFO - Predictive model loss: 0.021914450451731682
2024-07-21 20:43:00,514 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.09262968367997, Velocity: -0.09146587113671896, Movement: 0.6342936457719535, Collision: 0, Height: -1.0, Movement Penalty: -0.1464838428526826, Smoothness: -0.0, Curiosity: 86.57183837890625, Exploration: 0.3769646043067744, Total: 73.07232315608489
2024-07-21 20:43:00,607 - AirSimEnvLogger - INFO - Action: [0.73241921 0.73241921 0.73241921 0.73241921], Velocity: (0.7324192142634129, 0.7324192142634129, 0.7324192142634129), Duration: 1.0, Reward: 73.07232315608489, Done: False
2024-07-21 20:43:00,639 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:00,639 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:03,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.09396776556968689
2024-07-21 20:43:08,961 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.201295426218595, Velocity: -0.011108127602422541, Movement: 0.11010237012331173, Collision: 0, Height: -1.0, Movement Penalty: -0.025427053211643937, Smoothness: -0.0, Curiosity: 74.08613586425781, Exploration: 0.11944470421098348, Total: 60.41390271245895
2024-07-21 20:43:09,085 - AirSimEnvLogger - INFO - Action: [-0.12713527 -0.12713527 -0.12713527 -0.12713527], Velocity: (-0.12713526605821968, -0.12713526605821968, -0.12713526605821968), Duration: 1.0, Reward: 60.41390271245895, Done: False
2024-07-21 20:43:09,148 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:09,148 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:11,949 - AirSimEnvLogger - INFO - Predictive model loss: 0.04469144716858864
2024-07-21 20:43:17,689 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.29735626714909, Velocity: -0.04429920212330423, Movement: 0.311113883270859, Collision: 0, Height: -1.0, Movement Penalty: -0.07184867370202411, Smoothness: -0.0, Curiosity: 82.8777847290039, Exploration: 0.11656230328630093, Total: 69.11043947820524
2024-07-21 20:43:17,799 - AirSimEnvLogger - INFO - Action: [0.35924337 0.35924337 0.35924337 0.35924337], Velocity: (0.3592433685101205, 0.3592433685101205, 0.3592433685101205), Duration: 1.0, Reward: 69.11043947820524, Done: False
2024-07-21 20:43:17,877 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:17,877 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:20,845 - AirSimEnvLogger - INFO - Predictive model loss: 0.11622384935617447
2024-07-21 20:43:26,745 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.403452281177673, Velocity: -0.056588930659579796, Movement: 0.41211158672777454, Collision: 0, Height: -1.0, Movement Penalty: -0.09517309421337777, Smoothness: -0.0, Curiosity: 89.07013702392578, Exploration: 0.2710201800532597, Total: 75.2334695661296
2024-07-21 20:43:26,854 - AirSimEnvLogger - INFO - Action: [0.47586547 0.47586547 0.47586547 0.47586547], Velocity: (0.47586547106688887, 0.47586547106688887, 0.47586547106688887), Duration: 1.0, Reward: 75.2334695661296, Done: False
2024-07-21 20:43:26,916 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:26,916 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:29,847 - AirSimEnvLogger - INFO - Predictive model loss: 0.16801749169826508
2024-07-21 20:43:36,030 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.484459415948239, Velocity: -0.004452096010672549, Movement: 0.049898304113404325, Collision: 0, Height: -1.0, Movement Penalty: -0.01152351972479192, Smoothness: -0.0, Curiosity: 84.06645965576172, Exploration: 0.06084672377162793, Total: 70.09677377939336
2024-07-21 20:43:36,173 - AirSimEnvLogger - INFO - Action: [0.0576176 0.0576176 0.0576176 0.0576176], Velocity: (0.0576175986239596, 0.0576175986239596, 0.0576175986239596), Duration: 1.0, Reward: 70.09677377939336, Done: False
2024-07-21 20:43:36,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:36,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:39,231 - AirSimEnvLogger - INFO - Predictive model loss: 0.10226933658123016
2024-07-21 20:43:45,235 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.485893030056513, Velocity: -0.07141258472595852, Movement: 0.40595230134068433, Collision: 0, Height: -1.0, Movement Penalty: -0.09375066818287686, Smoothness: -0.0, Curiosity: 75.0792465209961, Exploration: 0.3410616874055629, Total: 61.17461440866652
2024-07-21 20:43:45,360 - AirSimEnvLogger - INFO - Action: [-0.46875334 -0.46875334 -0.46875334 -0.46875334], Velocity: (-0.4687533409143843, -0.4687533409143843, -0.4687533409143843), Duration: 1.0, Reward: 61.17461440866652, Done: False
2024-07-21 20:43:45,423 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:45,423 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:48,431 - AirSimEnvLogger - INFO - Predictive model loss: 0.02464885637164116
2024-07-21 20:43:54,514 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.474251968270412, Velocity: -0.028934435065117114, Movement: 0.20460026672838597, Collision: 0, Height: -1.0, Movement Penalty: -0.04725040762876115, Smoothness: -0.0, Curiosity: 83.9984359741211, Exploration: 0.01180785192753137, Total: 70.0290252018151
2024-07-21 20:43:54,621 - AirSimEnvLogger - INFO - Action: [0.23625204 0.23625204 0.23625204 0.23625204], Velocity: (0.23625203814380574, 0.23625203814380574, 0.23625203814380574), Duration: 1.0, Reward: 70.0290252018151, Done: False
2024-07-21 20:43:54,700 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:54,700 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:57,753 - AirSimEnvLogger - INFO - Predictive model loss: 0.02419404126703739
2024-07-21 20:44:03,734 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.430937535529777, Velocity: -0.032590968580894344, Movement: 0.1789233228992835, Collision: 0, Height: -1.0, Movement Penalty: -0.04132057145608147, Smoothness: -0.0, Curiosity: 77.31478118896484, Exploration: 0.08710429927934367, Total: 63.405007524889086
2024-07-21 20:44:03,843 - AirSimEnvLogger - INFO - Action: [-0.20660286 -0.20660286 -0.20660286 -0.20660286], Velocity: (-0.20660285728040734, -0.20660285728040734, -0.20660285728040734), Duration: 1.0, Reward: 63.405007524889086, Done: False
2024-07-21 20:44:03,875 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:03,875 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:06,858 - AirSimEnvLogger - INFO - Predictive model loss: 0.0049750301986932755
2024-07-21 20:44:12,896 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.378766338748061, Velocity: -0.04889232488027132, Movement: 0.32034884574541594, Collision: 0, Height: -1.0, Movement Penalty: -0.07398139693028072, Smoothness: -0.0, Curiosity: 72.63774108886719, Exploration: 0.24679339208632478, Total: 58.81870613775017
2024-07-21 20:44:13,022 - AirSimEnvLogger - INFO - Action: [-0.36990698 -0.36990698 -0.36990698 -0.36990698], Velocity: (-0.3699069846514036, -0.3699069846514036, -0.3699069846514036), Duration: 1.0, Reward: 58.81870613775017, Done: False
2024-07-21 20:44:13,085 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:13,085 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:16,105 - AirSimEnvLogger - INFO - Predictive model loss: 0.04087219759821892
2024-07-21 20:44:21,412 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.277797408713612, Velocity: -0.008151089897287571, Movement: 0.3852534219228595, Collision: 0, Height: -1.0, Movement Penalty: -0.08897046674135496, Smoothness: -0.0, Curiosity: 68.26104736328125, Exploration: 0.1928586783301897, Total: 54.53624258729237
2024-07-21 20:44:21,475 - AirSimEnvLogger - INFO - Action: [-0.44485233 -0.44485233 -0.44485233 -0.44485233], Velocity: (-0.4448523337067748, -0.4448523337067748, -0.4448523337067748), Duration: 1.0, Reward: 54.53624258729237, Done: False
2024-07-21 20:44:21,519 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:21,520 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:24,380 - AirSimEnvLogger - INFO - Predictive model loss: 0.09311705082654953
2024-07-21 20:44:30,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.155423955712617, Velocity: -0.031951446721156976, Movement: 0.27770801731878103, Collision: 0, Height: -1.0, Movement Penalty: -0.0641339194220462, Smoothness: -0.0, Curiosity: 66.32835388183594, Exploration: 0.11159282906241347, Total: 52.702213294849166
2024-07-21 20:44:30,735 - AirSimEnvLogger - INFO - Action: [-0.3206696 -0.3206696 -0.3206696 -0.3206696], Velocity: (-0.32066959711023096, -0.32066959711023096, -0.32066959711023096), Duration: 1.0, Reward: 52.702213294849166, Done: False
2024-07-21 20:44:30,829 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:30,829 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:33,786 - AirSimEnvLogger - INFO - Predictive model loss: 0.11639398336410522
2024-07-21 20:44:39,919 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.091674719618789, Velocity: -0.053926694720578705, Movement: 0.2923387315255986, Collision: 0, Height: -1.0, Movement Penalty: -0.06751273813634323, Smoothness: -0.0, Curiosity: 75.8900375366211, Exploration: 0.24474109119800985, Total: 62.3564316518512
2024-07-21 20:44:40,076 - AirSimEnvLogger - INFO - Action: [0.33756369 0.33756369 0.33756369 0.33756369], Velocity: (0.33756369068171616, 0.33756369068171616, 0.33756369068171616), Duration: 1.0, Reward: 62.3564316518512, Done: False
2024-07-21 20:44:40,139 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:40,139 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:43,179 - AirSimEnvLogger - INFO - Predictive model loss: 0.04809563606977463
2024-07-21 20:44:49,219 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.109627769642051, Velocity: -0.016568699394424524, Movement: 0.08498366107766958, Collision: 0, Height: -1.0, Movement Penalty: -0.019626135839964982, Smoothness: -0.0, Curiosity: 73.98028564453125, Exploration: 0.20251773530509892, Total: 60.41766472870082
2024-07-21 20:44:49,311 - AirSimEnvLogger - INFO - Action: [0.09813068 0.09813068 0.09813068 0.09813068], Velocity: (0.0981306791998249, 0.0981306791998249, 0.0981306791998249), Duration: 1.0, Reward: 60.41766472870082, Done: False
2024-07-21 20:44:49,389 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:49,389 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:52,269 - AirSimEnvLogger - INFO - Predictive model loss: 0.02242211066186428
2024-07-21 20:44:58,566 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.21362552935312, Velocity: -0.04727318902387965, Movement: 0.47240137060810283, Collision: 0, Height: -1.0, Movement Penalty: -0.10909642339445454, Smoothness: -0.0, Curiosity: 84.79529571533203, Exploration: 0.15392583476172664, Total: 71.12393378925962
2024-07-21 20:44:58,676 - AirSimEnvLogger - INFO - Action: [0.54548212 0.54548212 0.54548212 0.54548212], Velocity: (0.5454821169722727, 0.5454821169722727, 0.5454821169722727), Duration: 1.0, Reward: 71.12393378925962, Done: False
2024-07-21 20:44:58,754 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:58,754 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:01,770 - AirSimEnvLogger - INFO - Predictive model loss: 0.016884515061974525
2024-07-21 20:45:07,992 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.400989128193, Velocity: -0.1177078169729651, Movement: 0.6689268235596632, Collision: 0, Height: -1.0, Movement Penalty: -0.15448203266013316, Smoothness: -0.0, Curiosity: 95.91716003417969, Exploration: 0.36309927285707655, Total: 82.1043216960148
2024-07-21 20:45:08,115 - AirSimEnvLogger - INFO - Action: [0.77241016 0.77241016 0.77241016 0.77241016], Velocity: (0.7724101633006657, 0.7724101633006657, 0.7724101633006657), Duration: 1.0, Reward: 82.1043216960148, Done: False
2024-07-21 20:45:08,193 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:08,193 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:11,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.0754832774400711
2024-07-21 20:45:17,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.580639329025113, Velocity: -0.05071829186025026, Movement: 0.3408100656779404, Collision: 0, Height: -1.0, Movement Penalty: -0.07870671326467718, Smoothness: -0.0, Curiosity: 93.710693359375, Exploration: 0.15145767136627103, Total: 79.66817751814068
2024-07-21 20:45:17,454 - AirSimEnvLogger - INFO - Action: [0.39353357 0.39353357 0.39353357 0.39353357], Velocity: (0.3935335663233859, 0.3935335663233859, 0.3935335663233859), Duration: 1.0, Reward: 79.66817751814068, Done: False
2024-07-21 20:45:17,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:17,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:20,456 - AirSimEnvLogger - INFO - Predictive model loss: 0.07906585931777954
2024-07-21 20:45:26,578 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.65773732590542, Velocity: -0.03601140073813698, Movement: 0.19268681276979763, Collision: 0, Height: -1.0, Movement Penalty: -0.04449911328877348, Smoothness: -0.0, Curiosity: 84.21617889404297, Exploration: 0.29360194280833385, Total: 70.12709547205303
2024-07-21 20:45:26,734 - AirSimEnvLogger - INFO - Action: [-0.22249557 -0.22249557 -0.22249557 -0.22249557], Velocity: (-0.22249556644386737, -0.22249556644386737, -0.22249556644386737), Duration: 1.0, Reward: 70.12709547205303, Done: False
2024-07-21 20:45:26,811 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:26,811 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:29,794 - AirSimEnvLogger - INFO - Predictive model loss: 0.03653644770383835
2024-07-21 20:45:35,874 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.602187232805024, Velocity: -0.053762958801052675, Movement: 0.5266927156880815, Collision: 0, Height: -1.0, Movement Penalty: -0.12163447247309156, Smoothness: -0.0, Curiosity: 75.7376708984375, Exploration: 0.43015763833996207, Total: 61.74196337053035
2024-07-21 20:45:35,984 - AirSimEnvLogger - INFO - Action: [-0.60817236 -0.60817236 -0.60817236 -0.60817236], Velocity: (-0.6081723623654578, -0.6081723623654578, -0.6081723623654578), Duration: 1.0, Reward: 61.74196337053035, Done: False
2024-07-21 20:45:36,016 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:36,016 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:38,686 - AirSimEnvLogger - INFO - Predictive model loss: 0.029669880867004395
2024-07-21 20:45:44,879 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.511721154495483, Velocity: -0.01318230454412278, Movement: 0.07325235764461709, Collision: 0, Height: -1.0, Movement Penalty: -0.016916907361957768, Smoothness: -0.0, Curiosity: 78.95375061035156, Exploration: 0.086003585880233, Total: 64.9622889265434
2024-07-21 20:45:45,036 - AirSimEnvLogger - INFO - Action: [-0.08458454 -0.08458454 -0.08458454 -0.08458454], Velocity: (-0.08458453680978884, -0.08458453680978884, -0.08458453680978884), Duration: 1.0, Reward: 64.9622889265434, Done: False
2024-07-21 20:45:45,099 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:45,099 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:48,034 - AirSimEnvLogger - INFO - Predictive model loss: 0.00861362461000681
2024-07-21 20:45:53,778 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.47573277394233, Velocity: -0.02938966704202531, Movement: 0.19028036994814734, Collision: 0, Height: -1.0, Movement Penalty: -0.043943369124425785, Smoothness: -0.0, Curiosity: 84.42588806152344, Exploration: 0.2703925609586606, Total: 70.51407417712792
2024-07-21 20:45:53,842 - AirSimEnvLogger - INFO - Action: [0.21971685 0.21971685 0.21971685 0.21971685], Velocity: (0.2197168456221289, 0.2197168456221289, 0.2197168456221289), Duration: 1.0, Reward: 70.51407417712792, Done: False
2024-07-21 20:45:53,918 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:53,918 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:56,874 - AirSimEnvLogger - INFO - Predictive model loss: 0.006049488205462694
2024-07-21 20:46:02,659 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.578198679057335, Velocity: -0.07019552465564449, Movement: 0.5265132824222, Collision: 0, Height: -1.0, Movement Penalty: -0.12159303413534826, Smoothness: -0.0, Curiosity: 95.96745300292969, Exploration: 0.32532673941883167, Total: 81.96997526394455
2024-07-21 20:46:02,786 - AirSimEnvLogger - INFO - Action: [0.60796517 0.60796517 0.60796517 0.60796517], Velocity: (0.6079651706767413, 0.6079651706767413, 0.6079651706767413), Duration: 1.0, Reward: 81.96997526394455, Done: False
2024-07-21 20:46:02,879 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:02,879 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:46:05,781 - AirSimEnvLogger - INFO - Predictive model loss: 0.02577189728617668
2024-07-21 20:46:11,899 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.79089941891319, Velocity: -0.11062923505800783, Movement: 0.6944734875328622, Collision: 0, Height: -1.0, Movement Penalty: -0.1603817819888625, Smoothness: -0.0, Curiosity: 107.37232971191406, Exploration: 0.3589638614748605, Total: 93.16996445489447
2024-07-21 20:46:11,947 - AirSimEnvLogger - INFO - Action: [0.80190891 0.80190891 0.80190891 0.80190891], Velocity: (0.8019089099443124, 0.8019089099443124, 0.8019089099443124), Duration: 1.0, Reward: 93.16996445489447, Done: False
2024-07-21 20:46:12,025 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:12,025 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:46:14,927 - AirSimEnvLogger - INFO - Predictive model loss: 0.057107362896203995
2024-07-21 20:46:20,913 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.02261785973801, Velocity: -0.09380533354095962, Movement: 0.5589931275759465, Collision: 0, Height: -1.0, Movement Penalty: -0.12909393307244943, Smoothness: -0.0, Curiosity: 111.15745544433594, Exploration: 0.22390915950522575, Total: 96.69066822925444
2024-07-21 20:46:20,976 - AirSimEnvLogger - INFO - Action: [0.64546967 0.64546967 0.64546967 0.64546967], Velocity: (0.6454696653622471, 0.6454696653622471, 0.6454696653622471), Duration: 1.0, Reward: 96.69066822925444, Done: False
2024-07-21 20:46:21,069 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:21,069 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:46:23,883 - AirSimEnvLogger - INFO - Predictive model loss: 0.043482616543769836
2024-07-21 20:46:29,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.25112816523982, Velocity: -0.09016070457322141, Movement: 0.4869588161510633, Collision: 0, Height: -1.0, Movement Penalty: -0.11245832143563116, Smoothness: -0.0, Curiosity: 115.54595947265625, Exploration: 0.14914275068868066, Total: 100.83206314316084
2024-07-21 20:46:30,120 - AirSimEnvLogger - INFO - Action: [0.56229161 0.56229161 0.56229161 0.56229161], Velocity: (0.5622916071781557, 0.5622916071781557, 0.5622916071781557), Duration: 1.0, Reward: 100.83206314316084, Done: False
2024-07-21 20:46:30,181 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:30,181 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:46:33,185 - AirSimEnvLogger - INFO - Predictive model loss: 0.027596144005656242
2024-07-21 20:46:39,254 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.302609458557185, Velocity: -0.032443847180745645, Movement: 0.18782947516671536, Collision: 0, Height: -1.0, Movement Penalty: -0.0433773592196997, Smoothness: -0.0, Curiosity: 102.61820220947266, Exploration: 0.25113194326440463, Total: 87.8747161600617
2024-07-21 20:46:39,380 - AirSimEnvLogger - INFO - Action: [-0.2168868 -0.2168868 -0.2168868 -0.2168868], Velocity: (-0.21688679609849848, -0.21688679609849848, -0.21688679609849848), Duration: 1.0, Reward: 87.8747161600617, Done: False
2024-07-21 20:46:39,443 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:39,443 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:46:42,339 - AirSimEnvLogger - INFO - Predictive model loss: 0.013261711224913597
2024-07-21 20:46:48,449 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.227719005290515, Velocity: -0.09115415210449418, Movement: 0.49315213509011013, Collision: 0, Height: -1.0, Movement Penalty: -0.11388860717828551, Smoothness: -0.0, Curiosity: 93.67782592773438, Exploration: 0.4601936817029454, Total: 79.05893304934855
2024-07-21 20:46:48,561 - AirSimEnvLogger - INFO - Action: [-0.56944304 -0.56944304 -0.56944304 -0.56944304], Velocity: (-0.5694430358914275, -0.5694430358914275, -0.5694430358914275), Duration: 1.0, Reward: 79.05893304934855, Done: False
2024-07-21 20:46:48,670 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:48,670 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:46:51,713 - AirSimEnvLogger - INFO - Predictive model loss: 0.09598241746425629
2024-07-21 20:46:57,851 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.222704288448043, Velocity: -0.03317408260026364, Movement: 0.18542665432125016, Collision: 0, Height: -1.0, Movement Penalty: -0.04282245151492219, Smoothness: -0.0, Curiosity: 103.93050384521484, Exploration: 0.06507281608333688, Total: 89.22399740205346
2024-07-21 20:46:57,993 - AirSimEnvLogger - INFO - Action: [0.21411226 0.21411226 0.21411226 0.21411226], Velocity: (0.21411225757461094, 0.21411225757461094, 0.21411225757461094), Duration: 1.0, Reward: 89.22399740205346, Done: False
2024-07-21 20:46:58,024 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:46:58,024 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:01,007 - AirSimEnvLogger - INFO - Predictive model loss: 0.04648213088512421
2024-07-21 20:47:06,976 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.317218546758992, Velocity: -0.08885948027119465, Movement: 0.4852402111051347, Collision: 0, Height: -1.0, Movement Penalty: -0.11206142660127215, Smoothness: -0.0, Curiosity: 115.34249114990234, Exploration: 0.4390534828839889, Total: 100.62927187243363
2024-07-21 20:47:07,102 - AirSimEnvLogger - INFO - Action: [0.56030713 0.56030713 0.56030713 0.56030713], Velocity: (0.5603071330063607, 0.5603071330063607, 0.5603071330063607), Duration: 1.0, Reward: 100.62927187243363, Done: False
2024-07-21 20:47:07,149 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:47:07,149 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:10,053 - AirSimEnvLogger - INFO - Predictive model loss: 0.018306516110897064
2024-07-21 20:47:15,698 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.537868860219417, Velocity: -0.07718692403466364, Movement: 0.6756091658802426, Collision: 0, Height: -1.0, Movement Penalty: -0.15602525351517466, Smoothness: -0.0, Curiosity: 127.8166275024414, Exploration: 0.3524295000546223, Total: 112.86867139129673
2024-07-21 20:47:15,826 - AirSimEnvLogger - INFO - Action: [0.78012627 0.78012627 0.78012627 0.78012627], Velocity: (0.7801262675758732, 0.7801262675758732, 0.7801262675758732), Duration: 1.0, Reward: 112.86867139129673, Done: False
2024-07-21 20:47:15,888 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:47:15,888 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:18,872 - AirSimEnvLogger - INFO - Predictive model loss: 0.009267043322324753
2024-07-21 20:47:25,026 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.709059259549177, Velocity: -0.013242399804053664, Movement: 0.37484483965349563, Collision: 0, Height: -1.0, Movement Penalty: -0.08656670763131513, Smoothness: -0.0, Curiosity: 126.41658020019531, Exploration: 0.15966910871479442, Total: 111.25211551940596
2024-07-21 20:47:25,181 - AirSimEnvLogger - INFO - Action: [0.43283354 0.43283354 0.43283354 0.43283354], Velocity: (0.4328335381565756, 0.4328335381565756, 0.4328335381565756), Duration: 1.0, Reward: 111.25211551940596, Done: False
2024-07-21 20:47:25,244 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:47:25,244 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:28,167 - AirSimEnvLogger - INFO - Predictive model loss: 0.0024169536773115396
2024-07-21 20:47:34,119 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.72396788003234, Velocity: -0.015816921786811537, Movement: 0.24178700989337001, Collision: 0, Height: -1.0, Movement Penalty: -0.05583831809939676, Smoothness: -0.0, Curiosity: 113.61426544189453, Exploration: 0.313090229463229, Total: 98.46665764479543
2024-07-21 20:47:34,293 - AirSimEnvLogger - INFO - Action: [-0.27919159 -0.27919159 -0.27919159 -0.27919159], Velocity: (-0.2791915904969838, -0.2791915904969838, -0.2791915904969838), Duration: 1.0, Reward: 98.46665764479543, Done: False
2024-07-21 20:47:34,356 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:47:34,356 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:37,354 - AirSimEnvLogger - INFO - Predictive model loss: 0.0175972580909729
2024-07-21 20:47:43,540 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.830609269606175, Velocity: -0.03617208946073952, Movement: 0.23609904929583544, Collision: 0, Height: -1.0, Movement Penalty: -0.05452473986654613, Smoothness: -0.0, Curiosity: 123.69878387451172, Exploration: 0.09060653023404731, Total: 108.39118839488144
2024-07-21 20:47:43,682 - AirSimEnvLogger - INFO - Action: [0.2726237 0.2726237 0.2726237 0.2726237], Velocity: (0.2726236993327306, 0.2726236993327306, 0.2726236993327306), Duration: 1.0, Reward: 108.39118839488144, Done: False
2024-07-21 20:47:43,760 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:47:43,760 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:46,680 - AirSimEnvLogger - INFO - Predictive model loss: 0.007456670980900526
2024-07-21 20:47:52,754 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.7327375960258, Velocity: -0.04791064165654746, Movement: 0.31153280171470266, Collision: 0, Height: -1.0, Movement Penalty: -0.07194541877255275, Smoothness: -0.0, Curiosity: 111.91059112548828, Exploration: 0.044205868642691416, Total: 96.69087169793843
2024-07-21 20:47:52,755 - AirSimEnvLogger - INFO - Action: [-0.35972709 -0.35972709 -0.35972709 -0.35972709], Velocity: (-0.35972709386276375, -0.35972709386276375, -0.35972709386276375), Duration: 1.0, Reward: 96.69087169793843, Done: False
2024-07-21 20:47:52,803 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:47:52,803 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:47:55,641 - AirSimEnvLogger - INFO - Predictive model loss: 0.023210590705275536
2024-07-21 20:48:01,175 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.82808338212369, Velocity: -0.04603856978943304, Movement: 0.27415080465369573, Collision: 0, Height: -1.0, Movement Penalty: -0.0633124163461455, Smoothness: -0.0, Curiosity: 124.08943939208984, Exploration: 0.007582478461722892, Total: 108.76522102751184
2024-07-21 20:48:01,267 - AirSimEnvLogger - INFO - Action: [0.31656208 0.31656208 0.31656208 0.31656208], Velocity: (0.3165620817307275, 0.3165620817307275, 0.3165620817307275), Duration: 1.0, Reward: 108.76522102751184, Done: False
2024-07-21 20:48:01,330 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:01,330 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:04,181 - AirSimEnvLogger - INFO - Predictive model loss: 0.004721981938928366
2024-07-21 20:48:10,282 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.941654495251623, Velocity: -0.07332602550092407, Movement: 0.4252551050521566, Collision: 0, Height: -1.0, Movement Penalty: -0.09820845975045008, Smoothness: -0.0, Curiosity: 132.54891967773438, Exploration: 0.35038887992462103, Total: 117.1909535071474
2024-07-21 20:48:10,392 - AirSimEnvLogger - INFO - Action: [0.4910423 0.4910423 0.4910423 0.4910423], Velocity: (0.49104229875225036, 0.49104229875225036, 0.49104229875225036), Duration: 1.0, Reward: 117.1909535071474, Done: False
2024-07-21 20:48:10,469 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:10,469 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:13,312 - AirSimEnvLogger - INFO - Predictive model loss: 0.015091699548065662
2024-07-21 20:48:19,296 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.923377159485067, Velocity: -0.02013265574108845, Movement: 0.13456332778049793, Collision: 0, Height: -1.0, Movement Penalty: -0.031076069406848927, Smoothness: -0.0, Curiosity: 121.3230209350586, Exploration: 0.09274469280198555, Total: 105.92226262052972
2024-07-21 20:48:19,406 - AirSimEnvLogger - INFO - Action: [-0.15538035 -0.15538035 -0.15538035 -0.15538035], Velocity: (-0.15538034703424464, -0.15538034703424464, -0.15538034703424464), Duration: 1.0, Reward: 105.92226262052972, Done: False
2024-07-21 20:48:19,469 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:19,470 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:22,452 - AirSimEnvLogger - INFO - Predictive model loss: 0.006680525839328766
2024-07-21 20:48:28,541 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.07439279413848, Velocity: -0.03501481236872446, Movement: 0.3353098676814048, Collision: 0, Height: -1.0, Movement Penalty: -0.07743649694711874, Smoothness: -0.0, Curiosity: 133.55250549316406, Exploration: 0.045323552127744846, Total: 117.99326076777993
2024-07-21 20:48:28,680 - AirSimEnvLogger - INFO - Action: [0.38718248 0.38718248 0.38718248 0.38718248], Velocity: (0.3871824847355937, 0.3871824847355937, 0.3871824847355937), Duration: 1.0, Reward: 117.99326076777993, Done: False
2024-07-21 20:48:28,775 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:28,775 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:31,717 - AirSimEnvLogger - INFO - Predictive model loss: 0.005168071947991848
2024-07-21 20:48:37,610 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.993339853459725, Velocity: -0.03561018173123491, Movement: 0.2500688766610741, Collision: 0, Height: -1.0, Movement Penalty: -0.05775093330248737, Smoothness: -0.0, Curiosity: 120.99668884277344, Exploration: 0.04154511109985356, Total: 105.51547752277486
2024-07-21 20:48:37,673 - AirSimEnvLogger - INFO - Action: [-0.28875467 -0.28875467 -0.28875467 -0.28875467], Velocity: (-0.28875466651243686, -0.28875466651243686, -0.28875466651243686), Duration: 1.0, Reward: 105.51547752277486, Done: False
2024-07-21 20:48:37,735 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:37,735 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:40,592 - AirSimEnvLogger - INFO - Predictive model loss: 0.009284725412726402
2024-07-21 20:48:46,740 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.895147246852218, Velocity: -0.04335832451192132, Movement: 0.3835219103721986, Collision: 0, Height: -1.0, Movement Penalty: -0.08857059127740335, Smoothness: -0.0, Curiosity: 114.30152130126953, Exploration: 0.34137963726664133, Total: 98.98996331035084
2024-07-21 20:48:46,850 - AirSimEnvLogger - INFO - Action: [-0.44285296 -0.44285296 -0.44285296 -0.44285296], Velocity: (-0.44285295638701677, -0.44285295638701677, -0.44285295638701677), Duration: 1.0, Reward: 98.98996331035084, Done: False
2024-07-21 20:48:46,913 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:46,913 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:49,804 - AirSimEnvLogger - INFO - Predictive model loss: 0.03395383059978485
2024-07-21 20:48:55,979 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.748422926503489, Velocity: -0.07557320630431956, Movement: 0.3989235971758165, Collision: 0, Height: -1.0, Movement Penalty: -0.0921274584862206, Smoothness: -0.0, Curiosity: 108.94853210449219, Exploration: 0.19550850762570796, Total: 93.74730438847368
2024-07-21 20:48:56,026 - AirSimEnvLogger - INFO - Action: [-0.46063729 -0.46063729 -0.46063729 -0.46063729], Velocity: (-0.46063729243110296, -0.46063729243110296, -0.46063729243110296), Duration: 1.0, Reward: 93.74730438847368, Done: False
2024-07-21 20:48:56,088 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:48:56,088 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:48:58,979 - AirSimEnvLogger - INFO - Predictive model loss: 0.0562005452811718
2024-07-21 20:49:04,844 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.747500511994684, Velocity: -0.02910217562608286, Movement: 0.17797118525451028, Collision: 0, Height: -1.0, Movement Penalty: -0.04110068468587531, Smoothness: -0.0, Curiosity: 119.0340576171875, Exploration: 0.1697540734673563, Total: 103.82705594812039
2024-07-21 20:49:04,985 - AirSimEnvLogger - INFO - Action: [0.20550342 0.20550342 0.20550342 0.20550342], Velocity: (0.20550342342937655, 0.20550342342937655, 0.20550342342937655), Duration: 1.0, Reward: 103.82705594812039, Done: False
2024-07-21 20:49:05,048 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:05,048 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:49:07,945 - AirSimEnvLogger - INFO - Predictive model loss: 0.013535138219594955
2024-07-21 20:49:14,020 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.612907367196295, Velocity: -0.05816095151477868, Movement: 0.34389813885227144, Collision: 0, Height: -1.0, Movement Penalty: -0.0794198732160681, Smoothness: -0.0, Curiosity: 107.52857208251953, Exploration: 0.0448745517687506, Total: 92.42860556990986
2024-07-21 20:49:14,162 - AirSimEnvLogger - INFO - Action: [-0.39709937 -0.39709937 -0.39709937 -0.39709937], Velocity: (-0.39709936608034047, -0.39709936608034047, -0.39709936608034047), Duration: 1.0, Reward: 92.42860556990986, Done: False
2024-07-21 20:49:14,223 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:14,223 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:49:17,073 - AirSimEnvLogger - INFO - Predictive model loss: 0.020051730796694756
2024-07-21 20:49:23,030 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.681047402836162, Velocity: -0.04182960306943128, Movement: 0.24940916705205768, Collision: 0, Height: -1.0, Movement Penalty: -0.05759857989434634, Smoothness: -0.0, Curiosity: 119.07071685791016, Exploration: 0.0280673173546071, Total: 103.89805997119872
2024-07-21 20:49:23,171 - AirSimEnvLogger - INFO - Action: [0.2879929 0.2879929 0.2879929 0.2879929], Velocity: (0.2879928994717317, 0.2879928994717317, 0.2879928994717317), Duration: 1.0, Reward: 103.89805997119872, Done: False
2024-07-21 20:49:23,248 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:23,248 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:49:26,135 - AirSimEnvLogger - INFO - Predictive model loss: 0.007459246553480625
2024-07-21 20:49:31,909 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.833407921658937, Velocity: -0.08314872297282168, Movement: 0.5572184380825417, Collision: 0, Height: -1.0, Movement Penalty: -0.12868408608975132, Smoothness: -0.0, Curiosity: 132.44302368164062, Exploration: 0.42363035087409573, Total: 117.21240440667212
2024-07-21 20:49:31,971 - AirSimEnvLogger - INFO - Action: [0.64342043 0.64342043 0.64342043 0.64342043], Velocity: (0.6434204304487565, 0.6434204304487565, 0.6434204304487565), Duration: 1.0, Reward: 117.21240440667212, Done: False
2024-07-21 20:49:32,019 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:32,019 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:49:34,988 - AirSimEnvLogger - INFO - Predictive model loss: 0.05839565768837929
2024-07-21 20:49:41,003 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.093015471258285, Velocity: -0.0833027097548778, Movement: 0.71034065653539, Collision: 0, Height: -1.0, Movement Penalty: -0.1640461477068172, Smoothness: -0.0, Curiosity: 145.41802978515625, Exploration: 0.3551340422042966, Total: 129.915789490691
2024-07-21 20:49:41,096 - AirSimEnvLogger - INFO - Action: [0.82023074 0.82023074 0.82023074 0.82023074], Velocity: (0.8202307385340859, 0.8202307385340859, 0.8202307385340859), Duration: 1.0, Reward: 129.915789490691, Done: False
2024-07-21 20:49:41,144 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:41,144 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:49:44,061 - AirSimEnvLogger - INFO - Predictive model loss: 0.13231445848941803
2024-07-21 20:49:49,877 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.158222898108, Velocity: -0.009085780967979122, Movement: 0.06299360094121054, Collision: 0, Height: -1.0, Movement Penalty: -0.014547748984252706, Smoothness: -0.0, Curiosity: 133.13491821289062, Exploration: 0.12196771855743047, Total: 117.50538454159367
2024-07-21 20:49:49,972 - AirSimEnvLogger - INFO - Action: [0.07273874 0.07273874 0.07273874 0.07273874], Velocity: (0.07273874492126353, 0.07273874492126353, 0.07273874492126353), Duration: 1.0, Reward: 117.50538454159367, Done: False
2024-07-21 20:49:49,988 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:49,988 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:49:52,938 - AirSimEnvLogger - INFO - Predictive model loss: 0.06846631318330765
2024-07-21 20:49:58,764 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.367212928363232, Velocity: -0.03133679721627422, Movement: 0.3971741453913421, Collision: 0, Height: -1.0, Movement Penalty: -0.09172343990274037, Smoothness: -0.0, Curiosity: 144.7183837890625, Exploration: 0.11186337053751642, Total: 128.88350841660468
2024-07-21 20:49:58,874 - AirSimEnvLogger - INFO - Action: [0.4586172 0.4586172 0.4586172 0.4586172], Velocity: (0.45861719951370183, 0.45861719951370183, 0.45861719951370183), Duration: 1.0, Reward: 128.88350841660468, Done: False
2024-07-21 20:49:58,967 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:49:58,967 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:01,878 - AirSimEnvLogger - INFO - Predictive model loss: 0.08274534344673157
2024-07-21 20:50:08,016 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.443624826110664, Velocity: -0.03196699510554406, Movement: 0.18740531678362554, Collision: 0, Height: -1.0, Movement Penalty: -0.04327940403703732, Smoothness: -0.0, Curiosity: 143.27407836914062, Exploration: 0.1295926020877958, Total: 127.36166018433079
2024-07-21 20:50:08,125 - AirSimEnvLogger - INFO - Action: [0.21639702 0.21639702 0.21639702 0.21639702], Velocity: (0.21639702018518658, 0.21639702018518658, 0.21639702018518658), Duration: 1.0, Reward: 127.36166018433079, Done: False
2024-07-21 20:50:08,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:50:08,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:11,048 - AirSimEnvLogger - INFO - Predictive model loss: 0.03793875128030777
2024-07-21 20:50:17,172 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.532225493675838, Velocity: -0.01986072707684117, Movement: 0.14448113214524008, Collision: 0, Height: -1.0, Movement Penalty: -0.0333664882147505, Smoothness: -0.0, Curiosity: 144.559814453125, Exploration: 0.06994105278491644, Total: 128.54523344327498
2024-07-21 20:50:17,298 - AirSimEnvLogger - INFO - Action: [0.16683244 0.16683244 0.16683244 0.16683244], Velocity: (0.1668324410737525, 0.1668324410737525, 0.1668324410737525), Duration: 1.0, Reward: 128.54523344327498, Done: False
2024-07-21 20:50:17,329 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:50:17,329 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:19,947 - AirSimEnvLogger - INFO - Predictive model loss: 0.007545961067080498
2024-07-21 20:50:25,719 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.426008847475554, Velocity: -0.05184835719273516, Movement: 0.3605163113788127, Collision: 0, Height: -1.0, Movement Penalty: -0.08325767576872338, Smoothness: -0.0, Curiosity: 131.92803955078125, Exploration: 0.23185935451914416, Total: 116.0590169648345
2024-07-21 20:50:25,828 - AirSimEnvLogger - INFO - Action: [-0.41628838 -0.41628838 -0.41628838 -0.41628838], Velocity: (-0.4162883788436169, -0.4162883788436169, -0.4162883788436169), Duration: 1.0, Reward: 116.0590169648345, Done: False
2024-07-21 20:50:25,891 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:50:25,891 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:28,706 - AirSimEnvLogger - INFO - Predictive model loss: 0.025436725467443466
2024-07-21 20:50:34,738 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.539207574982106, Velocity: -0.032417250059918426, Movement: 0.24763088232709965, Collision: 0, Height: -1.0, Movement Penalty: -0.05718790262848621, Smoothness: -0.0, Curiosity: 144.91287231445312, Exploration: 0.007248831289161082, Total: 128.8781646178731
2024-07-21 20:50:34,786 - AirSimEnvLogger - INFO - Action: [0.28593951 0.28593951 0.28593951 0.28593951], Velocity: (0.28593951314243105, 0.28593951314243105, 0.28593951314243105), Duration: 1.0, Reward: 128.8781646178731, Done: False
2024-07-21 20:50:34,849 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:50:34,849 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:37,728 - AirSimEnvLogger - INFO - Predictive model loss: 0.03232070058584213
2024-07-21 20:50:43,727 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.413071566832887, Velocity: -0.045407630984742396, Movement: 0.2540082995426206, Collision: 0, Height: -1.0, Movement Penalty: -0.058660704046932445, Smoothness: -0.0, Curiosity: 133.2775421142578, Exploration: 0.03928388225856438, Total: 117.37519588715841
2024-07-21 20:50:43,744 - AirSimEnvLogger - INFO - Action: [-0.29330352 -0.29330352 -0.29330352 -0.29330352], Velocity: (-0.2933035202346622, -0.2933035202346622, -0.2933035202346622), Duration: 1.0, Reward: 117.37519588715841, Done: False
2024-07-21 20:50:43,790 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:50:43,791 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:46,680 - AirSimEnvLogger - INFO - Predictive model loss: 0.08864596486091614
2024-07-21 20:50:51,931 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.544415555024061, Velocity: -0.027796692764650018, Movement: 0.28737654735122026, Collision: 0, Height: -1.0, Movement Penalty: -0.06636677078880492, Smoothness: -0.0, Curiosity: 146.28538513183594, Exploration: 0.023028853027310274, Total: 130.25053587496646
2024-07-21 20:50:52,039 - AirSimEnvLogger - INFO - Action: [0.33183385 0.33183385 0.33183385 0.33183385], Velocity: (0.33183385394402454, 0.33183385394402454, 0.33183385394402454), Duration: 1.0, Reward: 130.25053587496646, Done: False
2024-07-21 20:50:52,119 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:50:52,119 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:50:55,071 - AirSimEnvLogger - INFO - Predictive model loss: 0.04111893102526665
2024-07-21 20:51:01,065 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.719516695009029, Velocity: -0.07098026128817253, Movement: 0.5205458331932368, Collision: 0, Height: -1.0, Movement Penalty: -0.12021491076786131, Smoothness: -0.0, Curiosity: 158.7462921142578, Exploration: 0.3789089982285233, Total: 142.61959542397565
2024-07-21 20:51:01,192 - AirSimEnvLogger - INFO - Action: [0.60107455 0.60107455 0.60107455 0.60107455], Velocity: (0.6010745538393065, 0.6010745538393065, 0.6010745538393065), Duration: 1.0, Reward: 142.61959542397565, Done: False
2024-07-21 20:51:01,255 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:01,255 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:03,996 - AirSimEnvLogger - INFO - Predictive model loss: 0.009576513431966305
2024-07-21 20:51:09,806 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.991157967374885, Velocity: -0.08678056708336697, Movement: 0.6841171823106315, Collision: 0, Height: -1.0, Movement Penalty: -0.15799009574571654, Smoothness: -0.0, Curiosity: 172.48423767089844, Exploration: 0.32113571042818856, Total: 156.07504421786433
2024-07-21 20:51:09,917 - AirSimEnvLogger - INFO - Action: [0.78995048 0.78995048 0.78995048 0.78995048], Velocity: (0.7899504787285827, 0.7899504787285827, 0.7899504787285827), Duration: 1.0, Reward: 156.07504421786433, Done: False
2024-07-21 20:51:09,994 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:09,994 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:12,874 - AirSimEnvLogger - INFO - Predictive model loss: 0.026755133643746376
2024-07-21 20:51:18,624 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.325042618240392, Velocity: -0.10242578795376536, Movement: 0.7743618396346676, Collision: 0, Height: -1.0, Movement Penalty: -0.1788312066252997, Smoothness: -0.0, Curiosity: 186.41937255859375, Exploration: 0.3095420966051759, Total: 169.67427709801336
2024-07-21 20:51:18,780 - AirSimEnvLogger - INFO - Action: [0.89415603 0.89415603 0.89415603 0.89415603], Velocity: (0.8941560331264984, 0.8941560331264984, 0.8941560331264984), Duration: 1.0, Reward: 169.67427709801336, Done: False
2024-07-21 20:51:18,843 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:18,843 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:21,644 - AirSimEnvLogger - INFO - Predictive model loss: 0.07724286615848541
2024-07-21 20:51:27,640 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.38955869958894, Velocity: -0.0239688674973139, Movement: 0.14546600219866318, Collision: 0, Height: -1.0, Movement Penalty: -0.033593934210934755, Smoothness: -0.0, Curiosity: 173.87242126464844, Exploration: 0.14974013736111946, Total: 157.01847418310518
2024-07-21 20:51:27,641 - AirSimEnvLogger - INFO - Action: [0.16796967 0.16796967 0.16796967 0.16796967], Velocity: (0.16796967105467375, 0.16796967105467375, 0.16796967105467375), Duration: 1.0, Reward: 157.01847418310518, Done: False
2024-07-21 20:51:27,702 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:27,702 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:30,580 - AirSimEnvLogger - INFO - Predictive model loss: 0.06081311032176018
2024-07-21 20:51:36,298 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.34353750248324, Velocity: -0.04192464794560904, Movement: 0.24978963584661537, Collision: 0, Height: -1.0, Movement Penalty: -0.057686445398728796, Smoothness: -0.0, Curiosity: 163.1400909423828, Exploration: 0.3932105504125929, Total: 146.38892672801907
2024-07-21 20:51:36,392 - AirSimEnvLogger - INFO - Action: [-0.28843223 -0.28843223 -0.28843223 -0.28843223], Velocity: (-0.28843222699364396, -0.28843222699364396, -0.28843222699364396), Duration: 1.0, Reward: 146.38892672801907, Done: False
2024-07-21 20:51:36,469 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:36,469 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:39,350 - AirSimEnvLogger - INFO - Predictive model loss: 0.045916467905044556
2024-07-21 20:51:45,168 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.33064542227733, Velocity: -0.015547948663595351, Movement: 0.12427225287573712, Collision: 0, Height: -1.0, Movement Penalty: -0.02869944746024323, Smoothness: -0.0, Curiosity: 163.21961975097656, Exploration: 0.1787143242904807, Total: 146.43157222009216
2024-07-21 20:51:45,309 - AirSimEnvLogger - INFO - Action: [-0.14349724 -0.14349724 -0.14349724 -0.14349724], Velocity: (-0.14349723730121614, -0.14349723730121614, -0.14349723730121614), Duration: 1.0, Reward: 146.43157222009216, Done: False
2024-07-21 20:51:45,372 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:45,372 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:48,232 - AirSimEnvLogger - INFO - Predictive model loss: 0.02581372857093811
2024-07-21 20:51:54,221 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.488093316629264, Velocity: -0.05655396179804228, Movement: 0.36992345390735654, Collision: 0, Height: -1.0, Movement Penalty: -0.08543016227718736, Smoothness: -0.0, Curiosity: 178.5372314453125, Exploration: 0.23606394838527572, Total: 161.6068516430405
2024-07-21 20:51:54,362 - AirSimEnvLogger - INFO - Action: [0.42715081 0.42715081 0.42715081 0.42715081], Velocity: (0.4271508113859368, 0.4271508113859368, 0.4271508113859368), Duration: 1.0, Reward: 161.6068516430405, Done: False
2024-07-21 20:51:54,470 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:51:54,470 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:51:57,345 - AirSimEnvLogger - INFO - Predictive model loss: 0.0021167080849409103
2024-07-21 20:52:03,168 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.72989760479052, Velocity: -0.07719547251925156, Movement: 0.6156120016325685, Collision: 0, Height: -1.0, Movement Penalty: -0.1421695019435711, Smoothness: -0.0, Curiosity: 194.1710662841797, Exploration: 0.4030853855700464, Total: 177.04125970007513
2024-07-21 20:52:03,263 - AirSimEnvLogger - INFO - Action: [0.71084751 0.71084751 0.71084751 0.71084751], Velocity: (0.7108475097178555, 0.7108475097178555, 0.7108475097178555), Duration: 1.0, Reward: 177.04125970007513, Done: False
2024-07-21 20:52:03,324 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:03,324 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:05,961 - AirSimEnvLogger - INFO - Predictive model loss: 0.0019048755057156086
2024-07-21 20:52:11,580 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.041166929366522, Velocity: -0.116180368725107, Movement: 0.739970084103473, Collision: 0, Height: -1.0, Movement Penalty: -0.17088877089976406, Smoothness: -0.0, Curiosity: 208.9701690673828, Exploration: 0.33409676124496873, Total: 191.51237778255472
2024-07-21 20:52:11,627 - AirSimEnvLogger - INFO - Action: [0.85444385 0.85444385 0.85444385 0.85444385], Velocity: (0.8544438544988203, 0.8544438544988203, 0.8544438544988203), Duration: 1.0, Reward: 191.51237778255472, Done: False
2024-07-21 20:52:11,689 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:11,689 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:14,653 - AirSimEnvLogger - INFO - Predictive model loss: 0.006658906117081642
2024-07-21 20:52:20,635 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.999606575399376, Velocity: -0.010864601273100082, Movement: 0.05437280826147075, Collision: 0, Height: -1.0, Movement Penalty: -0.012556862194542419, Smoothness: -0.0, Curiosity: 189.9993896484375, Exploration: 0.17656082967712974, Total: 172.5406393657959
2024-07-21 20:52:20,745 - AirSimEnvLogger - INFO - Action: [-0.06278431 -0.06278431 -0.06278431 -0.06278431], Velocity: (-0.06278431097271209, -0.06278431097271209, -0.06278431097271209), Duration: 1.0, Reward: 172.5406393657959, Done: False
2024-07-21 20:52:20,808 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:20,808 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:23,695 - AirSimEnvLogger - INFO - Predictive model loss: 0.013315831311047077
2024-07-21 20:52:29,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.241105959664576, Velocity: -0.06275002974649176, Movement: 0.35915950116392215, Collision: 0, Height: -1.0, Movement Penalty: -0.08294433387160086, Smoothness: -0.0, Curiosity: 204.4240264892578, Exploration: 0.14009368266685437, Total: 186.7176772368591
2024-07-21 20:52:29,584 - AirSimEnvLogger - INFO - Action: [0.41472167 0.41472167 0.41472167 0.41472167], Velocity: (0.4147216693580043, 0.4147216693580043, 0.4147216693580043), Duration: 1.0, Reward: 186.7176772368591, Done: False
2024-07-21 20:52:29,663 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:29,663 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:32,516 - AirSimEnvLogger - INFO - Predictive model loss: 0.004748423583805561
2024-07-21 20:52:38,139 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.089885529857003, Velocity: -0.04550006074561643, Movement: 0.24956672960112303, Collision: 0, Height: -1.0, Movement Penalty: -0.05763496740639317, Smoothness: -0.0, Curiosity: 188.28323364257812, Exploration: 0.08095220110158435, Total: 170.71353897134088
2024-07-21 20:52:38,233 - AirSimEnvLogger - INFO - Action: [-0.28817484 -0.28817484 -0.28817484 -0.28817484], Velocity: (-0.28817483703196584, -0.28817483703196584, -0.28817483703196584), Duration: 1.0, Reward: 170.71353897134088, Done: False
2024-07-21 20:52:38,279 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:38,279 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:40,913 - AirSimEnvLogger - INFO - Predictive model loss: 0.033469121903181076
2024-07-21 20:52:46,700 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.93273038899861, Velocity: -0.06018455735406023, Movement: 0.43458002629579007, Collision: 0, Height: -1.0, Movement Penalty: -0.10036195806652362, Smoothness: -0.0, Curiosity: 178.2996368408203, Exploration: 0.37958876845786954, Total: 160.95885363747976
2024-07-21 20:52:46,810 - AirSimEnvLogger - INFO - Action: [-0.50180979 -0.50180979 -0.50180979 -0.50180979], Velocity: (-0.5018097903326181, -0.5018097903326181, -0.5018097903326181), Duration: 1.0, Reward: 160.95885363747976, Done: False
2024-07-21 20:52:46,888 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:46,888 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:49,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.06541150063276291
2024-07-21 20:52:55,543 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.661554871827484, Velocity: -0.11031919622926434, Movement: 0.6383030208407816, Collision: 0, Height: -1.0, Movement Penalty: -0.14740976836279063, Smoothness: -0.0, Curiosity: 165.50286865234375, Exploration: 0.32302942339108465, Total: 148.42023616700962
2024-07-21 20:52:55,652 - AirSimEnvLogger - INFO - Action: [-0.73704884 -0.73704884 -0.73704884 -0.73704884], Velocity: (-0.7370488418139531, -0.7370488418139531, -0.7370488418139531), Duration: 1.0, Reward: 148.42023616700962, Done: False
2024-07-21 20:52:55,684 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:52:55,684 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:52:58,568 - AirSimEnvLogger - INFO - Predictive model loss: 0.10168667882680893
2024-07-21 20:53:04,487 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.357465001119483, Velocity: -0.12528224543215083, Movement: 0.7048572351834431, Collision: 0, Height: -1.0, Movement Penalty: -0.16277980578936652, Smoothness: -0.0, Curiosity: 153.79542541503906, Exploration: 0.33068130215683805, Total: 137.01877899890343
2024-07-21 20:53:04,533 - AirSimEnvLogger - INFO - Action: [-0.81389903 -0.81389903 -0.81389903 -0.81389903], Velocity: (-0.8138990289468325, -0.8138990289468325, -0.8138990289468325), Duration: 1.0, Reward: 137.01877899890343, Done: False
2024-07-21 20:53:04,611 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:04,611 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:53:07,466 - AirSimEnvLogger - INFO - Predictive model loss: 0.11117313802242279
2024-07-21 20:53:13,423 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.335795365771688, Velocity: -0.006716818892098161, Movement: 0.020047096661681692, Collision: 0, Height: -1.0, Movement Penalty: -0.004629678661636949, Smoothness: -0.0, Curiosity: 163.9656524658203, Exploration: 0.1787774415166097, Total: 147.17079598391004
2024-07-21 20:53:13,548 - AirSimEnvLogger - INFO - Action: [0.02314839 0.02314839 0.02314839 0.02314839], Velocity: (0.023148393308184745, 0.023148393308184745, 0.023148393308184745), Duration: 1.0, Reward: 147.17079598391004, Done: False
2024-07-21 20:53:13,610 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:13,610 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:53:16,281 - AirSimEnvLogger - INFO - Predictive model loss: 0.016297241672873497
2024-07-21 20:53:22,131 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.463185552854263, Velocity: -0.06829823956450491, Movement: 0.43892310837419474, Collision: 0, Height: -1.0, Movement Penalty: -0.10136494990935546, Smoothness: -0.0, Curiosity: 179.46299743652344, Exploration: 0.49187099995505557, Total: 162.6168791499201
2024-07-21 20:53:22,163 - AirSimEnvLogger - INFO - Action: [0.50682475 0.50682475 0.50682475 0.50682475], Velocity: (0.5068247495467773, 0.5068247495467773, 0.5068247495467773), Duration: 1.0, Reward: 162.6168791499201, Done: False
2024-07-21 20:53:22,241 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:22,241 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:53:24,983 - AirSimEnvLogger - INFO - Predictive model loss: 0.026528723537921906
2024-07-21 20:53:30,831 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.434375747815267, Velocity: -0.006146372089789419, Movement: 0.038027206146385814, Collision: 0, Height: -1.0, Movement Penalty: -0.008782007082058096, Smoothness: -0.0, Curiosity: 171.72607421875, Exploration: 0.11189316112408978, Total: 154.81775206610376
2024-07-21 20:53:30,941 - AirSimEnvLogger - INFO - Action: [0.04391004 0.04391004 0.04391004 0.04391004], Velocity: (0.04391003541029048, 0.04391003541029048, 0.04391003541029048), Duration: 1.0, Reward: 154.81775206610376, Done: False
2024-07-21 20:53:31,003 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:31,003 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:53:33,844 - AirSimEnvLogger - INFO - Predictive model loss: 0.045189034193754196
2024-07-21 20:53:39,712 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.456934419050505, Velocity: -0.0038390597477104666, Movement: 0.01956489100485652, Collision: 0, Height: -1.0, Movement Penalty: -0.00451831803532784, Smoothness: -0.0, Curiosity: 170.3234405517578, Exploration: 0.15283440432702466, Total: 153.40175406545077
2024-07-21 20:53:39,901 - AirSimEnvLogger - INFO - Action: [-0.02259159 -0.02259159 -0.02259159 -0.02259159], Velocity: (-0.022591590176639198, -0.022591590176639198, -0.022591590176639198), Duration: 1.0, Reward: 153.40175406545077, Done: False
2024-07-21 20:53:39,980 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:39,980 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:53:42,777 - AirSimEnvLogger - INFO - Predictive model loss: 0.0656537115573883
2024-07-21 20:53:48,685 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.37383413687384, Velocity: -0.03010760862800975, Movement: 0.2089530385313355, Collision: 0, Height: -1.0, Movement Penalty: -0.04825563721762272, Smoothness: -0.0, Curiosity: 163.88626098632812, Exploration: 0.13163135891584393, Total: 147.0448169079277
2024-07-21 20:53:48,686 - AirSimEnvLogger - INFO - Action: [-0.24127819 -0.24127819 -0.24127819 -0.24127819], Velocity: (-0.2412781860881136, -0.2412781860881136, -0.2412781860881136), Duration: 1.0, Reward: 147.0448169079277, Done: False
2024-07-21 20:53:48,780 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:48,780 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:53:51,549 - AirSimEnvLogger - INFO - Predictive model loss: 0.04831418767571449
2024-07-21 20:53:57,526 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.164637585439525, Velocity: -0.09129081396415403, Movement: 0.532686447644593, Collision: 0, Height: -1.0, Movement Penalty: -0.12301866557650852, Smoothness: -0.0, Curiosity: 151.5011749267578, Exploration: 0.2975178708458515, Total: 134.9089041400984
2024-07-21 20:53:57,635 - AirSimEnvLogger - INFO - Action: [-0.61509333 -0.61509333 -0.61509333 -0.61509333], Velocity: (-0.6150933278825426, -0.6150933278825426, -0.6150933278825426), Duration: 1.0, Reward: 134.9089041400984, Done: False
2024-07-21 20:53:57,745 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:53:57,745 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:00,202 - AirSimEnvLogger - INFO - Predictive model loss: 0.009682157076895237
2024-07-21 20:54:05,895 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.963488660773251, Velocity: -0.08597289252255487, Movement: 0.5218514199687485, Collision: 0, Height: -1.0, Movement Penalty: -0.12051642311837817, Smoothness: -0.0, Curiosity: 143.9385986328125, Exploration: 0.2885705129024601, Total: 127.545684887991
2024-07-21 20:54:06,002 - AirSimEnvLogger - INFO - Action: [-0.60258212 -0.60258212 -0.60258212 -0.60258212], Velocity: (-0.6025821155918908, -0.6025821155918908, -0.6025821155918908), Duration: 1.0, Reward: 127.545684887991, Done: False
2024-07-21 20:54:06,097 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:06,097 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:08,979 - AirSimEnvLogger - INFO - Predictive model loss: 0.002565308939665556
2024-07-21 20:54:14,812 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.00094225342444, Velocity: -0.023971895860932034, Movement: 0.170735189959637, Collision: 0, Height: -1.0, Movement Penalty: -0.039429603153335324, Smoothness: -0.0, Curiosity: 156.92042541503906, Exploration: 0.2077548003455364, Total: 140.46905770112568
2024-07-21 20:54:14,905 - AirSimEnvLogger - INFO - Action: [0.19714802 0.19714802 0.19714802 0.19714802], Velocity: (0.19714801576667662, 0.19714801576667662, 0.19714801576667662), Duration: 1.0, Reward: 140.46905770112568, Done: False
2024-07-21 20:54:14,968 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:14,968 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:17,832 - AirSimEnvLogger - INFO - Predictive model loss: 0.02574554644525051
2024-07-21 20:54:23,630 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.083427612200616, Velocity: -0.04606091157536661, Movement: 0.34135884300618996, Collision: 0, Height: -1.0, Movement Penalty: -0.07883344795995319, Smoothness: -0.0, Curiosity: 165.43789672851562, Exploration: 0.39569140504478245, Total: 148.94924556235867
2024-07-21 20:54:23,785 - AirSimEnvLogger - INFO - Action: [0.39416724 0.39416724 0.39416724 0.39416724], Velocity: (0.3941672397997659, 0.3941672397997659, 0.3941672397997659), Duration: 1.0, Reward: 148.94924556235867, Done: False
2024-07-21 20:54:23,863 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:23,863 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:26,652 - AirSimEnvLogger - INFO - Predictive model loss: 0.05108590051531792
2024-07-21 20:54:32,353 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.32504757864177, Velocity: -0.08327172357499153, Movement: 0.6036485052249219, Collision: 0, Height: -1.0, Movement Penalty: -0.13940665079500955, Smoothness: -0.0, Curiosity: 180.4977569580078, Exploration: 0.302042596715181, Total: 163.748659469587
2024-07-21 20:54:32,446 - AirSimEnvLogger - INFO - Action: [0.69703325 0.69703325 0.69703325 0.69703325], Velocity: (0.6970332539750477, 0.6970332539750477, 0.6970332539750477), Duration: 1.0, Reward: 163.748659469587, Done: False
2024-07-21 20:54:32,526 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:32,526 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:35,404 - AirSimEnvLogger - INFO - Predictive model loss: 0.08089219778776169
2024-07-21 20:54:40,634 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.606948959900656, Velocity: -0.10493363848552566, Movement: 0.6734982184368292, Collision: 0, Height: -1.0, Movement Penalty: -0.1555377510852947, Smoothness: -0.0, Curiosity: 192.6065673828125, Exploration: 0.3079006642905954, Total: 175.57646308688865
2024-07-21 20:54:40,758 - AirSimEnvLogger - INFO - Action: [0.77768876 0.77768876 0.77768876 0.77768876], Velocity: (0.7776887554264734, 0.7776887554264734, 0.7776887554264734), Duration: 1.0, Reward: 175.57646308688865, Done: False
2024-07-21 20:54:40,838 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:40,838 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:43,662 - AirSimEnvLogger - INFO - Predictive model loss: 0.0968175008893013
2024-07-21 20:54:49,319 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.947605040043264, Velocity: -0.12400078446475335, Movement: 0.7642121086913306, Collision: 0, Height: -1.0, Movement Penalty: -0.1764872266683645, Smoothness: -0.0, Curiosity: 206.8561248779297, Exploration: 0.2782393057664767, Total: 189.47886088164884
2024-07-21 20:54:49,398 - AirSimEnvLogger - INFO - Action: [0.88243613 0.88243613 0.88243613 0.88243613], Velocity: (0.8824361333418225, 0.8824361333418225, 0.8824361333418225), Duration: 1.0, Reward: 189.47886088164884, Done: False
2024-07-21 20:54:49,460 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:49,460 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:54:52,215 - AirSimEnvLogger - INFO - Predictive model loss: 0.12031538784503937
2024-07-21 20:54:57,298 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.26748975963598, Velocity: -0.058027562543667206, Movement: 0.7036923975831362, Collision: 0, Height: -1.0, Movement Penalty: -0.16251079806852675, Smoothness: -0.0, Curiosity: 216.53280639648438, Exploration: 0.2509914794981197, Total: 198.83450345750597
2024-07-21 20:54:57,439 - AirSimEnvLogger - INFO - Action: [0.81255399 0.81255399 0.81255399 0.81255399], Velocity: (0.8125539903426336, 0.8125539903426336, 0.8125539903426336), Duration: 1.0, Reward: 198.83450345750597, Done: False
2024-07-21 20:54:57,501 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:54:57,501 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:00,264 - AirSimEnvLogger - INFO - Predictive model loss: 0.10786032676696777
2024-07-21 20:55:06,126 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.479549580115517, Velocity: -0.07870717404440193, Movement: 0.4497648295295877, Collision: 0, Height: -1.0, Movement Penalty: -0.10386873816037344, Smoothness: -0.0, Curiosity: 217.42605590820312, Exploration: 0.16096819562861162, Total: 199.4866910027317
2024-07-21 20:55:06,143 - AirSimEnvLogger - INFO - Action: [0.51934369 0.51934369 0.51934369 0.51934369], Velocity: (0.5193436908018672, 0.5193436908018672, 0.5193436908018672), Duration: 1.0, Reward: 199.4866910027317, Done: False
2024-07-21 20:55:06,221 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:06,221 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:09,124 - AirSimEnvLogger - INFO - Predictive model loss: 0.05083209276199341
2024-07-21 20:55:15,172 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.828100324949848, Velocity: -0.11123670706778917, Movement: 0.6571765782761113, Collision: 0, Height: -1.0, Movement Penalty: -0.15176842974913204, Smoothness: -0.0, Curiosity: 233.93763732910156, Exploration: 0.18338429396873232, Total: 215.65671222714184
2024-07-21 20:55:15,297 - AirSimEnvLogger - INFO - Action: [0.75884215 0.75884215 0.75884215 0.75884215], Velocity: (0.7588421487456601, 0.7588421487456601, 0.7588421487456601), Duration: 1.0, Reward: 215.65671222714184, Done: False
2024-07-21 20:55:15,329 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:15,329 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:18,152 - AirSimEnvLogger - INFO - Predictive model loss: 0.03293682634830475
2024-07-21 20:55:23,777 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.178572687306044, Velocity: -0.06290573181877256, Movement: 0.7449069235044221, Collision: 0, Height: -1.0, Movement Penalty: -0.17202888512259765, Smoothness: -0.0, Curiosity: 249.35133361816406, Exploration: 0.3041492317737946, Total: 230.75469720771343
2024-07-21 20:55:23,872 - AirSimEnvLogger - INFO - Action: [0.86014443 0.86014443 0.86014443 0.86014443], Velocity: (0.8601444256129882, 0.8601444256129882, 0.8601444256129882), Duration: 1.0, Reward: 230.75469720771343, Done: False
2024-07-21 20:55:23,935 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:23,935 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:26,780 - AirSimEnvLogger - INFO - Predictive model loss: 0.01055347640067339
2024-07-21 20:55:32,588 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.172753361392292, Velocity: -0.008751358418778043, Movement: 0.0981788446044706, Collision: 0, Height: -1.0, Movement Penalty: -0.022673432944447014, Smoothness: -0.0, Curiosity: 234.355224609375, Exploration: 0.1563484882139883, Total: 215.71996458620245
2024-07-21 20:55:32,651 - AirSimEnvLogger - INFO - Action: [0.11336716 0.11336716 0.11336716 0.11336716], Velocity: (0.11336716472223507, 0.11336716472223507, 0.11336716472223507), Duration: 1.0, Reward: 215.71996458620245, Done: False
2024-07-21 20:55:32,746 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:32,746 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:35,604 - AirSimEnvLogger - INFO - Predictive model loss: 0.027189383283257484
2024-07-21 20:55:41,520 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.48755079852848, Velocity: -0.08002362696478883, Movement: 0.4819224121705244, Collision: 0, Height: -1.0, Movement Penalty: -0.11129521375806642, Smoothness: -0.0, Curiosity: 252.261474609375, Exploration: 0.1120181425563061, Total: 233.30350715176283
2024-07-21 20:55:41,583 - AirSimEnvLogger - INFO - Action: [0.55647607 0.55647607 0.55647607 0.55647607], Velocity: (0.5564760687903321, 0.5564760687903321, 0.5564760687903321), Duration: 1.0, Reward: 233.30350715176283, Done: False
2024-07-21 20:55:41,645 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:41,645 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:44,461 - AirSimEnvLogger - INFO - Predictive model loss: 0.04753493890166283
2024-07-21 20:55:50,322 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.430802651798672, Velocity: -0.0019441635989579412, Movement: 0.006455907460223772, Collision: 0, Height: -1.0, Movement Penalty: -0.001490927964009403, Smoothness: -0.0, Curiosity: 240.95985412597656, Exploration: 0.07761859409681587, Total: 222.04686769752269
2024-07-21 20:55:50,447 - AirSimEnvLogger - INFO - Action: [0.00745464 0.00745464 0.00745464 0.00745464], Velocity: (0.007454639820047015, 0.007454639820047015, 0.007454639820047015), Duration: 1.0, Reward: 222.04686769752269, Done: False
2024-07-21 20:55:50,494 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:50,494 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:55:53,383 - AirSimEnvLogger - INFO - Predictive model loss: 0.07565280050039291
2024-07-21 20:55:59,233 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.689174611003008, Velocity: -0.037408243545935284, Movement: 0.3866900011034892, Collision: 0, Height: -1.0, Movement Penalty: -0.08930223049201447, Smoothness: -0.0, Curiosity: 257.0011901855469, Exploration: 0.062120581020949896, Total: 237.83204796873258
2024-07-21 20:55:59,327 - AirSimEnvLogger - INFO - Action: [0.44651115 0.44651115 0.44651115 0.44651115], Velocity: (0.44651115246007234, 0.44651115246007234, 0.44651115246007234), Duration: 1.0, Reward: 237.83204796873258, Done: False
2024-07-21 20:55:59,389 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:55:59,389 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:02,290 - AirSimEnvLogger - INFO - Predictive model loss: 0.040348928421735764
2024-07-21 20:56:08,074 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.561179269333625, Velocity: -0.013097550725164567, Movement: 0.12740488038735537, Collision: 0, Height: -1.0, Movement Penalty: -0.029422896795084676, Smoothness: -0.0, Curiosity: 242.837646484375, Exploration: 0.0487375918674394, Total: 223.78949234095506
2024-07-21 20:56:08,137 - AirSimEnvLogger - INFO - Action: [-0.14711448 -0.14711448 -0.14711448 -0.14711448], Velocity: (-0.14711448397542337, -0.14711448397542337, -0.14711448397542337), Duration: 1.0, Reward: 223.78949234095506, Done: False
2024-07-21 20:56:08,184 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:56:08,184 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:11,009 - AirSimEnvLogger - INFO - Predictive model loss: 0.07098497450351715
2024-07-21 20:56:16,745 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.788267527432915, Velocity: -0.035102847117458466, Movement: 0.3047286824242106, Collision: 0, Height: -1.0, Movement Penalty: -0.07037407473096718, Smoothness: -0.0, Curiosity: 258.3542175292969, Exploration: 0.03523240083164526, Total: 239.0780181475174
2024-07-21 20:56:16,852 - AirSimEnvLogger - INFO - Action: [0.35187037 0.35187037 0.35187037 0.35187037], Velocity: (0.3518703736548359, 0.3518703736548359, 0.3518703736548359), Duration: 1.0, Reward: 239.0780181475174, Done: False
2024-07-21 20:56:16,930 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:56:16,930 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:19,807 - AirSimEnvLogger - INFO - Predictive model loss: 0.034267887473106384
2024-07-21 20:56:25,304 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.069375808799297, Velocity: -0.04594389081865063, Movement: 0.5818282274658909, Collision: 0, Height: -1.0, Movement Penalty: -0.13436747349982198, Smoothness: -0.0, Curiosity: 276.7308349609375, Exploration: 0.35879092512339844, Total: 257.25365889093
2024-07-21 20:56:25,416 - AirSimEnvLogger - INFO - Action: [0.67183737 0.67183737 0.67183737 0.67183737], Velocity: (0.6718373674991098, 0.6718373674991098, 0.6718373674991098), Duration: 1.0, Reward: 257.25365889093, Done: False
2024-07-21 20:56:25,494 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:56:25,494 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:28,339 - AirSimEnvLogger - INFO - Predictive model loss: 0.022673452273011208
2024-07-21 20:56:34,128 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.407101664586467, Velocity: -0.060755245311523966, Movement: 0.7234324075352023, Collision: 0, Height: -1.0, Movement Penalty: -0.16706955809237925, Smoothness: -0.0, Curiosity: 294.54681396484375, Exploration: 0.33833937794116825, Total: 274.7292005906613
2024-07-21 20:56:34,223 - AirSimEnvLogger - INFO - Action: [0.83534779 0.83534779 0.83534779 0.83534779], Velocity: (0.8353477904618962, 0.8353477904618962, 0.8353477904618962), Duration: 1.0, Reward: 274.7292005906613, Done: False
2024-07-21 20:56:34,301 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:56:34,301 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:37,124 - AirSimEnvLogger - INFO - Predictive model loss: 0.07583437114953995
2024-07-21 20:56:43,010 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.301085866699445, Velocity: -0.00903543226406452, Movement: 0.04093812248367669, Collision: 0, Height: -1.0, Movement Penalty: -0.009454254414427444, Smoothness: -0.0, Curiosity: 273.6061706542969, Exploration: 0.15931203436721492, Total: 253.84182722221598
2024-07-21 20:56:43,150 - AirSimEnvLogger - INFO - Action: [-0.04727127 -0.04727127 -0.04727127 -0.04727127], Velocity: (-0.047271272072137216, -0.047271272072137216, -0.047271272072137216), Duration: 1.0, Reward: 253.84182722221598, Done: False
2024-07-21 20:56:43,212 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:56:43,212 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:45,884 - AirSimEnvLogger - INFO - Predictive model loss: 0.07520122081041336
2024-07-21 20:56:51,707 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.6129155088158, Velocity: -0.045580466055895254, Movement: 0.4125373947348573, Collision: 0, Height: -1.0, Movement Penalty: -0.09527143036038271, Smoothness: -0.0, Curiosity: 292.9287414550781, Exploration: 0.11562413577145164, Total: 272.8479809709669
2024-07-21 20:56:51,755 - AirSimEnvLogger - INFO - Action: [0.47635715 0.47635715 0.47635715 0.47635715], Velocity: (0.4763571518019135, 0.4763571518019135, 0.4763571518019135), Duration: 1.0, Reward: 272.8479809709669, Done: False
2024-07-21 20:56:51,819 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:56:51,819 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:56:54,736 - AirSimEnvLogger - INFO - Predictive model loss: 0.018494220450520515
2024-07-21 20:57:00,632 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.494360484791468, Velocity: -0.012511957928882673, Movement: 0.07257165821852678, Collision: 0, Height: -1.0, Movement Penalty: -0.016759706563201582, Smoothness: -0.0, Curiosity: 279.41571044921875, Exploration: 0.058341927787922035, Total: 259.4352975908946
2024-07-21 20:57:00,773 - AirSimEnvLogger - INFO - Action: [-0.08379853 -0.08379853 -0.08379853 -0.08379853], Velocity: (-0.08379853281600791, -0.08379853281600791, -0.08379853281600791), Duration: 1.0, Reward: 259.4352975908946, Done: False
2024-07-21 20:57:00,836 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:00,836 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:03,679 - AirSimEnvLogger - INFO - Predictive model loss: 0.018875403329730034
2024-07-21 20:57:09,488 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.255565799262047, Velocity: -0.06775667472983958, Movement: 0.46748112444151224, Collision: 0, Height: -1.0, Movement Penalty: -0.10796014121495041, Smoothness: -0.0, Curiosity: 262.30328369140625, Exploration: 0.3839296103242062, Total: 242.64071332186012
2024-07-21 20:57:09,612 - AirSimEnvLogger - INFO - Action: [-0.53980071 -0.53980071 -0.53980071 -0.53980071], Velocity: (-0.5398007060747521, -0.5398007060747521, -0.5398007060747521), Duration: 1.0, Reward: 242.64071332186012, Done: False
2024-07-21 20:57:09,675 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:09,675 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:12,634 - AirSimEnvLogger - INFO - Predictive model loss: 0.06606602668762207
2024-07-21 20:57:18,287 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.00314698291148, Velocity: -0.04706224301519831, Movement: 0.5489663954514227, Collision: 0, Height: -1.0, Movement Penalty: -0.12677835847597496, Smoothness: -0.0, Curiosity: 250.01638793945312, Exploration: 0.3426603068666971, Total: 230.60081271894467
2024-07-21 20:57:18,397 - AirSimEnvLogger - INFO - Action: [-0.63389179 -0.63389179 -0.63389179 -0.63389179], Velocity: (-0.6338917923798748, -0.6338917923798748, -0.6338917923798748), Duration: 1.0, Reward: 230.60081271894467, Done: False
2024-07-21 20:57:18,428 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:18,428 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:21,301 - AirSimEnvLogger - INFO - Predictive model loss: 0.12423288077116013
2024-07-21 20:57:27,290 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.68099382120527, Velocity: -0.0842719955914487, Movement: 0.6457288279549127, Collision: 0, Height: -1.0, Movement Penalty: -0.14912468505730817, Smoothness: -0.0, Curiosity: 236.12855529785156, Exploration: 0.2766871259167856, Total: 217.01861200943304
2024-07-21 20:57:27,383 - AirSimEnvLogger - INFO - Action: [-0.74562343 -0.74562343 -0.74562343 -0.74562343], Velocity: (-0.7456234252865408, -0.7456234252865408, -0.7456234252865408), Duration: 1.0, Reward: 217.01861200943304, Done: False
2024-07-21 20:57:27,447 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:27,447 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:30,279 - AirSimEnvLogger - INFO - Predictive model loss: 0.1626824289560318
2024-07-21 20:57:35,936 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.692673018832032, Velocity: -0.013142988775127486, Movement: 0.07787883043770036, Collision: 0, Height: -1.0, Movement Penalty: -0.01798534548695181, Smoothness: -0.0, Curiosity: 244.7335205078125, Exploration: 0.12753296361892, Total: 225.5707761352531
2024-07-21 20:57:35,999 - AirSimEnvLogger - INFO - Action: [-0.08992673 -0.08992673 -0.08992673 -0.08992673], Velocity: (-0.08992672743475905, -0.08992672743475905, -0.08992672743475905), Duration: 1.0, Reward: 225.5707761352531, Done: False
2024-07-21 20:57:36,077 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:36,077 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:38,874 - AirSimEnvLogger - INFO - Predictive model loss: 0.09517212212085724
2024-07-21 20:57:44,331 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.788393084156567, Velocity: -0.025351478137704803, Movement: 0.2389007937867057, Collision: 0, Height: -1.0, Movement Penalty: -0.055171775040947925, Smoothness: -0.0, Curiosity: 256.3917236328125, Exploration: 0.3557487204101557, Total: 237.18847783014616
2024-07-21 20:57:44,346 - AirSimEnvLogger - INFO - Action: [0.27585888 0.27585888 0.27585888 0.27585888], Velocity: (0.2758588752047396, 0.2758588752047396, 0.2758588752047396), Duration: 1.0, Reward: 237.18847783014616, Done: False
2024-07-21 20:57:44,409 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:44,409 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:47,262 - AirSimEnvLogger - INFO - Predictive model loss: 0.031242704018950462
2024-07-21 20:57:53,377 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.05352417265386, Velocity: -0.058511497976929644, Movement: 0.5523185910127753, Collision: 0, Height: -1.0, Movement Penalty: -0.12755251487986424, Smoothness: -0.0, Curiosity: 275.1187438964844, Exploration: 0.3524611788718068, Total: 255.6539829904752
2024-07-21 20:57:53,519 - AirSimEnvLogger - INFO - Action: [0.63776257 0.63776257 0.63776257 0.63776257], Velocity: (0.6377625743993212, 0.6377625743993212, 0.6377625743993212), Duration: 1.0, Reward: 255.6539829904752, Done: False
2024-07-21 20:57:53,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:57:53,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:57:56,095 - AirSimEnvLogger - INFO - Predictive model loss: 0.09822576493024826
2024-07-21 20:58:02,160 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.306938987691158, Velocity: -0.08610727157996202, Movement: 0.5686043907238425, Collision: 0, Height: -1.0, Movement Penalty: -0.13131355921872545, Smoothness: -0.0, Curiosity: 286.4819641113281, Exploration: 0.29129552684822957, Total: 266.74736020274605
2024-07-21 20:58:02,271 - AirSimEnvLogger - INFO - Action: [0.6565678 0.6565678 0.6565678 0.6565678], Velocity: (0.6565677960936273, 0.6565677960936273, 0.6565677960936273), Duration: 1.0, Reward: 266.74736020274605, Done: False
2024-07-21 20:58:02,317 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:02,317 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:05,139 - AirSimEnvLogger - INFO - Predictive model loss: 0.23975741863250732
2024-07-21 20:58:10,871 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.43058080813346, Velocity: -0.03806556356337551, Movement: 0.31804074692215806, Collision: 0, Height: -1.0, Movement Penalty: -0.07344836433951105, Smoothness: -0.0, Curiosity: 285.688720703125, Exploration: 0.11505276718083372, Total: 265.78859699748614
2024-07-21 20:58:10,997 - AirSimEnvLogger - INFO - Action: [0.36724182 0.36724182 0.36724182 0.36724182], Velocity: (0.3672418216975552, 0.3672418216975552, 0.3672418216975552), Duration: 1.0, Reward: 265.78859699748614, Done: False
2024-07-21 20:58:11,013 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:11,013 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:13,759 - AirSimEnvLogger - INFO - Predictive model loss: 0.2859495282173157
2024-07-21 20:58:19,402 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.736720280330047, Velocity: -0.0644193286026178, Movement: 0.5282733501616921, Collision: 0, Height: -1.0, Movement Penalty: -0.12199950436862333, Smoothness: -0.0, Curiosity: 301.70062255859375, Exploration: 0.13822441584872125, Total: 281.502210477887
2024-07-21 20:58:19,496 - AirSimEnvLogger - INFO - Action: [0.60999752 0.60999752 0.60999752 0.60999752], Velocity: (0.6099975218431166, 0.6099975218431166, 0.6099975218431166), Duration: 1.0, Reward: 281.502210477887, Done: False
2024-07-21 20:58:19,527 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:19,527 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:22,430 - AirSimEnvLogger - INFO - Predictive model loss: 0.26447004079818726
2024-07-21 20:58:28,323 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.074919699775425, Velocity: -0.09633358070258975, Movement: 0.6734379975468309, Collision: 0, Height: -1.0, Movement Penalty: -0.15552384366647415, Smoothness: -0.0, Curiosity: 319.3829650878906, Exploration: 0.290115748648011, Total: 298.8816580500573
2024-07-21 20:58:28,417 - AirSimEnvLogger - INFO - Action: [0.77761922 0.77761922 0.77761922 0.77761922], Velocity: (0.7776192183323707, 0.7776192183323707, 0.7776192183323707), Duration: 1.0, Reward: 298.8816580500573, Done: False
2024-07-21 20:58:28,495 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:28,495 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:31,271 - AirSimEnvLogger - INFO - Predictive model loss: 0.20694005489349365
2024-07-21 20:58:37,273 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.94298549323337, Velocity: -0.012781323109083332, Movement: 0.06417679254313746, Collision: 0, Height: -1.0, Movement Penalty: -0.014820995380202873, Smoothness: -0.0, Curiosity: 298.1630554199219, Exploration: 0.16088898194594262, Total: 277.75737051349114
2024-07-21 20:58:37,429 - AirSimEnvLogger - INFO - Action: [-0.07410498 -0.07410498 -0.07410498 -0.07410498], Velocity: (-0.07410497690101436, -0.07410497690101436, -0.07410497690101436), Duration: 1.0, Reward: 277.75737051349114, Done: False
2024-07-21 20:58:37,493 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:37,493 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:40,288 - AirSimEnvLogger - INFO - Predictive model loss: 0.03025350160896778
2024-07-21 20:58:46,188 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.24558852562908, Velocity: -0.036709066391827125, Movement: 0.37890840835383427, Collision: 0, Height: -1.0, Movement Penalty: -0.08750514862451955, Smoothness: -0.0, Curiosity: 317.1982727050781, Exploration: 0.11238807949755697, Total: 296.48415713395156
2024-07-21 20:58:46,218 - AirSimEnvLogger - INFO - Action: [0.43752574 0.43752574 0.43752574 0.43752574], Velocity: (0.43752574312259773, 0.43752574312259773, 0.43752574312259773), Duration: 1.0, Reward: 296.48415713395156, Done: False
2024-07-21 20:58:46,266 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:46,266 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:48,931 - AirSimEnvLogger - INFO - Predictive model loss: 0.04879716783761978
2024-07-21 20:58:55,044 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.439332208851223, Velocity: -0.07866861401611103, Movement: 0.42384057231132355, Collision: 0, Height: -1.0, Movement Penalty: -0.09788178740697108, Smoothness: -0.0, Curiosity: 327.2171936035156, Exploration: 0.2707117426438942, Total: 306.342655020491
2024-07-21 20:58:55,091 - AirSimEnvLogger - INFO - Action: [0.48940894 0.48940894 0.48940894 0.48940894], Velocity: (0.4894089370348554, 0.4894089370348554, 0.4894089370348554), Duration: 1.0, Reward: 306.342655020491, Done: False
2024-07-21 20:58:55,195 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:58:55,196 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:58:58,031 - AirSimEnvLogger - INFO - Predictive model loss: 0.10990296304225922
2024-07-21 20:59:03,762 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.7029395983289, Velocity: -0.06886079691758229, Movement: 0.5058456375426519, Collision: 0, Height: -1.0, Movement Penalty: -0.11682004600145918, Smoothness: -0.0, Curiosity: 340.3413391113281, Exploration: 0.194182890666498, Total: 319.188583864416
2024-07-21 20:59:03,887 - AirSimEnvLogger - INFO - Action: [0.58410023 0.58410023 0.58410023 0.58410023], Velocity: (0.5841002300072958, 0.5841002300072958, 0.5841002300072958), Duration: 1.0, Reward: 319.188583864416, Done: False
2024-07-21 20:59:03,951 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:03,951 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:06,883 - AirSimEnvLogger - INFO - Predictive model loss: 0.15971103310585022
2024-07-21 20:59:12,878 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.067160472798374, Velocity: -0.08038961187954308, Movement: 0.6855243484312842, Collision: 0, Height: -1.0, Movement Penalty: -0.1583150668411379, Smoothness: -0.0, Curiosity: 360.4095764160156, Exploration: 0.26776384961495203, Total: 338.91277854280054
2024-07-21 20:59:13,004 - AirSimEnvLogger - INFO - Action: [0.79157533 0.79157533 0.79157533 0.79157533], Velocity: (0.7915753342056895, 0.7915753342056895, 0.7915753342056895), Duration: 1.0, Reward: 338.91277854280054, Done: False
2024-07-21 20:59:13,067 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:13,067 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:15,851 - AirSimEnvLogger - INFO - Predictive model loss: 0.06701312214136124
2024-07-21 20:59:21,534 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.01062621570336, Velocity: -0.015342779067720127, Movement: 0.09724568785637643, Collision: 0, Height: -1.0, Movement Penalty: -0.0224579296245637, Smoothness: -0.0, Curiosity: 345.0628662109375, Exploration: 0.11788698358580019, Total: 323.5802051550397
2024-07-21 20:59:21,660 - AirSimEnvLogger - INFO - Action: [0.11228965 0.11228965 0.11228965 0.11228965], Velocity: (0.1122896481228185, 0.1122896481228185, 0.1122896481228185), Duration: 1.0, Reward: 323.5802051550397, Done: False
2024-07-21 20:59:21,724 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:21,724 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:24,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.009876377880573273
2024-07-21 20:59:30,293 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.78781645126499, Velocity: -0.05435995769515639, Movement: 0.37900784030983986, Collision: 0, Height: -1.0, Movement Penalty: -0.0875281114511459, Smoothness: -0.0, Curiosity: 325.5564270019531, Exploration: 0.42317764874683667, Total: 304.3698024560482
2024-07-21 20:59:30,420 - AirSimEnvLogger - INFO - Action: [-0.43764056 -0.43764056 -0.43764056 -0.43764056], Velocity: (-0.4376405572557295, -0.4376405572557295, -0.4376405572557295), Duration: 1.0, Reward: 304.3698024560482, Done: False
2024-07-21 20:59:30,483 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:30,483 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:33,321 - AirSimEnvLogger - INFO - Predictive model loss: 0.06169766187667847
2024-07-21 20:59:38,863 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.055987556496657, Velocity: -0.0137510776980076, Movement: 0.24303114986671867, Collision: 0, Height: -1.0, Movement Penalty: -0.05612563991880573, Smoothness: -0.0, Curiosity: 346.26885986328125, Exploration: 0.017770403219321287, Total: 324.72154593277713
2024-07-21 20:59:39,053 - AirSimEnvLogger - INFO - Action: [0.2806282 0.2806282 0.2806282 0.2806282], Velocity: (0.2806281995940286, 0.2806281995940286, 0.2806281995940286), Duration: 1.0, Reward: 324.72154593277713, Done: False
2024-07-21 20:59:39,099 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:39,099 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:41,879 - AirSimEnvLogger - INFO - Predictive model loss: 0.030241118744015694
2024-07-21 20:59:47,695 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.301361253749242, Velocity: -0.08483649810056018, Movement: 0.4828816379495489, Collision: 0, Height: -1.0, Movement Penalty: -0.1115167374627598, Smoothness: -0.0, Curiosity: 363.8160400390625, Exploration: 0.3964032056342314, Total: 342.10921293338066
2024-07-21 20:59:47,835 - AirSimEnvLogger - INFO - Action: [0.55758369 0.55758369 0.55758369 0.55758369], Velocity: (0.5575836873137989, 0.5575836873137989, 0.5575836873137989), Duration: 1.0, Reward: 342.10921293338066, Done: False
2024-07-21 20:59:47,928 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:47,928 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:50,808 - AirSimEnvLogger - INFO - Predictive model loss: 0.042722128331661224
2024-07-21 20:59:56,525 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.083025912470795, Velocity: -0.024765586460235425, Movement: 0.1773382673452467, Collision: 0, Height: -1.0, Movement Penalty: -0.04095451855576, Smoothness: -0.0, Curiosity: 342.0665283203125, Exploration: 0.0903440034417243, Total: 320.50615506813773
2024-07-21 20:59:56,667 - AirSimEnvLogger - INFO - Action: [-0.20477259 -0.20477259 -0.20477259 -0.20477259], Velocity: (-0.2047725927788, -0.2047725927788, -0.2047725927788), Duration: 1.0, Reward: 320.50615506813773, Done: False
2024-07-21 20:59:56,761 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:59:56,761 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:59:59,512 - AirSimEnvLogger - INFO - Predictive model loss: 0.09896152466535568
2024-07-21 21:00:05,404 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.80359501086782, Velocity: -0.0701037221243285, Movement: 0.5205296448177085, Collision: 0, Height: -1.0, Movement Penalty: -0.1202111722226737, Smoothness: -0.0, Curiosity: 323.2195739746094, Exploration: 0.45348105120364357, Total: 302.0260377974691
2024-07-21 21:00:05,497 - AirSimEnvLogger - INFO - Action: [-0.60105586 -0.60105586 -0.60105586 -0.60105586], Velocity: (-0.6010558611133685, -0.6010558611133685, -0.6010558611133685), Duration: 1.0, Reward: 302.0260377974691, Done: False
2024-07-21 21:00:05,591 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:05,591 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:00:08,356 - AirSimEnvLogger - INFO - Predictive model loss: 0.10684263706207275
2024-07-21 21:00:14,305 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.44677240320756, Velocity: -0.08744842436716115, Movement: 0.6878095865940798, Collision: 0, Height: -1.0, Movement Penalty: -0.15884281998851887, Smoothness: -0.0, Curiosity: 304.9081115722656, Exploration: 0.39039181223789365, Total: 284.0592563755649
2024-07-21 21:00:14,430 - AirSimEnvLogger - INFO - Action: [-0.7942141 -0.7942141 -0.7942141 -0.7942141], Velocity: (-0.7942140999425943, -0.7942140999425943, -0.7942140999425943), Duration: 1.0, Reward: 284.0592563755649, Done: False
2024-07-21 21:00:14,492 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:14,492 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:00:17,394 - AirSimEnvLogger - INFO - Predictive model loss: 0.11865466088056564
2024-07-21 21:00:23,348 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.04372665567298, Velocity: -0.13018453735511526, Movement: 0.7762306509582048, Collision: 0, Height: -1.0, Movement Penalty: -0.17926279011358318, Smoothness: -0.0, Curiosity: 286.69775390625, Exploration: 0.3528151212005984, Total: 266.241197170795
2024-07-21 21:00:23,473 - AirSimEnvLogger - INFO - Action: [-0.89631395 -0.89631395 -0.89631395 -0.89631395], Velocity: (-0.8963139505679159, -0.8963139505679159, -0.8963139505679159), Duration: 1.0, Reward: 266.241197170795, Done: False
2024-07-21 21:00:23,552 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:23,552 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:00:26,315 - AirSimEnvLogger - INFO - Predictive model loss: 0.16757310926914215
2024-07-21 21:00:31,963 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.902406608881034, Velocity: -0.0481911009729072, Movement: 0.3832723394486052, Collision: 0, Height: -1.0, Movement Penalty: -0.08851295534143595, Smoothness: -0.0, Curiosity: 285.640869140625, Exploration: 0.1663285472663813, Total: 265.28130063737007
2024-07-21 21:00:32,088 - AirSimEnvLogger - INFO - Action: [-0.44256478 -0.44256478 -0.44256478 -0.44256478], Velocity: (-0.44256477670717975, -0.44256477670717975, -0.44256477670717975), Duration: 1.0, Reward: 265.28130063737007, Done: False
2024-07-21 21:00:32,120 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:32,120 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:00:34,961 - AirSimEnvLogger - INFO - Predictive model loss: 0.1611824780702591
2024-07-21 21:00:40,568 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.071628150819524, Velocity: -0.03412225550284696, Movement: 0.24017388371479603, Collision: 0, Height: -1.0, Movement Penalty: -0.05546578256602215, Smoothness: -0.0, Curiosity: 305.176513671875, Exploration: 0.3699365919056699, Total: 284.6924501640806
2024-07-21 21:00:40,693 - AirSimEnvLogger - INFO - Action: [0.27732891 0.27732891 0.27732891 0.27732891], Velocity: (0.27732891283011074, 0.27732891283011074, 0.27732891283011074), Duration: 1.0, Reward: 284.6924501640806, Done: False
2024-07-21 21:00:40,754 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:40,754 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:00:43,576 - AirSimEnvLogger - INFO - Predictive model loss: 0.12021658569574356
2024-07-21 21:00:49,158 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.323077628772243, Velocity: -0.08939762762179716, Movement: 0.5299158635181529, Collision: 0, Height: -1.0, Movement Penalty: -0.12237882658002341, Smoothness: -0.0, Curiosity: 324.2465515136719, Exploration: 0.4585640564025771, Total: 303.5330026627126
2024-07-21 21:00:49,236 - AirSimEnvLogger - INFO - Action: [0.61189413 0.61189413 0.61189413 0.61189413], Velocity: (0.6118941329001171, 0.6118941329001171, 0.6118941329001171), Duration: 1.0, Reward: 303.5330026627126, Done: False
2024-07-21 21:00:49,316 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:49,316 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:00:52,208 - AirSimEnvLogger - INFO - Predictive model loss: 0.04223192483186722
2024-07-21 21:00:58,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.579965788647403, Velocity: -0.0830367229208573, Movement: 0.5599776862698229, Collision: 0, Height: -1.0, Movement Penalty: -0.12932130716322643, Smoothness: -0.0, Curiosity: 336.85845947265625, Exploration: 0.28966302950397294, Total: 315.8505487309988
2024-07-21 21:00:58,254 - AirSimEnvLogger - INFO - Action: [0.64660654 0.64660654 0.64660654 0.64660654], Velocity: (0.6466065358161321, 0.6466065358161321, 0.6466065358161321), Duration: 1.0, Reward: 315.8505487309988, Done: False
2024-07-21 21:00:58,318 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:00:58,318 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:01,259 - AirSimEnvLogger - INFO - Predictive model loss: 0.06570815294981003
2024-07-21 21:01:07,271 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.95405785592853, Velocity: -0.12193347100608716, Movement: 0.7104022892152688, Collision: 0, Height: -1.0, Movement Penalty: -0.16406038116454474, Smoothness: -0.0, Curiosity: 356.53753662109375, Exploration: 0.26244291540939774, Total: 335.14907341851665
2024-07-21 21:01:07,364 - AirSimEnvLogger - INFO - Action: [0.82030191 0.82030191 0.82030191 0.82030191], Velocity: (0.8203019058227237, 0.8203019058227237, 0.8203019058227237), Duration: 1.0, Reward: 335.14907341851665, Done: False
2024-07-21 21:01:07,426 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:07,426 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:10,204 - AirSimEnvLogger - INFO - Predictive model loss: 0.22566479444503784
2024-07-21 21:01:15,957 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.90831385388118, Velocity: -0.010201865496834267, Movement: 0.11590011848144223, Collision: 0, Height: -1.0, Movement Penalty: -0.026765985841748077, Smoothness: -0.0, Curiosity: 341.27899169921875, Exploration: 0.12953208270347757, Total: 319.9022930614762
2024-07-21 21:01:16,066 - AirSimEnvLogger - INFO - Action: [0.13382993 0.13382993 0.13382993 0.13382993], Velocity: (0.13382992920874037, 0.13382992920874037, 0.13382992920874037), Duration: 1.0, Reward: 319.9022930614762, Done: False
2024-07-21 21:01:16,129 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:16,129 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:18,932 - AirSimEnvLogger - INFO - Predictive model loss: 0.23368626832962036
2024-07-21 21:01:24,793 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.241510423628533, Velocity: -0.0631533099828466, Movement: 0.4627084847364184, Collision: 0, Height: -1.0, Movement Penalty: -0.106857947287558, Smoothness: -0.0, Curiosity: 361.06146240234375, Exploration: 0.10565087401892168, Total: 339.34928656296063
2024-07-21 21:01:24,809 - AirSimEnvLogger - INFO - Action: [0.53428974 0.53428974 0.53428974 0.53428974], Velocity: (0.53428973643779, 0.53428973643779, 0.53428973643779), Duration: 1.0, Reward: 339.34928656296063, Done: False
2024-07-21 21:01:24,841 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:24,841 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:27,366 - AirSimEnvLogger - INFO - Predictive model loss: 0.20864629745483398
2024-07-21 21:01:32,982 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.246217700366046, Velocity: -0.005771356903945733, Movement: 0.1520324250535292, Collision: 0, Height: -1.0, Movement Penalty: -0.03511038461208268, Smoothness: -0.0, Curiosity: 355.3058776855469, Exploration: 0.11289905196112016, Total: 333.58877897864295
2024-07-21 21:01:33,109 - AirSimEnvLogger - INFO - Action: [0.17555192 0.17555192 0.17555192 0.17555192], Velocity: (0.1755519230604134, 0.1755519230604134, 0.1755519230604134), Duration: 1.0, Reward: 333.58877897864295, Done: False
2024-07-21 21:01:33,172 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:33,172 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:36,120 - AirSimEnvLogger - INFO - Predictive model loss: 0.08828667551279068
2024-07-21 21:01:41,927 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.092059884379065, Velocity: -0.028615158753839555, Movement: 0.2224842203621577, Collision: 0, Height: -1.0, Movement Penalty: -0.05138052980661431, Smoothness: -0.0, Curiosity: 340.8396911621094, Exploration: 0.25306143134357556, Total: 319.3084314170113
2024-07-21 21:01:42,006 - AirSimEnvLogger - INFO - Action: [-0.25690265 -0.25690265 -0.25690265 -0.25690265], Velocity: (-0.25690264903307153, -0.25690264903307153, -0.25690264903307153), Duration: 1.0, Reward: 319.3084314170113, Done: False
2024-07-21 21:01:42,069 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:42,069 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:44,883 - AirSimEnvLogger - INFO - Predictive model loss: 0.013253966346383095
2024-07-21 21:01:50,542 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.106574224112606, Velocity: -0.0081142991982409, Movement: 0.08228768472105417, Collision: 0, Height: -1.0, Movement Penalty: -0.019003526769876673, Smoothness: -0.0, Curiosity: 342.50311279296875, Exploration: 0.1444575564617586, Total: 320.9309708893985
2024-07-21 21:01:50,573 - AirSimEnvLogger - INFO - Action: [-0.09501763 -0.09501763 -0.09501763 -0.09501763], Velocity: (-0.09501763384938336, -0.09501763384938336, -0.09501763384938336), Duration: 1.0, Reward: 320.9309708893985, Done: False
2024-07-21 21:01:50,620 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:50,620 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:01:53,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.04451914131641388
2024-07-21 21:01:59,237 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.7909998358485, Velocity: -0.04275993428906061, Movement: 0.4726285661914624, Collision: 0, Height: -1.0, Movement Penalty: -0.10914889196693907, Smoothness: -0.0, Curiosity: 323.2906799316406, Exploration: 0.17902475948169766, Total: 302.0481733503274
2024-07-21 21:01:59,237 - AirSimEnvLogger - INFO - Action: [-0.54574446 -0.54574446 -0.54574446 -0.54574446], Velocity: (-0.5457444598346953, -0.5457444598346953, -0.5457444598346953), Duration: 1.0, Reward: 302.0481733503274, Done: False
2024-07-21 21:01:59,300 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:01:59,300 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:02,262 - AirSimEnvLogger - INFO - Predictive model loss: 0.13181085884571075
2024-07-21 21:02:07,956 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.66664737731773, Velocity: -0.06432915327400104, Movement: 0.33576887995641735, Collision: 0, Height: -1.0, Movement Penalty: -0.07754250129133468, Smoothness: -0.0, Curiosity: 318.8274841308594, Exploration: 0.22140348487789216, Total: 297.71356303228436
2024-07-21 21:02:08,113 - AirSimEnvLogger - INFO - Action: [-0.38771251 -0.38771251 -0.38771251 -0.38771251], Velocity: (-0.3877125064566734, -0.3877125064566734, -0.3877125064566734), Duration: 1.0, Reward: 297.71356303228436, Done: False
2024-07-21 21:02:08,175 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:02:08,175 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:11,002 - AirSimEnvLogger - INFO - Predictive model loss: 0.23833489418029785
2024-07-21 21:02:17,225 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.523807776618956, Velocity: -0.04558127790285881, Movement: 0.2728203632594336, Collision: 0, Height: -1.0, Movement Penalty: -0.0630051640672982, Smoothness: -0.0, Curiosity: 314.1772155761719, Exploration: 0.08102442464589857, Total: 293.17417755825386
2024-07-21 21:02:17,400 - AirSimEnvLogger - INFO - Action: [-0.31502582 -0.31502582 -0.31502582 -0.31502582], Velocity: (-0.31502582033649096, -0.31502582033649096, -0.31502582033649096), Duration: 1.0, Reward: 293.17417755825386, Done: False
2024-07-21 21:02:17,477 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:02:17,477 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:20,416 - AirSimEnvLogger - INFO - Predictive model loss: 0.21373601257801056
2024-07-21 21:02:26,164 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.64040153392665, Velocity: -0.024900354191071825, Movement: 0.12734533301261372, Collision: 0, Height: -1.0, Movement Penalty: -0.029409144917950027, Smoothness: -0.0, Curiosity: 326.47149658203125, Exploration: 0.1607907571510856, Total: 305.36871066099303
2024-07-21 21:02:26,213 - AirSimEnvLogger - INFO - Action: [0.14704572 0.14704572 0.14704572 0.14704572], Velocity: (0.14704572458975013, 0.14704572458975013, 0.14704572458975013), Duration: 1.0, Reward: 305.36871066099303, Done: False
2024-07-21 21:02:26,338 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:02:26,338 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:29,163 - AirSimEnvLogger - INFO - Predictive model loss: 0.10088986158370972
2024-07-21 21:02:35,081 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.759416065286096, Velocity: -0.04462933440760685, Movement: 0.2579999133805846, Collision: 0, Height: -1.0, Movement Penalty: -0.059582527776472265, Smoothness: -0.0, Curiosity: 335.5413818359375, Exploration: 0.2541976658631371, Total: 314.34229702434425
2024-07-21 21:02:35,191 - AirSimEnvLogger - INFO - Action: [0.29791264 0.29791264 0.29791264 0.29791264], Velocity: (0.2979126388823613, 0.2979126388823613, 0.2979126388823613), Duration: 1.0, Reward: 314.34229702434425, Done: False
2024-07-21 21:02:35,269 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:02:35,269 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:38,206 - AirSimEnvLogger - INFO - Predictive model loss: 0.01481733750551939
2024-07-21 21:02:43,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.999471182801415, Velocity: -0.042655477259069145, Movement: 0.4441202804058322, Collision: 0, Height: -1.0, Movement Penalty: -0.10256518537795173, Smoothness: -0.0, Curiosity: 350.4356994628906, Exploration: 0.2225478129240778, Total: 328.9940429759037
2024-07-21 21:02:44,117 - AirSimEnvLogger - INFO - Action: [0.51282593 0.51282593 0.51282593 0.51282593], Velocity: (0.5128259268897586, 0.5128259268897586, 0.5128259268897586), Duration: 1.0, Reward: 328.9940429759037, Done: False
2024-07-21 21:02:44,149 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:02:44,149 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:46,978 - AirSimEnvLogger - INFO - Predictive model loss: 0.0700855553150177
2024-07-21 21:02:52,868 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.208482230411654, Velocity: -0.042141391909796995, Movement: 0.4412943560396246, Collision: 0, Height: -1.0, Movement Penalty: -0.10191256610053592, Smoothness: -0.0, Curiosity: 359.96636962890625, Exploration: 0.20364288735916328, Total: 338.31133605019147
2024-07-21 21:02:52,979 - AirSimEnvLogger - INFO - Action: [0.50956283 0.50956283 0.50956283 0.50956283], Velocity: (0.5095628305026796, 0.5095628305026796, 0.5095628305026796), Duration: 1.0, Reward: 338.31133605019147, Done: False
2024-07-21 21:02:53,041 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:02:53,041 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:02:55,872 - AirSimEnvLogger - INFO - Predictive model loss: 0.22546643018722534
2024-07-21 21:03:01,479 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.45176002082266, Velocity: -0.0561760496315385, Movement: 0.47201257712344574, Collision: 0, Height: -1.0, Movement Penalty: -0.10900663538524416, Smoothness: -0.0, Curiosity: 371.18389892578125, Exploration: 0.1637273759292405, Total: 349.27575703956387
2024-07-21 21:03:01,510 - AirSimEnvLogger - INFO - Action: [0.54503318 0.54503318 0.54503318 0.54503318], Velocity: (0.5450331769262208, 0.5450331769262208, 0.5450331769262208), Duration: 1.0, Reward: 349.27575703956387, Done: False
2024-07-21 21:03:01,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:01,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:04,368 - AirSimEnvLogger - INFO - Predictive model loss: 0.2848091125488281
2024-07-21 21:03:09,449 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.491011845292444, Velocity: -0.02037237319877204, Movement: 0.17636380248794623, Collision: 0, Height: -1.0, Movement Penalty: -0.0407294755366887, Smoothness: -0.0, Curiosity: 366.4121398925781, Exploration: 0.09378183351034129, Total: 344.44498682625317
2024-07-21 21:03:09,529 - AirSimEnvLogger - INFO - Action: [0.20364738 0.20364738 0.20364738 0.20364738], Velocity: (0.2036473776834435, 0.2036473776834435, 0.2036473776834435), Duration: 1.0, Reward: 344.44498682625317, Done: False
2024-07-21 21:03:09,592 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:09,592 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:12,500 - AirSimEnvLogger - INFO - Predictive model loss: 0.2852696478366852
2024-07-21 21:03:18,196 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.256856319012545, Velocity: -0.05264869489427329, Movement: 0.34227444033886484, Collision: 0, Height: -1.0, Movement Penalty: -0.07904489610654886, Smoothness: -0.0, Curiosity: 345.9017333984375, Exploration: 0.3184474094270667, Total: 324.2212510876979
2024-07-21 21:03:18,274 - AirSimEnvLogger - INFO - Action: [-0.39522448 -0.39522448 -0.39522448 -0.39522448], Velocity: (-0.3952244805327442, -0.3952244805327442, -0.3952244805327442), Duration: 1.0, Reward: 324.2212510876979, Done: False
2024-07-21 21:03:18,336 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:18,336 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:21,145 - AirSimEnvLogger - INFO - Predictive model loss: 0.16623982787132263
2024-07-21 21:03:26,869 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.955016013671074, Velocity: -0.05320494762014083, Movement: 0.5779159670509137, Collision: 0, Height: -1.0, Movement Penalty: -0.13346397565833118, Smoothness: -0.0, Curiosity: 328.0543212890625, Exploration: 0.4183726734488941, Total: 306.70438674308065
2024-07-21 21:03:26,980 - AirSimEnvLogger - INFO - Action: [-0.66731988 -0.66731988 -0.66731988 -0.66731988], Velocity: (-0.6673198782916558, -0.6673198782916558, -0.6673198782916558), Duration: 1.0, Reward: 306.70438674308065, Done: False
2024-07-21 21:03:27,029 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:27,029 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:29,826 - AirSimEnvLogger - INFO - Predictive model loss: 0.05473362281918526
2024-07-21 21:03:35,565 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.15611719698376, Velocity: -0.015689383943160794, Movement: 0.1172271432855956, Collision: 0, Height: -1.0, Movement Penalty: -0.02707244909290778, Smoothness: -0.0, Curiosity: 346.9278869628906, Exploration: 0.09509952083521121, Total: 325.2949492928182
2024-07-21 21:03:35,691 - AirSimEnvLogger - INFO - Action: [0.13536225 0.13536225 0.13536225 0.13536225], Velocity: (0.1353622454645389, 0.1353622454645389, 0.1353622454645389), Duration: 1.0, Reward: 325.2949492928182, Done: False
2024-07-21 21:03:35,738 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:35,738 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:38,477 - AirSimEnvLogger - INFO - Predictive model loss: 0.06798681616783142
2024-07-21 21:03:44,162 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.840994212467326, Velocity: -0.03619039912730309, Movement: 0.3588084475770863, Collision: 0, Height: -1.0, Movement Penalty: -0.08286326151845701, Smoothness: -0.0, Curiosity: 327.4556884765625, Exploration: 0.07964391496269259, Total: 306.13819487652546
2024-07-21 21:03:44,195 - AirSimEnvLogger - INFO - Action: [-0.41431631 -0.41431631 -0.41431631 -0.41431631], Velocity: (-0.414316307592285, -0.414316307592285, -0.414316307592285), Duration: 1.0, Reward: 306.13819487652546, Done: False
2024-07-21 21:03:44,225 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:44,225 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:46,500 - AirSimEnvLogger - INFO - Predictive model loss: 0.152028426527977
2024-07-21 21:03:52,173 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.58651067525405, Velocity: -0.08819930411576482, Movement: 0.49698025049717426, Collision: 0, Height: -1.0, Movement Penalty: -0.11477267256258848, Smoothness: -0.0, Curiosity: 313.6766052246094, Exploration: 0.33398798069513863, Total: 292.67028275329193
2024-07-21 21:03:52,296 - AirSimEnvLogger - INFO - Action: [-0.57386336 -0.57386336 -0.57386336 -0.57386336], Velocity: (-0.5738633628129424, -0.5738633628129424, -0.5738633628129424), Duration: 1.0, Reward: 292.67028275329193, Done: False
2024-07-21 21:03:52,374 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:03:52,374 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:03:55,273 - AirSimEnvLogger - INFO - Predictive model loss: 0.2509048879146576
2024-07-21 21:04:01,225 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.800483957136734, Velocity: -0.015706615457963563, Movement: 0.1841235349084626, Collision: 0, Height: -1.0, Movement Penalty: -0.042521508977418535, Smoothness: -0.0, Curiosity: 333.6483459472656, Exploration: 0.14396310655482206, Total: 312.3839193834892
2024-07-21 21:04:01,319 - AirSimEnvLogger - INFO - Action: [0.21260754 0.21260754 0.21260754 0.21260754], Velocity: (0.21260754488709266, 0.21260754488709266, 0.21260754488709266), Duration: 1.0, Reward: 312.3839193834892, Done: False
2024-07-21 21:04:01,413 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:01,413 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:04,235 - AirSimEnvLogger - INFO - Predictive model loss: 0.28313663601875305
2024-07-21 21:04:10,244 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.514146118094637, Velocity: -0.03715079026648044, Movement: 0.31361145237956517, Collision: 0, Height: -1.0, Movement Penalty: -0.07242546258091657, Smoothness: -0.0, Curiosity: 314.9017639160156, Exploration: 0.07921243214815207, Total: 293.9098144503311
2024-07-21 21:04:10,385 - AirSimEnvLogger - INFO - Action: [-0.36212731 -0.36212731 -0.36212731 -0.36212731], Velocity: (-0.3621273129045829, -0.3621273129045829, -0.3621273129045829), Duration: 1.0, Reward: 293.9098144503311, Done: False
2024-07-21 21:04:10,431 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:10,432 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:13,292 - AirSimEnvLogger - INFO - Predictive model loss: 0.08575042337179184
2024-07-21 21:04:18,726 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.775836853715145, Velocity: -0.026319503346993657, Movement: 0.2722558147040936, Collision: 0, Height: -1.0, Movement Penalty: -0.06287478716313973, Smoothness: -0.0, Curiosity: 335.35369873046875, Exploration: 0.03965501106622619, Total: 314.0910289994411
2024-07-21 21:04:18,774 - AirSimEnvLogger - INFO - Action: [0.31437394 0.31437394 0.31437394 0.31437394], Velocity: (0.3143739358156986, 0.3143739358156986, 0.3143739358156986), Duration: 1.0, Reward: 314.0910289994411, Done: False
2024-07-21 21:04:18,883 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:18,883 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:21,441 - AirSimEnvLogger - INFO - Predictive model loss: 0.030617864802479744
2024-07-21 21:04:27,289 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.789259404890434, Velocity: -0.01186034406272699, Movement: 0.14499555720842552, Collision: 0, Height: -1.0, Movement Penalty: -0.033485289594233705, Smoothness: -0.0, Curiosity: 334.9392395019531, Exploration: 0.22187451969756886, Total: 313.7033819353989
2024-07-21 21:04:27,336 - AirSimEnvLogger - INFO - Action: [0.16742645 0.16742645 0.16742645 0.16742645], Velocity: (0.1674264479711685, 0.1674264479711685, 0.1674264479711685), Duration: 1.0, Reward: 313.7033819353989, Done: False
2024-07-21 21:04:27,415 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:27,415 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:30,177 - AirSimEnvLogger - INFO - Predictive model loss: 0.016185924410820007
2024-07-21 21:04:35,887 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.53455604038684, Velocity: -0.033240865609310125, Movement: 0.3600898396984429, Collision: 0, Height: -1.0, Movement Penalty: -0.08315918635293808, Smoothness: -0.0, Curiosity: 315.260498046875, Exploration: 0.23574531180056468, Total: 294.2856723260227
2024-07-21 21:04:35,903 - AirSimEnvLogger - INFO - Action: [-0.41579593 -0.41579593 -0.41579593 -0.41579593], Velocity: (-0.4157959317646904, -0.4157959317646904, -0.4157959317646904), Duration: 1.0, Reward: 294.2856723260227, Done: False
2024-07-21 21:04:35,981 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:35,981 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:38,804 - AirSimEnvLogger - INFO - Predictive model loss: 0.1277323067188263
2024-07-21 21:04:44,924 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.43942236391439, Velocity: -0.052570274043101106, Movement: 0.28065595958341594, Collision: 0, Height: -1.0, Movement Penalty: -0.06481471752606316, Smoothness: -0.0, Curiosity: 310.9385070800781, Exploration: 0.2547737377472696, Total: 290.05931012402
2024-07-21 21:04:45,095 - AirSimEnvLogger - INFO - Action: [-0.32407359 -0.32407359 -0.32407359 -0.32407359], Velocity: (-0.3240735876303158, -0.3240735876303158, -0.3240735876303158), Duration: 1.0, Reward: 290.05931012402, Done: False
2024-07-21 21:04:45,157 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:45,157 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:47,827 - AirSimEnvLogger - INFO - Predictive model loss: 0.12992216646671295
2024-07-21 21:04:53,713 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.380732334585204, Velocity: -0.025090251009313006, Movement: 0.14835143657746705, Collision: 0, Height: -1.0, Movement Penalty: -0.03426029673706732, Smoothness: -0.0, Curiosity: 310.70196533203125, Exploration: 0.04969405144816135, Total: 289.83379265693054
2024-07-21 21:04:53,714 - AirSimEnvLogger - INFO - Action: [-0.17130148 -0.17130148 -0.17130148 -0.17130148], Velocity: (-0.1713014836853366, -0.1713014836853366, -0.1713014836853366), Duration: 1.0, Reward: 289.83379265693054, Done: False
2024-07-21 21:04:53,761 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:04:53,761 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:04:56,688 - AirSimEnvLogger - INFO - Predictive model loss: 0.11796694993972778
2024-07-21 21:05:02,349 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.62033618809796, Velocity: -0.0413949930153836, Movement: 0.34595075654331986, Collision: 0, Height: -1.0, Movement Penalty: -0.07989390496665616, Smoothness: -0.0, Curiosity: 330.8976135253906, Exploration: 0.2527538842262487, Total: 309.8397573847959
2024-07-21 21:05:02,426 - AirSimEnvLogger - INFO - Action: [0.39946952 0.39946952 0.39946952 0.39946952], Velocity: (0.3994695248332808, 0.3994695248332808, 0.3994695248332808), Duration: 1.0, Reward: 309.8397573847959, Done: False
2024-07-21 21:05:02,473 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:02,473 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:05,409 - AirSimEnvLogger - INFO - Predictive model loss: 0.074726901948452
2024-07-21 21:05:11,151 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.878840520716196, Velocity: -0.04566137791834626, Movement: 0.5351003200239721, Collision: 0, Height: -1.0, Movement Penalty: -0.12357612552371809, Smoothness: -0.0, Curiosity: 347.7286071777344, Exploration: 0.37303713209128925, Total: 326.4441250416489
2024-07-21 21:05:11,246 - AirSimEnvLogger - INFO - Action: [0.61788063 0.61788063 0.61788063 0.61788063], Velocity: (0.6178806276185904, 0.6178806276185904, 0.6178806276185904), Duration: 1.0, Reward: 326.4441250416489, Done: False
2024-07-21 21:05:11,324 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:11,324 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:14,143 - AirSimEnvLogger - INFO - Predictive model loss: 0.0806984156370163
2024-07-21 21:05:19,827 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.183777162929918, Velocity: -0.074857981890697, Movement: 0.6188326311779143, Collision: 0, Height: -1.0, Movement Penalty: -0.14291327447755728, Smoothness: -0.0, Curiosity: 363.60595703125, Exploration: 0.27549106739883095, Total: 341.9932369467589
2024-07-21 21:05:19,998 - AirSimEnvLogger - INFO - Action: [0.71456637 0.71456637 0.71456637 0.71456637], Velocity: (0.7145663723877864, 0.7145663723877864, 0.7145663723877864), Duration: 1.0, Reward: 341.9932369467589, Done: False
2024-07-21 21:05:20,077 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:20,077 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:22,984 - AirSimEnvLogger - INFO - Predictive model loss: 0.07385193556547165
2024-07-21 21:05:28,624 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.567492327442448, Velocity: -0.12159480501402574, Movement: 0.7393361020568322, Collision: 0, Height: -1.0, Movement Penalty: -0.17074235901764828, Smoothness: -0.0, Curiosity: 383.5726623535156, Exploration: 0.2803550864994401, Total: 361.57562809024574
2024-07-21 21:05:28,750 - AirSimEnvLogger - INFO - Action: [0.8537118 0.8537118 0.8537118 0.8537118], Velocity: (0.8537117950882414, 0.8537117950882414, 0.8537117950882414), Duration: 1.0, Reward: 361.57562809024574, Done: False
2024-07-21 21:05:28,812 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:28,812 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:31,735 - AirSimEnvLogger - INFO - Predictive model loss: 0.06383100152015686
2024-07-21 21:05:37,393 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.75178727184389, Velocity: -0.07022851793215223, Movement: 0.4743434976941721, Collision: 0, Height: -1.0, Movement Penalty: -0.10954493843283156, Smoothness: -0.0, Curiosity: 385.4564514160156, Exploration: 0.17870525579279326, Total: 363.25037912167164
2024-07-21 21:05:37,518 - AirSimEnvLogger - INFO - Action: [0.54772469 0.54772469 0.54772469 0.54772469], Velocity: (0.5477246921641578, 0.5477246921641578, 0.5477246921641578), Duration: 1.0, Reward: 363.25037912167164, Done: False
2024-07-21 21:05:37,564 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:37,564 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:40,327 - AirSimEnvLogger - INFO - Predictive model loss: 0.029588134959340096
2024-07-21 21:05:46,004 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.563279543263103, Velocity: -0.014504288338246107, Movement: 0.18750474639610004, Collision: 0, Height: -1.0, Movement Penalty: -0.043302366322448355, Smoothness: -0.0, Curiosity: 362.56756591796875, Exploration: 0.3108263346567034, Total: 340.5789254841771
2024-07-21 21:05:46,067 - AirSimEnvLogger - INFO - Action: [-0.21651183 -0.21651183 -0.21651183 -0.21651183], Velocity: (-0.21651183161224175, -0.21651183161224175, -0.21651183161224175), Duration: 1.0, Reward: 340.5789254841771, Done: False
2024-07-21 21:05:46,145 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:46,145 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:49,001 - AirSimEnvLogger - INFO - Predictive model loss: 0.005904506891965866
2024-07-21 21:05:54,828 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.280680953293903, Velocity: -0.07065281727022758, Movement: 0.5250788843754319, Collision: 0, Height: -1.0, Movement Penalty: -0.1212617740959776, Smoothness: -0.0, Curiosity: 343.2034606933594, Exploration: 0.47664953105591806, Total: 321.53822400724584
2024-07-21 21:05:54,969 - AirSimEnvLogger - INFO - Action: [-0.60630887 -0.60630887 -0.60630887 -0.60630887], Velocity: (-0.606308870479888, -0.606308870479888, -0.606308870479888), Duration: 1.0, Reward: 321.53822400724584, Done: False
2024-07-21 21:05:55,015 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:05:55,015 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:05:57,927 - AirSimEnvLogger - INFO - Predictive model loss: 0.04372485354542732
2024-07-21 21:06:03,563 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.459684012852787, Velocity: -0.005828482778176294, Movement: 0.07900871801547757, Collision: 0, Height: -1.0, Movement Penalty: -0.018246281845825284, Smoothness: -0.0, Curiosity: 359.2880859375, Exploration: 0.03597757229860936, Total: 337.33803199759734
2024-07-21 21:06:03,674 - AirSimEnvLogger - INFO - Action: [0.09123141 0.09123141 0.09123141 0.09123141], Velocity: (0.09123140922912643, 0.09123140922912643, 0.09123140922912643), Duration: 1.0, Reward: 337.33803199759734, Done: False
2024-07-21 21:06:03,752 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:03,752 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:06,640 - AirSimEnvLogger - INFO - Predictive model loss: 0.14731042087078094
2024-07-21 21:06:12,502 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.725846935760494, Velocity: -0.05573100130813414, Movement: 0.45987452778348736, Collision: 0, Height: -1.0, Movement Penalty: -0.10620347296369939, Smoothness: -0.0, Curiosity: 380.9315185546875, Exploration: 0.42060408558798096, Total: 358.80811815588925
2024-07-21 21:06:12,564 - AirSimEnvLogger - INFO - Action: [0.53101736 0.53101736 0.53101736 0.53101736], Velocity: (0.5310173648184969, 0.5310173648184969, 0.5310173648184969), Duration: 1.0, Reward: 358.80811815588925, Done: False
2024-07-21 21:06:12,626 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:12,626 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:15,324 - AirSimEnvLogger - INFO - Predictive model loss: 0.16669608652591705
2024-07-21 21:06:21,183 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.041249077331734, Velocity: -0.10041968901139207, Movement: 0.6271299241611197, Collision: 0, Height: -1.0, Movement Penalty: -0.14482945221251683, Smoothness: -0.0, Curiosity: 400.3438720703125, Exploration: 0.36321784111477934, Total: 377.89150459081236
2024-07-21 21:06:21,278 - AirSimEnvLogger - INFO - Action: [0.72414726 0.72414726 0.72414726 0.72414726], Velocity: (0.7241472610625841, 0.7241472610625841, 0.7241472610625841), Duration: 1.0, Reward: 377.89150459081236, Done: False
2024-07-21 21:06:21,339 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:21,339 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:24,121 - AirSimEnvLogger - INFO - Predictive model loss: 0.07270047068595886
2024-07-21 21:06:29,628 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.395555879140044, Velocity: -0.12484223708324522, Movement: 0.7103017737549829, Collision: 0, Height: -1.0, Movement Penalty: -0.16403716811332325, Smoothness: -0.0, Curiosity: 419.06591796875, Exploration: 0.29273585854014017, Total: 396.2426307776349
2024-07-21 21:06:29,707 - AirSimEnvLogger - INFO - Action: [0.82018584 0.82018584 0.82018584 0.82018584], Velocity: (0.8201858405666161, 0.8201858405666161, 0.8201858405666161), Duration: 1.0, Reward: 396.2426307776349, Done: False
2024-07-21 21:06:29,801 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:29,801 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:32,472 - AirSimEnvLogger - INFO - Predictive model loss: 0.011759325861930847
2024-07-21 21:06:38,382 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.356566942897953, Velocity: -0.020309005762975806, Movement: 0.14834264897460442, Collision: 0, Height: -1.0, Movement Penalty: -0.03425826732711601, Smoothness: -0.0, Curiosity: 404.5921325683594, Exploration: 0.13255939184401075, Total: 381.76766222220215
2024-07-21 21:06:38,506 - AirSimEnvLogger - INFO - Action: [0.17129134 0.17129134 0.17129134 0.17129134], Velocity: (0.17129133663558005, 0.17129133663558005, 0.17129133663558005), Duration: 1.0, Reward: 381.76766222220215, Done: False
2024-07-21 21:06:38,585 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:38,585 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:41,352 - AirSimEnvLogger - INFO - Predictive model loss: 0.05301346629858017
2024-07-21 21:06:47,348 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.7190285576365, Velocity: -0.06558748296103972, Movement: 0.4936430621912407, Collision: 0, Height: -1.0, Movement Penalty: -0.11400198193588161, Smoothness: -0.0, Curiosity: 426.6944274902344, Exploration: 0.10906421695891456, Total: 403.5060339919412
2024-07-21 21:06:47,458 - AirSimEnvLogger - INFO - Action: [0.57000991 0.57000991 0.57000991 0.57000991], Velocity: (0.570009909679408, 0.570009909679408, 0.570009909679408), Duration: 1.0, Reward: 403.5060339919412, Done: False
2024-07-21 21:06:47,551 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:47,551 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:50,440 - AirSimEnvLogger - INFO - Predictive model loss: 0.09834179282188416
2024-07-21 21:06:56,442 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.0845300472581, Velocity: -0.11148390476844462, Movement: 0.6719535135998034, Collision: 0, Height: -1.0, Movement Penalty: -0.15518101678390456, Smoothness: -0.0, Curiosity: 448.7934265136719, Exploration: 0.3339145007352992, Total: 425.2910313946181
2024-07-21 21:06:56,583 - AirSimEnvLogger - INFO - Action: [0.77590508 0.77590508 0.77590508 0.77590508], Velocity: (0.7759050839195227, 0.7759050839195227, 0.7759050839195227), Duration: 1.0, Reward: 425.2910313946181, Done: False
2024-07-21 21:06:56,662 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:06:56,662 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:06:59,556 - AirSimEnvLogger - INFO - Predictive model loss: 0.12258651852607727
2024-07-21 21:07:05,450 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.93494686073161, Velocity: -0.004790328499869014, Movement: 0.005358009892756775, Collision: 0, Height: -1.0, Movement Penalty: -0.001237379381561521, Smoothness: -0.0, Curiosity: 427.3872985839844, Exploration: 0.12638829153125916, Total: 403.98107342914915
2024-07-21 21:07:05,496 - AirSimEnvLogger - INFO - Action: [-0.0061869 -0.0061869 -0.0061869 -0.0061869], Velocity: (-0.006186896907807604, -0.006186896907807604, -0.006186896907807604), Duration: 1.0, Reward: 403.98107342914915, Done: False
2024-07-21 21:07:05,544 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:05,544 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:07:08,416 - AirSimEnvLogger - INFO - Predictive model loss: 0.1661675125360489
2024-07-21 21:07:14,149 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.661971671902158, Velocity: -0.04653251695762588, Movement: 0.4333190590362299, Collision: 0, Height: -1.0, Movement Penalty: -0.10007075015182508, Smoothness: -0.0, Curiosity: 405.7421875, Exploration: 0.4576889313752893, Total: 382.6914603238076
2024-07-21 21:07:14,180 - AirSimEnvLogger - INFO - Action: [-0.50035375 -0.50035375 -0.50035375 -0.50035375], Velocity: (-0.5003537507591254, -0.5003537507591254, -0.5003537507591254), Duration: 1.0, Reward: 382.6914603238076, Done: False
2024-07-21 21:07:14,212 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:14,212 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:07:16,995 - AirSimEnvLogger - INFO - Predictive model loss: 0.13302966952323914
2024-07-21 21:07:22,782 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.317933866635823, Velocity: -0.11503481630533144, Movement: 0.6280268304961705, Collision: 0, Height: -1.0, Movement Penalty: -0.14503658385810858, Smoothness: -0.0, Curiosity: 385.93048095703125, Exploration: 0.4056961158206015, Total: 363.209759179744
2024-07-21 21:07:22,875 - AirSimEnvLogger - INFO - Action: [-0.72518292 -0.72518292 -0.72518292 -0.72518292], Velocity: (-0.7251829192905429, -0.7251829192905429, -0.7251829192905429), Duration: 1.0, Reward: 363.209759179744, Done: False
2024-07-21 21:07:22,969 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:22,969 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:07:25,846 - AirSimEnvLogger - INFO - Predictive model loss: 0.08792614936828613
2024-07-21 21:07:31,699 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.250765260932972, Velocity: -0.05297850715753957, Movement: 0.2924469787080725, Collision: 0, Height: -1.0, Movement Penalty: -0.06753773675231937, Smoothness: -0.0, Curiosity: 386.5906982421875, Exploration: 0.14016686592025718, Total: 363.8740472183434
2024-07-21 21:07:31,731 - AirSimEnvLogger - INFO - Action: [-0.33768868 -0.33768868 -0.33768868 -0.33768868], Velocity: (-0.33768868376159683, -0.33768868376159683, -0.33768868376159683), Duration: 1.0, Reward: 363.8740472183434, Done: False
2024-07-21 21:07:31,777 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:31,777 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:07:34,558 - AirSimEnvLogger - INFO - Predictive model loss: 0.11897580325603485
2024-07-21 21:07:40,165 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.485882872396527, Velocity: -0.027761190782286685, Movement: 0.2717530484403333, Collision: 0, Height: -1.0, Movement Penalty: -0.06275867826805114, Smoothness: -0.0, Curiosity: 408.876220703125, Exploration: 0.3268649978446553, Total: 385.96940674880216
2024-07-21 21:07:40,258 - AirSimEnvLogger - INFO - Action: [0.31379339 0.31379339 0.31379339 0.31379339], Velocity: (0.3137933913402557, 0.3137933913402557, 0.3137933913402557), Duration: 1.0, Reward: 385.96940674880216, Done: False
2024-07-21 21:07:40,289 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:40,289 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:07:43,054 - AirSimEnvLogger - INFO - Predictive model loss: 0.2625049352645874
2024-07-21 21:07:48,346 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.73380412761536, Velocity: -0.04140301241424123, Movement: 0.48284358395062293, Collision: 0, Height: -1.0, Movement Penalty: -0.11150794926815033, Smoothness: -0.0, Curiosity: 426.9033203125, Exploration: 0.4002703241844607, Total: 403.76928218532464
2024-07-21 21:07:48,471 - AirSimEnvLogger - INFO - Action: [0.55753975 0.55753975 0.55753975 0.55753975], Velocity: (0.5575397463407517, 0.5575397463407517, 0.5575397463407517), Duration: 1.0, Reward: 403.76928218532464, Done: False
2024-07-21 21:07:48,535 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:48,535 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:07:51,408 - AirSimEnvLogger - INFO - Predictive model loss: 0.23504455387592316
2024-07-21 21:07:57,320 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.06608331973374, Velocity: -0.09504588131441029, Movement: 0.6360283076935115, Collision: 0, Height: -1.0, Movement Penalty: -0.1468844458636284, Smoothness: -0.0, Curiosity: 446.9518737792969, Exploration: 0.3010121081409317, Total: 423.4611203964494
2024-07-21 21:07:57,461 - AirSimEnvLogger - INFO - Action: [0.73442223 0.73442223 0.73442223 0.73442223], Velocity: (0.7344222293181419, 0.7344222293181419, 0.7344222293181419), Duration: 1.0, Reward: 423.4611203964494, Done: False
2024-07-21 21:07:57,523 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:07:57,523 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:00,385 - AirSimEnvLogger - INFO - Predictive model loss: 0.08116088807582855
2024-07-21 21:08:06,050 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.41873280788279, Velocity: -0.10499079978141106, Movement: 0.6888159369302542, Collision: 0, Height: -1.0, Movement Penalty: -0.15907522664351464, Smoothness: -0.0, Curiosity: 465.4653625488281, Exploration: 0.27642115641615667, Total: 441.6166041447914
2024-07-21 21:08:06,128 - AirSimEnvLogger - INFO - Action: [0.79537613 0.79537613 0.79537613 0.79537613], Velocity: (0.7953761332175732, 0.7953761332175732, 0.7953761332175732), Duration: 1.0, Reward: 441.6166041447914, Done: False
2024-07-21 21:08:06,238 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:06,238 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:09,001 - AirSimEnvLogger - INFO - Predictive model loss: 0.027002841234207153
2024-07-21 21:08:14,945 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.509507972771022, Velocity: -0.03377469173562126, Movement: 0.3317932411257325, Collision: 0, Height: -1.0, Movement Penalty: -0.07662436683169604, Smoothness: -0.0, Curiosity: 461.3802185058594, Exploration: 0.13640471593224635, Total: 437.406845018921
2024-07-21 21:08:15,023 - AirSimEnvLogger - INFO - Action: [0.38312183 0.38312183 0.38312183 0.38312183], Velocity: (0.38312183415848017, 0.38312183415848017, 0.38312183415848017), Duration: 1.0, Reward: 437.406845018921, Done: False
2024-07-21 21:08:15,086 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:15,086 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:17,915 - AirSimEnvLogger - INFO - Predictive model loss: 0.13704894483089447
2024-07-21 21:08:23,853 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.902522929210466, Velocity: -0.0687069218903898, Movement: 0.5978866957407145, Collision: 0, Height: -1.0, Movement Penalty: -0.13807601789231896, Smoothness: -0.0, Curiosity: 485.05194091796875, Exploration: 0.14707896279443358, Total: 460.6910415864695
2024-07-21 21:08:23,901 - AirSimEnvLogger - INFO - Action: [0.69038009 0.69038009 0.69038009 0.69038009], Velocity: (0.6903800894615948, 0.6903800894615948, 0.6903800894615948), Duration: 1.0, Reward: 460.6910415864695, Done: False
2024-07-21 21:08:23,978 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:23,978 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:26,760 - AirSimEnvLogger - INFO - Predictive model loss: 0.21522463858127594
2024-07-21 21:08:32,336 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.284527455760152, Velocity: -0.09523122150118608, Movement: 0.7197392712844448, Collision: 0, Height: -1.0, Movement Penalty: -0.1662166648089677, Smoothness: -0.0, Curiosity: 507.3482971191406, Exploration: 0.31506373657703185, Total: 482.64436636621843
2024-07-21 21:08:32,353 - AirSimEnvLogger - INFO - Action: [0.83108332 0.83108332 0.83108332 0.83108332], Velocity: (0.8310833240448385, 0.8310833240448385, 0.8310833240448385), Duration: 1.0, Reward: 482.64436636621843, Done: False
2024-07-21 21:08:32,384 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:32,384 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:35,257 - AirSimEnvLogger - INFO - Predictive model loss: 0.26377174258232117
2024-07-21 21:08:41,112 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.108370461545004, Velocity: -0.005714706453357583, Movement: 0.000928742899849393, Collision: 0, Height: -1.0, Movement Penalty: -0.00021448398529440028, Smoothness: -0.0, Curiosity: 482.8847961425781, Exploration: 0.15975734713228432, Total: 458.3126211822415
2024-07-21 21:08:41,206 - AirSimEnvLogger - INFO - Action: [0.00107242 0.00107242 0.00107242 0.00107242], Velocity: (0.0010724199264720014, 0.0010724199264720014, 0.0010724199264720014), Duration: 1.0, Reward: 458.3126211822415, Done: False
2024-07-21 21:08:41,268 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:41,268 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:44,153 - AirSimEnvLogger - INFO - Predictive model loss: 0.25381630659103394
2024-07-21 21:08:50,113 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.452741781005606, Velocity: -0.03501648324909023, Movement: 0.3908465318895429, Collision: 0, Height: -1.0, Movement Penalty: -0.09026214015930371, Smoothness: -0.0, Curiosity: 505.4271240234375, Exploration: 0.122211688973499, Total: 480.5085767269558
2024-07-21 21:08:50,225 - AirSimEnvLogger - INFO - Action: [0.4513107 0.4513107 0.4513107 0.4513107], Velocity: (0.45131070079651847, 0.45131070079651847, 0.45131070079651847), Duration: 1.0, Reward: 480.5085767269558, Done: False
2024-07-21 21:08:50,303 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:50,303 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:08:53,198 - AirSimEnvLogger - INFO - Predictive model loss: 0.04961305484175682
2024-07-21 21:08:58,957 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.801767237578105, Velocity: -0.049407045626055066, Movement: 0.603685133857867, Collision: 0, Height: -1.0, Movement Penalty: -0.13941510981544591, Smoothness: -0.0, Curiosity: 528.7213745117188, Exploration: 0.3319862345152367, Total: 503.5058317673489
2024-07-21 21:08:59,052 - AirSimEnvLogger - INFO - Action: [0.69707555 0.69707555 0.69707555 0.69707555], Velocity: (0.6970755490772296, 0.6970755490772296, 0.6970755490772296), Duration: 1.0, Reward: 503.5058317673489, Done: False
2024-07-21 21:08:59,130 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:08:59,130 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:01,969 - AirSimEnvLogger - INFO - Predictive model loss: 0.008978849276900291
2024-07-21 21:09:07,940 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.185232194607654, Velocity: -0.11356829312586005, Movement: 0.7170991633701661, Collision: 0, Height: -1.0, Movement Penalty: -0.16560695800296832, Smoothness: -0.0, Curiosity: 551.73583984375, Exploration: 0.3110376706619232, Total: 526.1283798879523
2024-07-21 21:09:08,067 - AirSimEnvLogger - INFO - Action: [0.82803479 0.82803479 0.82803479 0.82803479], Velocity: (0.8280347900148416, 0.8280347900148416, 0.8280347900148416), Duration: 1.0, Reward: 526.1283798879523, Done: False
2024-07-21 21:09:08,147 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:09:08,147 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:10,939 - AirSimEnvLogger - INFO - Predictive model loss: 0.035214729607105255
2024-07-21 21:09:16,617 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.04492112417197, Velocity: -0.00547848987086399, Movement: 0.06011950567929066, Collision: 0, Height: -1.0, Movement Penalty: -0.013884005114994281, Smoothness: -0.0, Curiosity: 530.019287109375, Exploration: 0.14162644715111397, Total: 504.5078669472983
2024-07-21 21:09:16,727 - AirSimEnvLogger - INFO - Action: [0.06942003 0.06942003 0.06942003 0.06942003], Velocity: (0.0694200255749714, 0.0694200255749714, 0.0694200255749714), Duration: 1.0, Reward: 504.5078669472983, Done: False
2024-07-21 21:09:16,790 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:09:16,790 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:19,626 - AirSimEnvLogger - INFO - Predictive model loss: 0.057023417204618454
2024-07-21 21:09:25,532 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.432829752557993, Velocity: -0.0725393626020803, Movement: 0.46297863695116864, Collision: 0, Height: -1.0, Movement Penalty: -0.1069203362691213, Smoothness: -0.0, Curiosity: 556.1504516601562, Exploration: 0.10541024082715546, Total: 530.2459691677669
2024-07-21 21:09:25,659 - AirSimEnvLogger - INFO - Action: [0.53460168 0.53460168 0.53460168 0.53460168], Velocity: (0.5346016813456065, 0.5346016813456065, 0.5346016813456065), Duration: 1.0, Reward: 530.2459691677669, Done: False
2024-07-21 21:09:25,738 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:09:25,738 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:28,618 - AirSimEnvLogger - INFO - Predictive model loss: 0.1448991596698761
2024-07-21 21:09:34,454 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.809371022728136, Velocity: -0.08155948667379215, Movement: 0.6561264799367125, Collision: 0, Height: -1.0, Movement Penalty: -0.15152591992556103, Smoothness: -0.0, Curiosity: 581.2881469726562, Exploration: 0.349066175150075, Total: 555.0669999687736
2024-07-21 21:09:34,548 - AirSimEnvLogger - INFO - Action: [0.7576296 0.7576296 0.7576296 0.7576296], Velocity: (0.7576295996278051, 0.7576295996278051, 0.7576295996278051), Duration: 1.0, Reward: 555.0669999687736, Done: False
2024-07-21 21:09:34,611 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:09:34,611 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:37,481 - AirSimEnvLogger - INFO - Predictive model loss: 0.12166762351989746
2024-07-21 21:09:43,375 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.143248039287837, Velocity: -0.053462987834015506, Movement: 0.6582880198994134, Collision: 0, Height: -1.0, Movement Penalty: -0.15202510619729284, Smoothness: -0.0, Curiosity: 599.666748046875, Exploration: 0.27380647439112205, Total: 573.0972769676009
2024-07-21 21:09:43,483 - AirSimEnvLogger - INFO - Action: [0.76012553 0.76012553 0.76012553 0.76012553], Velocity: (0.7601255309864642, 0.7601255309864642, 0.7601255309864642), Duration: 1.0, Reward: 573.0972769676009, Done: False
2024-07-21 21:09:43,561 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:09:43,561 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:46,381 - AirSimEnvLogger - INFO - Predictive model loss: 0.025586433708667755
2024-07-21 21:09:52,418 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.344843457977078, Velocity: -0.07366269299381148, Movement: 0.471371623532289, Collision: 0, Height: -1.0, Movement Penalty: -0.1088586134938872, Smoothness: -0.0, Curiosity: 605.8035888671875, Exploration: 0.1624925964948569, Total: 579.0003151576524
2024-07-21 21:09:52,543 - AirSimEnvLogger - INFO - Action: [0.54429307 0.54429307 0.54429307 0.54429307], Velocity: (0.544293067469436, 0.544293067469436, 0.544293067469436), Duration: 1.0, Reward: 579.0003151576524, Done: False
2024-07-21 21:09:52,622 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:09:52,622 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:09:55,524 - AirSimEnvLogger - INFO - Predictive model loss: 0.023358924314379692
2024-07-21 21:10:01,280 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.195253463026237, Velocity: -0.005985456752316397, Movement: 0.03867862267920872, Collision: 0, Height: -1.0, Movement Penalty: -0.008932445286290048, Smoothness: -0.0, Curiosity: 586.2855834960938, Exploration: 0.21925626760590308, Total: 559.6411092134737
2024-07-21 21:10:01,404 - AirSimEnvLogger - INFO - Action: [-0.04466223 -0.04466223 -0.04466223 -0.04466223], Velocity: (-0.044662226431450236, -0.044662226431450236, -0.044662226431450236), Duration: 1.0, Reward: 559.6411092134737, Done: False
2024-07-21 21:10:01,529 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:01,529 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:04,422 - AirSimEnvLogger - INFO - Predictive model loss: 0.1524585336446762
2024-07-21 21:10:10,371 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.45021581544602, Velocity: -0.03492787388536247, Movement: 0.25736021798626996, Collision: 0, Height: -1.0, Movement Penalty: -0.059434796453229494, Smoothness: -0.0, Curiosity: 603.7398071289062, Exploration: 0.0986549387474952, Total: 576.815102194275
2024-07-21 21:10:10,465 - AirSimEnvLogger - INFO - Action: [0.29717398 0.29717398 0.29717398 0.29717398], Velocity: (0.29717398226614744, 0.29717398226614744, 0.29717398226614744), Duration: 1.0, Reward: 576.815102194275, Done: False
2024-07-21 21:10:10,527 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:10,527 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:13,343 - AirSimEnvLogger - INFO - Predictive model loss: 0.10531331598758698
2024-07-21 21:10:18,958 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.2976516472105, Velocity: -0.013153809093642273, Movement: 0.0800024455677789, Collision: 0, Height: -1.0, Movement Penalty: -0.018475773393754214, Smoothness: -0.0, Curiosity: 589.8062133789062, Exploration: 0.03497028664884331, Total: 563.0172519723983
2024-07-21 21:10:19,020 - AirSimEnvLogger - INFO - Action: [-0.09237887 -0.09237887 -0.09237887 -0.09237887], Velocity: (-0.09237886696877107, -0.09237886696877107, -0.09237886696877107), Duration: 1.0, Reward: 563.0172519723983, Done: False
2024-07-21 21:10:19,098 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:19,098 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:21,947 - AirSimEnvLogger - INFO - Predictive model loss: 0.11237319558858871
2024-07-21 21:10:27,929 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.9702846504235, Velocity: -0.08481814736493054, Movement: 0.44382929077181493, Collision: 0, Height: -1.0, Movement Penalty: -0.10249798420053922, Smoothness: -0.0, Curiosity: 565.77490234375, Exploration: 0.3212135226178016, Total: 539.3809020973999
2024-07-21 21:10:28,039 - AirSimEnvLogger - INFO - Action: [-0.51248992 -0.51248992 -0.51248992 -0.51248992], Velocity: (-0.5124899210026961, -0.5124899210026961, -0.5124899210026961), Duration: 1.0, Reward: 539.3809020973999, Done: False
2024-07-21 21:10:28,101 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:28,101 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:31,068 - AirSimEnvLogger - INFO - Predictive model loss: 0.10891982913017273
2024-07-21 21:10:37,194 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.615862303696314, Velocity: -0.09220493511292718, Movement: 0.5936248478441402, Collision: 0, Height: -1.0, Movement Penalty: -0.137091786280186, Smoothness: -0.0, Curiosity: 544.0858764648438, Exploration: 0.35617004017992165, Total: 518.0572743624357
2024-07-21 21:10:37,320 - AirSimEnvLogger - INFO - Action: [-0.68545893 -0.68545893 -0.68545893 -0.68545893], Velocity: (-0.68545893140093, -0.68545893140093, -0.68545893140093), Duration: 1.0, Reward: 518.0572743624357, Done: False
2024-07-21 21:10:37,368 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:37,368 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:40,080 - AirSimEnvLogger - INFO - Predictive model loss: 0.07107888907194138
2024-07-21 21:10:45,353 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.81969739340522, Velocity: -0.005506235795963006, Movement: 0.02569880519469727, Collision: 0, Height: -1.0, Movement Penalty: -0.005934884838804089, Smoothness: -0.0, Curiosity: 563.6573486328125, Exploration: 0.10901043290896566, Total: 537.3628034057045
2024-07-21 21:10:45,354 - AirSimEnvLogger - INFO - Action: [0.02967442 0.02967442 0.02967442 0.02967442], Velocity: (0.029674424194020443, 0.029674424194020443, 0.029674424194020443), Duration: 1.0, Reward: 537.3628034057045, Done: False
2024-07-21 21:10:45,430 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:45,430 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:48,224 - AirSimEnvLogger - INFO - Predictive model loss: 0.1930505633354187
2024-07-21 21:10:54,194 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.136147742880834, Velocity: -0.05679391174365736, Movement: 0.44168560015057634, Collision: 0, Height: -1.0, Movement Penalty: -0.10200292005764666, Smoothness: -0.0, Curiosity: 590.9043579101562, Exploration: 0.442520449428443, Total: 564.3751450034619
2024-07-21 21:10:54,335 - AirSimEnvLogger - INFO - Action: [0.5100146 0.5100146 0.5100146 0.5100146], Velocity: (0.5100146002882333, 0.5100146002882333, 0.5100146002882333), Duration: 1.0, Reward: 564.3751450034619, Done: False
2024-07-21 21:10:54,414 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:10:54,414 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:10:57,244 - AirSimEnvLogger - INFO - Predictive model loss: 0.29998260736465454
2024-07-21 21:11:02,967 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.500429623898842, Velocity: -0.10716035521388613, Movement: 0.6428999080433543, Collision: 0, Height: -1.0, Movement Penalty: -0.14847137398832652, Smoothness: -0.0, Curiosity: 616.8097534179688, Exploration: 0.390068432824741, Total: 589.9040937982301
2024-07-21 21:11:02,984 - AirSimEnvLogger - INFO - Action: [0.74235687 0.74235687 0.74235687 0.74235687], Velocity: (0.7423568699416325, 0.7423568699416325, 0.7423568699416325), Duration: 1.0, Reward: 589.9040937982301, Done: False
2024-07-21 21:11:03,062 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:03,062 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:05,860 - AirSimEnvLogger - INFO - Predictive model loss: 0.19304953515529633
2024-07-21 21:11:11,415 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.7608324098047, Velocity: -0.04377084444686023, Movement: 0.5675021179705843, Collision: 0, Height: -1.0, Movement Penalty: -0.13105900023039982, Smoothness: -0.0, Curiosity: 629.9347534179688, Exploration: 0.24173493578763192, Total: 602.7390637553511
2024-07-21 21:11:11,558 - AirSimEnvLogger - INFO - Action: [0.655295 0.655295 0.655295 0.655295], Velocity: (0.6552950011519991, 0.6552950011519991, 0.6552950011519991), Duration: 1.0, Reward: 602.7390637553511, Done: False
2024-07-21 21:11:11,620 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:11,620 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:14,463 - AirSimEnvLogger - INFO - Predictive model loss: 0.03666452318429947
2024-07-21 21:11:20,061 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.172841972083503, Velocity: -0.10553467607443194, Movement: 0.7110154796398654, Collision: 0, Height: -1.0, Movement Penalty: -0.1642019914272269, Smoothness: -0.0, Curiosity: 655.687255859375, Exploration: 0.22923619869400746, Total: 628.0740259168197
2024-07-21 21:11:20,201 - AirSimEnvLogger - INFO - Action: [0.82100996 0.82100996 0.82100996 0.82100996], Velocity: (0.8210099571361344, 0.8210099571361344, 0.8210099571361344), Duration: 1.0, Reward: 628.0740259168197, Done: False
2024-07-21 21:11:20,233 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:20,233 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:23,109 - AirSimEnvLogger - INFO - Predictive model loss: 0.11127037554979324
2024-07-21 21:11:29,157 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.46580195602518, Velocity: -0.10406872117369569, Movement: 0.612667670907569, Collision: 0, Height: -1.0, Movement Penalty: -0.14148953788890642, Smoothness: -0.0, Curiosity: 669.6007690429688, Exploration: 0.23349661744982397, Total: 641.6932931418223
2024-07-21 21:11:29,252 - AirSimEnvLogger - INFO - Action: [0.70744769 0.70744769 0.70744769 0.70744769], Velocity: (0.7074476894445321, 0.7074476894445321, 0.7074476894445321), Duration: 1.0, Reward: 641.6932931418223, Done: False
2024-07-21 21:11:29,298 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:29,298 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:32,105 - AirSimEnvLogger - INFO - Predictive model loss: 0.35271549224853516
2024-07-21 21:11:38,026 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.893007061230254, Velocity: -0.11655181129920188, Movement: 0.7371566987157598, Collision: 0, Height: -1.0, Movement Penalty: -0.17023904737539194, Smoothness: -0.0, Curiosity: 696.0895385742188, Exploration: 0.23324616032104017, Total: 667.7566053628649
2024-07-21 21:11:38,138 - AirSimEnvLogger - INFO - Action: [0.85119524 0.85119524 0.85119524 0.85119524], Velocity: (0.8511952368769596, 0.8511952368769596, 0.8511952368769596), Duration: 1.0, Reward: 667.7566053628649, Done: False
2024-07-21 21:11:38,185 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:38,185 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:41,067 - AirSimEnvLogger - INFO - Predictive model loss: 0.39290937781333923
2024-07-21 21:11:46,926 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.27061551098838, Velocity: -0.1172810795612013, Movement: 0.7245175634997767, Collision: 0, Height: -1.0, Movement Penalty: -0.16732016412768314, Smoothness: -0.0, Curiosity: 717.3907470703125, Exploration: 0.2696558671480478, Total: 688.6881966776382
2024-07-21 21:11:47,021 - AirSimEnvLogger - INFO - Action: [0.83660082 0.83660082 0.83660082 0.83660082], Velocity: (0.8366008206384157, 0.8366008206384157, 0.8366008206384157), Duration: 1.0, Reward: 688.6881966776382, Done: False
2024-07-21 21:11:47,036 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:47,036 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:49,880 - AirSimEnvLogger - INFO - Predictive model loss: 0.3172997832298279
2024-07-21 21:11:55,740 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.00995524536002, Velocity: -0.011088513462031678, Movement: 0.05677579348653176, Collision: 0, Height: -1.0, Movement Penalty: -0.013111807861161486, Smoothness: -0.0, Curiosity: 685.011474609375, Exploration: 0.2355421019496102, Total: 656.5559779032748
2024-07-21 21:11:55,897 - AirSimEnvLogger - INFO - Action: [-0.06555904 -0.06555904 -0.06555904 -0.06555904], Velocity: (-0.06555903930580742, -0.06555903930580742, -0.06555903930580742), Duration: 1.0, Reward: 656.5559779032748, Done: False
2024-07-21 21:11:55,943 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:11:55,943 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:11:58,856 - AirSimEnvLogger - INFO - Predictive model loss: 0.18206331133842468
2024-07-21 21:12:04,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.269376238476504, Velocity: -0.021182487198967384, Movement: 0.23512462081371296, Collision: 0, Height: -1.0, Movement Penalty: -0.05429970524796235, Smoothness: -0.0, Curiosity: 702.7523803710938, Exploration: 0.19204069584595979, Total: 674.0308228382318
2024-07-21 21:12:04,775 - AirSimEnvLogger - INFO - Action: [0.27149853 0.27149853 0.27149853 0.27149853], Velocity: (0.27149852623981174, 0.27149852623981174, 0.27149852623981174), Duration: 1.0, Reward: 674.0308228382318, Done: False
2024-07-21 21:12:04,838 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:04,838 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:07,572 - AirSimEnvLogger - INFO - Predictive model loss: 0.009855317883193493
2024-07-21 21:12:13,346 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.647178867220102, Velocity: -0.09156675005136453, Movement: 0.5442815555506616, Collision: 0, Height: -1.0, Movement Penalty: -0.1256964410448491, Smoothness: -0.0, Curiosity: 732.005615234375, Exploration: 0.29777755358595925, Total: 702.9311197267292
2024-07-21 21:12:13,424 - AirSimEnvLogger - INFO - Action: [0.62848221 0.62848221 0.62848221 0.62848221], Velocity: (0.6284822052242455, 0.6284822052242455, 0.6284822052242455), Duration: 1.0, Reward: 702.9311197267292, Done: False
2024-07-21 21:12:13,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:13,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:16,388 - AirSimEnvLogger - INFO - Predictive model loss: 0.15479932725429535
2024-07-21 21:12:22,046 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.047147241401472, Velocity: -0.10466272545392521, Movement: 0.7043340516842438, Collision: 0, Height: -1.0, Movement Penalty: -0.16265898173572718, Smoothness: -0.0, Curiosity: 760.3501586914062, Exploration: 0.34299501566828045, Total: 730.8887113074311
2024-07-21 21:12:22,169 - AirSimEnvLogger - INFO - Action: [0.81329491 0.81329491 0.81329491 0.81329491], Velocity: (0.8132949086786359, 0.8132949086786359, 0.8132949086786359), Duration: 1.0, Reward: 730.8887113074311, Done: False
2024-07-21 21:12:22,232 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:22,232 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:25,100 - AirSimEnvLogger - INFO - Predictive model loss: 0.34105318784713745
2024-07-21 21:12:30,968 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.416461350270207, Velocity: -0.09200025815846055, Movement: 0.7090260671879324, Collision: 0, Height: -1.0, Movement Penalty: -0.1637425563013658, Smoothness: -0.0, Curiosity: 782.7846069335938, Exploration: 0.27890646249924633, Total: 752.9404864151381
2024-07-21 21:12:31,109 - AirSimEnvLogger - INFO - Action: [0.81871278 0.81871278 0.81871278 0.81871278], Velocity: (0.8187127815068289, 0.8187127815068289, 0.8187127815068289), Duration: 1.0, Reward: 752.9404864151381, Done: False
2024-07-21 21:12:31,187 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:31,187 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:33,999 - AirSimEnvLogger - INFO - Predictive model loss: 0.19798117876052856
2024-07-21 21:12:39,293 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.842742523640943, Velocity: -0.043858230404082946, Movement: 0.7807887154973195, Collision: 0, Height: -1.0, Movement Penalty: -0.18031543002903982, Smoothness: -0.0, Curiosity: 810.0349731445312, Exploration: 0.26574761840269734, Total: 779.7681194552376
2024-07-21 21:12:39,402 - AirSimEnvLogger - INFO - Action: [0.90157715 0.90157715 0.90157715 0.90157715], Velocity: (0.9015771501451991, 0.9015771501451991, 0.9015771501451991), Duration: 1.0, Reward: 779.7681194552376, Done: False
2024-07-21 21:12:39,482 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:39,482 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:42,320 - AirSimEnvLogger - INFO - Predictive model loss: 0.026241328567266464
2024-07-21 21:12:47,976 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.61278752925759, Velocity: -0.007257708030821692, Movement: 0.0198401658536667, Collision: 0, Height: -1.0, Movement Penalty: -0.004581890038552516, Smoothness: -0.0, Curiosity: 778.8414306640625, Exploration: 0.19938891351205176, Total: 748.7742634923098
2024-07-21 21:12:48,070 - AirSimEnvLogger - INFO - Action: [0.02290945 0.02290945 0.02290945 0.02290945], Velocity: (0.02290945019276258, 0.02290945019276258, 0.02290945019276258), Duration: 1.0, Reward: 748.7742634923098, Done: False
2024-07-21 21:12:48,133 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:48,133 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:50,937 - AirSimEnvLogger - INFO - Predictive model loss: 0.09519659727811813
2024-07-21 21:12:56,822 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.614648601265074, Velocity: -0.003307759048766235, Movement: 0.03099489974842427, Collision: 0, Height: -1.0, Movement Penalty: -0.007157965485303286, Smoothness: -0.0, Curiosity: 775.6106567382812, Exploration: 0.3117107017153612, Total: 745.5681311257279
2024-07-21 21:12:56,932 - AirSimEnvLogger - INFO - Action: [-0.03578983 -0.03578983 -0.03578983 -0.03578983], Velocity: (-0.03578982742651643, -0.03578982742651643, -0.03578982742651643), Duration: 1.0, Reward: 745.5681311257279, Done: False
2024-07-21 21:12:56,996 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:12:56,996 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:12:59,891 - AirSimEnvLogger - INFO - Predictive model loss: 0.18190070986747742
2024-07-21 21:13:05,750 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.71519948072071, Velocity: -0.009440831116946009, Movement: 0.0891671696776506, Collision: 0, Height: -1.0, Movement Penalty: -0.020592275766507448, Smoothness: -0.0, Curiosity: 783.2123413085938, Exploration: 0.03342863953290555, Total: 753.0060735977256
2024-07-21 21:13:05,827 - AirSimEnvLogger - INFO - Action: [0.10296138 0.10296138 0.10296138 0.10296138], Velocity: (0.10296137883253723, 0.10296137883253723, 0.10296137883253723), Duration: 1.0, Reward: 753.0060735977256, Done: False
2024-07-21 21:13:05,891 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:05,891 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:13:08,700 - AirSimEnvLogger - INFO - Predictive model loss: 0.13322408497333527
2024-07-21 21:13:14,689 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.097721598775394, Velocity: -0.047549101326874246, Movement: 0.47459711165869156, Collision: 0, Height: -1.0, Movement Penalty: -0.10960350806910578, Smoothness: -0.0, Curiosity: 814.8211059570312, Exploration: 0.24666911287439408, Total: 784.2870051856819
2024-07-21 21:13:14,815 - AirSimEnvLogger - INFO - Action: [0.54801754 0.54801754 0.54801754 0.54801754], Velocity: (0.5480175403455289, 0.5480175403455289, 0.5480175403455289), Duration: 1.0, Reward: 784.2870051856819, Done: False
2024-07-21 21:13:14,894 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:14,894 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:13:17,658 - AirSimEnvLogger - INFO - Predictive model loss: 0.03798457235097885
2024-07-21 21:13:23,495 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.76107286726761, Velocity: -0.02825056189425946, Movement: 0.18344484511329298, Collision: 0, Height: -1.0, Movement Penalty: -0.04236477228304356, Smoothness: -0.0, Curiosity: 782.5686645507812, Exploration: 0.05888591631349731, Total: 752.3228102802507
2024-07-21 21:13:23,496 - AirSimEnvLogger - INFO - Action: [-0.21182386 -0.21182386 -0.21182386 -0.21182386], Velocity: (-0.2118238614152178, -0.2118238614152178, -0.2118238614152178), Duration: 1.0, Reward: 752.3228102802507, Done: False
2024-07-21 21:13:23,557 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:23,557 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:13:26,394 - AirSimEnvLogger - INFO - Predictive model loss: 0.07722435146570206
2024-07-21 21:13:32,256 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.608669873863192, Velocity: -0.02901261710560196, Movement: 0.26844924729579955, Collision: 0, Height: -1.0, Movement Penalty: -0.061995698075992915, Smoothness: -0.0, Curiosity: 770.2222900390625, Exploration: 0.32440490768022406, Total: 740.1919170778386
2024-07-21 21:13:32,397 - AirSimEnvLogger - INFO - Action: [-0.30997849 -0.30997849 -0.30997849 -0.30997849], Velocity: (-0.30997849037996456, -0.30997849037996456, -0.30997849037996456), Duration: 1.0, Reward: 740.1919170778386, Done: False
2024-07-21 21:13:32,428 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:32,428 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:13:35,205 - AirSimEnvLogger - INFO - Predictive model loss: 0.035958338528871536
2024-07-21 21:13:41,047 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.381356062342856, Velocity: -0.06664662518380053, Movement: 0.3607093150248269, Collision: 0, Height: -1.0, Movement Penalty: -0.08330224805151575, Smoothness: -0.0, Curiosity: 754.6592407226562, Exploration: 0.17733657151270318, Total: 724.8208555893207
2024-07-21 21:13:41,189 - AirSimEnvLogger - INFO - Action: [-0.41651124 -0.41651124 -0.41651124 -0.41651124], Velocity: (-0.4165112402575787, -0.4165112402575787, -0.4165112402575787), Duration: 1.0, Reward: 724.8208555893207, Done: False
2024-07-21 21:13:41,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:41,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:13:43,905 - AirSimEnvLogger - INFO - Predictive model loss: 0.06290657073259354
2024-07-21 21:13:49,788 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.966854702700402, Velocity: -0.061818773274451996, Movement: 0.5954825932557726, Collision: 0, Height: -1.0, Movement Penalty: -0.13752081420558268, Smoothness: -0.0, Curiosity: 726.8709106445312, Exploration: 0.2721705668590506, Total: 697.4750804508341
2024-07-21 21:13:49,914 - AirSimEnvLogger - INFO - Action: [-0.68760407 -0.68760407 -0.68760407 -0.68760407], Velocity: (-0.6876040710279134, -0.6876040710279134, -0.6876040710279134), Duration: 1.0, Reward: 697.4750804508341, Done: False
2024-07-21 21:13:49,945 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:49,945 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:13:52,865 - AirSimEnvLogger - INFO - Predictive model loss: 0.12465083599090576
2024-07-21 21:13:58,602 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.930462719035567, Velocity: -0.04879243126049131, Movement: 0.28040653560525863, Collision: 0, Height: -1.0, Movement Penalty: -0.06475711552569059, Smoothness: -0.0, Curiosity: 728.2376098632812, Exploration: 0.14418313174190106, Total: 698.8423083786422
2024-07-21 21:13:58,664 - AirSimEnvLogger - INFO - Action: [-0.32378558 -0.32378558 -0.32378558 -0.32378558], Velocity: (-0.32378557762845295, -0.32378557762845295, -0.32378557762845295), Duration: 1.0, Reward: 698.8423083786422, Done: False
2024-07-21 21:13:58,759 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:13:58,759 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:01,592 - AirSimEnvLogger - INFO - Predictive model loss: 0.21172496676445007
2024-07-21 21:14:07,484 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.55676986358867, Velocity: -0.07441308958906243, Movement: 0.49385438628784206, Collision: 0, Height: -1.0, Movement Penalty: -0.11405078514550522, Smoothness: -0.0, Curiosity: 704.6588134765625, Exploration: 0.12443753189523951, Total: 675.6353371578583
2024-07-21 21:14:07,577 - AirSimEnvLogger - INFO - Action: [-0.57025393 -0.57025393 -0.57025393 -0.57025393], Velocity: (-0.5702539257275261, -0.5702539257275261, -0.5702539257275261), Duration: 1.0, Reward: 675.6353371578583, Done: False
2024-07-21 21:14:07,609 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:14:07,609 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:10,586 - AirSimEnvLogger - INFO - Predictive model loss: 0.11113141477108002
2024-07-21 21:14:16,454 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.849522501329286, Velocity: -0.017570524176532397, Movement: 0.13554128908896795, Collision: 0, Height: -1.0, Movement Penalty: -0.03130191989672981, Smoothness: -0.0, Curiosity: 731.6945190429688, Exploration: 0.1315034222556157, Total: 702.3768100968647
2024-07-21 21:14:16,549 - AirSimEnvLogger - INFO - Action: [0.1565096 0.1565096 0.1565096 0.1565096], Velocity: (0.15650959948364906, 0.15650959948364906, 0.15650959948364906), Duration: 1.0, Reward: 702.3768100968647, Done: False
2024-07-21 21:14:16,612 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:14:16,612 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:19,481 - AirSimEnvLogger - INFO - Predictive model loss: 0.15289439260959625
2024-07-21 21:14:25,246 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.46881940674869, Velocity: -0.034391743072327584, Movement: 0.3425620024603822, Collision: 0, Height: -1.0, Movement Penalty: -0.07911130573385557, Smoothness: -0.0, Curiosity: 703.3980102539062, Exploration: 0.06698553548303281, Total: 674.449561373489
2024-07-21 21:14:25,293 - AirSimEnvLogger - INFO - Action: [-0.39555653 -0.39555653 -0.39555653 -0.39555653], Velocity: (-0.3955565286692778, -0.3955565286692778, -0.3955565286692778), Duration: 1.0, Reward: 674.449561373489, Done: False
2024-07-21 21:14:25,388 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:14:25,388 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:28,181 - AirSimEnvLogger - INFO - Predictive model loss: 0.01474734116345644
2024-07-21 21:14:34,201 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.827269061192922, Velocity: -0.04452585677740605, Movement: 0.25306639620786286, Collision: 0, Height: -1.0, Movement Penalty: -0.058443180789383244, Smoothness: -0.0, Curiosity: 733.5451049804688, Exploration: 0.04669555581768791, Total: 704.2303310165106
2024-07-21 21:14:34,326 - AirSimEnvLogger - INFO - Action: [0.2922159 0.2922159 0.2922159 0.2922159], Velocity: (0.2922159039469162, 0.2922159039469162, 0.2922159039469162), Duration: 1.0, Reward: 704.2303310165106, Done: False
2024-07-21 21:14:34,374 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:14:34,374 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:37,266 - AirSimEnvLogger - INFO - Predictive model loss: 0.07234548032283783
2024-07-21 21:14:42,983 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.749793468909107, Velocity: -0.004659418642567009, Movement: 0.037265508795329576, Collision: 0, Height: -1.0, Movement Penalty: -0.00860610061378876, Smoothness: -0.0, Curiosity: 726.5825805664062, Exploration: 0.1796558560869865, Total: 697.3745561234574
2024-07-21 21:14:43,061 - AirSimEnvLogger - INFO - Action: [0.0430305 0.0430305 0.0430305 0.0430305], Velocity: (0.04303050306894379, 0.04303050306894379, 0.04303050306894379), Duration: 1.0, Reward: 697.3745561234574, Done: False
2024-07-21 21:14:43,139 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:14:43,139 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:45,768 - AirSimEnvLogger - INFO - Predictive model loss: 0.17081059515476227
2024-07-21 21:14:51,528 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.5107609433216, Velocity: -0.047361563723807336, Movement: 0.26362166507073703, Collision: 0, Height: -1.0, Movement Penalty: -0.06088081571712295, Smoothness: -0.0, Curiosity: 707.1268310546875, Exploration: 0.19948152634279162, Total: 678.1636813313037
2024-07-21 21:14:51,624 - AirSimEnvLogger - INFO - Action: [-0.30440408 -0.30440408 -0.30440408 -0.30440408], Velocity: (-0.30440407858561475, -0.30440407858561475, -0.30440407858561475), Duration: 1.0, Reward: 678.1636813313037, Done: False
2024-07-21 21:14:51,686 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:14:51,686 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:14:54,541 - AirSimEnvLogger - INFO - Predictive model loss: 0.2697703242301941
2024-07-21 21:15:00,517 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.248123793526208, Velocity: -0.047987969966959296, Movement: 0.41062524950520307, Collision: 0, Height: -1.0, Movement Penalty: -0.0948298393351545, Smoothness: -0.0, Curiosity: 688.8862915039062, Exploration: 0.26710420287709274, Total: 660.2048754953082
2024-07-21 21:15:00,642 - AirSimEnvLogger - INFO - Action: [-0.4741492 -0.4741492 -0.4741492 -0.4741492], Velocity: (-0.4741491966757725, -0.4741491966757725, -0.4741491966757725), Duration: 1.0, Reward: 660.2048754953082, Done: False
2024-07-21 21:15:00,704 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:00,704 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:03,447 - AirSimEnvLogger - INFO - Predictive model loss: 0.19644172489643097
2024-07-21 21:15:08,991 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.582893799657974, Velocity: -0.026297990331474234, Movement: 0.21373005468825831, Collision: 0, Height: -1.0, Movement Penalty: -0.04935884184327175, Smoothness: -0.0, Curiosity: 718.3570556640625, Exploration: 0.13214216561190617, Total: 689.3071675501968
2024-07-21 21:15:09,087 - AirSimEnvLogger - INFO - Action: [0.24679421 0.24679421 0.24679421 0.24679421], Velocity: (0.24679420921635875, 0.24679420921635875, 0.24679420921635875), Duration: 1.0, Reward: 689.3071675501968, Done: False
2024-07-21 21:15:09,103 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:09,103 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:11,851 - AirSimEnvLogger - INFO - Predictive model loss: 0.03841570392251015
2024-07-21 21:15:17,861 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.774646996818777, Velocity: -0.042431673524534586, Movement: 0.3420658938343006, Collision: 0, Height: -1.0, Movement Penalty: -0.07899673435432936, Smoothness: -0.0, Curiosity: 734.197998046875, Exploration: 0.35030071660738826, Total: 705.0080679054831
2024-07-21 21:15:17,985 - AirSimEnvLogger - INFO - Action: [0.39498367 0.39498367 0.39498367 0.39498367], Velocity: (0.3949836717716468, 0.3949836717716468, 0.3949836717716468), Duration: 1.0, Reward: 705.0080679054831, Done: False
2024-07-21 21:15:18,001 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:18,001 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:20,669 - AirSimEnvLogger - INFO - Predictive model loss: 0.059388190507888794
2024-07-21 21:15:26,463 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.106547248099428, Velocity: -0.06683455291877592, Movement: 0.5331804462321389, Collision: 0, Height: -1.0, Movement Penalty: -0.12313274966350807, Smoothness: -0.0, Curiosity: 758.6188354492188, Exploration: 0.25071066061829395, Total: 729.0763470854104
2024-07-21 21:15:26,511 - AirSimEnvLogger - INFO - Action: [0.61566375 0.61566375 0.61566375 0.61566375], Velocity: (0.6156637483175403, 0.6156637483175403, 0.6156637483175403), Duration: 1.0, Reward: 729.0763470854104, Done: False
2024-07-21 21:15:26,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:26,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:29,354 - AirSimEnvLogger - INFO - Predictive model loss: 0.10135408490896225
2024-07-21 21:15:35,218 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.48824692872776, Velocity: -0.06926358672180656, Movement: 0.6682053012910043, Collision: 0, Height: -1.0, Movement Penalty: -0.15431540422971854, Smoothness: -0.0, Curiosity: 785.2197265625, Exploration: 0.2905159210480584, Total: 755.307762977007
2024-07-21 21:15:35,328 - AirSimEnvLogger - INFO - Action: [0.77157702 0.77157702 0.77157702 0.77157702], Velocity: (0.7715770211485926, 0.7715770211485926, 0.7715770211485926), Duration: 1.0, Reward: 755.307762977007, Done: False
2024-07-21 21:15:35,405 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:35,405 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:38,307 - AirSimEnvLogger - INFO - Predictive model loss: 0.07095547020435333
2024-07-21 21:15:44,235 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -29.9205684051597, Velocity: -0.07456478745248352, Movement: 0.7663375295784817, Collision: 0, Height: -1.0, Movement Penalty: -0.17697807159689968, Smoothness: -0.0, Curiosity: 813.999755859375, Exploration: 0.29872012451044067, Total: 783.65923482245
2024-07-21 21:15:44,360 - AirSimEnvLogger - INFO - Action: [0.88489036 0.88489036 0.88489036 0.88489036], Velocity: (0.8848903579844983, 0.8848903579844983, 0.8848903579844983), Duration: 1.0, Reward: 783.65923482245, Done: False
2024-07-21 21:15:44,392 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:44,392 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:47,050 - AirSimEnvLogger - INFO - Predictive model loss: 0.04083523154258728
2024-07-21 21:15:52,760 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.35411273010157, Velocity: -0.09106054598647909, Movement: 0.8115796722763587, Collision: 0, Height: -1.0, Movement Penalty: -0.18742629690303358, Smoothness: -0.0, Curiosity: 841.8525390625, Exploration: 0.2962361975614081, Total: 811.0773626089061
2024-07-21 21:15:52,808 - AirSimEnvLogger - INFO - Action: [0.93713148 0.93713148 0.93713148 0.93713148], Velocity: (0.9371314845151679, 0.9371314845151679, 0.9371314845151679), Duration: 1.0, Reward: 811.0773626089061, Done: False
2024-07-21 21:15:52,901 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:15:52,918 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:15:55,922 - AirSimEnvLogger - INFO - Predictive model loss: 0.023780060932040215
2024-07-21 21:16:02,790 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.44824043632755, Velocity: -0.05643284253786234, Movement: 0.41851882082324937, Collision: 0, Height: -1.0, Movement Penalty: -0.09665278154529111, Smoothness: -0.0, Curiosity: 837.7903442382812, Exploration: 0.1648338182707997, Total: 806.8846385400449
2024-07-21 21:16:02,916 - AirSimEnvLogger - INFO - Action: [0.48326391 0.48326391 0.48326391 0.48326391], Velocity: (0.48326390772645555, 0.48326390772645555, 0.48326390772645555), Duration: 1.0, Reward: 806.8846385400449, Done: False
2024-07-21 21:16:02,995 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:02,995 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:05,821 - AirSimEnvLogger - INFO - Predictive model loss: 0.03791574761271477
2024-07-21 21:16:11,643 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.17760173257972, Velocity: -0.030121740144285522, Movement: 0.19019535527917789, Collision: 0, Height: -1.0, Movement Penalty: -0.04392373582761995, Smoothness: -0.0, Curiosity: 807.6461791992188, Exploration: 0.3469896627684043, Total: 777.0500383968866
2024-07-21 21:16:11,752 - AirSimEnvLogger - INFO - Action: [-0.21961868 -0.21961868 -0.21961868 -0.21961868], Velocity: (-0.21961867913809974, -0.21961867913809974, -0.21961867913809974), Duration: 1.0, Reward: 777.0500383968866, Done: False
2024-07-21 21:16:11,815 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:11,816 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:14,651 - AirSimEnvLogger - INFO - Predictive model loss: 0.18096069991588593
2024-07-21 21:16:20,375 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.534214488077488, Velocity: -0.05002820433368351, Movement: 0.28860037242613623, Collision: 0, Height: -1.0, Movement Penalty: -0.0666494010833824, Smoothness: -0.0, Curiosity: 836.3680419921875, Exploration: 0.08995941249494312, Total: 805.3565947001485
2024-07-21 21:16:20,376 - AirSimEnvLogger - INFO - Action: [0.33324701 0.33324701 0.33324701 0.33324701], Velocity: (0.333247005416912, 0.333247005416912, 0.333247005416912), Duration: 1.0, Reward: 805.3565947001485, Done: False
2024-07-21 21:16:20,391 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:20,391 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:23,287 - AirSimEnvLogger - INFO - Predictive model loss: 0.04452279955148697
2024-07-21 21:16:29,150 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.177664865261008, Velocity: -0.050437080260207705, Movement: 0.26680935460657973, Collision: 0, Height: -1.0, Movement Penalty: -0.061616981081767656, Smoothness: -0.0, Curiosity: 805.4319458007812, Exploration: 0.0380990497706763, Total: 774.7645448280455
2024-07-21 21:16:29,260 - AirSimEnvLogger - INFO - Action: [-0.30808491 -0.30808491 -0.30808491 -0.30808491], Velocity: (-0.30808490540883826, -0.30808490540883826, -0.30808490540883826), Duration: 1.0, Reward: 774.7645448280455, Done: False
2024-07-21 21:16:29,321 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:29,321 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:32,196 - AirSimEnvLogger - INFO - Predictive model loss: 0.10352765023708344
2024-07-21 21:16:38,086 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.014022656164236, Velocity: -0.0389566017363857, Movement: 0.3066221234243403, Collision: 0, Height: -1.0, Movement Penalty: -0.07081134619941502, Smoothness: -0.0, Curiosity: 793.3411865234375, Exploration: 0.29020268067585947, Total: 762.8975362478348
2024-07-21 21:16:38,103 - AirSimEnvLogger - INFO - Action: [-0.35405673 -0.35405673 -0.35405673 -0.35405673], Velocity: (-0.3540567309970751, -0.3540567309970751, -0.3540567309970751), Duration: 1.0, Reward: 762.8975362478348, Done: False
2024-07-21 21:16:38,165 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:38,165 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:41,088 - AirSimEnvLogger - INFO - Predictive model loss: 0.1212247908115387
2024-07-21 21:16:46,972 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.380218896856444, Velocity: -0.05051733274066778, Movement: 0.27788382066698614, Collision: 0, Height: -1.0, Movement Penalty: -0.06417451946621046, Smoothness: -0.0, Curiosity: 825.8895874023438, Exploration: 0.17164118268126277, Total: 795.0506107193902
2024-07-21 21:16:47,081 - AirSimEnvLogger - INFO - Action: [0.3208726 0.3208726 0.3208726 0.3208726], Velocity: (0.3208725973310523, 0.3208725973310523, 0.3208725973310523), Duration: 1.0, Reward: 795.0506107193902, Done: False
2024-07-21 21:16:47,143 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:47,143 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:50,034 - AirSimEnvLogger - INFO - Predictive model loss: 0.22444798052310944
2024-07-21 21:16:56,023 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.551971745789324, Velocity: -0.039009607465292515, Movement: 0.3312939509473393, Collision: 0, Height: -1.0, Movement Penalty: -0.07650906070413642, Smoothness: -0.0, Curiosity: 839.4697875976562, Exploration: 0.3187697968486699, Total: 808.4953585671761
2024-07-21 21:16:56,024 - AirSimEnvLogger - INFO - Action: [0.3825453 0.3825453 0.3825453 0.3825453], Velocity: (0.38254530352068206, 0.38254530352068206, 0.38254530352068206), Duration: 1.0, Reward: 808.4953585671761, Done: False
2024-07-21 21:16:56,055 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:16:56,055 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:16:58,882 - AirSimEnvLogger - INFO - Predictive model loss: 0.15698736906051636
2024-07-21 21:17:04,933 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -30.772669269093747, Velocity: -0.04630387464485934, Movement: 0.38532114121127076, Collision: 0, Height: -1.0, Movement Penalty: -0.0889861058411124, Smoothness: -0.0, Curiosity: 854.737060546875, Exploration: 0.16006157052531822, Total: 823.5060269583867
2024-07-21 21:17:04,963 - AirSimEnvLogger - INFO - Action: [0.44493053 0.44493053 0.44493053 0.44493053], Velocity: (0.444930529205562, 0.444930529205562, 0.444930529205562), Duration: 1.0, Reward: 823.5060269583867, Done: False
2024-07-21 21:17:05,026 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:05,026 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:17:07,825 - AirSimEnvLogger - INFO - Predictive model loss: 0.0405985489487648
2024-07-21 21:17:13,839 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -31.161541173485983, Velocity: -0.07446243741547545, Movement: 0.5956863739731603, Collision: 0, Height: -1.0, Movement Penalty: -0.13756787534639847, Smoothness: -0.0, Curiosity: 884.1487426757812, Exploration: 0.23727603976292136, Total: 852.5489409023822
2024-07-21 21:17:13,886 - AirSimEnvLogger - INFO - Action: [0.68783938 0.68783938 0.68783938 0.68783938], Velocity: (0.6878393767319924, 0.6878393767319924, 0.6878393767319924), Duration: 1.0, Reward: 852.5489409023822, Done: False
2024-07-21 21:17:13,964 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:13,964 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:17:16,774 - AirSimEnvLogger - INFO - Predictive model loss: 0.006510341539978981
2024-07-21 21:17:22,725 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -31.5702210763809, Velocity: -0.0925926756966404, Movement: 0.7143993984289595, Collision: 0, Height: -1.0, Movement Penalty: -0.16498347399674662, Smoothness: -0.0, Curiosity: 913.61962890625, Exploration: 0.3055581016252717, Total: 881.627951104455
2024-07-21 21:17:22,881 - AirSimEnvLogger - INFO - Action: [0.82491737 0.82491737 0.82491737 0.82491737], Velocity: (0.824917369983733, 0.824917369983733, 0.824917369983733), Duration: 1.0, Reward: 881.627951104455, Done: False
2024-07-21 21:17:22,989 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:22,989 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:17:26,007 - AirSimEnvLogger - INFO - Predictive model loss: 0.0757531225681305
2024-07-21 21:17:31,930 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -31.936772001466736, Velocity: -0.1016946290928422, Movement: 0.6923919177044545, Collision: 0, Height: -1.0, Movement Penalty: -0.1599010640285552, Smoothness: -0.0, Curiosity: 936.7662963867188, Exploration: 0.2619331110176617, Total: 904.3965838743412
2024-07-21 21:17:32,040 - AirSimEnvLogger - INFO - Action: [0.79950532 0.79950532 0.79950532 0.79950532], Velocity: (0.799505320142776, 0.799505320142776, 0.799505320142776), Duration: 1.0, Reward: 904.3965838743412, Done: False
2024-07-21 21:17:32,102 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:32,102 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:17:34,832 - AirSimEnvLogger - INFO - Predictive model loss: 0.22298692166805267
2024-07-21 21:17:40,735 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -32.3208566398779, Velocity: -0.09187380496328423, Movement: 0.7075036664345369, Collision: 0, Height: -1.0, Movement Penalty: -0.16339097290745083, Smoothness: -0.0, Curiosity: 961.8814086914062, Exploration: 0.23717791944899308, Total: 929.1232706193524
2024-07-21 21:17:40,829 - AirSimEnvLogger - INFO - Action: [0.81695486 0.81695486 0.81695486 0.81695486], Velocity: (0.8169548645372541, 0.8169548645372541, 0.8169548645372541), Duration: 1.0, Reward: 929.1232706193524, Done: False
2024-07-21 21:17:40,860 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:40,860 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:17:43,861 - AirSimEnvLogger - INFO - Predictive model loss: 0.24000078439712524
2024-07-21 21:17:49,798 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -32.710335047298535, Velocity: -0.12483996370185227, Movement: 0.7145418266694592, Collision: 0, Height: -1.0, Movement Penalty: -0.16501636638994369, Smoothness: -0.0, Curiosity: 987.4305419921875, Exploration: 0.24444997353044223, Total: 954.2814741149696
2024-07-21 21:17:49,861 - AirSimEnvLogger - INFO - Action: [0.82508183 0.82508183 0.82508183 0.82508183], Velocity: (0.8250818319497184, 0.8250818319497184, 0.8250818319497184), Duration: 1.0, Reward: 954.2814741149696, Done: False
2024-07-21 21:17:49,922 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:49,922 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:17:52,841 - AirSimEnvLogger - INFO - Predictive model loss: 0.1057039350271225
2024-07-21 21:17:58,519 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -33.150993178183946, Velocity: -0.04248080193932361, Movement: 0.7833936993555951, Collision: 0, Height: -1.0, Movement Penalty: -0.18091702528176387, Smoothness: -0.0, Curiosity: 1018.2266235351562, Exploration: 0.2675418080225348, Total: 984.6521334978925
2024-07-21 21:17:58,596 - AirSimEnvLogger - INFO - Action: [0.90458513 0.90458513 0.90458513 0.90458513], Velocity: (0.9045851264088192, 0.9045851264088192, 0.9045851264088192), Duration: 1.0, Reward: 984.6521334978925, Done: False
2024-07-21 21:17:58,659 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:17:58,659 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:01,505 - AirSimEnvLogger - INFO - Predictive model loss: 0.027445849031209946
2024-07-21 21:18:07,530 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -33.6013140766822, Velocity: -0.10824173171918396, Movement: 0.8232063248847129, Collision: 0, Height: -1.0, Movement Penalty: -0.19011135730831663, Smoothness: -0.0, Curiosity: 1049.7115478515625, Exploration: 0.29010264883084214, Total: 1015.6863264177691
2024-07-21 21:18:07,686 - AirSimEnvLogger - INFO - Action: [0.95055679 0.95055679 0.95055679 0.95055679], Velocity: (0.950556786541583, 0.950556786541583, 0.950556786541583), Duration: 1.0, Reward: 1015.6863264177691, Done: False
2024-07-21 21:18:07,749 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:18:07,749 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:10,571 - AirSimEnvLogger - INFO - Predictive model loss: 0.0267939493060112
2024-07-21 21:18:16,512 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -33.990771913904084, Velocity: -0.09853580678362926, Movement: 0.7620548494149353, Collision: 0, Height: -1.0, Movement Penalty: -0.17598902897878907, Smoothness: -0.0, Curiosity: 1074.4844970703125, Exploration: 0.2625987515049032, Total: 1040.062962453005
2024-07-21 21:18:16,655 - AirSimEnvLogger - INFO - Action: [0.87994514 0.87994514 0.87994514 0.87994514], Velocity: (0.8799451448939453, 0.8799451448939453, 0.8799451448939453), Duration: 1.0, Reward: 1040.062962453005, Done: False
2024-07-21 21:18:16,719 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:18:16,719 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:19,562 - AirSimEnvLogger - INFO - Predictive model loss: 0.03821079060435295
2024-07-21 21:18:25,346 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.031984585940144, Velocity: -0.05873512259847368, Movement: 0.344927020196796, Collision: 0, Height: -1.0, Movement Penalty: -0.07965748318455826, Smoothness: -0.0, Curiosity: 1066.71533203125, Exploration: 0.16824175070526187, Total: 1032.2246305769477
2024-07-21 21:18:25,347 - AirSimEnvLogger - INFO - Action: [0.39828742 0.39828742 0.39828742 0.39828742], Velocity: (0.39828741592279127, 0.39828741592279127, 0.39828741592279127), Duration: 1.0, Reward: 1032.2246305769477, Done: False
2024-07-21 21:18:25,393 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:18:25,393 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:28,230 - AirSimEnvLogger - INFO - Predictive model loss: 0.02608051896095276
2024-07-21 21:18:34,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.38081979945151, Velocity: -0.057619622848798034, Movement: 0.49934894484717807, Collision: 0, Height: -1.0, Movement Penalty: -0.11531969909082954, Smoothness: -0.0, Curiosity: 1092.6317138671875, Exploration: 0.13677961412077988, Total: 1057.788840419432
2024-07-21 21:18:34,236 - AirSimEnvLogger - INFO - Action: [0.5765985 0.5765985 0.5765985 0.5765985], Velocity: (0.5765984954541477, 0.5765984954541477, 0.5765984954541477), Duration: 1.0, Reward: 1057.788840419432, Done: False
2024-07-21 21:18:34,299 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:18:34,299 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:37,238 - AirSimEnvLogger - INFO - Predictive model loss: 0.07425453513860703
2024-07-21 21:18:43,170 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.69389755260749, Velocity: -0.08078048362004538, Movement: 0.5426507285562364, Collision: 0, Height: -1.0, Movement Penalty: -0.12531981768315587, Smoothness: -0.0, Curiosity: 1115.6546630859375, Exploration: 0.22168118611714016, Total: 1080.5169853509312
2024-07-21 21:18:43,312 - AirSimEnvLogger - INFO - Action: [0.62659909 0.62659909 0.62659909 0.62659909], Velocity: (0.6265990884157793, 0.6265990884157793, 0.6265990884157793), Duration: 1.0, Reward: 1080.5169853509312, Done: False
2024-07-21 21:18:43,327 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:18:43,327 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:46,169 - AirSimEnvLogger - INFO - Predictive model loss: 0.01635649800300598
2024-07-21 21:18:51,835 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.98512694794616, Velocity: -0.08009458384981842, Movement: 0.540019952763827, Collision: 0, Height: -1.0, Movement Penalty: -0.1247122660383858, Smoothness: -0.0, Curiosity: 1136.0068359375, Exploration: 0.19455721065662004, Total: 1100.5716943499876
2024-07-21 21:18:51,944 - AirSimEnvLogger - INFO - Action: [0.62356133 0.62356133 0.62356133 0.62356133], Velocity: (0.6235613301919289, 0.6235613301919289, 0.6235613301919289), Duration: 1.0, Reward: 1100.5716943499876, Done: False
2024-07-21 21:18:52,021 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:18:52,021 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:18:54,961 - AirSimEnvLogger - INFO - Predictive model loss: 0.0064513604156672955
2024-07-21 21:19:00,944 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.643603714688446, Velocity: -0.01950801221418988, Movement: 0.15341844556798, Collision: 0, Height: -1.0, Movement Penalty: -0.03543047233893089, Smoothness: -0.0, Curiosity: 1097.6302490234375, Exploration: 0.2169193412680024, Total: 1062.5383493022293
2024-07-21 21:19:01,039 - AirSimEnvLogger - INFO - Action: [-0.17715236 -0.17715236 -0.17715236 -0.17715236], Velocity: (-0.17715236169465443, -0.17715236169465443, -0.17715236169465443), Duration: 1.0, Reward: 1062.5383493022293, Done: False
2024-07-21 21:19:01,054 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:01,055 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:03,997 - AirSimEnvLogger - INFO - Predictive model loss: 0.1339847296476364
2024-07-21 21:19:09,245 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -35.06261248221824, Velocity: -0.04221772193599248, Movement: 0.34520531312890007, Collision: 0, Height: -1.0, Movement Penalty: -0.0797217521842638, Smoothness: -0.0, Curiosity: 1134.3748779296875, Exploration: 0.09088695983477922, Total: 1098.8374155437834
2024-07-21 21:19:09,339 - AirSimEnvLogger - INFO - Action: [0.39860876 0.39860876 0.39860876 0.39860876], Velocity: (0.398608760921319, 0.398608760921319, 0.398608760921319), Duration: 1.0, Reward: 1098.8374155437834, Done: False
2024-07-21 21:19:09,403 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:09,403 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:12,287 - AirSimEnvLogger - INFO - Predictive model loss: 0.018784943968057632
2024-07-21 21:19:17,563 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.666597121303106, Velocity: -0.025536558709171073, Movement: 0.2576669786014539, Collision: 0, Height: -1.0, Movement Penalty: -0.05950563978273078, Smoothness: -0.0, Curiosity: 1096.4844970703125, Exploration: 0.04410784920174054, Total: 1061.331811655568
2024-07-21 21:19:17,657 - AirSimEnvLogger - INFO - Action: [-0.2975282 -0.2975282 -0.2975282 -0.2975282], Velocity: (-0.2975281989136539, -0.2975281989136539, -0.2975281989136539), Duration: 1.0, Reward: 1061.331811655568, Done: False
2024-07-21 21:19:17,720 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:17,720 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:20,551 - AirSimEnvLogger - INFO - Predictive model loss: 0.09851183742284775
2024-07-21 21:19:26,243 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -35.077668765247985, Velocity: -0.03539406161715517, Movement: 0.2934024789755162, Collision: 0, Height: -1.0, Movement Penalty: -0.06775840008696712, Smoothness: -0.0, Curiosity: 1133.7218017578125, Exploration: 0.022678745249546443, Total: 1098.1530068448233
2024-07-21 21:19:26,245 - AirSimEnvLogger - INFO - Action: [0.338792 0.338792 0.338792 0.338792], Velocity: (0.3387920004348356, 0.3387920004348356, 0.3387920004348356), Duration: 1.0, Reward: 1098.1530068448233, Done: False
2024-07-21 21:19:26,289 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:26,289 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:29,145 - AirSimEnvLogger - INFO - Predictive model loss: 0.03389037400484085
2024-07-21 21:19:34,714 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.7510660794881, Velocity: -0.021124797359205104, Movement: 0.19470052884415395, Collision: 0, Height: -1.0, Movement Penalty: -0.044964161095813915, Smoothness: -0.0, Curiosity: 1103.102294921875, Exploration: 0.03496981544772699, Total: 1067.861935413694
2024-07-21 21:19:34,825 - AirSimEnvLogger - INFO - Action: [-0.22482081 -0.22482081 -0.22482081 -0.22482081], Velocity: (-0.22482080547906957, -0.22482080547906957, -0.22482080547906957), Duration: 1.0, Reward: 1067.861935413694, Done: False
2024-07-21 21:19:34,902 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:34,902 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:37,720 - AirSimEnvLogger - INFO - Predictive model loss: 0.01718207448720932
2024-07-21 21:19:43,740 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.34834457976917, Velocity: -0.04463941109329492, Movement: 0.5113886750500846, Collision: 0, Height: -1.0, Movement Penalty: -0.11810015568027694, Smoothness: -0.0, Curiosity: 1069.093017578125, Exploration: 0.37859024032697725, Total: 1034.3398291491885
2024-07-21 21:19:43,821 - AirSimEnvLogger - INFO - Action: [-0.59050078 -0.59050078 -0.59050078 -0.59050078], Velocity: (-0.5905007784013847, -0.5905007784013847, -0.5905007784013847), Duration: 1.0, Reward: 1034.3398291491885, Done: False
2024-07-21 21:19:43,882 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:43,882 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:46,730 - AirSimEnvLogger - INFO - Predictive model loss: 0.02806948684155941
2024-07-21 21:19:52,744 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.73929527653259, Velocity: -0.030768730263737824, Movement: 0.17348071385012728, Collision: 0, Height: -1.0, Movement Penalty: -0.040063654736231774, Smoothness: -0.0, Curiosity: 1106.4593505859375, Exploration: 0.07274274232616422, Total: 1071.2379647396845
2024-07-21 21:19:52,870 - AirSimEnvLogger - INFO - Action: [0.20031827 0.20031827 0.20031827 0.20031827], Velocity: (0.20031827368115884, 0.20031827368115884, 0.20031827368115884), Duration: 1.0, Reward: 1071.2379647396845, Done: False
2024-07-21 21:19:52,963 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:19:52,963 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:19:55,807 - AirSimEnvLogger - INFO - Predictive model loss: 0.14846155047416687
2024-07-21 21:20:01,794 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.31051076723209, Velocity: -0.055875761068419516, Movement: 0.3442857824966812, Collision: 0, Height: -1.0, Movement Penalty: -0.07950939568104794, Smoothness: -0.0, Curiosity: 1069.75830078125, Exploration: 0.06078157026057322, Total: 1034.96462751078
2024-07-21 21:20:01,921 - AirSimEnvLogger - INFO - Action: [-0.39754698 -0.39754698 -0.39754698 -0.39754698], Velocity: (-0.39754697840523967, -0.39754697840523967, -0.39754697840523967), Duration: 1.0, Reward: 1034.96462751078, Done: False
2024-07-21 21:20:02,000 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:02,000 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:04,724 - AirSimEnvLogger - INFO - Predictive model loss: 0.025223927572369576
2024-07-21 21:20:10,574 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.71642511992452, Velocity: -0.023279891781269976, Movement: 0.2594955801808358, Collision: 0, Height: -1.0, Movement Penalty: -0.05992793722836947, Smoothness: -0.0, Curiosity: 1107.3521728515625, Exploration: 0.03507272557106172, Total: 1072.1478518819524
2024-07-21 21:20:10,731 - AirSimEnvLogger - INFO - Action: [0.29963969 0.29963969 0.29963969 0.29963969], Velocity: (0.29963968614184733, 0.29963968614184733, 0.29963968614184733), Duration: 1.0, Reward: 1072.1478518819524, Done: False
2024-07-21 21:20:10,794 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:10,794 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:13,608 - AirSimEnvLogger - INFO - Predictive model loss: 0.07070492953062057
2024-07-21 21:20:19,569 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.30572329145323, Velocity: -0.050385342474152134, Movement: 0.2990306456490003, Collision: 0, Height: -1.0, Movement Penalty: -0.06905816950455916, Smoothness: -0.0, Curiosity: 1070.5003662109375, Exploration: 0.019351945449981502, Total: 1035.701390538329
2024-07-21 21:20:19,695 - AirSimEnvLogger - INFO - Action: [-0.34529085 -0.34529085 -0.34529085 -0.34529085], Velocity: (-0.3452908475227958, -0.3452908475227958, -0.3452908475227958), Duration: 1.0, Reward: 1035.701390538329, Done: False
2024-07-21 21:20:19,758 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:19,758 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:22,727 - AirSimEnvLogger - INFO - Predictive model loss: 0.043800871819257736
2024-07-21 21:20:28,675 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.702654007675804, Velocity: -0.04150598780027314, Movement: 0.2699458329564438, Collision: 0, Height: -1.0, Movement Penalty: -0.06234131972960824, Smoothness: -0.0, Curiosity: 1106.8504638671875, Exploration: 0.00872305731621871, Total: 1071.6522873206634
2024-07-21 21:20:28,785 - AirSimEnvLogger - INFO - Action: [0.3117066 0.3117066 0.3117066 0.3117066], Velocity: (0.3117065986480412, 0.3117065986480412, 0.3117065986480412), Duration: 1.0, Reward: 1071.6522873206634, Done: False
2024-07-21 21:20:28,863 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:28,863 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:31,656 - AirSimEnvLogger - INFO - Predictive model loss: 0.048523202538490295
2024-07-21 21:20:37,504 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -34.83510528820455, Velocity: -0.022596609927887118, Movement: 0.27382700010534083, Collision: 0, Height: -1.0, Movement Penalty: -0.0632376368888825, Smoothness: -0.0, Curiosity: 1117.8643798828125, Exploration: 0.2742541040887304, Total: 1082.5968103391258
2024-07-21 21:20:37,630 - AirSimEnvLogger - INFO - Action: [0.31618818 0.31618818 0.31618818 0.31618818], Velocity: (0.31618818444441243, 0.31618818444441243, 0.31618818444441243), Duration: 1.0, Reward: 1082.5968103391258, Done: False
2024-07-21 21:20:37,692 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:37,692 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:40,541 - AirSimEnvLogger - INFO - Predictive model loss: 0.08076909929513931
2024-07-21 21:20:46,427 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -35.244746993097614, Velocity: -0.07928766580643953, Movement: 0.557760216697132, Collision: 0, Height: -1.0, Movement Penalty: -0.12880920450134128, Smoothness: -0.0, Curiosity: 1153.4599609375, Exploration: 0.22957724786179756, Total: 1117.773769772917
2024-07-21 21:20:46,537 - AirSimEnvLogger - INFO - Action: [0.64404602 0.64404602 0.64404602 0.64404602], Velocity: (0.6440460225067064, 0.6440460225067064, 0.6440460225067064), Duration: 1.0, Reward: 1117.773769772917, Done: False
2024-07-21 21:20:46,601 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:46,601 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:49,305 - AirSimEnvLogger - INFO - Predictive model loss: 0.08890355378389359
2024-07-21 21:20:55,490 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -35.61606773238041, Velocity: -0.06318225611334913, Movement: 0.6459316579227775, Collision: 0, Height: -1.0, Movement Penalty: -0.14917152663192676, Smoothness: -0.0, Curiosity: 1183.022216796875, Exploration: 0.3056285348448923, Total: 1146.9859700706909
2024-07-21 21:20:55,584 - AirSimEnvLogger - INFO - Action: [0.74585763 0.74585763 0.74585763 0.74585763], Velocity: (0.7458576331596337, 0.7458576331596337, 0.7458576331596337), Duration: 1.0, Reward: 1146.9859700706909, Done: False
2024-07-21 21:20:55,647 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:20:55,647 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:20:58,444 - AirSimEnvLogger - INFO - Predictive model loss: 0.07786539942026138
2024-07-21 21:21:04,325 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.03935501052851, Velocity: -0.06649392902094355, Movement: 0.7305975562649398, Collision: 0, Height: -1.0, Movement Penalty: -0.16872427831153833, Smoothness: -0.0, Curiosity: 1215.99365234375, Exploration: 0.2754196895641801, Total: 1179.5289159876406
2024-07-21 21:21:04,451 - AirSimEnvLogger - INFO - Action: [0.84362139 0.84362139 0.84362139 0.84362139], Velocity: (0.8436213915576916, 0.8436213915576916, 0.8436213915576916), Duration: 1.0, Reward: 1179.5289159876406, Done: False
2024-07-21 21:21:04,514 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:04,514 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:21:07,260 - AirSimEnvLogger - INFO - Predictive model loss: 0.05296502262353897
2024-07-21 21:21:12,980 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -35.85230069721152, Velocity: -0.011379661322318352, Movement: 0.10658240102973732, Collision: 0, Height: -1.0, Movement Penalty: -0.024614151170158194, Smoothness: -0.0, Curiosity: 1188.4114990234375, Exploration: 0.15044141564784938, Total: 1152.095276346252
2024-07-21 21:21:13,010 - AirSimEnvLogger - INFO - Action: [0.12307076 0.12307076 0.12307076 0.12307076], Velocity: (0.12307075585079097, 0.12307075585079097, 0.12307075585079097), Duration: 1.0, Reward: 1152.095276346252, Done: False
2024-07-21 21:21:13,058 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:13,058 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:21:15,899 - AirSimEnvLogger - INFO - Predictive model loss: 0.03985893353819847
2024-07-21 21:21:21,799 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.189443231514254, Velocity: -0.05480828363168487, Movement: 0.37089523060508306, Collision: 0, Height: -1.0, Movement Penalty: -0.08565458449239721, Smoothness: -0.0, Curiosity: 1216.2164306640625, Exploration: 0.12851987936450812, Total: 1179.5601642164775
2024-07-21 21:21:21,925 - AirSimEnvLogger - INFO - Action: [0.42827292 0.42827292 0.42827292 0.42827292], Velocity: (0.42827292246198606, 0.42827292246198606, 0.42827292246198606), Duration: 1.0, Reward: 1179.5601642164775, Done: False
2024-07-21 21:21:21,987 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:21,987 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:21:24,858 - AirSimEnvLogger - INFO - Predictive model loss: 0.010299529880285263
2024-07-21 21:21:30,790 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.59665094738824, Velocity: -0.0824989848144402, Movement: 0.5979525762407899, Collision: 0, Height: -1.0, Movement Penalty: -0.1380912323421001, Smoothness: -0.0, Curiosity: 1251.80517578125, Exploration: 0.2920111254940433, Total: 1214.7821052387467
2024-07-21 21:21:30,791 - AirSimEnvLogger - INFO - Action: [0.69045616 0.69045616 0.69045616 0.69045616], Velocity: (0.6904561617105005, 0.6904561617105005, 0.6904561617105005), Duration: 1.0, Reward: 1214.7821052387467, Done: False
2024-07-21 21:21:30,885 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:30,885 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:21:33,857 - AirSimEnvLogger - INFO - Predictive model loss: 0.042326461523771286
2024-07-21 21:21:39,787 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.36870594584044, Velocity: -0.00368547043770641, Movement: 0.014517705711354013, Collision: 0, Height: -1.0, Movement Penalty: -0.003352720520186403, Smoothness: -0.0, Curiosity: 1223.1436767578125, Exploration: 0.09184723641326828, Total: 1186.2960832478427
2024-07-21 21:21:39,818 - AirSimEnvLogger - INFO - Action: [0.0167636 0.0167636 0.0167636 0.0167636], Velocity: (0.016763602600932015, 0.016763602600932015, 0.016763602600932015), Duration: 1.0, Reward: 1186.2960832478427, Done: False
2024-07-21 21:21:39,911 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:39,911 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:21:42,864 - AirSimEnvLogger - INFO - Predictive model loss: 0.026574568822979927
2024-07-21 21:21:48,801 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.77304456288243, Velocity: -0.05286653161585776, Movement: 0.4040434122314172, Collision: 0, Height: -1.0, Movement Penalty: -0.09330982912644148, Smoothness: -0.0, Curiosity: 1259.0625, Exploration: 0.07920002589140063, Total: 1221.8122959530604
2024-07-21 21:21:48,927 - AirSimEnvLogger - INFO - Action: [0.46654915 0.46654915 0.46654915 0.46654915], Velocity: (0.46654914563220734, 0.46654914563220734, 0.46654914563220734), Duration: 1.0, Reward: 1221.8122959530604, Done: False
2024-07-21 21:21:48,958 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:48,958 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:21:51,626 - AirSimEnvLogger - INFO - Predictive model loss: 0.05101975053548813
2024-07-21 21:21:57,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.19137939723271, Velocity: -0.09476985075286029, Movement: 0.6315738094738736, Collision: 0, Height: -1.0, Movement Penalty: -0.14585572356514334, Smoothness: -0.0, Curiosity: 1296.501220703125, Exploration: 0.34488916332181213, Total: 1258.8951812999248
2024-07-21 21:21:57,470 - AirSimEnvLogger - INFO - Action: [0.72927862 0.72927862 0.72927862 0.72927862], Velocity: (0.7292786178257167, 0.7292786178257167, 0.7292786178257167), Duration: 1.0, Reward: 1258.8951812999248, Done: False
2024-07-21 21:21:57,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:21:57,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:00,513 - AirSimEnvLogger - INFO - Predictive model loss: 0.022957200184464455
2024-07-21 21:22:06,457 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.90368165471525, Velocity: -0.005507236273318922, Movement: 0.030869898173231387, Collision: 0, Height: -1.0, Movement Penalty: -0.007129097608068591, Smoothness: -0.0, Curiosity: 1261.5013427734375, Exploration: 0.10255123486401978, Total: 1224.121454416054
2024-07-21 21:22:06,535 - AirSimEnvLogger - INFO - Action: [-0.03564549 -0.03564549 -0.03564549 -0.03564549], Velocity: (-0.035645488040342954, -0.035645488040342954, -0.035645488040342954), Duration: 1.0, Reward: 1224.121454416054, Done: False
2024-07-21 21:22:06,614 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:22:06,614 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:09,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.09872882068157196
2024-07-21 21:22:15,410 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -36.87233074972694, Velocity: -0.005830073528964929, Movement: 0.06942031654410569, Collision: 0, Height: -1.0, Movement Penalty: -0.016031935377587382, Smoothness: -0.0, Curiosity: 1256.484619140625, Exploration: 0.27082800185984446, Total: 1219.1756987006002
2024-07-21 21:22:15,536 - AirSimEnvLogger - INFO - Action: [-0.08015968 -0.08015968 -0.08015968 -0.08015968], Velocity: (-0.0801596768879369, -0.0801596768879369, -0.0801596768879369), Duration: 1.0, Reward: 1219.1756987006002, Done: False
2024-07-21 21:22:15,614 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:22:15,614 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:18,512 - AirSimEnvLogger - INFO - Predictive model loss: 0.1037972941994667
2024-07-21 21:22:24,375 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.18011636628541, Velocity: -0.04650713174428732, Movement: 0.2795274007796226, Collision: 0, Height: -1.0, Movement Penalty: -0.06455408803439662, Smoothness: -0.0, Curiosity: 1285.6146240234375, Exploration: 0.11587176683117881, Total: 1247.9633642424408
2024-07-21 21:22:24,485 - AirSimEnvLogger - INFO - Action: [0.32277044 0.32277044 0.32277044 0.32277044], Velocity: (0.3227704401719831, 0.3227704401719831, 0.3227704401719831), Duration: 1.0, Reward: 1247.9633642424408, Done: False
2024-07-21 21:22:24,516 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:22:24,516 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:27,328 - AirSimEnvLogger - INFO - Predictive model loss: 0.020910728722810745
2024-07-21 21:22:33,433 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.33142118276498, Velocity: -0.039811450727714456, Movement: 0.2866635168797616, Collision: 0, Height: -1.0, Movement Penalty: -0.06620210345495008, Smoothness: -0.0, Curiosity: 1298.24072265625, Exploration: 0.2064410143841943, Total: 1260.4598336023562
2024-07-21 21:22:33,557 - AirSimEnvLogger - INFO - Action: [0.33101052 0.33101052 0.33101052 0.33101052], Velocity: (0.3310105172747504, 0.3310105172747504, 0.3310105172747504), Duration: 1.0, Reward: 1260.4598336023562, Done: False
2024-07-21 21:22:33,574 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:22:33,574 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:36,510 - AirSimEnvLogger - INFO - Predictive model loss: 0.02008996345102787
2024-07-21 21:22:42,555 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.7580486786617, Velocity: -0.09408812532286379, Movement: 0.5705707824792672, Collision: 0, Height: -1.0, Movement Penalty: -0.1317676779424561, Smoothness: -0.0, Curiosity: 1337.0726318359375, Exploration: 0.23104332530706842, Total: 1298.87231038014
2024-07-21 21:22:42,680 - AirSimEnvLogger - INFO - Action: [0.65883839 0.65883839 0.65883839 0.65883839], Velocity: (0.6588383897122806, 0.6588383897122806, 0.6588383897122806), Duration: 1.0, Reward: 1298.87231038014, Done: False
2024-07-21 21:22:42,744 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:22:42,744 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:45,726 - AirSimEnvLogger - INFO - Predictive model loss: 0.029217781499028206
2024-07-21 21:22:51,446 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.026303798260194, Velocity: -0.06146003181599725, Movement: 0.5328506411289273, Collision: 0, Height: -1.0, Movement Penalty: -0.12305658443746036, Smoothness: -0.0, Curiosity: 1357.505126953125, Exploration: 0.2543632795638534, Total: 1319.0442515035222
2024-07-21 21:22:51,555 - AirSimEnvLogger - INFO - Action: [0.61528292 0.61528292 0.61528292 0.61528292], Velocity: (0.6152829221873017, 0.6152829221873017, 0.6152829221873017), Duration: 1.0, Reward: 1319.0442515035222, Done: False
2024-07-21 21:22:51,617 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:22:51,617 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:22:54,470 - AirSimEnvLogger - INFO - Predictive model loss: 0.014094258658587933
2024-07-21 21:23:00,140 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.98318196621248, Velocity: -0.009873057303707668, Movement: 0.1812561003651603, Collision: 0, Height: -1.0, Movement Penalty: -0.04185930333523485, Smoothness: -0.0, Curiosity: 1346.40283203125, Exploration: 0.10978576020786047, Total: 1307.9483596865384
2024-07-21 21:23:00,203 - AirSimEnvLogger - INFO - Action: [0.20929652 0.20929652 0.20929652 0.20929652], Velocity: (0.20929651667617422, 0.20929651667617422, 0.20929651667617422), Duration: 1.0, Reward: 1307.9483596865384, Done: False
2024-07-21 21:23:00,251 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:00,251 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:03,075 - AirSimEnvLogger - INFO - Predictive model loss: 0.023201532661914825
2024-07-21 21:23:09,067 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.422221842550094, Velocity: -0.07390242459518326, Movement: 0.5189079999202477, Collision: 0, Height: -1.0, Movement Penalty: -0.11983666937544213, Smoothness: -0.0, Curiosity: 1386.135986328125, Exploration: 0.1191763785387752, Total: 1347.2465135954806
2024-07-21 21:23:09,193 - AirSimEnvLogger - INFO - Action: [0.59918335 0.59918335 0.59918335 0.59918335], Velocity: (0.5991833468772106, 0.5991833468772106, 0.5991833468772106), Duration: 1.0, Reward: 1347.2465135954806, Done: False
2024-07-21 21:23:09,224 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:09,224 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:11,916 - AirSimEnvLogger - INFO - Predictive model loss: 0.016130581498146057
2024-07-21 21:23:17,892 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.35445987322834, Velocity: -0.012244877043779441, Movement: 0.1563893026411243, Collision: 0, Height: -1.0, Movement Penalty: -0.036116562391292385, Smoothness: -0.0, Curiosity: 1373.8924560546875, Exploration: 0.11292400440296271, Total: 1335.0665804359414
2024-07-21 21:23:18,065 - AirSimEnvLogger - INFO - Action: [0.18058281 0.18058281 0.18058281 0.18058281], Velocity: (0.18058281195646192, 0.18058281195646192, 0.18058281195646192), Duration: 1.0, Reward: 1335.0665804359414, Done: False
2024-07-21 21:23:18,159 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:18,159 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:21,142 - AirSimEnvLogger - INFO - Predictive model loss: 0.03155071288347244
2024-07-21 21:23:26,876 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.79323566576, Velocity: -0.07244906005673096, Movement: 0.5070087558833426, Collision: 0, Height: -1.0, Movement Penalty: -0.11708865667629804, Smoothness: -0.0, Curiosity: 1414.3839111328125, Exploration: 0.11362587666930032, Total: 1375.1220014101664
2024-07-21 21:23:26,985 - AirSimEnvLogger - INFO - Action: [0.58544328 0.58544328 0.58544328 0.58544328], Velocity: (0.5854432833814902, 0.5854432833814902, 0.5854432833814902), Duration: 1.0, Reward: 1375.1220014101664, Done: False
2024-07-21 21:23:27,047 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:27,047 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:29,963 - AirSimEnvLogger - INFO - Predictive model loss: 0.014347654767334461
2024-07-21 21:23:35,855 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.71048585462053, Velocity: -0.013008184887787455, Movement: 0.1408862882811844, Collision: 0, Height: -1.0, Movement Penalty: -0.032536294585707615, Smoothness: -0.0, Curiosity: 1401.011474609375, Exploration: 0.1091558984007226, Total: 1361.8282497259643
2024-07-21 21:23:35,949 - AirSimEnvLogger - INFO - Action: [0.16268147 0.16268147 0.16268147 0.16268147], Velocity: (0.16268147292853807, 0.16268147292853807, 0.16268147292853807), Duration: 1.0, Reward: 1361.8282497259643, Done: False
2024-07-21 21:23:36,011 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:36,011 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:38,882 - AirSimEnvLogger - INFO - Predictive model loss: 0.005237278062850237
2024-07-21 21:23:44,642 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.94470739553782, Velocity: -0.04724565558107821, Movement: 0.28498823889792546, Collision: 0, Height: -1.0, Movement Penalty: -0.06581521457743786, Smoothness: -0.0, Curiosity: 1420.8853759765625, Exploration: 0.072140675423331, Total: 1381.4595271169708
2024-07-21 21:23:44,735 - AirSimEnvLogger - INFO - Action: [0.32907607 0.32907607 0.32907607 0.32907607], Velocity: (0.32907607288718926, 0.32907607288718926, 0.32907607288718926), Duration: 1.0, Reward: 1381.4595271169708, Done: False
2024-07-21 21:23:44,798 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:44,798 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:47,634 - AirSimEnvLogger - INFO - Predictive model loss: 0.0049974252469837666
2024-07-21 21:23:53,671 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.813479932834476, Velocity: -0.0012743258128548821, Movement: 0.007438708457355131, Collision: 0, Height: -1.0, Movement Penalty: -0.0017178961321108522, Smoothness: -0.0, Curiosity: 1405.16796875, Exploration: 0.056864272680692426, Total: 1365.8676226384187
2024-07-21 21:23:53,797 - AirSimEnvLogger - INFO - Action: [-0.00858948 -0.00858948 -0.00858948 -0.00858948], Velocity: (-0.00858948066055426, -0.00858948066055426, -0.00858948066055426), Duration: 1.0, Reward: 1365.8676226384187, Done: False
2024-07-21 21:23:53,812 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:23:53,812 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:23:56,725 - AirSimEnvLogger - INFO - Predictive model loss: 0.004163526929914951
2024-07-21 21:24:02,583 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.4431913855671, Velocity: -0.05529626084371657, Movement: 0.3825882275927588, Collision: 0, Height: -1.0, Movement Penalty: -0.08835496647578445, Smoothness: -0.0, Curiosity: 1369.3109130859375, Exploration: 0.29359056155629953, Total: 1330.4391027720692
2024-07-21 21:24:02,694 - AirSimEnvLogger - INFO - Action: [-0.44177483 -0.44177483 -0.44177483 -0.44177483], Velocity: (-0.4417748323789222, -0.4417748323789222, -0.4417748323789222), Duration: 1.0, Reward: 1330.4391027720692, Done: False
2024-07-21 21:24:02,726 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:02,726 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:05,445 - AirSimEnvLogger - INFO - Predictive model loss: 0.03320150822401047
2024-07-21 21:24:11,485 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.986363801514855, Velocity: -0.11613168983938153, Movement: 0.622332874097013, Collision: 0, Height: -1.0, Movement Penalty: -0.14372162095418559, Smoothness: -0.0, Curiosity: 1329.615966796875, Exploration: 0.3841421242436219, Total: 1291.2216083068486
2024-07-21 21:24:11,515 - AirSimEnvLogger - INFO - Action: [-0.7186081 -0.7186081 -0.7186081 -0.7186081], Velocity: (-0.7186081047709278, -0.7186081047709278, -0.7186081047709278), Duration: 1.0, Reward: 1291.2216083068486, Done: False
2024-07-21 21:24:11,593 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:11,593 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:14,509 - AirSimEnvLogger - INFO - Predictive model loss: 0.0330214649438858
2024-07-21 21:24:20,215 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.39816980354601, Velocity: -0.021678002698058557, Movement: 0.12135998676775844, Collision: 0, Height: -1.0, Movement Penalty: -0.028026888411685904, Smoothness: -0.0, Curiosity: 1372.0235595703125, Exploration: 0.11532277244093603, Total: 1333.1527231580997
2024-07-21 21:24:20,373 - AirSimEnvLogger - INFO - Action: [0.14013444 0.14013444 0.14013444 0.14013444], Velocity: (0.1401344420584295, 0.1401344420584295, 0.1401344420584295), Duration: 1.0, Reward: 1333.1527231580997, Done: False
2024-07-21 21:24:20,452 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:20,452 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:23,279 - AirSimEnvLogger - INFO - Predictive model loss: 0.2748775780200958
2024-07-21 21:24:29,424 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.94191739880961, Velocity: -0.05255202085903853, Movement: 0.3722514599873962, Collision: 0, Height: -1.0, Movement Penalty: -0.08596779225198177, Smoothness: -0.0, Curiosity: 1331.1163330078125, Exploration: 0.09465263924402538, Total: 1292.7000618222155
2024-07-21 21:24:29,425 - AirSimEnvLogger - INFO - Action: [-0.42983896 -0.42983896 -0.42983896 -0.42983896], Velocity: (-0.42983896125990884, -0.42983896125990884, -0.42983896125990884), Duration: 1.0, Reward: 1292.7000618222155, Done: False
2024-07-21 21:24:29,518 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:29,518 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:32,536 - AirSimEnvLogger - INFO - Predictive model loss: 0.04223477467894554
2024-07-21 21:24:38,454 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.48411648111188, Velocity: -0.0664321047223403, Movement: 0.6161466641596606, Collision: 0, Height: -1.0, Movement Penalty: -0.14229297696514798, Smoothness: -0.0, Curiosity: 1291.7042236328125, Exploration: 0.3961640927981529, Total: 1253.8196957270718
2024-07-21 21:24:38,565 - AirSimEnvLogger - INFO - Action: [-0.71146488 -0.71146488 -0.71146488 -0.71146488], Velocity: (-0.7114648848257399, -0.7114648848257399, -0.7114648848257399), Duration: 1.0, Reward: 1253.8196957270718, Done: False
2024-07-21 21:24:38,643 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:38,643 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:41,488 - AirSimEnvLogger - INFO - Predictive model loss: 0.028050171211361885
2024-07-21 21:24:47,575 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.67715808886274, Velocity: -0.01712333206797357, Movement: 0.08461220581728032, Collision: 0, Height: -1.0, Movement Penalty: -0.019540351922133926, Smoothness: -0.0, Curiosity: 1313.2625732421875, Exploration: 0.08600250872093368, Total: 1275.1055589299772
2024-07-21 21:24:47,701 - AirSimEnvLogger - INFO - Action: [-0.09770176 -0.09770176 -0.09770176 -0.09770176], Velocity: (-0.09770175961066963, -0.09770175961066963, -0.09770175961066963), Duration: 1.0, Reward: 1275.1055589299772, Done: False
2024-07-21 21:24:47,763 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:47,763 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:50,576 - AirSimEnvLogger - INFO - Predictive model loss: 0.011441714130342007
2024-07-21 21:24:56,453 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.06814369299599, Velocity: -0.06585976056394022, Movement: 0.3805617868373588, Collision: 0, Height: -1.0, Movement Penalty: -0.08788698002953364, Smoothness: -0.0, Curiosity: 1354.0494384765625, Exploration: 0.40080400802564803, Total: 1315.5762288894985
2024-07-21 21:24:56,578 - AirSimEnvLogger - INFO - Action: [0.4394349 0.4394349 0.4394349 0.4394349], Velocity: (0.4394349001476682, 0.4394349001476682, 0.4394349001476682), Duration: 1.0, Reward: 1315.5762288894985, Done: False
2024-07-21 21:24:56,610 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:24:56,610 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:24:59,453 - AirSimEnvLogger - INFO - Predictive model loss: 0.04190312698483467
2024-07-21 21:25:05,042 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.44400705459932, Velocity: -0.04754496731864171, Movement: 0.578074354159862, Collision: 0, Height: -1.0, Movement Penalty: -0.13350055359432617, Smoothness: -0.0, Curiosity: 1389.542236328125, Exploration: 0.3881834762785259, Total: 1350.696937109123
2024-07-21 21:25:05,150 - AirSimEnvLogger - INFO - Action: [0.66750277 0.66750277 0.66750277 0.66750277], Velocity: (0.6675027679716308, 0.6675027679716308, 0.6675027679716308), Duration: 1.0, Reward: 1350.696937109123, Done: False
2024-07-21 21:25:05,213 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:05,213 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:25:08,054 - AirSimEnvLogger - INFO - Predictive model loss: 0.06996257603168488
2024-07-21 21:25:13,781 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.38253601081478, Velocity: -0.006808485718071091, Movement: 0.19534010773472693, Collision: 0, Height: -1.0, Movement Penalty: -0.04511186551367004, Smoothness: -0.0, Curiosity: 1377.14111328125, Exploration: 0.09736521556757217, Total: 1338.28508210377
2024-07-21 21:25:13,908 - AirSimEnvLogger - INFO - Action: [0.22555933 0.22555933 0.22555933 0.22555933], Velocity: (0.22555932756835018, 0.22555932756835018, 0.22555932756835018), Duration: 1.0, Reward: 1338.28508210377, Done: False
2024-07-21 21:25:13,970 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:13,970 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:25:16,888 - AirSimEnvLogger - INFO - Predictive model loss: 0.17694558203220367
2024-07-21 21:25:22,742 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.59700484124895, Velocity: -0.04553000047251149, Movement: 0.28625736889933373, Collision: 0, Height: -1.0, Movement Penalty: -0.06610830759661773, Smoothness: -0.0, Curiosity: 1394.267333984375, Exploration: 0.089431545954357, Total: 1355.1933672764958
2024-07-21 21:25:22,867 - AirSimEnvLogger - INFO - Action: [0.33054154 0.33054154 0.33054154 0.33054154], Velocity: (0.33054153798308866, 0.33054153798308866, 0.33054153798308866), Duration: 1.0, Reward: 1355.1933672764958, Done: False
2024-07-21 21:25:22,931 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:22,931 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:25:25,681 - AirSimEnvLogger - INFO - Predictive model loss: 0.16694991290569305
2024-07-21 21:25:31,827 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.214301495249885, Velocity: -0.03628700710699868, Movement: 0.2675716714744776, Collision: 0, Height: -1.0, Movement Penalty: -0.0617930306213231, Smoothness: -0.0, Curiosity: 1354.401611328125, Exploration: 0.1772099491601446, Total: 1315.731002939167
2024-07-21 21:25:31,875 - AirSimEnvLogger - INFO - Action: [-0.30896515 -0.30896515 -0.30896515 -0.30896515], Velocity: (-0.3089651531066155, -0.3089651531066155, -0.3089651531066155), Duration: 1.0, Reward: 1315.731002939167, Done: False
2024-07-21 21:25:31,936 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:31,936 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:25:34,787 - AirSimEnvLogger - INFO - Predictive model loss: 0.13586758077144623
2024-07-21 21:25:40,389 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.352636996060056, Velocity: -0.0011871642766850888, Movement: 0.0008736381387405033, Collision: 0, Height: -1.0, Movement Penalty: -0.0002017580858304613, Smoothness: -0.0, Curiosity: 1367.833984375, Exploration: 0.1514651283954255, Total: 1329.0160870723396
2024-07-21 21:25:40,516 - AirSimEnvLogger - INFO - Action: [0.00100879 0.00100879 0.00100879 0.00100879], Velocity: (0.0010087904291523064, 0.0010087904291523064, 0.0010087904291523064), Duration: 1.0, Reward: 1329.0160870723396, Done: False
2024-07-21 21:25:40,595 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:40,595 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:25:43,548 - AirSimEnvLogger - INFO - Predictive model loss: 0.0029695809353142977
2024-07-21 21:25:49,568 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.930266493107595, Velocity: -0.053482202562994186, Movement: 0.4094802938861956, Collision: 0, Height: -1.0, Movement Penalty: -0.09456542316121685, Smoothness: -0.0, Curiosity: 1329.473388671875, Exploration: 0.1201770274326293, Total: 1291.075459204384
2024-07-21 21:25:49,647 - AirSimEnvLogger - INFO - Action: [-0.47282712 -0.47282712 -0.47282712 -0.47282712], Velocity: (-0.4728271158060842, -0.4728271158060842, -0.4728271158060842), Duration: 1.0, Reward: 1291.075459204384, Done: False
2024-07-21 21:25:49,710 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:49,710 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:25:52,475 - AirSimEnvLogger - INFO - Predictive model loss: 0.04320906475186348
2024-07-21 21:25:58,516 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.35137730547259, Velocity: -0.03160697526877514, Movement: 0.2277070156889807, Collision: 0, Height: -1.0, Movement Penalty: -0.05258668272175974, Smoothness: -0.0, Curiosity: 1371.518798828125, Exploration: 0.05932605691493752, Total: 1332.6834914590515
2024-07-21 21:25:58,626 - AirSimEnvLogger - INFO - Action: [0.26293341 0.26293341 0.26293341 0.26293341], Velocity: (0.2629334136087987, 0.2629334136087987, 0.2629334136087987), Duration: 1.0, Reward: 1332.6834914590515, Done: False
2024-07-21 21:25:58,689 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:25:58,689 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:01,601 - AirSimEnvLogger - INFO - Predictive model loss: 0.4311941862106323
2024-07-21 21:26:07,481 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.00922440811413, Velocity: -0.03966656172928655, Movement: 0.22118132389811576, Collision: 0, Height: -1.0, Movement Penalty: -0.05107963875691798, Smoothness: -0.0, Curiosity: 1339.1199951171875, Exploration: 0.07747115604731024, Total: 1300.6300479847587
2024-07-21 21:26:07,591 - AirSimEnvLogger - INFO - Action: [-0.25539819 -0.25539819 -0.25539819 -0.25539819], Velocity: (-0.2553981937845899, -0.2553981937845899, -0.2553981937845899), Duration: 1.0, Reward: 1300.6300479847587, Done: False
2024-07-21 21:26:07,654 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:26:07,654 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:10,375 - AirSimEnvLogger - INFO - Predictive model loss: 0.12909428775310516
2024-07-21 21:26:16,415 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.67239093668571, Velocity: -0.062411380471850735, Movement: 0.43767276560812796, Collision: 0, Height: -1.0, Movement Penalty: -0.10107619561632826, Smoothness: -0.0, Curiosity: 1309.0262451171875, Exploration: 0.319975239408111, Total: 1270.9319434368954
2024-07-21 21:26:16,540 - AirSimEnvLogger - INFO - Action: [-0.50538098 -0.50538098 -0.50538098 -0.50538098], Velocity: (-0.5053809780816413, -0.5053809780816413, -0.5053809780816413), Duration: 1.0, Reward: 1270.9319434368954, Done: False
2024-07-21 21:26:16,602 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:26:16,602 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:19,460 - AirSimEnvLogger - INFO - Predictive model loss: 0.029294082894921303
2024-07-21 21:26:25,270 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.08741101728595, Velocity: -0.032745858572349254, Movement: 0.21406211174851844, Collision: 0, Height: -1.0, Movement Penalty: -0.04943552713652275, Smoothness: -0.0, Curiosity: 1350.9034423828125, Exploration: 0.11067564611579174, Total: 1312.3434631103526
2024-07-21 21:26:25,364 - AirSimEnvLogger - INFO - Action: [0.24717764 0.24717764 0.24717764 0.24717764], Velocity: (0.24717763568261375, 0.24717763568261375, 0.24717763568261375), Duration: 1.0, Reward: 1312.3434631103526, Done: False
2024-07-21 21:26:25,458 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:26:25,458 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:28,211 - AirSimEnvLogger - INFO - Predictive model loss: 0.005691815167665482
2024-07-21 21:26:34,261 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.77495751052115, Velocity: -0.028218854383825317, Movement: 0.19619676077901266, Collision: 0, Height: -1.0, Movement Penalty: -0.045309701059958234, Smoothness: -0.0, Curiosity: 1321.7462158203125, Exploration: 0.10383006581992717, Total: 1283.4971300354705
2024-07-21 21:26:34,369 - AirSimEnvLogger - INFO - Action: [-0.22654851 -0.22654851 -0.22654851 -0.22654851], Velocity: (-0.22654850529979115, -0.22654850529979115, -0.22654850529979115), Duration: 1.0, Reward: 1283.4971300354705, Done: False
2024-07-21 21:26:34,416 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:26:34,416 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:37,296 - AirSimEnvLogger - INFO - Predictive model loss: 0.130537211894989
2024-07-21 21:26:43,093 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.17884694721626, Velocity: -0.04862330642540172, Movement: 0.31051514289139764, Collision: 0, Height: -1.0, Movement Penalty: -0.07171040053432141, Smoothness: -0.0, Curiosity: 1361.037109375, Exploration: 0.057872110513837206, Total: 1322.374327602297
2024-07-21 21:26:43,234 - AirSimEnvLogger - INFO - Action: [0.358552 0.358552 0.358552 0.358552], Velocity: (0.3585520026716071, 0.3585520026716071, 0.3585520026716071), Duration: 1.0, Reward: 1322.374327602297, Done: False
2024-07-21 21:26:43,250 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:26:43,250 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:46,205 - AirSimEnvLogger - INFO - Predictive model loss: 0.07410261034965515
2024-07-21 21:26:52,026 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.95938755801116, Velocity: -0.004686818500943658, Movement: 0.07117206812208816, Collision: 0, Height: -1.0, Movement Penalty: -0.016436485075627994, Smoothness: -0.0, Curiosity: 1338.3504638671875, Exploration: 0.082276156626837, Total: 1299.9112769903497
2024-07-21 21:26:52,058 - AirSimEnvLogger - INFO - Action: [-0.08218243 -0.08218243 -0.08218243 -0.08218243], Velocity: (-0.08218242537813997, -0.08218242537813997, -0.08218242537813997), Duration: 1.0, Reward: 1299.9112769903497, Done: False
2024-07-21 21:26:52,121 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:26:52,121 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:26:54,949 - AirSimEnvLogger - INFO - Predictive model loss: 0.170609712600708
2024-07-21 21:27:00,773 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.56485466279824, Velocity: -0.037570591076037364, Movement: 0.43691395497930086, Collision: 0, Height: -1.0, Movement Penalty: -0.10090095580800136, Smoothness: -0.0, Curiosity: 1301.5439453125, Exploration: 0.32616703814048353, Total: 1263.5610694851596
2024-07-21 21:27:00,883 - AirSimEnvLogger - INFO - Action: [-0.50450478 -0.50450478 -0.50450478 -0.50450478], Velocity: (-0.5045047790400068, -0.5045047790400068, -0.5045047790400068), Duration: 1.0, Reward: 1263.5610694851596, Done: False
2024-07-21 21:27:00,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:00,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:03,884 - AirSimEnvLogger - INFO - Predictive model loss: 0.14347214996814728
2024-07-21 21:27:09,639 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -37.98363391481861, Velocity: -0.0180609750621546, Movement: 0.21254717217992025, Collision: 0, Height: -1.0, Movement Penalty: -0.049085666829428276, Smoothness: -0.0, Curiosity: 1343.1064453125, Exploration: 0.05193817646294179, Total: 1304.638164851452
2024-07-21 21:27:09,670 - AirSimEnvLogger - INFO - Action: [0.24542833 0.24542833 0.24542833 0.24542833], Velocity: (0.24542833414714138, 0.24542833414714138, 0.24542833414714138), Duration: 1.0, Reward: 1304.638164851452, Done: False
2024-07-21 21:27:09,747 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:09,747 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:12,560 - AirSimEnvLogger - INFO - Predictive model loss: 0.048360422253608704
2024-07-21 21:27:18,564 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.39413920521361, Velocity: -0.06467110641868744, Movement: 0.5390396001285639, Collision: 0, Height: -1.0, Movement Penalty: -0.12448586329523784, Smoothness: -0.0, Curiosity: 1383.9852294921875, Exploration: 0.44533115200424556, Total: 1345.2002719536633
2024-07-21 21:27:18,674 - AirSimEnvLogger - INFO - Action: [0.62242932 0.62242932 0.62242932 0.62242932], Velocity: (0.6224293164761892, 0.6224293164761892, 0.6224293164761892), Duration: 1.0, Reward: 1345.2002719536633, Done: False
2024-07-21 21:27:18,768 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:18,768 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:21,614 - AirSimEnvLogger - INFO - Predictive model loss: 0.24978123605251312
2024-07-21 21:27:27,311 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -38.807811077159315, Velocity: -0.11350188509780151, Movement: 0.6872149103261316, Collision: 0, Height: -1.0, Movement Penalty: -0.15870548538716667, Smoothness: -0.0, Curiosity: 1421.11572265625, Exploration: 0.3558564134586719, Total: 1381.8952657104285
2024-07-21 21:27:27,400 - AirSimEnvLogger - INFO - Action: [0.79352743 0.79352743 0.79352743 0.79352743], Velocity: (0.7935274269358332, 0.7935274269358332, 0.7935274269358332), Duration: 1.0, Reward: 1381.8952657104285, Done: False
2024-07-21 21:27:27,467 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:27,467 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:30,315 - AirSimEnvLogger - INFO - Predictive model loss: 0.19034621119499207
2024-07-21 21:27:36,239 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -39.2540836964634, Velocity: -0.06527549074216, Movement: 0.7732303799765898, Collision: 0, Height: -1.0, Movement Penalty: -0.1785699072100323, Smoothness: -0.0, Curiosity: 1459.00927734375, Exploration: 0.30612520072424876, Total: 1419.3380421939705
2024-07-21 21:27:36,364 - AirSimEnvLogger - INFO - Action: [0.89284954 0.89284954 0.89284954 0.89284954], Velocity: (0.8928495360501615, 0.8928495360501615, 0.8928495360501615), Duration: 1.0, Reward: 1419.3380421939705, Done: False
2024-07-21 21:27:36,412 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:36,412 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:39,264 - AirSimEnvLogger - INFO - Predictive model loss: 0.06564860045909882
2024-07-21 21:27:45,046 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -39.643262017940884, Velocity: -0.11758455902706022, Movement: 0.7435950459232219, Collision: 0, Height: -1.0, Movement Penalty: -0.17172591997273773, Smoothness: -0.0, Curiosity: 1489.326904296875, Exploration: 0.2679433712077549, Total: 1449.251751144852
2024-07-21 21:27:45,077 - AirSimEnvLogger - INFO - Action: [0.8586296 0.8586296 0.8586296 0.8586296], Velocity: (0.8586295998636886, 0.8586295998636886, 0.8586295998636886), Duration: 1.0, Reward: 1449.251751144852, Done: False
2024-07-21 21:27:45,124 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:45,124 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:48,018 - AirSimEnvLogger - INFO - Predictive model loss: 0.10191355645656586
2024-07-21 21:27:53,885 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -40.09064047260396, Velocity: -0.06157384781275678, Movement: 0.7906612904751572, Collision: 0, Height: -1.0, Movement Penalty: -0.18259540355745957, Smoothness: -0.0, Curiosity: 1526.18115234375, Exploration: 0.260688376973533, Total: 1485.6637076919565
2024-07-21 21:27:53,996 - AirSimEnvLogger - INFO - Action: [0.91297702 0.91297702 0.91297702 0.91297702], Velocity: (0.9129770177872978, 0.9129770177872978, 0.9129770177872978), Duration: 1.0, Reward: 1485.6637076919565, Done: False
2024-07-21 21:27:54,091 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:27:54,091 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:27:57,003 - AirSimEnvLogger - INFO - Predictive model loss: 0.18953509628772736
2024-07-21 21:28:02,791 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -40.551653244469584, Velocity: -0.04319500987761842, Movement: 0.8280012154926847, Collision: 0, Height: -1.0, Movement Penalty: -0.19121868986161553, Smoothness: -0.0, Curiosity: 1564.55419921875, Exploration: 0.2881570711110278, Total: 1523.5848134248863
2024-07-21 21:28:02,949 - AirSimEnvLogger - INFO - Action: [0.95609345 0.95609345 0.95609345 0.95609345], Velocity: (0.9560934493080776, 0.9560934493080776, 0.9560934493080776), Duration: 1.0, Reward: 1523.5848134248863, Done: False
2024-07-21 21:28:03,028 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:03,028 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:05,837 - AirSimEnvLogger - INFO - Predictive model loss: 0.1961148977279663
2024-07-21 21:28:11,598 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -40.976609812076994, Velocity: -0.11620426995104295, Movement: 0.8064225499862273, Collision: 0, Height: -1.0, Movement Penalty: -0.18623531052605313, Smoothness: -0.0, Curiosity: 1598.7305908203125, Exploration: 0.2755240952577009, Total: 1557.3255126248848
2024-07-21 21:28:11,739 - AirSimEnvLogger - INFO - Action: [0.93117655 0.93117655 0.93117655 0.93117655], Velocity: (0.9311765526302656, 0.9311765526302656, 0.9311765526302656), Duration: 1.0, Reward: 1557.3255126248848, Done: False
2024-07-21 21:28:11,770 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:11,770 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:14,650 - AirSimEnvLogger - INFO - Predictive model loss: 0.14451757073402405
2024-07-21 21:28:20,623 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -40.78905640972932, Velocity: -0.02545972110419732, Movement: 0.14961768424077285, Collision: 0, Height: -1.0, Movement Penalty: -0.03455272410877546, Smoothness: -0.0, Curiosity: 1567.442138671875, Exploration: 0.19882579414034873, Total: 1526.1999363364268
2024-07-21 21:28:20,750 - AirSimEnvLogger - INFO - Action: [0.17276362 0.17276362 0.17276362 0.17276362], Velocity: (0.17276362054387728, 0.17276362054387728, 0.17276362054387728), Duration: 1.0, Reward: 1526.1999363364268, Done: False
2024-07-21 21:28:20,782 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:20,782 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:23,695 - AirSimEnvLogger - INFO - Predictive model loss: 0.12897351384162903
2024-07-21 21:28:29,400 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -41.244908321453494, Velocity: -0.08360368757063498, Movement: 0.5054682017673111, Collision: 0, Height: -1.0, Movement Penalty: -0.11673288094286126, Smoothness: -0.0, Curiosity: 1610.380126953125, Exploration: 0.1319468649154085, Total: 1568.669605149621
2024-07-21 21:28:29,509 - AirSimEnvLogger - INFO - Action: [0.5836644 0.5836644 0.5836644 0.5836644], Velocity: (0.5836644047143063, 0.5836644047143063, 0.5836644047143063), Duration: 1.0, Reward: 1568.669605149621, Done: False
2024-07-21 21:28:29,541 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:29,541 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:32,310 - AirSimEnvLogger - INFO - Predictive model loss: 0.08553583174943924
2024-07-21 21:28:38,202 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -41.64340986761669, Velocity: -0.09419912745331005, Movement: 0.6419579020893207, Collision: 0, Height: -1.0, Movement Penalty: -0.1482538270318707, Smoothness: -0.0, Curiosity: 1647.81396484375, Exploration: 0.31838093889702235, Total: 1605.750109872111
2024-07-21 21:28:38,312 - AirSimEnvLogger - INFO - Action: [0.74126914 0.74126914 0.74126914 0.74126914], Velocity: (0.7412691351593534, 0.7412691351593534, 0.7412691351593534), Duration: 1.0, Reward: 1605.750109872111, Done: False
2024-07-21 21:28:38,328 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:38,328 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:41,218 - AirSimEnvLogger - INFO - Predictive model loss: 0.3884007930755615
2024-07-21 21:28:47,187 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -41.35807423272585, Velocity: -0.004654850246545918, Movement: 0.008794290489346724, Collision: 0, Height: -1.0, Movement Penalty: -0.002030954392542439, Smoothness: -0.0, Curiosity: 1610.2564697265625, Exploration: 0.13295525297230423, Total: 1568.428725440468
2024-07-21 21:28:47,234 - AirSimEnvLogger - INFO - Action: [-0.01015477 -0.01015477 -0.01015477 -0.01015477], Velocity: (-0.010154771962712195, -0.010154771962712195, -0.010154771962712195), Duration: 1.0, Reward: 1568.428725440468, Done: False
2024-07-21 21:28:47,265 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:47,265 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:50,182 - AirSimEnvLogger - INFO - Predictive model loss: 0.12179122865200043
2024-07-21 21:28:56,069 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -41.80824065145844, Velocity: -0.035137392355240715, Movement: 0.42383823136857546, Collision: 0, Height: -1.0, Movement Penalty: -0.09788124678940079, Smoothness: -0.0, Curiosity: 1654.318359375, Exploration: 0.09162301279640896, Total: 1612.0380750062461
2024-07-21 21:28:56,197 - AirSimEnvLogger - INFO - Action: [0.48940623 0.48940623 0.48940623 0.48940623], Velocity: (0.4894062339470039, 0.4894062339470039, 0.4894062339470039), Duration: 1.0, Reward: 1612.0380750062461, Done: False
2024-07-21 21:28:56,244 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:28:56,244 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:28:59,083 - AirSimEnvLogger - INFO - Predictive model loss: 0.15631666779518127
2024-07-21 21:29:04,879 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -42.24379334804003, Velocity: -0.08859022163766427, Movement: 0.6447159463474171, Collision: 0, Height: -1.0, Movement Penalty: -0.14889077006981025, Smoothness: -0.0, Curiosity: 1697.9315185546875, Exploration: 0.3612265988184554, Total: 1655.2777631496658
2024-07-21 21:29:04,988 - AirSimEnvLogger - INFO - Action: [0.74445385 0.74445385 0.74445385 0.74445385], Velocity: (0.7444538503490512, 0.7444538503490512, 0.7444538503490512), Duration: 1.0, Reward: 1655.2777631496658, Done: False
2024-07-21 21:29:05,052 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:05,052 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:29:07,909 - AirSimEnvLogger - INFO - Predictive model loss: 0.009173392318189144
2024-07-21 21:29:13,557 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -42.62732281393972, Velocity: -0.07812197668656087, Movement: 0.688342297099562, Collision: 0, Height: -1.0, Movement Penalty: -0.158965844210015, Smoothness: -0.0, Curiosity: 1732.3819580078125, Exploration: 0.2957320736479252, Total: 1689.3317263726321
2024-07-21 21:29:13,667 - AirSimEnvLogger - INFO - Action: [0.79482922 0.79482922 0.79482922 0.79482922], Velocity: (0.7948292210500749, 0.7948292210500749, 0.7948292210500749), Duration: 1.0, Reward: 1689.3317263726321, Done: False
2024-07-21 21:29:13,714 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:13,714 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:29:16,520 - AirSimEnvLogger - INFO - Predictive model loss: 0.04834119603037834
2024-07-21 21:29:22,388 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -43.080587987117376, Velocity: -0.06272773373218586, Movement: 0.7750493217165794, Collision: 0, Height: -1.0, Movement Penalty: -0.1789899738113216, Smoothness: -0.0, Curiosity: 1773.925537109375, Exploration: 0.27510615159367646, Total: 1730.4209626818856
2024-07-21 21:29:22,514 - AirSimEnvLogger - INFO - Action: [0.89494987 0.89494987 0.89494987 0.89494987], Velocity: (0.8949498690566079, 0.8949498690566079, 0.8949498690566079), Duration: 1.0, Reward: 1730.4209626818856, Done: False
2024-07-21 21:29:22,608 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:22,608 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:29:25,514 - AirSimEnvLogger - INFO - Predictive model loss: 0.2824608087539673
2024-07-21 21:29:31,465 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -43.47163952878997, Velocity: -0.11013454211365346, Movement: 0.7392681425328267, Collision: 0, Height: -1.0, Movement Penalty: -0.17072666443785686, Smoothness: -0.0, Curiosity: 1807.0792236328125, Exploration: 0.2658612144775627, Total: 1763.1758529368767
2024-07-21 21:29:31,606 - AirSimEnvLogger - INFO - Action: [0.85363332 0.85363332 0.85363332 0.85363332], Velocity: (0.8536333221892842, 0.8536333221892842, 0.8536333221892842), Duration: 1.0, Reward: 1763.1758529368767, Done: False
2024-07-21 21:29:31,638 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:31,638 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:29:34,556 - AirSimEnvLogger - INFO - Predictive model loss: 0.2650255262851715
2024-07-21 21:29:40,216 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -43.933411846895254, Velocity: -0.10574795239557007, Movement: 0.8026371719992327, Collision: 0, Height: -1.0, Movement Penalty: -0.1853611149261428, Smoothness: -0.0, Curiosity: 1849.0888671875, Exploration: 0.264866819221842, Total: 1804.7254885604032
2024-07-21 21:29:40,327 - AirSimEnvLogger - INFO - Action: [0.92680557 0.92680557 0.92680557 0.92680557], Velocity: (0.9268055746307139, 0.9268055746307139, 0.9268055746307139), Duration: 1.0, Reward: 1804.7254885604032, Done: False
2024-07-21 21:29:40,422 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:40,422 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:29:43,323 - AirSimEnvLogger - INFO - Predictive model loss: 0.09090415388345718
2024-07-21 21:29:49,304 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.31782736175109, Velocity: -0.07953785316060989, Movement: 0.7440363842181011, Collision: 0, Height: -1.0, Movement Penalty: -0.17182784268607862, Smoothness: -0.0, Curiosity: 1881.58544921875, Exploration: 0.2604924374248899, Total: 1836.8378325047381
2024-07-21 21:29:49,462 - AirSimEnvLogger - INFO - Action: [0.85913921 0.85913921 0.85913921 0.85913921], Velocity: (0.859139213430393, 0.859139213430393, 0.859139213430393), Duration: 1.0, Reward: 1836.8378325047381, Done: False
2024-07-21 21:29:49,509 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:49,509 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:29:52,461 - AirSimEnvLogger - INFO - Predictive model loss: 0.017617227509617805
2024-07-21 21:29:58,423 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -43.98834593056509, Velocity: -0.007505064143871986, Movement: 0.011912066642485477, Collision: 0, Height: -1.0, Movement Penalty: -0.002750973953057501, Smoothness: -0.0, Curiosity: 1834.97998046875, Exploration: 0.24368462348794742, Total: 1790.547223697526
2024-07-21 21:29:58,516 - AirSimEnvLogger - INFO - Action: [-0.01375487 -0.01375487 -0.01375487 -0.01375487], Velocity: (-0.013754869765287503, -0.013754869765287503, -0.013754869765287503), Duration: 1.0, Reward: 1790.547223697526, Done: False
2024-07-21 21:29:58,578 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:29:58,578 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:01,479 - AirSimEnvLogger - INFO - Predictive model loss: 0.0064324201084673405
2024-07-21 21:30:07,380 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.44324052679486, Velocity: -0.043260329780004166, Movement: 0.4185608039325663, Collision: 0, Height: -1.0, Movement Penalty: -0.09666247712907733, Smoothness: -0.0, Curiosity: 1881.368896484375, Exploration: 0.13445861046636928, Total: 1836.4625226789954
2024-07-21 21:30:07,521 - AirSimEnvLogger - INFO - Action: [0.48331239 0.48331239 0.48331239 0.48331239], Velocity: (0.48331238564538664, 0.48331238564538664, 0.48331238564538664), Duration: 1.0, Reward: 1836.4625226789954, Done: False
2024-07-21 21:30:07,584 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:30:07,584 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:10,435 - AirSimEnvLogger - INFO - Predictive model loss: 0.21175755560398102
2024-07-21 21:30:16,178 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.8694538797966, Velocity: -0.09825299855700419, Movement: 0.6248566499106575, Collision: 0, Height: -1.0, Movement Penalty: -0.14430446201233835, Smoothness: -0.0, Curiosity: 1925.7100830078125, Exploration: 0.34997972952146006, Total: 1880.4266268659314
2024-07-21 21:30:16,179 - AirSimEnvLogger - INFO - Action: [0.72152231 0.72152231 0.72152231 0.72152231], Velocity: (0.7215223100616918, 0.7215223100616918, 0.7215223100616918), Duration: 1.0, Reward: 1880.4266268659314, Done: False
2024-07-21 21:30:16,242 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:30:16,242 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:19,181 - AirSimEnvLogger - INFO - Predictive model loss: 0.5219243168830872
2024-07-21 21:30:25,029 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.82702155391853, Velocity: -0.020342286824926687, Movement: 0.2417882915041606, Collision: 0, Height: -1.0, Movement Penalty: -0.05583861407473073, Smoothness: -0.0, Curiosity: 1913.9691162109375, Exploration: 0.11578598297135682, Total: 1868.672622258193
2024-07-21 21:30:25,201 - AirSimEnvLogger - INFO - Action: [0.27919307 0.27919307 0.27919307 0.27919307], Velocity: (0.27919307037365365, 0.27919307037365365, 0.27919307037365365), Duration: 1.0, Reward: 1868.672622258193, Done: False
2024-07-21 21:30:25,296 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:30:25,296 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:28,075 - AirSimEnvLogger - INFO - Predictive model loss: 0.018123764544725418
2024-07-21 21:30:33,863 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.4380566277067, Velocity: -0.04765022222529402, Movement: 0.2841924742138593, Collision: 0, Height: -1.0, Movement Penalty: -0.06563144059561497, Smoothness: -0.0, Curiosity: 1868.0257568359375, Exploration: 0.3328871850723999, Total: 1823.166470464667
2024-07-21 21:30:33,990 - AirSimEnvLogger - INFO - Action: [-0.3281572 -0.3281572 -0.3281572 -0.3281572], Velocity: (-0.3281572029780749, -0.3281572029780749, -0.3281572029780749), Duration: 1.0, Reward: 1823.166470464667, Done: False
2024-07-21 21:30:34,038 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:30:34,038 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:36,866 - AirSimEnvLogger - INFO - Predictive model loss: 0.2579064965248108
2024-07-21 21:30:42,725 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.89459585927543, Velocity: -0.024760761767346297, Movement: 0.2907809903613905, Collision: 0, Height: -1.0, Movement Penalty: -0.0671529932241499, Smoothness: -0.0, Curiosity: 1917.4373779296875, Exploration: 0.03510937648040072, Total: 1872.0555139928642
2024-07-21 21:30:42,804 - AirSimEnvLogger - INFO - Action: [0.33576497 0.33576497 0.33576497 0.33576497], Velocity: (0.3357649661207495, 0.3357649661207495, 0.3357649661207495), Duration: 1.0, Reward: 1872.0555139928642, Done: False
2024-07-21 21:30:42,867 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:30:42,867 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:45,758 - AirSimEnvLogger - INFO - Predictive model loss: 0.04835395887494087
2024-07-21 21:30:51,770 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.54372889975945, Velocity: -0.030963304304101633, Movement: 0.19266818558153925, Collision: 0, Height: -1.0, Movement Penalty: -0.04449481152391138, Smoothness: -0.0, Curiosity: 1878.726318359375, Exploration: 0.044549050540788226, Total: 1833.694465551029
2024-07-21 21:30:51,893 - AirSimEnvLogger - INFO - Action: [-0.22247406 -0.22247406 -0.22247406 -0.22247406], Velocity: (-0.2224740576195569, -0.2224740576195569, -0.2224740576195569), Duration: 1.0, Reward: 1833.694465551029, Done: False
2024-07-21 21:30:51,956 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:30:51,956 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:30:54,765 - AirSimEnvLogger - INFO - Predictive model loss: 0.20276683568954468
2024-07-21 21:31:00,691 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.519234723645326, Velocity: -0.01724709076464755, Movement: 0.12338552232311484, Collision: 0, Height: -1.0, Movement Penalty: -0.028494665810941174, Smoothness: -0.0, Curiosity: 1876.1165771484375, Exploration: 0.18389261767892076, Total: 1831.1409396580016
2024-07-21 21:31:00,723 - AirSimEnvLogger - INFO - Action: [-0.14247333 -0.14247333 -0.14247333 -0.14247333], Velocity: (-0.14247332905470586, -0.14247332905470586, -0.14247332905470586), Duration: 1.0, Reward: 1831.1409396580016, Done: False
2024-07-21 21:31:00,817 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:00,817 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:03,752 - AirSimEnvLogger - INFO - Predictive model loss: 0.04641556367278099
2024-07-21 21:31:09,497 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.141032088877175, Velocity: -0.06384883651782357, Movement: 0.40756203671481267, Collision: 0, Height: -1.0, Movement Penalty: -0.09412242064350769, Smoothness: -0.0, Curiosity: 1838.154052734375, Exploration: 0.16558270452595303, Total: 1793.5547172587053
2024-07-21 21:31:09,639 - AirSimEnvLogger - INFO - Action: [-0.4706121 -0.4706121 -0.4706121 -0.4706121], Velocity: (-0.47061210321753844, -0.47061210321753844, -0.47061210321753844), Duration: 1.0, Reward: 1793.5547172587053, Done: False
2024-07-21 21:31:09,750 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:09,750 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:12,598 - AirSimEnvLogger - INFO - Predictive model loss: 0.018155233934521675
2024-07-21 21:31:18,728 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.5375691526244, Velocity: -0.034570022085243984, Movement: 0.18172641149255453, Collision: 0, Height: -1.0, Movement Penalty: -0.04196791703763643, Smoothness: -0.0, Curiosity: 1882.50634765625, Exploration: 0.07963524440866583, Total: 1837.4880953467211
2024-07-21 21:31:18,760 - AirSimEnvLogger - INFO - Action: [0.20983959 0.20983959 0.20983959 0.20983959], Velocity: (0.20983958518818213, 0.20983958518818213, 0.20983958518818213), Duration: 1.0, Reward: 1837.4880953467211, Done: False
2024-07-21 21:31:18,821 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:18,821 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:21,638 - AirSimEnvLogger - INFO - Predictive model loss: 0.34125620126724243
2024-07-21 21:31:27,544 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.84232268105812, Velocity: -0.034515331305666405, Movement: 0.3993865780000041, Collision: 0, Height: -1.0, Movement Penalty: -0.09223437932761033, Smoothness: -0.0, Curiosity: 1916.377685546875, Exploration: 0.3679334375921682, Total: 1871.1263329545889
2024-07-21 21:31:27,654 - AirSimEnvLogger - INFO - Action: [0.4611719 0.4611719 0.4611719 0.4611719], Velocity: (0.46117189663805164, 0.46117189663805164, 0.46117189663805164), Duration: 1.0, Reward: 1871.1263329545889, Done: False
2024-07-21 21:31:27,733 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:27,733 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:30,661 - AirSimEnvLogger - INFO - Predictive model loss: 0.4565280079841614
2024-07-21 21:31:36,710 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.914600293481016, Velocity: -0.04496738563218307, Movement: 0.25394511766660577, Collision: 0, Height: -1.0, Movement Penalty: -0.05864611281768242, Smoothness: -0.0, Curiosity: 1920.96484375, Exploration: 0.14471772417341702, Total: 1875.58526105458
2024-07-21 21:31:36,819 - AirSimEnvLogger - INFO - Action: [0.29323056 0.29323056 0.29323056 0.29323056], Velocity: (0.2932305640884121, 0.2932305640884121, 0.2932305640884121), Duration: 1.0, Reward: 1875.58526105458, Done: False
2024-07-21 21:31:36,882 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:36,882 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:39,661 - AirSimEnvLogger - INFO - Predictive model loss: 0.07051575183868408
2024-07-21 21:31:45,642 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.60126172499415, Velocity: -0.03424024755716054, Movement: 0.19211069504371553, Collision: 0, Height: -1.0, Movement Penalty: -0.04436606459907811, Smoothness: -0.0, Curiosity: 1883.9573974609375, Exploration: 0.195845632882056, Total: 1838.902468671757
2024-07-21 21:31:45,798 - AirSimEnvLogger - INFO - Action: [-0.22183032 -0.22183032 -0.22183032 -0.22183032], Velocity: (-0.22183032299539052, -0.22183032299539052, -0.22183032299539052), Duration: 1.0, Reward: 1838.902468671757, Done: False
2024-07-21 21:31:45,875 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:45,875 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:48,638 - AirSimEnvLogger - INFO - Predictive model loss: 0.13991732895374298
2024-07-21 21:31:54,349 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.00750333727144, Velocity: -0.05396949342603553, Movement: 0.2898891157477802, Collision: 0, Height: -1.0, Movement Penalty: -0.06694702360484939, Smoothness: -0.0, Curiosity: 1927.985107421875, Exploration: 0.030585635198934792, Total: 1882.486352795759
2024-07-21 21:31:54,492 - AirSimEnvLogger - INFO - Action: [0.33473512 0.33473512 0.33473512 0.33473512], Velocity: (0.33473511802424694, 0.33473511802424694, 0.33473511802424694), Duration: 1.0, Reward: 1882.486352795759, Done: False
2024-07-21 21:31:54,522 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:31:54,522 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:31:57,397 - AirSimEnvLogger - INFO - Predictive model loss: 0.11178134381771088
2024-07-21 21:32:03,501 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.839271168955875, Velocity: -0.0034736919904389285, Movement: 0.01900779523574732, Collision: 0, Height: -1.0, Movement Penalty: -0.004389662278424001, Smoothness: -0.0, Curiosity: 1908.2880859375, Exploration: 0.09761595677118816, Total: 1862.9713853295962
2024-07-21 21:32:03,627 - AirSimEnvLogger - INFO - Action: [-0.02194831 -0.02194831 -0.02194831 -0.02194831], Velocity: (-0.02194831139212, -0.02194831139212, -0.02194831139212), Duration: 1.0, Reward: 1862.9713853295962, Done: False
2024-07-21 21:32:03,689 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:03,689 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:32:06,446 - AirSimEnvLogger - INFO - Predictive model loss: 0.2785302996635437
2024-07-21 21:32:12,415 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.2757529139116, Velocity: -0.04930207990578907, Movement: 0.4069935712471015, Collision: 0, Height: -1.0, Movement Penalty: -0.09399113916718449, Smoothness: -0.0, Curiosity: 1955.300048828125, Exploration: 0.10076306224954523, Total: 1909.5525247409312
2024-07-21 21:32:12,541 - AirSimEnvLogger - INFO - Action: [0.4699557 0.4699557 0.4699557 0.4699557], Velocity: (0.4699556958359224, 0.4699556958359224, 0.4699556958359224), Duration: 1.0, Reward: 1909.5525247409312, Done: False
2024-07-21 21:32:12,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:12,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:32:15,497 - AirSimEnvLogger - INFO - Predictive model loss: 0.053578946739435196
2024-07-21 21:32:21,356 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.84805382147412, Velocity: -0.03592848770913401, Movement: 0.2241821883167282, Collision: 0, Height: -1.0, Movement Penalty: -0.051772658708872966, Smoothness: -0.0, Curiosity: 1905.8759765625, Exploration: 0.05658654296083374, Total: 1860.5428439741459
2024-07-21 21:32:21,496 - AirSimEnvLogger - INFO - Action: [-0.25886329 -0.25886329 -0.25886329 -0.25886329], Velocity: (-0.2588632935443648, -0.2588632935443648, -0.2588632935443648), Duration: 1.0, Reward: 1860.5428439741459, Done: False
2024-07-21 21:32:21,560 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:21,560 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:32:24,289 - AirSimEnvLogger - INFO - Predictive model loss: 0.07213573157787323
2024-07-21 21:32:30,243 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.533705116714344, Velocity: -0.07296969983564511, Movement: 0.41025122788952095, Collision: 0, Height: -1.0, Movement Penalty: -0.09474346274295578, Smoothness: -0.0, Curiosity: 1872.827880859375, Exploration: 0.37725774696352193, Total: 1827.8837114950118
2024-07-21 21:32:30,305 - AirSimEnvLogger - INFO - Action: [-0.47371731 -0.47371731 -0.47371731 -0.47371731], Velocity: (-0.4737173137147789, -0.4737173137147789, -0.4737173137147789), Duration: 1.0, Reward: 1827.8837114950118, Done: False
2024-07-21 21:32:30,368 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:30,368 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:32:33,198 - AirSimEnvLogger - INFO - Predictive model loss: 0.017852943390607834
2024-07-21 21:32:39,289 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.05335123031225, Velocity: -0.07094526881401535, Movement: 0.6349670941824754, Collision: 0, Height: -1.0, Movement Penalty: -0.14663936910112266, Smoothness: -0.0, Curiosity: 1826.1766357421875, Exploration: 0.32601166180984226, Total: 1781.7067483758449
2024-07-21 21:32:39,351 - AirSimEnvLogger - INFO - Action: [-0.73319685 -0.73319685 -0.73319685 -0.73319685], Velocity: (-0.7331968455056133, -0.7331968455056133, -0.7331968455056133), Duration: 1.0, Reward: 1781.7067483758449, Done: False
2024-07-21 21:32:39,382 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:39,382 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:32:42,297 - AirSimEnvLogger - INFO - Predictive model loss: 0.11293944716453552
2024-07-21 21:32:48,321 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.48592015839701, Velocity: -0.02086217217515078, Movement: 0.11464762899639469, Collision: 0, Height: -1.0, Movement Penalty: -0.02647673578520833, Smoothness: -0.0, Curiosity: 1875.6435546875, Exploration: 0.13014346946668215, Total: 1830.6882936100296
2024-07-21 21:32:48,447 - AirSimEnvLogger - INFO - Action: [0.13238368 0.13238368 0.13238368 0.13238368], Velocity: (0.13238367892604164, 0.13238367892604164, 0.13238367892604164), Duration: 1.0, Reward: 1830.6882936100296, Done: False
2024-07-21 21:32:48,541 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:48,541 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:32:51,386 - AirSimEnvLogger - INFO - Predictive model loss: 0.7516632676124573
2024-07-21 21:32:57,548 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.01393466669084, Velocity: -0.05942598133743378, Movement: 0.37318437850731406, Collision: 0, Height: -1.0, Movement Penalty: -0.0861832405554244, Smoothness: -0.0, Curiosity: 1827.874755859375, Exploration: 0.09889320573514365, Total: 1783.386778224583
2024-07-21 21:32:57,689 - AirSimEnvLogger - INFO - Action: [-0.4309162 -0.4309162 -0.4309162 -0.4309162], Velocity: (-0.43091620277712195, -0.43091620277712195, -0.43091620277712195), Duration: 1.0, Reward: 1783.386778224583, Done: False
2024-07-21 21:32:57,752 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:32:57,752 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:00,599 - AirSimEnvLogger - INFO - Predictive model loss: 0.14982368052005768
2024-07-21 21:33:06,646 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.06422503225196, Velocity: -0.01900556349215759, Movement: 0.1307637637515227, Collision: 0, Height: -1.0, Movement Penalty: -0.030198597680876106, Smoothness: -0.0, Curiosity: 1834.775634765625, Exploration: 0.15414090264076838, Total: 1790.248169212716
2024-07-21 21:33:06,724 - AirSimEnvLogger - INFO - Action: [-0.15099299 -0.15099299 -0.15099299 -0.15099299], Velocity: (-0.15099298840438052, -0.15099298840438052, -0.15099298840438052), Duration: 1.0, Reward: 1790.248169212716, Done: False
2024-07-21 21:33:06,817 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:33:06,817 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:09,742 - AirSimEnvLogger - INFO - Predictive model loss: 0.04464932903647423
2024-07-21 21:33:15,756 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.20509061263964, Velocity: -0.012798558589532882, Movement: 0.08423886963570468, Collision: 0, Height: -1.0, Movement Penalty: -0.019454133624161558, Smoothness: -0.0, Curiosity: 1851.9173583984375, Exploration: 0.1527535092802146, Total: 1807.2481876120103
2024-07-21 21:33:15,787 - AirSimEnvLogger - INFO - Action: [0.09727067 0.09727067 0.09727067 0.09727067], Velocity: (0.09727066812080779, 0.09727066812080779, 0.09727066812080779), Duration: 1.0, Reward: 1807.2481876120103, Done: False
2024-07-21 21:33:15,817 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:33:15,817 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:18,756 - AirSimEnvLogger - INFO - Predictive model loss: 0.010739879682660103
2024-07-21 21:33:24,635 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.640196942473935, Velocity: -0.07285360535556654, Movement: 0.4727648831074743, Collision: 0, Height: -1.0, Movement Penalty: -0.10918037301020089, Smoothness: -0.0, Curiosity: 1899.855224609375, Exploration: 0.297466737782544, Total: 1854.7877565531855
2024-07-21 21:33:24,777 - AirSimEnvLogger - INFO - Action: [0.54590187 0.54590187 0.54590187 0.54590187], Velocity: (0.5459018650510045, 0.5459018650510045, 0.5459018650510045), Duration: 1.0, Reward: 1854.7877565531855, Done: False
2024-07-21 21:33:24,808 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:33:24,808 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:27,705 - AirSimEnvLogger - INFO - Predictive model loss: 0.08027820289134979
2024-07-21 21:33:33,712 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.21469845282657, Velocity: -0.016553159571989485, Movement: 0.19610650676938587, Collision: 0, Height: -1.0, Movement Penalty: -0.04528885778925684, Smoothness: -0.0, Curiosity: 1850.2337646484375, Exploration: 0.05164179725490572, Total: 1805.5340989750755
2024-07-21 21:33:33,807 - AirSimEnvLogger - INFO - Action: [-0.22644429 -0.22644429 -0.22644429 -0.22644429], Velocity: (-0.2264442889462842, -0.2264442889462842, -0.2264442889462842), Duration: 1.0, Reward: 1805.5340989750755, Done: False
2024-07-21 21:33:33,869 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:33:33,869 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:36,822 - AirSimEnvLogger - INFO - Predictive model loss: 0.6208125948905945
2024-07-21 21:33:42,773 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.22056804381196, Velocity: -0.008498308404277392, Movement: 0.09968499115302917, Collision: 0, Height: -1.0, Movement Penalty: -0.023021262590546744, Smoothness: -0.0, Curiosity: 1850.1199951171875, Exploration: 0.2430098154080062, Total: 1805.456914767545
2024-07-21 21:33:42,884 - AirSimEnvLogger - INFO - Action: [-0.11510631 -0.11510631 -0.11510631 -0.11510631], Velocity: (-0.11510631295273371, -0.11510631295273371, -0.11510631295273371), Duration: 1.0, Reward: 1805.456914767545, Done: False
2024-07-21 21:33:42,932 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:33:42,932 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:45,762 - AirSimEnvLogger - INFO - Predictive model loss: 0.4287910461425781
2024-07-21 21:33:51,697 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.648700531229565, Velocity: -0.05632688746376195, Movement: 0.3690787756551584, Collision: 0, Height: -1.0, Movement Penalty: -0.08523509219067328, Smoothness: -0.0, Curiosity: 1897.5888671875, Exploration: 0.21491239121856523, Total: 1852.4930173000007
2024-07-21 21:33:51,761 - AirSimEnvLogger - INFO - Action: [0.42617546 0.42617546 0.42617546 0.42617546], Velocity: (0.4261754609533664, 0.4261754609533664, 0.4261754609533664), Duration: 1.0, Reward: 1852.4930173000007, Done: False
2024-07-21 21:33:51,809 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:33:51,809 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:33:54,672 - AirSimEnvLogger - INFO - Predictive model loss: 0.028925137594342232
2024-07-21 21:34:00,760 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.219573589532985, Velocity: -0.037536199479710236, Movement: 0.23938615221694606, Collision: 0, Height: -1.0, Movement Penalty: -0.05528386376908902, Smoothness: -0.0, Curiosity: 1849.4488525390625, Exploration: 0.022055729014662833, Total: 1804.7364702766808
2024-07-21 21:34:00,932 - AirSimEnvLogger - INFO - Action: [-0.27641932 -0.27641932 -0.27641932 -0.27641932], Velocity: (-0.2764193188454451, -0.2764193188454451, -0.2764193188454451), Duration: 1.0, Reward: 1804.7364702766808, Done: False
2024-07-21 21:34:00,995 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:00,995 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:03,943 - AirSimEnvLogger - INFO - Predictive model loss: 0.01206566859036684
2024-07-21 21:34:09,622 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.655290968091236, Velocity: -0.04637417781059425, Movement: 0.29656640272739676, Collision: 0, Height: -1.0, Movement Penalty: -0.06848907697890459, Smoothness: -0.0, Curiosity: 1896.32861328125, Exploration: 0.024677996304699513, Total: 1851.1816355924202
2024-07-21 21:34:09,732 - AirSimEnvLogger - INFO - Action: [0.34244538 0.34244538 0.34244538 0.34244538], Velocity: (0.342445384894523, 0.342445384894523, 0.342445384894523), Duration: 1.0, Reward: 1851.1816355924202, Done: False
2024-07-21 21:34:09,811 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:09,811 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:12,543 - AirSimEnvLogger - INFO - Predictive model loss: 0.2897026538848877
2024-07-21 21:34:18,264 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.06904207529641, Velocity: -0.06665040771569566, Movement: 0.5606987321478581, Collision: 0, Height: -1.0, Movement Penalty: -0.12948782557593913, Smoothness: -0.0, Curiosity: 1941.284912109375, Exploration: 0.387957161684701, Total: 1895.8121890502073
2024-07-21 21:34:18,391 - AirSimEnvLogger - INFO - Action: [0.64743913 0.64743913 0.64743913 0.64743913], Velocity: (0.6474391278796956, 0.6474391278796956, 0.6474391278796956), Duration: 1.0, Reward: 1895.8121890502073, Done: False
2024-07-21 21:34:18,484 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:18,484 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:21,279 - AirSimEnvLogger - INFO - Predictive model loss: 0.529260516166687
2024-07-21 21:34:27,470 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.506101152739795, Velocity: -0.09839975522104899, Movement: 0.7056661450869935, Collision: 0, Height: -1.0, Movement Penalty: -0.1629666155295925, Smoothness: -0.0, Curiosity: 1985.9833984375, Exploration: 0.3334660223554518, Total: 1940.0614644469272
2024-07-21 21:34:27,516 - AirSimEnvLogger - INFO - Action: [0.81483308 0.81483308 0.81483308 0.81483308], Velocity: (0.8148330776479624, 0.8148330776479624, 0.8148330776479624), Duration: 1.0, Reward: 1940.0614644469272, Done: False
2024-07-21 21:34:27,564 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:27,564 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:30,335 - AirSimEnvLogger - INFO - Predictive model loss: 0.14135564863681793
2024-07-21 21:34:36,325 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.13846507461685, Velocity: -0.009242952035103618, Movement: 0.05113733521349027, Collision: 0, Height: -1.0, Movement Penalty: -0.011809661700459495, Smoothness: -0.0, Curiosity: 1936.3184814453125, Exploration: 0.15658774465324823, Total: 1890.716361652842
2024-07-21 21:34:36,326 - AirSimEnvLogger - INFO - Action: [-0.05904831 -0.05904831 -0.05904831 -0.05904831], Velocity: (-0.059048308502297475, -0.059048308502297475, -0.059048308502297475), Duration: 1.0, Reward: 1890.716361652842, Done: False
2024-07-21 21:34:36,356 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:36,356 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:39,229 - AirSimEnvLogger - INFO - Predictive model loss: 0.08529006689786911
2024-07-21 21:34:45,050 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.70679090521481, Velocity: -0.04077700141210667, Movement: 0.45814299398173686, Collision: 0, Height: -1.0, Movement Penalty: -0.10580359236107875, Smoothness: -0.0, Curiosity: 1888.276123046875, Exploration: 0.48965068642237153, Total: 1843.189112322373
2024-07-21 21:34:45,176 - AirSimEnvLogger - INFO - Action: [-0.52901796 -0.52901796 -0.52901796 -0.52901796], Velocity: (-0.5290179618053937, -0.5290179618053937, -0.5290179618053937), Duration: 1.0, Reward: 1843.189112322373, Done: False
2024-07-21 21:34:45,239 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:45,239 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:48,113 - AirSimEnvLogger - INFO - Predictive model loss: 0.5820114016532898
2024-07-21 21:34:54,125 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.86977239316624, Velocity: -0.00947930439893624, Movement: 0.07014082124788783, Collision: 0, Height: -1.0, Movement Penalty: -0.016198328811459783, Smoothness: -0.0, Curiosity: 1907.19140625, Exploration: 0.11712274443518113, Total: 1861.849344708183
2024-07-21 21:34:54,252 - AirSimEnvLogger - INFO - Action: [-0.08099164 -0.08099164 -0.08099164 -0.08099164], Velocity: (-0.08099164405729892, -0.08099164405729892, -0.08099164405729892), Duration: 1.0, Reward: 1861.849344708183, Done: False
2024-07-21 21:34:54,315 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:34:54,315 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:34:57,042 - AirSimEnvLogger - INFO - Predictive model loss: 0.14630068838596344
2024-07-21 21:35:02,913 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.53951772668395, Velocity: -0.04050002859632032, Movement: 0.3278852795261395, Collision: 0, Height: -1.0, Movement Penalty: -0.07572186175909294, Smoothness: -0.0, Curiosity: 1874.4677734375, Exploration: 0.05323627296411893, Total: 1829.4444930589923
2024-07-21 21:35:03,022 - AirSimEnvLogger - INFO - Action: [-0.37860931 -0.37860931 -0.37860931 -0.37860931], Velocity: (-0.37860930879546467, -0.37860930879546467, -0.37860930879546467), Duration: 1.0, Reward: 1829.4444930589923, Done: False
2024-07-21 21:35:03,069 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:03,069 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:05,926 - AirSimEnvLogger - INFO - Predictive model loss: 0.12118250876665115
2024-07-21 21:35:11,881 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.95911686893805, Velocity: -0.03452581165996939, Movement: 0.24392759376095244, Collision: 0, Height: -1.0, Movement Penalty: -0.05633266476826543, Smoothness: -0.0, Curiosity: 1921.67333984375, Exploration: 0.10965944218849498, Total: 1876.2419755960916
2024-07-21 21:35:12,039 - AirSimEnvLogger - INFO - Action: [0.28166332 0.28166332 0.28166332 0.28166332], Velocity: (0.28166332384132714, 0.28166332384132714, 0.28166332384132714), Duration: 1.0, Reward: 1876.2419755960916, Done: False
2024-07-21 21:35:12,101 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:12,101 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:15,016 - AirSimEnvLogger - INFO - Predictive model loss: 0.13180753588676453
2024-07-21 21:35:21,115 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.1331483438678, Velocity: -0.037028834491478306, Movement: 0.296040650006098, Collision: 0, Height: -1.0, Movement Penalty: -0.06836765958883699, Smoothness: -0.0, Curiosity: 1940.5943603515625, Exploration: 0.3013999335101816, Total: 1895.0340929701592
2024-07-21 21:35:21,224 - AirSimEnvLogger - INFO - Action: [0.3418383 0.3418383 0.3418383 0.3418383], Velocity: (0.34183829794418497, 0.34183829794418497, 0.34183829794418497), Duration: 1.0, Reward: 1895.0340929701592, Done: False
2024-07-21 21:35:21,303 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:21,303 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:24,110 - AirSimEnvLogger - INFO - Predictive model loss: 0.19237449765205383
2024-07-21 21:35:29,753 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.57933958306726, Velocity: -0.0786259748407011, Movement: 0.5805146159068059, Collision: 0, Height: -1.0, Movement Penalty: -0.13406410790492265, Smoothness: -0.0, Curiosity: 1988.249755859375, Exploration: 0.25452654097913685, Total: 1942.2353347755234
2024-07-21 21:35:29,830 - AirSimEnvLogger - INFO - Action: [0.67032054 0.67032054 0.67032054 0.67032054], Velocity: (0.6703205395246132, 0.6703205395246132, 0.6703205395246132), Duration: 1.0, Reward: 1942.2353347755234, Done: False
2024-07-21 21:35:29,894 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:29,894 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:32,750 - AirSimEnvLogger - INFO - Predictive model loss: 0.32204440236091614
2024-07-21 21:35:38,638 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -46.021883390551274, Velocity: -0.07702390201794754, Movement: 0.7207219085980903, Collision: 0, Height: -1.0, Movement Penalty: -0.16644359517598734, Smoothness: -0.0, Curiosity: 2033.5889892578125, Exploration: 0.3356014136482686, Total: 1987.1542710718143
2024-07-21 21:35:38,796 - AirSimEnvLogger - INFO - Action: [0.83221798 0.83221798 0.83221798 0.83221798], Velocity: (0.8322179758799366, 0.8322179758799366, 0.8322179758799366), Duration: 1.0, Reward: 1987.1542710718143, Done: False
2024-07-21 21:35:38,876 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:38,876 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:41,692 - AirSimEnvLogger - INFO - Predictive model loss: 0.031440459191799164
2024-07-21 21:35:47,484 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.62607513196171, Velocity: -0.00935594380061977, Movement: 0.07082275253704612, Collision: 0, Height: -1.0, Movement Penalty: -0.016355814096805532, Smoothness: -0.0, Curiosity: 1980.1065673828125, Exploration: 0.171776169065145, Total: 1934.0208024536626
2024-07-21 21:35:47,485 - AirSimEnvLogger - INFO - Action: [-0.08177907 -0.08177907 -0.08177907 -0.08177907], Velocity: (-0.08177907048402766, -0.08177907048402766, -0.08177907048402766), Duration: 1.0, Reward: 1934.0208024536626, Done: False
2024-07-21 21:35:47,547 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:47,547 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:50,483 - AirSimEnvLogger - INFO - Predictive model loss: 0.2880696654319763
2024-07-21 21:35:56,164 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.741544619929854, Velocity: -0.004871286801907961, Movement: 0.0550688981769871, Collision: 0, Height: -1.0, Movement Penalty: -0.012717617274583837, Smoothness: -0.0, Curiosity: 1990.3629150390625, Exploration: 0.2567667406341987, Total: 1944.1812904779058
2024-07-21 21:35:56,257 - AirSimEnvLogger - INFO - Action: [0.06358809 0.06358809 0.06358809 0.06358809], Velocity: (0.06358808637291918, 0.06358808637291918, 0.06358808637291918), Duration: 1.0, Reward: 1944.1812904779058, Done: False
2024-07-21 21:35:56,337 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:35:56,337 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:35:59,155 - AirSimEnvLogger - INFO - Predictive model loss: 0.294144868850708
2024-07-21 21:36:05,066 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.28913414273027, Velocity: -0.0768933789293807, Movement: 0.4036515289896064, Collision: 0, Height: -1.0, Movement Penalty: -0.0932193275683813, Smoothness: -0.0, Curiosity: 1941.418701171875, Exploration: 0.18747798318273753, Total: 1895.6748991776583
2024-07-21 21:36:05,129 - AirSimEnvLogger - INFO - Action: [-0.46609664 -0.46609664 -0.46609664 -0.46609664], Velocity: (-0.4660966378419065, -0.4660966378419065, -0.4660966378419065), Duration: 1.0, Reward: 1895.6748991776583, Done: False
2024-07-21 21:36:05,175 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:05,175 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:36:08,022 - AirSimEnvLogger - INFO - Predictive model loss: 0.409363716840744
2024-07-21 21:36:13,832 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.742585258040755, Velocity: -0.041314942243461036, Movement: 0.23102348540150192, Collision: 0, Height: -1.0, Movement Penalty: -0.05335258859427309, Smoothness: -0.0, Curiosity: 1992.283447265625, Exploration: 0.0316334771188833, Total: 1946.0496732067563
2024-07-21 21:36:13,957 - AirSimEnvLogger - INFO - Action: [0.26676294 0.26676294 0.26676294 0.26676294], Velocity: (0.26676294297136544, 0.26676294297136544, 0.26676294297136544), Duration: 1.0, Reward: 1946.0496732067563, Done: False
2024-07-21 21:36:14,018 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:14,018 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:36:16,907 - AirSimEnvLogger - INFO - Predictive model loss: 0.004704680293798447
2024-07-21 21:36:23,046 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.28856362446915, Velocity: -0.04250461131807851, Movement: 0.311554486279004, Collision: 0, Height: -1.0, Movement Penalty: -0.0719504266081674, Smoothness: -0.0, Curiosity: 1943.22900390625, Exploration: 0.034091675842648626, Total: 1897.4516733208677
2024-07-21 21:36:23,126 - AirSimEnvLogger - INFO - Action: [-0.35975213 -0.35975213 -0.35975213 -0.35975213], Velocity: (-0.35975213304083703, -0.35975213304083703, -0.35975213304083703), Duration: 1.0, Reward: 1897.4516733208677, Done: False
2024-07-21 21:36:23,220 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:23,220 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:36:26,151 - AirSimEnvLogger - INFO - Predictive model loss: 0.009905397891998291
2024-07-21 21:36:32,040 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -44.83637002772484, Velocity: -0.07351153701441039, Movement: 0.5695233862163123, Collision: 0, Height: -1.0, Movement Penalty: -0.13152579213671003, Smoothness: -0.0, Curiosity: 1896.9779052734375, Exploration: 0.40348453632529274, Total: 1851.740955913368
2024-07-21 21:36:32,150 - AirSimEnvLogger - INFO - Action: [-0.65762896 -0.65762896 -0.65762896 -0.65762896], Velocity: (-0.6576289606835501, -0.6576289606835501, -0.6576289606835501), Duration: 1.0, Reward: 1851.740955913368, Done: False
2024-07-21 21:36:32,228 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:32,228 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:36:35,146 - AirSimEnvLogger - INFO - Predictive model loss: 0.040716689079999924
2024-07-21 21:36:41,094 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.24580064141775, Velocity: -0.010849690285023904, Movement: 0.11451888380632799, Collision: 0, Height: -1.0, Movement Penalty: -0.02644700335715158, Smoothness: -0.0, Curiosity: 1944.0042724609375, Exploration: 0.08846171494853007, Total: 1898.2805421869837
2024-07-21 21:36:41,253 - AirSimEnvLogger - INFO - Action: [0.13223502 0.13223502 0.13223502 0.13223502], Velocity: (0.1322350167857579, 0.1322350167857579, 0.1322350167857579), Duration: 1.0, Reward: 1898.2805421869837, Done: False
2024-07-21 21:36:41,315 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:41,315 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:36:44,222 - AirSimEnvLogger - INFO - Predictive model loss: 0.6270490884780884
2024-07-21 21:36:50,124 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.675374880313804, Velocity: -0.05100457652316799, Movement: 0.4889471837981859, Collision: 0, Height: -1.0, Movement Penalty: -0.11291751527415683, Smoothness: -0.0, Curiosity: 1993.4635009765625, Exploration: 0.4631284691959997, Total: 1947.401539034606
2024-07-21 21:36:50,140 - AirSimEnvLogger - INFO - Action: [0.56458758 0.56458758 0.56458758 0.56458758], Velocity: (0.5645875763707842, 0.5645875763707842, 0.5645875763707842), Duration: 1.0, Reward: 1947.401539034606, Done: False
2024-07-21 21:36:50,187 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:50,187 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:36:53,104 - AirSimEnvLogger - INFO - Predictive model loss: 0.5203887224197388
2024-07-21 21:36:58,110 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -46.09418606584725, Velocity: -0.02655225630010143, Movement: 0.6619364676655636, Collision: 0, Height: -1.0, Movement Penalty: -0.15286767911725727, Smoothness: -0.0, Curiosity: 2038.16552734375, Exploration: 0.3720818191305956, Total: 1991.6705021365874
2024-07-21 21:36:58,204 - AirSimEnvLogger - INFO - Action: [0.7643384 0.7643384 0.7643384 0.7643384], Velocity: (0.7643383955862862, 0.7643383955862862, 0.7643383955862862), Duration: 1.0, Reward: 1991.6705021365874, Done: False
2024-07-21 21:36:58,282 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:36:58,282 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:00,862 - AirSimEnvLogger - INFO - Predictive model loss: 0.18556083738803864
2024-07-21 21:37:06,874 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -46.35169638627286, Velocity: -0.08586942234462465, Movement: 0.5601202666626923, Collision: 0, Height: -1.0, Movement Penalty: -0.12935423469450816, Smoothness: -0.0, Curiosity: 2060.295654296875, Exploration: 0.22659581266217466, Total: 2013.5012277252654
2024-07-21 21:37:06,968 - AirSimEnvLogger - INFO - Action: [0.64677117 0.64677117 0.64677117 0.64677117], Velocity: (0.6467711734725408, 0.6467711734725408, 0.6467711734725408), Duration: 1.0, Reward: 2013.5012277252654, Done: False
2024-07-21 21:37:07,047 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:37:07,047 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:09,911 - AirSimEnvLogger - INFO - Predictive model loss: 0.15769711136817932
2024-07-21 21:37:15,871 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -45.96106286889247, Velocity: -0.02586357940885197, Movement: 0.14475459795954507, Collision: 0, Height: -1.0, Movement Penalty: -0.03342964243935176, Smoothness: -0.0, Curiosity: 2008.9124755859375, Exploration: 0.2444148208468172, Total: 1962.5085925903936
2024-07-21 21:37:15,965 - AirSimEnvLogger - INFO - Action: [-0.16714821 -0.16714821 -0.16714821 -0.16714821], Velocity: (-0.1671482121967588, -0.1671482121967588, -0.1671482121967588), Duration: 1.0, Reward: 1962.5085925903936, Done: False
2024-07-21 21:37:16,075 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:37:16,075 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:18,776 - AirSimEnvLogger - INFO - Predictive model loss: 0.8935858011245728
2024-07-21 21:37:24,238 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -46.32479418080033, Velocity: -0.022884551674052265, Movement: 0.26328971232827214, Collision: 0, Height: -1.0, Movement Penalty: -0.06080415451503482, Smoothness: -0.0, Curiosity: 2047.8773193359375, Exploration: 0.1300952909902418, Total: 2001.086617099366
2024-07-21 21:37:24,397 - AirSimEnvLogger - INFO - Action: [0.30402077 0.30402077 0.30402077 0.30402077], Velocity: (0.30402077257517407, 0.30402077257517407, 0.30402077257517407), Duration: 1.0, Reward: 2001.086617099366, Done: False
2024-07-21 21:37:24,460 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:37:24,460 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:27,358 - AirSimEnvLogger - INFO - Predictive model loss: 0.6015000343322754
2024-07-21 21:37:33,240 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -46.69429884612439, Velocity: -0.0784688519345259, Movement: 0.49386307369621046, Collision: 0, Height: -1.0, Movement Penalty: -0.11405279141652924, Smoothness: -0.0, Curiosity: 2088.68798828125, Exploration: 0.3111309382936933, Total: 2041.569517100349
2024-07-21 21:37:33,351 - AirSimEnvLogger - INFO - Action: [0.57026396 0.57026396 0.57026396 0.57026396], Velocity: (0.5702639570826462, 0.5702639570826462, 0.5702639570826462), Duration: 1.0, Reward: 2041.569517100349, Done: False
2024-07-21 21:37:33,382 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:37:33,382 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:36,280 - AirSimEnvLogger - INFO - Predictive model loss: 0.31424832344055176
2024-07-21 21:37:42,218 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -47.1307472975516, Velocity: -0.11264631737090448, Movement: 0.6660783062778364, Collision: 0, Height: -1.0, Movement Penalty: -0.15382419577235154, Smoothness: -0.0, Curiosity: 2134.772216796875, Exploration: 0.312931097603787, Total: 2087.218517885028
2024-07-21 21:37:42,297 - AirSimEnvLogger - INFO - Action: [0.76912098 0.76912098 0.76912098 0.76912098], Velocity: (0.7691209788617577, 0.7691209788617577, 0.7691209788617577), Duration: 1.0, Reward: 2087.218517885028, Done: False
2024-07-21 21:37:42,360 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:37:42,360 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:45,174 - AirSimEnvLogger - INFO - Predictive model loss: 0.027715278789401054
2024-07-21 21:37:51,163 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -46.775256930840335, Velocity: -0.008341574255974787, Movement: 0.04389854508821635, Collision: 0, Height: -1.0, Movement Penalty: -0.010137934729485855, Smoothness: -0.0, Curiosity: 2086.486083984375, Exploration: 0.13665518717332825, Total: 2039.2425004181066
2024-07-21 21:37:51,289 - AirSimEnvLogger - INFO - Action: [-0.05068967 -0.05068967 -0.05068967 -0.05068967], Velocity: (-0.05068967364742927, -0.05068967364742927, -0.05068967364742927), Duration: 1.0, Reward: 2039.2425004181066, Done: False
2024-07-21 21:37:51,337 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:37:51,337 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:37:54,189 - AirSimEnvLogger - INFO - Predictive model loss: 0.016064174473285675
2024-07-21 21:37:59,818 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -47.22496763570444, Velocity: -0.04075542234566348, Movement: 0.39314880152875153, Collision: 0, Height: -1.0, Movement Penalty: -0.09079382655768138, Smoothness: -0.0, Curiosity: 2135.224365234375, Exploration: 0.10319025829726118, Total: 2087.52869973478
2024-07-21 21:37:59,927 - AirSimEnvLogger - INFO - Action: [0.45396913 0.45396913 0.45396913 0.45396913], Velocity: (0.4539691327884069, 0.4539691327884069, 0.4539691327884069), Duration: 1.0, Reward: 2087.52869973478, Done: False
2024-07-21 21:38:00,005 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:00,005 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:02,831 - AirSimEnvLogger - INFO - Predictive model loss: 0.3835124373435974
2024-07-21 21:38:08,673 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -47.14704576948854, Velocity: -0.01987630468166707, Movement: 0.11370953624977256, Collision: 0, Height: -1.0, Movement Penalty: -0.02626009254529348, Smoothness: -0.0, Curiosity: 2123.374267578125, Exploration: 0.12547741215166985, Total: 2075.7568832717657
2024-07-21 21:38:08,783 - AirSimEnvLogger - INFO - Action: [0.13130046 0.13130046 0.13130046 0.13130046], Velocity: (0.1313004627264674, 0.1313004627264674, 0.1313004627264674), Duration: 1.0, Reward: 2075.7568832717657, Done: False
2024-07-21 21:38:08,909 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:08,909 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:11,828 - AirSimEnvLogger - INFO - Predictive model loss: 0.30530381202697754
2024-07-21 21:38:17,516 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -47.475693916479514, Velocity: -0.049720613755950924, Movement: 0.3604866851887106, Collision: 0, Height: -1.0, Movement Penalty: -0.08325083389319118, Smoothness: -0.0, Curiosity: 2158.710693359375, Exploration: 0.07614916571425524, Total: 2110.756384408614
2024-07-21 21:38:17,532 - AirSimEnvLogger - INFO - Action: [0.41625417 0.41625417 0.41625417 0.41625417], Velocity: (0.4162541694659559, 0.4162541694659559, 0.4162541694659559), Duration: 1.0, Reward: 2110.756384408614, Done: False
2024-07-21 21:38:17,624 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:17,624 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:20,464 - AirSimEnvLogger - INFO - Predictive model loss: 0.330720454454422
2024-07-21 21:38:26,241 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -47.905941769232044, Velocity: -0.04489305189104413, Movement: 0.5972173593280435, Collision: 0, Height: -1.0, Movement Penalty: -0.13792144126910535, Smoothness: -0.0, Curiosity: 2206.2333984375, Exploration: 0.29478938864017445, Total: 2157.9054186321373
2024-07-21 21:38:26,333 - AirSimEnvLogger - INFO - Action: [0.68960721 0.68960721 0.68960721 0.68960721], Velocity: (0.6896072063455267, 0.6896072063455267, 0.6896072063455267), Duration: 1.0, Reward: 2157.9054186321373, Done: False
2024-07-21 21:38:26,379 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:26,379 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:29,305 - AirSimEnvLogger - INFO - Predictive model loss: 0.11742039024829865
2024-07-21 21:38:35,028 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.327179254238445, Velocity: -0.03355382569634011, Movement: 0.6973355897197736, Collision: 0, Height: -1.0, Movement Penalty: -0.1610427561760871, Smoothness: -0.0, Curiosity: 2250.2353515625, Exploration: 0.30900630094683373, Total: 2201.4929939793824
2024-07-21 21:38:35,092 - AirSimEnvLogger - INFO - Action: [0.80521378 0.80521378 0.80521378 0.80521378], Velocity: (0.8052137808804355, 0.8052137808804355, 0.8052137808804355), Duration: 1.0, Reward: 2201.4929939793824, Done: False
2024-07-21 21:38:35,156 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:35,156 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:37,965 - AirSimEnvLogger - INFO - Predictive model loss: 0.04016684368252754
2024-07-21 21:38:44,066 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.784857833657874, Velocity: -0.10297175999576491, Movement: 0.7800965119276556, Collision: 0, Height: -1.0, Movement Penalty: -0.18015557246212804, Smoothness: -0.0, Curiosity: 2297.31884765625, Exploration: 0.2925380864717789, Total: 2248.11011213192
2024-07-21 21:38:44,193 - AirSimEnvLogger - INFO - Action: [0.90077786 0.90077786 0.90077786 0.90077786], Velocity: (0.9007778623106402, 0.9007778623106402, 0.9007778623106402), Duration: 1.0, Reward: 2248.11011213192, Done: False
2024-07-21 21:38:44,303 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:44,303 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:47,035 - AirSimEnvLogger - INFO - Predictive model loss: 0.354543000459671
2024-07-21 21:38:52,739 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.241434069851834, Velocity: -0.09382578186933213, Movement: 0.8059276114780769, Collision: 0, Height: -1.0, Movement Penalty: -0.18612100937368792, Smoothness: -0.0, Curiosity: 2343.10791015625, Exploration: 0.29145133470752677, Total: 2293.443896676114
2024-07-21 21:38:52,850 - AirSimEnvLogger - INFO - Action: [0.93060505 0.93060505 0.93060505 0.93060505], Velocity: (0.9306050468684396, 0.9306050468684396, 0.9306050468684396), Duration: 1.0, Reward: 2293.443896676114, Done: False
2024-07-21 21:38:52,929 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:38:52,929 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:38:55,665 - AirSimEnvLogger - INFO - Predictive model loss: 0.4181653559207916
2024-07-21 21:39:01,519 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.83386897727132, Velocity: -0.010846519000100075, Movement: 0.029782386551126645, Collision: 0, Height: -1.0, Movement Penalty: -0.006877947556960984, Smoothness: -0.0, Curiosity: 2283.896484375, Exploration: 0.23632507704794825, Total: 2234.6166160738885
2024-07-21 21:39:01,614 - AirSimEnvLogger - INFO - Action: [-0.03438974 -0.03438974 -0.03438974 -0.03438974], Velocity: (-0.03438973778480492, -0.03438973778480492, -0.03438973778480492), Duration: 1.0, Reward: 2234.6166160738885, Done: False
2024-07-21 21:39:01,677 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:01,677 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:04,586 - AirSimEnvLogger - INFO - Predictive model loss: 0.8507266640663147
2024-07-21 21:39:10,381 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.2506331872278, Velocity: -0.06404323067057122, Movement: 0.36018238307301476, Collision: 0, Height: -1.0, Movement Penalty: -0.0831805583298264, Smoothness: -0.0, Curiosity: 2329.73291015625, Exploration: 0.16956046672960118, Total: 2280.023706807768
2024-07-21 21:39:10,475 - AirSimEnvLogger - INFO - Action: [0.41590279 0.41590279 0.41590279 0.41590279], Velocity: (0.41590279164913196, 0.41590279164913196, 0.41590279164913196), Duration: 1.0, Reward: 2280.023706807768, Done: False
2024-07-21 21:39:10,570 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:10,570 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:13,556 - AirSimEnvLogger - INFO - Predictive model loss: 0.10761716961860657
2024-07-21 21:39:19,383 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.31984295490772, Velocity: -0.042477306971467975, Movement: 0.23644273182092307, Collision: 0, Height: -1.0, Movement Penalty: -0.05460410994589618, Smoothness: -0.0, Curiosity: 2335.054931640625, Exploration: 0.16983379598660453, Total: 2285.2757026555787
2024-07-21 21:39:19,509 - AirSimEnvLogger - INFO - Action: [0.27302055 0.27302055 0.27302055 0.27302055], Velocity: (0.27302054972948087, 0.27302054972948087, 0.27302054972948087), Duration: 1.0, Reward: 2285.2757026555787, Done: False
2024-07-21 21:39:19,556 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:19,556 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:22,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.022231897339224815
2024-07-21 21:39:28,219 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.700966870590385, Velocity: -0.07890739060365586, Movement: 0.4753805209162263, Collision: 0, Height: -1.0, Movement Penalty: -0.10978442868739512, Smoothness: -0.0, Curiosity: 2377.73876953125, Exploration: 0.15178856812228353, Total: 2327.576484350852
2024-07-21 21:39:28,328 - AirSimEnvLogger - INFO - Action: [0.54892214 0.54892214 0.54892214 0.54892214], Velocity: (0.5489221434369755, 0.5489221434369755, 0.5489221434369755), Duration: 1.0, Reward: 2327.576484350852, Done: False
2024-07-21 21:39:28,392 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:28,392 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:30,983 - AirSimEnvLogger - INFO - Predictive model loss: 0.3557904362678528
2024-07-21 21:39:36,479 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.67544172185531, Velocity: -0.018760660142285158, Movement: 0.2003213479284635, Collision: 0, Height: -1.0, Movement Penalty: -0.046262233660370834, Smoothness: -0.0, Curiosity: 2369.97998046875, Exploration: 0.10852432012270737, Total: 2319.8325371463884
2024-07-21 21:39:36,589 - AirSimEnvLogger - INFO - Action: [0.23131117 0.23131117 0.23131117 0.23131117], Velocity: (0.23131116830185416, 0.23131116830185416, 0.23131116830185416), Duration: 1.0, Reward: 2319.8325371463884, Done: False
2024-07-21 21:39:36,651 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:36,651 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:39,402 - AirSimEnvLogger - INFO - Predictive model loss: 0.3676685392856598
2024-07-21 21:39:45,053 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.5841535767015, Velocity: -0.0005325707265729169, Movement: 0.0027534180714647617, Collision: 0, Height: -1.0, Movement Penalty: -0.0006358746659007042, Smoothness: -0.0, Curiosity: 2355.95166015625, Exploration: 0.15326658509868166, Total: 2305.9027721782454
2024-07-21 21:39:45,054 - AirSimEnvLogger - INFO - Action: [-0.00317937 -0.00317937 -0.00317937 -0.00317937], Velocity: (-0.003179373329503521, -0.003179373329503521, -0.003179373329503521), Duration: 1.0, Reward: 2305.9027721782454, Done: False
2024-07-21 21:39:45,101 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:45,101 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:47,991 - AirSimEnvLogger - INFO - Predictive model loss: 0.16984260082244873
2024-07-21 21:39:53,996 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.37503333550867, Velocity: -0.02088613294501521, Movement: 0.20157878146093056, Collision: 0, Height: -1.0, Movement Penalty: -0.04655262549575401, Smoothness: -0.0, Curiosity: 2330.780029296875, Exploration: 0.18509449838279635, Total: 2280.9504237993565
2024-07-21 21:39:54,122 - AirSimEnvLogger - INFO - Action: [-0.23276313 -0.23276313 -0.23276313 -0.23276313], Velocity: (-0.23276312747877, -0.23276312747877, -0.23276312747877), Duration: 1.0, Reward: 2280.9504237993565, Done: False
2024-07-21 21:39:54,186 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:39:54,186 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:39:56,998 - AirSimEnvLogger - INFO - Predictive model loss: 0.018143022432923317
2024-07-21 21:40:02,842 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.6771064503338, Velocity: -0.0323260112056776, Movement: 0.18159356038796856, Collision: 0, Height: -1.0, Movement Penalty: -0.041937236389238484, Smoothness: -0.0, Curiosity: 2366.909912109375, Exploration: 0.045645949672631934, Total: 2316.744526106605
2024-07-21 21:40:02,934 - AirSimEnvLogger - INFO - Action: [0.20968618 0.20968618 0.20968618 0.20968618], Velocity: (0.2096861819461924, 0.2096861819461924, 0.2096861819461924), Duration: 1.0, Reward: 2316.744526106605, Done: False
2024-07-21 21:40:02,997 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:02,997 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:05,792 - AirSimEnvLogger - INFO - Predictive model loss: 0.005185508169233799
2024-07-21 21:40:11,629 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.48356489580444, Velocity: -0.009865965969596116, Movement: 0.08620015458199806, Collision: 0, Height: -1.0, Movement Penalty: -0.019907072980841573, Smoothness: -0.0, Curiosity: 2343.90966796875, Exploration: 0.05639373375085964, Total: 2293.9402015202627
2024-07-21 21:40:11,706 - AirSimEnvLogger - INFO - Action: [-0.09953536 -0.09953536 -0.09953536 -0.09953536], Velocity: (-0.09953536490420786, -0.09953536490420786, -0.09953536490420786), Duration: 1.0, Reward: 2293.9402015202627, Done: False
2024-07-21 21:40:11,785 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:11,785 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:14,644 - AirSimEnvLogger - INFO - Predictive model loss: 0.09379061311483383
2024-07-21 21:40:20,474 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.290789594326974, Velocity: -0.03608337126612022, Movement: 0.2225502244367337, Collision: 0, Height: -1.0, Movement Penalty: -0.051395772794703934, Smoothness: -0.0, Curiosity: 2321.837646484375, Exploration: 0.17966597999942244, Total: 2272.0900308733453
2024-07-21 21:40:20,537 - AirSimEnvLogger - INFO - Action: [-0.25697886 -0.25697886 -0.25697886 -0.25697886], Velocity: (-0.25697886397351966, -0.25697886397351966, -0.25697886397351966), Duration: 1.0, Reward: 2272.0900308733453, Done: False
2024-07-21 21:40:20,600 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:20,600 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:23,384 - AirSimEnvLogger - INFO - Predictive model loss: 0.21260404586791992
2024-07-21 21:40:29,382 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.74021500465721, Velocity: -0.04988307696603901, Movement: 0.3100956935195786, Collision: 0, Height: -1.0, Movement Penalty: -0.07161353285122896, Smoothness: -0.0, Curiosity: 2375.766357421875, Exploration: 0.13812685997528726, Total: 2325.5605299179824
2024-07-21 21:40:29,460 - AirSimEnvLogger - INFO - Action: [0.35806766 0.35806766 0.35806766 0.35806766], Velocity: (0.3580676642561448, 0.3580676642561448, 0.3580676642561448), Duration: 1.0, Reward: 2325.5605299179824, Done: False
2024-07-21 21:40:29,523 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:29,523 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:32,242 - AirSimEnvLogger - INFO - Predictive model loss: 0.03161027282476425
2024-07-21 21:40:38,322 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.04747276058946, Velocity: -0.04360590321994075, Movement: 0.4545344116335355, Collision: 0, Height: -1.0, Movement Penalty: -0.10497022596502795, Smoothness: -0.0, Curiosity: 2411.572265625, Exploration: 0.34677198906838735, Total: 2361.1113395362204
2024-07-21 21:40:38,432 - AirSimEnvLogger - INFO - Action: [0.52485113 0.52485113 0.52485113 0.52485113], Velocity: (0.5248511298251397, 0.5248511298251397, 0.5248511298251397), Duration: 1.0, Reward: 2361.1113395362204, Done: False
2024-07-21 21:40:38,479 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:38,479 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:41,379 - AirSimEnvLogger - INFO - Predictive model loss: 0.028787825256586075
2024-07-21 21:40:47,226 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.47229098663799, Velocity: -0.09094270527324028, Movement: 0.6313037893068234, Collision: 0, Height: -1.0, Movement Penalty: -0.14579336507869012, Smoothness: -0.0, Curiosity: 2459.5517578125, Exploration: 0.27959116834839, Total: 2408.6501643720185
2024-07-21 21:40:47,242 - AirSimEnvLogger - INFO - Action: [0.72896683 0.72896683 0.72896683 0.72896683], Velocity: (0.7289668253934506, 0.7289668253934506, 0.7289668253934506), Duration: 1.0, Reward: 2408.6501643720185, Done: False
2024-07-21 21:40:47,336 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:47,336 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:50,391 - AirSimEnvLogger - INFO - Predictive model loss: 0.04865412041544914
2024-07-21 21:40:56,287 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.17077570852298, Velocity: -0.004105743315952273, Movement: 0.004350437950040375, Collision: 0, Height: -1.0, Movement Penalty: -0.0010046906086194297, Smoothness: -0.0, Curiosity: 2415.9755859375, Exploration: 0.11468366121353551, Total: 2365.330883612733
2024-07-21 21:40:56,350 - AirSimEnvLogger - INFO - Action: [-0.00502345 -0.00502345 -0.00502345 -0.00502345], Velocity: (-0.005023453043097148, -0.005023453043097148, -0.005023453043097148), Duration: 1.0, Reward: 2365.330883612733, Done: False
2024-07-21 21:40:56,428 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:40:56,428 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:40:59,319 - AirSimEnvLogger - INFO - Predictive model loss: 0.034213293343782425
2024-07-21 21:41:05,707 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.61629386713615, Velocity: -0.0558277058385652, Movement: 0.4057605192332954, Collision: 0, Height: -1.0, Movement Penalty: -0.09370637800234617, Smoothness: -0.0, Curiosity: 2466.981201171875, Exploration: 0.0888906121051445, Total: 2415.8897226586305
2024-07-21 21:41:05,709 - AirSimEnvLogger - INFO - Action: [0.46853189 0.46853189 0.46853189 0.46853189], Velocity: (0.46853189001173084, 0.46853189001173084, 0.46853189001173084), Duration: 1.0, Reward: 2415.8897226586305, Done: False
2024-07-21 21:41:05,768 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:05,768 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:41:08,394 - AirSimEnvLogger - INFO - Predictive model loss: 0.0076100509613752365
2024-07-21 21:41:14,343 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.99702405399114, Velocity: -0.07643126830349739, Movement: 0.565686367494993, Collision: 0, Height: -1.0, Movement Penalty: -0.1306396706200543, Smoothness: -0.0, Curiosity: 2510.629638671875, Exploration: 0.320927921993939, Total: 2459.2126611692515
2024-07-21 21:41:14,438 - AirSimEnvLogger - INFO - Action: [0.65319835 0.65319835 0.65319835 0.65319835], Velocity: (0.6531983531002715, 0.6531983531002715, 0.6531983531002715), Duration: 1.0, Reward: 2459.2126611692515, Done: False
2024-07-21 21:41:14,502 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:14,502 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:41:17,328 - AirSimEnvLogger - INFO - Predictive model loss: 0.05397249013185501
2024-07-21 21:41:23,385 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.23492191673599, Velocity: -0.0751463918291336, Movement: 0.492479724475942, Collision: 0, Height: -1.0, Movement Penalty: -0.11373332059864714, Smoothness: -0.0, Curiosity: 2534.15966796875, Exploration: 0.20510545923876417, Total: 2482.4764861695835
2024-07-21 21:41:23,464 - AirSimEnvLogger - INFO - Action: [0.5686666 0.5686666 0.5686666 0.5686666], Velocity: (0.5686666029932357, 0.5686666029932357, 0.5686666029932357), Duration: 1.0, Reward: 2482.4764861695835, Done: False
2024-07-21 21:41:23,543 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:23,543 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:41:26,244 - AirSimEnvLogger - INFO - Predictive model loss: 0.003206981113180518
2024-07-21 21:41:32,236 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.883851100890716, Velocity: -0.011682195945783437, Movement: 0.11075054981038226, Collision: 0, Height: -1.0, Movement Penalty: -0.025576743898369305, Smoothness: -0.0, Curiosity: 2485.251953125, Exploration: 0.20332353080539964, Total: 2433.9164149216244
2024-07-21 21:41:32,391 - AirSimEnvLogger - INFO - Action: [-0.12788372 -0.12788372 -0.12788372 -0.12788372], Velocity: (-0.12788371949184651, -0.12788371949184651, -0.12788371949184651), Duration: 1.0, Reward: 2433.9164149216244, Done: False
2024-07-21 21:41:32,454 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:32,454 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:41:35,455 - AirSimEnvLogger - INFO - Predictive model loss: 0.07183191180229187
2024-07-21 21:41:41,362 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.791915515915335, Velocity: -0.017826426495025225, Movement: 0.15031093897973719, Collision: 0, Height: -1.0, Movement Penalty: -0.03471282443283868, Smoothness: -0.0, Curiosity: 2473.282958984375, Exploration: 0.2819912845913282, Total: 2422.057805940507
2024-07-21 21:41:41,472 - AirSimEnvLogger - INFO - Action: [-0.17356412 -0.17356412 -0.17356412 -0.17356412], Velocity: (-0.17356412216419337, -0.17356412216419337, -0.17356412216419337), Duration: 1.0, Reward: 2422.057805940507, Done: False
2024-07-21 21:41:41,488 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:41,488 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:41:44,266 - AirSimEnvLogger - INFO - Predictive model loss: 0.05614963918924332
2024-07-21 21:41:50,006 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.32248102678262, Velocity: -0.07112113377466618, Movement: 0.499386749680233, Collision: 0, Height: -1.0, Movement Penalty: -0.11532842974304591, Smoothness: -0.0, Curiosity: 2420.006103515625, Exploration: 0.2514972203267069, Total: 2369.246604666224
2024-07-21 21:41:50,039 - AirSimEnvLogger - INFO - Action: [-0.57664215 -0.57664215 -0.57664215 -0.57664215], Velocity: (-0.5766421487152296, -0.5766421487152296, -0.5766421487152296), Duration: 1.0, Reward: 2369.246604666224, Done: False
2024-07-21 21:41:50,101 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:50,101 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:41:52,996 - AirSimEnvLogger - INFO - Predictive model loss: 0.14302830398082733
2024-07-21 21:41:58,659 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.78864577330858, Velocity: -0.019536808395719935, Movement: 0.17999502884813265, Collision: 0, Height: -1.0, Movement Penalty: -0.041568071343305535, Smoothness: -0.0, Curiosity: 2477.153564453125, Exploration: 0.07291559949258009, Total: 2425.8841508552273
2024-07-21 21:41:58,675 - AirSimEnvLogger - INFO - Action: [0.20784036 0.20784036 0.20784036 0.20784036], Velocity: (0.20784035671652767, 0.20784035671652767, 0.20784035671652767), Duration: 1.0, Reward: 2425.8841508552273, Done: False
2024-07-21 21:41:58,721 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:41:58,721 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:01,464 - AirSimEnvLogger - INFO - Predictive model loss: 0.20062139630317688
2024-07-21 21:42:07,375 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.62670071508358, Velocity: -0.008436428455707001, Movement: 0.047214616088246245, Collision: 0, Height: -1.0, Movement Penalty: -0.010903748523293523, Smoothness: -0.0, Curiosity: 2458.619384765625, Exploration: 0.19018965325093273, Total: 2407.5367421999345
2024-07-21 21:42:07,484 - AirSimEnvLogger - INFO - Action: [-0.05451874 -0.05451874 -0.05451874 -0.05451874], Velocity: (-0.05451874261646761, -0.05451874261646761, -0.05451874261646761), Duration: 1.0, Reward: 2407.5367421999345, Done: False
2024-07-21 21:42:07,563 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:42:07,563 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:10,401 - AirSimEnvLogger - INFO - Predictive model loss: 0.14285911619663239
2024-07-21 21:42:16,494 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.06338192271197, Velocity: -0.04679555423702902, Movement: 0.3852485412733573, Collision: 0, Height: -1.0, Movement Penalty: -0.0889693396036334, Smoothness: -0.0, Curiosity: 2511.3642578125, Exploration: 0.1304138198537936, Total: 2459.8356416389074
2024-07-21 21:42:16,589 - AirSimEnvLogger - INFO - Action: [0.4448467 0.4448467 0.4448467 0.4448467], Velocity: (0.44484669801816695, 0.44484669801816695, 0.44484669801816695), Duration: 1.0, Reward: 2459.8356416389074, Done: False
2024-07-21 21:42:16,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:42:16,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:19,461 - AirSimEnvLogger - INFO - Predictive model loss: 0.17651358246803284
2024-07-21 21:42:25,487 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.6104638750986, Velocity: -0.03256160311957649, Movement: 0.24036731902868802, Collision: 0, Height: -1.0, Movement Penalty: -0.055510454538240675, Smoothness: -0.0, Curiosity: 2454.651123046875, Exploration: 0.04668722524889577, Total: 2403.554037270667
2024-07-21 21:42:25,628 - AirSimEnvLogger - INFO - Action: [-0.27755227 -0.27755227 -0.27755227 -0.27755227], Velocity: (-0.27755227269120336, -0.27755227269120336, -0.27755227269120336), Duration: 1.0, Reward: 2403.554037270667, Done: False
2024-07-21 21:42:25,660 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:42:25,660 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:28,530 - AirSimEnvLogger - INFO - Predictive model loss: 0.02625712752342224
2024-07-21 21:42:34,350 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.227745137137866, Velocity: -0.041068530128147034, Movement: 0.4708214076646499, Collision: 0, Height: -1.0, Movement Penalty: -0.10873154658216966, Smoothness: -0.0, Curiosity: 2410.749755859375, Exploration: 0.40037187980432004, Total: 2360.1215386253616
2024-07-21 21:42:34,367 - AirSimEnvLogger - INFO - Action: [-0.54365773 -0.54365773 -0.54365773 -0.54365773], Velocity: (-0.5436577329108483, -0.5436577329108483, -0.5436577329108483), Duration: 1.0, Reward: 2360.1215386253616, Done: False
2024-07-21 21:42:34,445 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:42:34,445 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:37,326 - AirSimEnvLogger - INFO - Predictive model loss: 0.18375740945339203
2024-07-21 21:42:42,974 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.00927696067156, Velocity: -0.03725483042705391, Movement: 0.4234554630247073, Collision: 0, Height: -1.0, Movement Penalty: -0.09779285022685297, Smoothness: -0.0, Curiosity: 2388.093017578125, Exploration: 0.22585160287531295, Total: 2337.642348342782
2024-07-21 21:42:43,067 - AirSimEnvLogger - INFO - Action: [-0.48896425 -0.48896425 -0.48896425 -0.48896425], Velocity: (-0.4889642511342648, -0.4889642511342648, -0.4889642511342648), Duration: 1.0, Reward: 2337.642348342782, Done: False
2024-07-21 21:42:43,115 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:42:43,115 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:45,960 - AirSimEnvLogger - INFO - Predictive model loss: 0.1303163766860962
2024-07-21 21:42:51,655 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.330522618232756, Velocity: -0.012407148112653938, Movement: 0.09694701385933246, Collision: 0, Height: -1.0, Movement Penalty: -0.022388953819526394, Smoothness: -0.0, Curiosity: 2429.2197265625, Exploration: 0.16180680376149859, Total: 2378.4275568993517
2024-07-21 21:42:51,736 - AirSimEnvLogger - INFO - Action: [0.11194477 0.11194477 0.11194477 0.11194477], Velocity: (0.11194476909763196, 0.11194476909763196, 0.11194476909763196), Duration: 1.0, Reward: 2378.4275568993517, Done: False
2024-07-21 21:42:51,798 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:42:51,799 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:42:54,653 - AirSimEnvLogger - INFO - Predictive model loss: 0.017911294475197792
2024-07-21 21:43:00,345 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.01289515382018, Velocity: -0.02906314360736271, Movement: 0.23021514001394225, Collision: 0, Height: -1.0, Movement Penalty: -0.05316590922343078, Smoothness: -0.0, Curiosity: 2393.509521484375, Exploration: 0.08485228706511412, Total: 2343.0188832072095
2024-07-21 21:43:00,501 - AirSimEnvLogger - INFO - Action: [-0.26582955 -0.26582955 -0.26582955 -0.26582955], Velocity: (-0.26582954611715387, -0.26582954611715387, -0.26582954611715387), Duration: 1.0, Reward: 2343.0188832072095, Done: False
2024-07-21 21:43:00,580 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:00,580 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:03,439 - AirSimEnvLogger - INFO - Predictive model loss: 0.00961343478411436
2024-07-21 21:43:09,466 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.476611531439325, Velocity: -0.05620842945955361, Movement: 0.31773786007322863, Collision: 0, Height: -1.0, Movement Penalty: -0.07337841561800568, Smoothness: -0.0, Curiosity: 2449.69384765625, Exploration: 0.09977848484765252, Total: 2398.742358425981
2024-07-21 21:43:09,530 - AirSimEnvLogger - INFO - Action: [0.36689208 0.36689208 0.36689208 0.36689208], Velocity: (0.3668920780900284, 0.3668920780900284, 0.3668920780900284), Duration: 1.0, Reward: 2398.742358425981, Done: False
2024-07-21 21:43:09,576 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:09,576 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:12,520 - AirSimEnvLogger - INFO - Predictive model loss: 0.1054464727640152
2024-07-21 21:43:18,413 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.92228409871292, Velocity: -0.09163122423659237, Movement: 0.5905284362639385, Collision: 0, Height: -1.0, Movement Penalty: -0.13637670065644544, Smoothness: -0.0, Curiosity: 2503.180908203125, Exploration: 0.41336376336728475, Total: 2451.8590202783116
2024-07-21 21:43:18,522 - AirSimEnvLogger - INFO - Action: [0.6818835 0.6818835 0.6818835 0.6818835], Velocity: (0.6818835032822272, 0.6818835032822272, 0.6818835032822272), Duration: 1.0, Reward: 2451.8590202783116, Done: False
2024-07-21 21:43:18,585 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:18,585 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:21,440 - AirSimEnvLogger - INFO - Predictive model loss: 0.15520347654819489
2024-07-21 21:43:27,230 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.68824828653308, Velocity: -0.009790210474858526, Movement: 0.05432513613578087, Collision: 0, Height: -1.0, Movement Penalty: -0.01254585278870246, Smoothness: -0.0, Curiosity: 2468.955078125, Exploration: 0.06981570243206944, Total: 2417.783241021644
2024-07-21 21:43:27,372 - AirSimEnvLogger - INFO - Action: [0.06272926 0.06272926 0.06272926 0.06272926], Velocity: (0.0627292639435123, 0.0627292639435123, 0.0627292639435123), Duration: 1.0, Reward: 2417.783241021644, Done: False
2024-07-21 21:43:27,498 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:27,498 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:30,385 - AirSimEnvLogger - INFO - Predictive model loss: 0.02800595946609974
2024-07-21 21:43:36,215 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.400563429608304, Velocity: -0.034804298543115696, Movement: 0.25519181185845796, Collision: 0, Height: -1.0, Movement Penalty: -0.05893402450858762, Smoothness: -0.0, Curiosity: 2432.532958984375, Exploration: 0.3276548166532817, Total: 2381.7105355741483
2024-07-21 21:43:36,340 - AirSimEnvLogger - INFO - Action: [-0.29467012 -0.29467012 -0.29467012 -0.29467012], Velocity: (-0.29467012254293806, -0.29467012254293806, -0.29467012254293806), Duration: 1.0, Reward: 2381.7105355741483, Done: False
2024-07-21 21:43:36,433 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:36,433 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:39,308 - AirSimEnvLogger - INFO - Predictive model loss: 0.09473758190870285
2024-07-21 21:43:45,004 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.20063187769465, Velocity: -0.032503890111827916, Movement: 0.309910631248739, Collision: 0, Height: -1.0, Movement Penalty: -0.07157079455047453, Smoothness: -0.0, Curiosity: 2410.121826171875, Exploration: 0.2324240916059055, Total: 2359.479003537338
2024-07-21 21:43:45,129 - AirSimEnvLogger - INFO - Action: [-0.35785397 -0.35785397 -0.35785397 -0.35785397], Velocity: (-0.35785397275237263, -0.35785397275237263, -0.35785397275237263), Duration: 1.0, Reward: 2359.479003537338, Done: False
2024-07-21 21:43:45,192 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:45,192 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:47,912 - AirSimEnvLogger - INFO - Predictive model loss: 0.09952356666326523
2024-07-21 21:43:53,761 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.73313971345805, Velocity: -0.0933798974547039, Movement: 0.5660556682377099, Collision: 0, Height: -1.0, Movement Penalty: -0.13072495697334213, Smoothness: -0.0, Curiosity: 2359.2353515625, Exploration: 0.25751827462049043, Total: 2309.065988377526
2024-07-21 21:43:53,888 - AirSimEnvLogger - INFO - Action: [-0.65362478 -0.65362478 -0.65362478 -0.65362478], Velocity: (-0.6536247848667106, -0.6536247848667106, -0.6536247848667106), Duration: 1.0, Reward: 2309.065988377526, Done: False
2024-07-21 21:43:53,999 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:43:53,999 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:43:56,868 - AirSimEnvLogger - INFO - Predictive model loss: 0.07892470061779022
2024-07-21 21:44:02,798 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.17784957551219, Velocity: -0.02175517799761258, Movement: 0.135150870023848, Collision: 0, Height: -1.0, Movement Penalty: -0.03121175647579231, Smoothness: -0.0, Curiosity: 2414.401123046875, Exploration: 0.1126605318959841, Total: 2363.7503251194644
2024-07-21 21:44:02,908 - AirSimEnvLogger - INFO - Action: [0.15605878 0.15605878 0.15605878 0.15605878], Velocity: (0.15605878237896154, 0.15605878237896154, 0.15605878237896154), Duration: 1.0, Reward: 2363.7503251194644, Done: False
2024-07-21 21:44:02,971 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:02,971 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:05,768 - AirSimEnvLogger - INFO - Predictive model loss: 0.12721306085586548
2024-07-21 21:44:11,755 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.48956992768135, Velocity: -0.0660498522217362, Movement: 0.3774697082943857, Collision: 0, Height: -1.0, Movement Penalty: -0.08717289507787723, Smoothness: -0.0, Curiosity: 2453.2490234375, Exploration: 0.4153806000384237, Total: 2402.3576453742276
2024-07-21 21:44:11,882 - AirSimEnvLogger - INFO - Action: [0.43586448 0.43586448 0.43586448 0.43586448], Velocity: (0.43586447538938616, 0.43586447538938616, 0.43586447538938616), Duration: 1.0, Reward: 2402.3576453742276, Done: False
2024-07-21 21:44:11,945 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:11,945 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:14,661 - AirSimEnvLogger - INFO - Predictive model loss: 0.1945967823266983
2024-07-21 21:44:20,510 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.87433303641164, Velocity: -0.08466469539070907, Movement: 0.5582199621049118, Collision: 0, Height: -1.0, Movement Penalty: -0.1289153781553174, Smoothness: -0.0, Curiosity: 2497.7744140625, Exploration: 0.299996387686934, Total: 2446.474306831334
2024-07-21 21:44:20,525 - AirSimEnvLogger - INFO - Action: [0.64457689 0.64457689 0.64457689 0.64457689], Velocity: (0.644576890776587, 0.644576890776587, 0.644576890776587), Duration: 1.0, Reward: 2446.474306831334, Done: False
2024-07-21 21:44:20,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:20,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:23,510 - AirSimEnvLogger - INFO - Predictive model loss: 0.1454170197248459
2024-07-21 21:44:29,546 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.274531338058004, Velocity: -0.1046280577998837, Movement: 0.6563963665470467, Collision: 0, Height: -1.0, Movement Penalty: -0.15158824756841188, Smoothness: -0.0, Curiosity: 2542.197509765625, Exploration: 0.2825350218145056, Total: 2490.4936000443367
2024-07-21 21:44:29,688 - AirSimEnvLogger - INFO - Action: [0.75794124 0.75794124 0.75794124 0.75794124], Velocity: (0.7579412378420594, 0.7579412378420594, 0.7579412378420594), Duration: 1.0, Reward: 2490.4936000443367, Done: False
2024-07-21 21:44:29,768 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:29,768 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:32,511 - AirSimEnvLogger - INFO - Predictive model loss: 0.010914729908108711
2024-07-21 21:44:38,506 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.495903994044546, Velocity: -0.0915302200236699, Movement: 0.5124122347227231, Collision: 0, Height: -1.0, Movement Penalty: -0.11833653666128877, Smoothness: -0.0, Curiosity: 2561.614501953125, Exploration: 0.19107813561133283, Total: 2509.665962352799
2024-07-21 21:44:38,585 - AirSimEnvLogger - INFO - Action: [0.59168268 0.59168268 0.59168268 0.59168268], Velocity: (0.5916826833064438, 0.5916826833064438, 0.5916826833064438), Duration: 1.0, Reward: 2509.665962352799, Done: False
2024-07-21 21:44:38,664 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:38,664 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:41,541 - AirSimEnvLogger - INFO - Predictive model loss: 0.0810508280992508
2024-07-21 21:44:47,487 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.82763602632469, Velocity: -0.09277675848260872, Movement: 0.5518651748783917, Collision: 0, Height: -1.0, Movement Penalty: -0.12744780290896776, Smoothness: -0.0, Curiosity: 2596.4326171875, Exploration: 0.16608946586895054, Total: 2544.1474412855055
2024-07-21 21:44:47,488 - AirSimEnvLogger - INFO - Action: [0.63723901 0.63723901 0.63723901 0.63723901], Velocity: (0.6372390145448388, 0.6372390145448388, 0.6372390145448388), Duration: 1.0, Reward: 2544.1474412855055, Done: False
2024-07-21 21:44:47,564 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:47,564 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:50,467 - AirSimEnvLogger - INFO - Predictive model loss: 0.19352757930755615
2024-07-21 21:44:56,293 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.4176796776858, Velocity: -0.02585383101210299, Movement: 0.1413228568270122, Collision: 0, Height: -1.0, Movement Penalty: -0.03263711577268898, Smoothness: -0.0, Curiosity: 2539.553466796875, Exploration: 0.21014788440007134, Total: 2487.6850023915695
2024-07-21 21:44:56,404 - AirSimEnvLogger - INFO - Action: [-0.16318558 -0.16318558 -0.16318558 -0.16318558], Velocity: (-0.16318557886344487, -0.16318557886344487, -0.16318557886344487), Duration: 1.0, Reward: 2487.6850023915695, Done: False
2024-07-21 21:44:56,466 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:44:56,466 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:44:59,354 - AirSimEnvLogger - INFO - Predictive model loss: 0.4561969041824341
2024-07-21 21:45:04,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.96330028110119, Velocity: -0.05603472636991826, Movement: 0.4990928286978379, Collision: 0, Height: -1.0, Movement Penalty: -0.1152605515997234, Smoothness: -0.0, Curiosity: 2485.14501953125, Exploration: 0.4749089994427711, Total: 2433.797587567601
2024-07-21 21:45:05,115 - AirSimEnvLogger - INFO - Action: [-0.57630276 -0.57630276 -0.57630276 -0.57630276], Velocity: (-0.576302757998617, -0.576302757998617, -0.576302757998617), Duration: 1.0, Reward: 2433.797587567601, Done: False
2024-07-21 21:45:05,177 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:05,177 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:45:08,118 - AirSimEnvLogger - INFO - Predictive model loss: 0.3728538453578949
2024-07-21 21:45:13,980 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.3045925423007, Velocity: -0.007188897836983893, Movement: 0.06550796591966669, Collision: 0, Height: -1.0, Movement Penalty: -0.015128416703113757, Smoothness: -0.0, Curiosity: 2527.53857421875, Exploration: 0.03730073025188145, Total: 2475.743448861501
2024-07-21 21:45:14,091 - AirSimEnvLogger - INFO - Action: [0.07564208 0.07564208 0.07564208 0.07564208], Velocity: (0.07564208351556878, 0.07564208351556878, 0.07564208351556878), Duration: 1.0, Reward: 2475.743448861501, Done: False
2024-07-21 21:45:14,139 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:14,139 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:45:16,933 - AirSimEnvLogger - INFO - Predictive model loss: 0.08269824087619781
2024-07-21 21:45:22,829 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.7290296231495, Velocity: -0.041885948716118976, Movement: 0.4445786036782336, Collision: 0, Height: -1.0, Movement Penalty: -0.10267103060383045, Smoothness: -0.0, Curiosity: 2581.184814453125, Exploration: 0.39962739473060777, Total: 2529.054416025105
2024-07-21 21:45:22,955 - AirSimEnvLogger - INFO - Action: [0.51335515 0.51335515 0.51335515 0.51335515], Velocity: (0.5133551530191522, 0.5133551530191522, 0.5133551530191522), Duration: 1.0, Reward: 2529.054416025105, Done: False
2024-07-21 21:45:23,019 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:23,019 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:45:25,950 - AirSimEnvLogger - INFO - Predictive model loss: 0.6236015558242798
2024-07-21 21:45:31,992 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.17193592643872, Velocity: -0.10994367282495399, Movement: 0.6475775612792324, Collision: 0, Height: -1.0, Movement Penalty: -0.14955163173029049, Smoothness: -0.0, Curiosity: 2633.9140625, Exploration: 0.37455167214833945, Total: 2581.333164133587
2024-07-21 21:45:32,070 - AirSimEnvLogger - INFO - Action: [0.74775816 0.74775816 0.74775816 0.74775816], Velocity: (0.7477581586514523, 0.7477581586514523, 0.7477581586514523), Duration: 1.0, Reward: 2581.333164133587, Done: False
2024-07-21 21:45:32,133 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:32,133 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:45:35,011 - AirSimEnvLogger - INFO - Predictive model loss: 0.4042724668979645
2024-07-21 21:45:40,475 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.049935805330264, Velocity: -0.03258736901777616, Movement: 0.1855546557473554, Collision: 0, Height: -1.0, Movement Penalty: -0.0428520121780496, Smoothness: -0.0, Curiosity: 2611.505126953125, Exploration: 0.09935203828535592, Total: 2558.979335025414
2024-07-21 21:45:40,615 - AirSimEnvLogger - INFO - Action: [0.21426006 0.21426006 0.21426006 0.21426006], Velocity: (0.21426006089024796, 0.21426006089024796, 0.21426006089024796), Duration: 1.0, Reward: 2558.979335025414, Done: False
2024-07-21 21:45:40,676 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:40,676 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:45:43,504 - AirSimEnvLogger - INFO - Predictive model loss: 0.026199903339147568
2024-07-21 21:45:49,451 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.63082599130929, Velocity: -0.0524686935994687, Movement: 0.3189563657356737, Collision: 0, Height: -1.0, Movement Penalty: -0.07365981744689439, Smoothness: -0.0, Curiosity: 2557.192138671875, Exploration: 0.3650012021054914, Total: 2505.1478400701694
2024-07-21 21:45:49,577 - AirSimEnvLogger - INFO - Action: [-0.36829909 -0.36829909 -0.36829909 -0.36829909], Velocity: (-0.3682990872344719, -0.3682990872344719, -0.3682990872344719), Duration: 1.0, Reward: 2505.1478400701694, Done: False
2024-07-21 21:45:49,624 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:49,624 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:45:52,515 - AirSimEnvLogger - INFO - Predictive model loss: 0.11143043637275696
2024-07-21 21:45:58,472 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.1039750844406, Velocity: -0.0421006938536963, Movement: 0.27324963300971317, Collision: 0, Height: -1.0, Movement Penalty: -0.06310429966964974, Smoothness: -0.0, Curiosity: 2615.255859375, Exploration: 0.027392829432107694, Total: 2562.66067737073
2024-07-21 21:45:58,596 - AirSimEnvLogger - INFO - Action: [0.3155215 0.3155215 0.3155215 0.3155215], Velocity: (0.3155214983482487, 0.3155214983482487, 0.3155214983482487), Duration: 1.0, Reward: 2562.66067737073, Done: False
2024-07-21 21:45:58,659 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:45:58,659 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:01,344 - AirSimEnvLogger - INFO - Predictive model loss: 0.050787270069122314
2024-07-21 21:46:07,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.78108754211852, Velocity: -0.01926817382017951, Movement: 0.1580543549428734, Collision: 0, Height: -1.0, Movement Penalty: -0.03650108974914425, Smoothness: -0.0, Curiosity: 2575.188232421875, Exploration: 0.07146470501399863, Total: 2522.9255320093453
2024-07-21 21:46:07,268 - AirSimEnvLogger - INFO - Action: [-0.18250545 -0.18250545 -0.18250545 -0.18250545], Velocity: (-0.18250544874572122, -0.18250544874572122, -0.18250544874572122), Duration: 1.0, Reward: 2522.9255320093453, Done: False
2024-07-21 21:46:07,331 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:46:07,331 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:10,286 - AirSimEnvLogger - INFO - Predictive model loss: 0.2060076743364334
2024-07-21 21:46:16,111 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.22700677725446, Velocity: -0.03359676447223496, Movement: 0.33077732833772566, Collision: 0, Height: -1.0, Movement Penalty: -0.07638975182304446, Smoothness: -0.0, Curiosity: 2629.994873046875, Exploration: 0.04846664303565256, Total: 2577.283767871127
2024-07-21 21:46:16,159 - AirSimEnvLogger - INFO - Action: [0.38194876 0.38194876 0.38194876 0.38194876], Velocity: (0.38194875911522225, 0.38194875911522225, 0.38194875911522225), Duration: 1.0, Reward: 2577.283767871127, Done: False
2024-07-21 21:46:16,221 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:46:16,221 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:19,108 - AirSimEnvLogger - INFO - Predictive model loss: 0.033951692283153534
2024-07-21 21:46:24,673 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.76655258928781, Velocity: -0.031267678714195384, Movement: 0.26572576978596624, Collision: 0, Height: -1.0, Movement Penalty: -0.061366737886619255, Smoothness: -0.0, Curiosity: 2572.226806640625, Exploration: 0.030752056948967935, Total: 2519.970718495391
2024-07-21 21:46:24,799 - AirSimEnvLogger - INFO - Action: [-0.30683369 -0.30683369 -0.30683369 -0.30683369], Velocity: (-0.30683368943309625, -0.30683368943309625, -0.30683368943309625), Duration: 1.0, Reward: 2519.970718495391, Done: False
2024-07-21 21:46:24,877 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:46:24,877 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:27,672 - AirSimEnvLogger - INFO - Predictive model loss: 0.07165995240211487
2024-07-21 21:46:33,468 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.298302449184874, Velocity: -0.09779297785216869, Movement: 0.560081003182063, Collision: 0, Height: -1.0, Movement Penalty: -0.12934516718206387, Smoothness: -0.0, Curiosity: 2517.690185546875, Exploration: 0.42922396754552655, Total: 2465.994564069295
2024-07-21 21:46:33,562 - AirSimEnvLogger - INFO - Action: [-0.64672584 -0.64672584 -0.64672584 -0.64672584], Velocity: (-0.6467258359103194, -0.6467258359103194, -0.6467258359103194), Duration: 1.0, Reward: 2465.994564069295, Done: False
2024-07-21 21:46:33,593 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:46:33,593 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:36,582 - AirSimEnvLogger - INFO - Predictive model loss: 0.026426810771226883
2024-07-21 21:46:42,662 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.663341233728254, Velocity: -0.007990956405847151, Movement: 0.06357397264862026, Collision: 0, Height: -1.0, Movement Penalty: -0.014681780088853924, Smoothness: -0.0, Curiosity: 2563.908447265625, Exploration: 0.05852303434226913, Total: 2511.7593267002812
2024-07-21 21:46:42,787 - AirSimEnvLogger - INFO - Action: [0.0734089 0.0734089 0.0734089 0.0734089], Velocity: (0.07340890044426962, 0.07340890044426962, 0.07340890044426962), Duration: 1.0, Reward: 2511.7593267002812, Done: False
2024-07-21 21:46:42,850 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:46:42,850 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:45,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.2745667099952698
2024-07-21 21:46:51,519 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.301384784454264, Velocity: -0.032792219440715024, Movement: 0.2839562771312242, Collision: 0, Height: -1.0, Movement Penalty: -0.0655768932159185, Smoothness: -0.0, Curiosity: 2522.7880859375, Exploration: 0.09406323808133503, Total: 2471.012021908051
2024-07-21 21:46:51,629 - AirSimEnvLogger - INFO - Action: [-0.32788447 -0.32788447 -0.32788447 -0.32788447], Velocity: (-0.3278844660795925, -0.3278844660795925, -0.3278844660795925), Duration: 1.0, Reward: 2471.012021908051, Done: False
2024-07-21 21:46:51,739 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:46:51,739 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:46:54,594 - AirSimEnvLogger - INFO - Predictive model loss: 0.16638216376304626
2024-07-21 21:47:00,499 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.767547717992784, Velocity: -0.04667959297551164, Movement: 0.2897466658694593, Collision: 0, Height: -1.0, Movement Penalty: -0.06691412621461156, Smoothness: -0.0, Curiosity: 2580.652587890625, Exploration: 0.09495299326905464, Total: 2528.4093188719007
2024-07-21 21:47:00,563 - AirSimEnvLogger - INFO - Action: [0.33457063 0.33457063 0.33457063 0.33457063], Velocity: (0.3345706310730578, 0.3345706310730578, 0.3345706310730578), Duration: 1.0, Reward: 2528.4093188719007, Done: False
2024-07-21 21:47:00,626 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:00,626 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:03,522 - AirSimEnvLogger - INFO - Predictive model loss: 0.33007678389549255
2024-07-21 21:47:09,596 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.325944785298276, Velocity: -0.03576359478783327, Movement: 0.2628117427464553, Collision: 0, Height: -1.0, Movement Penalty: -0.060693772168344255, Smoothness: -0.0, Curiosity: 2526.544677734375, Exploration: 0.022241729413734788, Total: 2474.7267189452637
2024-07-21 21:47:09,737 - AirSimEnvLogger - INFO - Action: [-0.30346886 -0.30346886 -0.30346886 -0.30346886], Velocity: (-0.30346886084172126, -0.30346886084172126, -0.30346886084172126), Duration: 1.0, Reward: 2474.7267189452637, Done: False
2024-07-21 21:47:09,783 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:09,783 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:12,649 - AirSimEnvLogger - INFO - Predictive model loss: 0.005332713946700096
2024-07-21 21:47:18,380 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.98786572968764, Velocity: -0.06527837606103412, Movement: 0.43512448277690285, Collision: 0, Height: -1.0, Movement Penalty: -0.10048769490489662, Smoothness: -0.0, Curiosity: 2487.971923828125, Exploration: 0.34966518126584123, Total: 2436.568626832659
2024-07-21 21:47:18,442 - AirSimEnvLogger - INFO - Action: [-0.50243847 -0.50243847 -0.50243847 -0.50243847], Velocity: (-0.5024384745244831, -0.5024384745244831, -0.5024384745244831), Duration: 1.0, Reward: 2436.568626832659, Done: False
2024-07-21 21:47:18,522 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:18,522 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:21,366 - AirSimEnvLogger - INFO - Predictive model loss: 0.19083094596862793
2024-07-21 21:47:26,989 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.643541348973976, Velocity: -0.06471923494848375, Movement: 0.5168647582307963, Collision: 0, Height: -1.0, Movement Penalty: -0.11936480291967243, Smoothness: -0.0, Curiosity: 2450.652587890625, Exploration: 0.25622329451877596, Total: 2399.574184640588
2024-07-21 21:47:27,114 - AirSimEnvLogger - INFO - Action: [-0.59682401 -0.59682401 -0.59682401 -0.59682401], Velocity: (-0.5968240145983621, -0.5968240145983621, -0.5968240145983621), Duration: 1.0, Reward: 2399.574184640588, Done: False
2024-07-21 21:47:27,160 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:27,160 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:29,896 - AirSimEnvLogger - INFO - Predictive model loss: 0.2283497452735901
2024-07-21 21:47:35,792 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.167406757878844, Velocity: -0.11260665194524251, Movement: 0.6635543065707441, Collision: 0, Height: -1.0, Movement Penalty: -0.15324130300822184, Smoothness: -0.0, Curiosity: 2400.447021484375, Exploration: 0.28392080834780387, Total: 2349.8499327986374
2024-07-21 21:47:35,855 - AirSimEnvLogger - INFO - Action: [-0.76620652 -0.76620652 -0.76620652 -0.76620652], Velocity: (-0.7662065150411091, -0.7662065150411091, -0.7662065150411091), Duration: 1.0, Reward: 2349.8499327986374, Done: False
2024-07-21 21:47:35,934 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:35,934 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:38,699 - AirSimEnvLogger - INFO - Predictive model loss: 0.14807048439979553
2024-07-21 21:47:44,565 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.65004860532817, Velocity: -0.10353901103288768, Movement: 0.7645923861706848, Collision: 0, Height: -1.0, Movement Penalty: -0.17657504799039328, Smoothness: -0.0, Curiosity: 2346.9541015625, Exploration: 0.3362170156178841, Total: 2296.8897843797295
2024-07-21 21:47:44,672 - AirSimEnvLogger - INFO - Action: [-0.88287524 -0.88287524 -0.88287524 -0.88287524], Velocity: (-0.8828752399519664, -0.8828752399519664, -0.8828752399519664), Duration: 1.0, Reward: 2296.8897843797295, Done: False
2024-07-21 21:47:44,735 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:44,735 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:47,527 - AirSimEnvLogger - INFO - Predictive model loss: 0.042536184191703796
2024-07-21 21:47:53,094 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.09794446698327, Velocity: -0.01014336348947446, Movement: 0.04696712701334307, Collision: 0, Height: -1.0, Movement Penalty: -0.010846593369686786, Smoothness: -0.0, Curiosity: 2403.520751953125, Exploration: 0.19897878292758367, Total: 2352.968710370963
2024-07-21 21:47:53,218 - AirSimEnvLogger - INFO - Action: [0.05423297 0.05423297 0.05423297 0.05423297], Velocity: (0.05423296684843393, 0.05423296684843393, 0.05423296684843393), Duration: 1.0, Reward: 2352.968710370963, Done: False
2024-07-21 21:47:53,281 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:47:53,281 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:47:55,974 - AirSimEnvLogger - INFO - Predictive model loss: 0.296564519405365
2024-07-21 21:48:01,967 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.716459741906675, Velocity: -0.048314129805867446, Movement: 0.3028174651875554, Collision: 0, Height: -1.0, Movement Penalty: -0.0699326980165421, Smoothness: -0.0, Curiosity: 2362.46142578125, Exploration: 0.17968558075696714, Total: 2312.288890406055
2024-07-21 21:48:02,078 - AirSimEnvLogger - INFO - Action: [-0.34966349 -0.34966349 -0.34966349 -0.34966349], Velocity: (-0.3496634900827105, -0.3496634900827105, -0.3496634900827105), Duration: 1.0, Reward: 2312.288890406055, Done: False
2024-07-21 21:48:02,140 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:02,140 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:05,032 - AirSimEnvLogger - INFO - Predictive model loss: 0.16716943681240082
2024-07-21 21:48:10,961 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.22954388512849, Velocity: -0.09637815222542462, Movement: 0.583560401478624, Collision: 0, Height: -1.0, Movement Penalty: -0.13476750195283588, Smoothness: -0.0, Curiosity: 2309.2919921875, Exploration: 0.33920390375723447, Total: 2259.6451420902436
2024-07-21 21:48:11,024 - AirSimEnvLogger - INFO - Action: [-0.67383751 -0.67383751 -0.67383751 -0.67383751], Velocity: (-0.6738375097641793, -0.6738375097641793, -0.6738375097641793), Duration: 1.0, Reward: 2259.6451420902436, Done: False
2024-07-21 21:48:11,087 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:11,087 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:13,972 - AirSimEnvLogger - INFO - Predictive model loss: 0.012192955240607262
2024-07-21 21:48:19,757 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.73974888304054, Velocity: -0.07017903702813073, Movement: 0.70953684101194, Collision: 0, Height: -1.0, Movement Penalty: -0.1638605144632801, Smoothness: -0.0, Curiosity: 2258.62548828125, Exploration: 0.3626935593409386, Total: 2209.4795459136653
2024-07-21 21:48:19,881 - AirSimEnvLogger - INFO - Action: [-0.81930257 -0.81930257 -0.81930257 -0.81930257], Velocity: (-0.8193025723164006, -0.8193025723164006, -0.8193025723164006), Duration: 1.0, Reward: 2209.4795459136653, Done: False
2024-07-21 21:48:19,928 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:19,928 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:22,806 - AirSimEnvLogger - INFO - Predictive model loss: 0.004422065336257219
2024-07-21 21:48:28,973 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.597962561199594, Velocity: -0.0695007887547096, Movement: 0.4591568970344181, Collision: 0, Height: -1.0, Movement Penalty: -0.10603774324123783, Smoothness: -0.0, Curiosity: 2247.35205078125, Exploration: 0.19273373069415886, Total: 2198.3027299931987
2024-07-21 21:48:29,099 - AirSimEnvLogger - INFO - Action: [-0.53018872 -0.53018872 -0.53018872 -0.53018872], Velocity: (-0.5301887162061891, -0.5301887162061891, -0.5301887162061891), Duration: 1.0, Reward: 2198.3027299931987, Done: False
2024-07-21 21:48:29,161 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:29,161 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:31,990 - AirSimEnvLogger - INFO - Predictive model loss: 0.027557823807001114
2024-07-21 21:48:37,900 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.07847966113504, Velocity: -0.10090170526311894, Movement: 0.6614182861981023, Collision: 0, Height: -1.0, Movement Penalty: -0.15274801023336612, Smoothness: -0.0, Curiosity: 2195.537353515625, Exploration: 0.2008454715191967, Total: 2147.0112026973643
2024-07-21 21:48:37,901 - AirSimEnvLogger - INFO - Action: [-0.76374005 -0.76374005 -0.76374005 -0.76374005], Velocity: (-0.7637400511668305, -0.7637400511668305, -0.7637400511668305), Duration: 1.0, Reward: 2147.0112026973643, Done: False
2024-07-21 21:48:37,931 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:37,931 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:40,715 - AirSimEnvLogger - INFO - Predictive model loss: 0.008230815641582012
2024-07-21 21:48:46,590 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.44069884516542, Velocity: -0.005987734909958403, Movement: 0.019352199962868317, Collision: 0, Height: -1.0, Movement Penalty: -0.004469199143189395, Smoothness: -0.0, Curiosity: 2240.598388671875, Exploration: 0.14720811374747408, Total: 2191.6914236278035
2024-07-21 21:48:46,732 - AirSimEnvLogger - INFO - Action: [0.022346 0.022346 0.022346 0.022346], Velocity: (0.022345995715946976, 0.022345995715946976, 0.022345995715946976), Duration: 1.0, Reward: 2191.6914236278035, Done: False
2024-07-21 21:48:46,795 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:46,795 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:49,722 - AirSimEnvLogger - INFO - Predictive model loss: 0.04141999036073685
2024-07-21 21:48:55,309 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.87079626656382, Velocity: -0.07511188748558388, Movement: 0.43675167202490633, Collision: 0, Height: -1.0, Movement Penalty: -0.10086347816503954, Smoothness: -0.0, Curiosity: 2293.550537109375, Exploration: 0.47064776670174685, Total: 2244.2911921354053
2024-07-21 21:48:55,418 - AirSimEnvLogger - INFO - Action: [0.50431739 0.50431739 0.50431739 0.50431739], Velocity: (0.5043173908251977, 0.5043173908251977, 0.5043173908251977), Duration: 1.0, Reward: 2244.2911921354053, Done: False
2024-07-21 21:48:55,450 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:48:55,450 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:48:58,358 - AirSimEnvLogger - INFO - Predictive model loss: 0.06880811601877213
2024-07-21 21:49:04,210 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.309008609811066, Velocity: -0.06581135211742781, Movement: 0.6494280947172969, Collision: 0, Height: -1.0, Movement Penalty: -0.14997899412173485, Smoothness: -0.0, Curiosity: 2343.9306640625, Exploration: 0.3955385490443065, Total: 2294.221978619957
2024-07-21 21:49:04,413 - AirSimEnvLogger - INFO - Action: [0.74989497 0.74989497 0.74989497 0.74989497], Velocity: (0.7498949706086743, 0.7498949706086743, 0.7498949706086743), Duration: 1.0, Reward: 2294.221978619957, Done: False
2024-07-21 21:49:04,461 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:04,461 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:49:07,248 - AirSimEnvLogger - INFO - Predictive model loss: 0.08577599376440048
2024-07-21 21:49:12,957 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.90005291208951, Velocity: -0.013027851681480192, Movement: 0.09776961534437324, Collision: 0, Height: -1.0, Movement Penalty: -0.022578925495056024, Smoothness: -0.0, Curiosity: 2288.427001953125, Exploration: 0.12232981858688823, Total: 2239.0561803975434
2024-07-21 21:49:13,066 - AirSimEnvLogger - INFO - Action: [-0.11289463 -0.11289463 -0.11289463 -0.11289463], Velocity: (-0.11289462747528012, -0.11289462747528012, -0.11289462747528012), Duration: 1.0, Reward: 2239.0561803975434, Done: False
2024-07-21 21:49:13,161 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:13,161 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:49:16,020 - AirSimEnvLogger - INFO - Predictive model loss: 0.2821073532104492
2024-07-21 21:49:21,770 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.22251441044155, Velocity: -0.036866901403583445, Movement: 0.2405331090913319, Collision: 0, Height: -1.0, Movement Penalty: -0.055548742113159244, Smoothness: -0.0, Curiosity: 2324.10400390625, Exploration: 0.14749500255038983, Total: 2274.417626920471
2024-07-21 21:49:21,881 - AirSimEnvLogger - INFO - Action: [0.27774371 0.27774371 0.27774371 0.27774371], Velocity: (0.2777437105657962, 0.2777437105657962, 0.2777437105657962), Duration: 1.0, Reward: 2274.417626920471, Done: False
2024-07-21 21:49:21,944 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:21,944 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:49:24,852 - AirSimEnvLogger - INFO - Predictive model loss: 0.17250977456569672
2024-07-21 21:49:30,776 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.67373132044369, Velocity: -0.07340242113951745, Movement: 0.5532161778530101, Collision: 0, Height: -1.0, Movement Penalty: -0.1277598036813965, Smoothness: -0.0, Curiosity: 2376.70703125, Exploration: 0.3193812027734438, Total: 2326.612987727139
2024-07-21 21:49:30,901 - AirSimEnvLogger - INFO - Action: [0.63879902 0.63879902 0.63879902 0.63879902], Velocity: (0.6387990184069825, 0.6387990184069825, 0.6387990184069825), Duration: 1.0, Reward: 2326.612987727139, Done: False
2024-07-21 21:49:30,964 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:30,964 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:49:33,821 - AirSimEnvLogger - INFO - Predictive model loss: 0.0466010756790638
2024-07-21 21:49:39,737 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.10732904959599, Velocity: -0.06985154079119676, Movement: 0.692567118578033, Collision: 0, Height: -1.0, Movement Penalty: -0.15994152493716432, Smoothness: -0.0, Curiosity: 2424.927978515625, Exploration: 0.34018946391090243, Total: 2374.408896522782
2024-07-21 21:49:39,846 - AirSimEnvLogger - INFO - Action: [0.79970762 0.79970762 0.79970762 0.79970762], Velocity: (0.7997076246858216, 0.7997076246858216, 0.7997076246858216), Duration: 1.0, Reward: 2374.408896522782, Done: False
2024-07-21 21:49:39,941 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:39,941 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:49:42,826 - AirSimEnvLogger - INFO - Predictive model loss: 0.007189081981778145
2024-07-21 21:49:48,588 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.54185751361774, Velocity: -0.05846177525787976, Movement: 0.7461085632367888, Collision: 0, Height: -1.0, Movement Penalty: -0.17230639193177796, Smoothness: -0.0, Curiosity: 2470.6806640625, Exploration: 0.29229686910764596, Total: 2419.7184906541383
2024-07-21 21:49:48,713 - AirSimEnvLogger - INFO - Action: [0.86153196 0.86153196 0.86153196 0.86153196], Velocity: (0.8615319596588897, 0.8615319596588897, 0.8615319596588897), Duration: 1.0, Reward: 2419.7184906541383, Done: False
2024-07-21 21:49:48,840 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:48,840 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:49:51,653 - AirSimEnvLogger - INFO - Predictive model loss: 0.036184195429086685
2024-07-21 21:49:57,382 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.45758923169251, Velocity: -0.030117924810418775, Movement: 0.2623071920864104, Collision: 0, Height: -1.0, Movement Penalty: -0.06057725118458557, Smoothness: -0.0, Curiosity: 2450.73583984375, Exploration: 0.1405488932254707, Total: 2399.813999446318
2024-07-21 21:49:57,508 - AirSimEnvLogger - INFO - Action: [0.30288626 0.30288626 0.30288626 0.30288626], Velocity: (0.30288625592292784, 0.30288625592292784, 0.30288625592292784), Duration: 1.0, Reward: 2399.813999446318, Done: False
2024-07-21 21:49:57,586 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:49:57,586 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:00,371 - AirSimEnvLogger - INFO - Predictive model loss: 0.016846928745508194
2024-07-21 21:50:06,250 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.150941586037035, Velocity: -0.03475406720021888, Movement: 0.1839313169446551, Collision: 0, Height: -1.0, Movement Penalty: -0.0424771181401596, Smoothness: -0.0, Curiosity: 2409.46533203125, Exploration: 0.3379299334045848, Total: 2358.893150748478
2024-07-21 21:50:06,282 - AirSimEnvLogger - INFO - Action: [-0.21238559 -0.21238559 -0.21238559 -0.21238559], Velocity: (-0.212385590700798, -0.212385590700798, -0.212385590700798), Duration: 1.0, Reward: 2358.893150748478, Done: False
2024-07-21 21:50:06,330 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:50:06,330 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:09,199 - AirSimEnvLogger - INFO - Predictive model loss: 0.07505439966917038
2024-07-21 21:50:15,293 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.81755790326077, Velocity: -0.06144057484978391, Movement: 0.4021283512648962, Collision: 0, Height: -1.0, Movement Penalty: -0.0928675647406273, Smoothness: -0.0, Curiosity: 2371.0166015625, Exploration: 0.34432285810616026, Total: 2320.781958045926
2024-07-21 21:50:15,402 - AirSimEnvLogger - INFO - Action: [-0.46433782 -0.46433782 -0.46433782 -0.46433782], Velocity: (-0.4643378237031364, -0.4643378237031364, -0.4643378237031364), Duration: 1.0, Reward: 2320.781958045926, Done: False
2024-07-21 21:50:15,466 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:50:15,466 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:18,232 - AirSimEnvLogger - INFO - Predictive model loss: 0.1063319742679596
2024-07-21 21:50:24,262 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.32524267047388, Velocity: -0.06611449633507739, Movement: 0.6324107150348148, Collision: 0, Height: -1.0, Movement Penalty: -0.14604899862550164, Smoothness: -0.0, Curiosity: 2317.805419921875, Exploration: 0.33630016300159393, Total: 2268.06642784005
2024-07-21 21:50:24,405 - AirSimEnvLogger - INFO - Action: [-0.73024499 -0.73024499 -0.73024499 -0.73024499], Velocity: (-0.7302449931275081, -0.7302449931275081, -0.7302449931275081), Duration: 1.0, Reward: 2268.06642784005, Done: False
2024-07-21 21:50:24,467 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:50:24,467 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:27,421 - AirSimEnvLogger - INFO - Predictive model loss: 0.13212384283542633
2024-07-21 21:50:33,310 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.40941528247859, Velocity: -0.03169519554071772, Movement: 0.22028812329923095, Collision: 0, Height: -1.0, Movement Penalty: -0.05087336291443539, Smoothness: -0.0, Curiosity: 2330.939208984375, Exploration: 0.123328718551299, Total: 2281.0603934434134
2024-07-21 21:50:33,436 - AirSimEnvLogger - INFO - Action: [-0.25436681 -0.25436681 -0.25436681 -0.25436681], Velocity: (-0.2543668145721769, -0.2543668145721769, -0.2543668145721769), Duration: 1.0, Reward: 2281.0603934434134, Done: False
2024-07-21 21:50:33,499 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:50:33,499 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:36,263 - AirSimEnvLogger - INFO - Predictive model loss: 0.16925114393234253
2024-07-21 21:50:42,240 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.92961629501196, Velocity: -0.02079839129169315, Movement: 0.5239560707446751, Collision: 0, Height: -1.0, Movement Penalty: -0.12100247139519071, Smoothness: -0.0, Curiosity: 2280.633544921875, Exploration: 0.12009240124587318, Total: 2231.2423226542273
2024-07-21 21:50:42,333 - AirSimEnvLogger - INFO - Action: [-0.60501236 -0.60501236 -0.60501236 -0.60501236], Velocity: (-0.6050123569759536, -0.6050123569759536, -0.6050123569759536), Duration: 1.0, Reward: 2231.2423226542273, Done: False
2024-07-21 21:50:42,396 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:50:42,396 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:45,397 - AirSimEnvLogger - INFO - Predictive model loss: 0.07670069485902786
2024-07-21 21:50:51,410 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.623084364638395, Velocity: -0.016515186920033703, Movement: 0.5218872532922446, Collision: 0, Height: -1.0, Movement Penalty: -0.12052469846996475, Smoothness: -0.0, Curiosity: 2249.952880859375, Exploration: 0.2714724179230963, Total: 2200.9033854983927
2024-07-21 21:50:51,535 - AirSimEnvLogger - INFO - Action: [-0.60262349 -0.60262349 -0.60262349 -0.60262349], Velocity: (-0.6026234923498237, -0.6026234923498237, -0.6026234923498237), Duration: 1.0, Reward: 2200.9033854983927, Done: False
2024-07-21 21:50:51,598 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:50:51,599 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:50:55,003 - AirSimEnvLogger - INFO - Predictive model loss: 0.025752900168299675
2024-07-21 21:51:01,750 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.06579230320963, Velocity: -0.025930771227605284, Movement: 0.17065533033401814, Collision: 0, Height: -1.0, Movement Penalty: -0.039411160362795954, Smoothness: -0.0, Curiosity: 2304.91552734375, Exploration: 0.20731288749925522, Total: 2255.399010093608
2024-07-21 21:51:01,800 - AirSimEnvLogger - INFO - Action: [0.1970558 0.1970558 0.1970558 0.1970558], Velocity: (0.19705580181397975, 0.19705580181397975, 0.19705580181397975), Duration: 1.0, Reward: 2255.399010093608, Done: False
2024-07-21 21:51:01,876 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:01,876 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:51:04,672 - AirSimEnvLogger - INFO - Predictive model loss: 0.18250194191932678
2024-07-21 21:51:11,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.602011158674735, Velocity: -0.06002869369863721, Movement: 0.3338895901936935, Collision: 0, Height: -1.0, Movement Penalty: -0.07710849791117712, Smoothness: -0.0, Curiosity: 2253.19873046875, Exploration: 0.09064510176250148, Total: 2204.1197551077826
2024-07-21 21:51:11,221 - AirSimEnvLogger - INFO - Action: [-0.38554249 -0.38554249 -0.38554249 -0.38554249], Velocity: (-0.38554248955588555, -0.38554248955588555, -0.38554248955588555), Duration: 1.0, Reward: 2204.1197551077826, Done: False
2024-07-21 21:51:11,316 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:11,316 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:51:14,199 - AirSimEnvLogger - INFO - Predictive model loss: 0.012562202289700508
2024-07-21 21:51:20,466 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.162510464511826, Velocity: -0.07614969808096292, Movement: 0.5572527272180932, Collision: 0, Height: -1.0, Movement Penalty: -0.12869200482640766, Smoothness: -0.0, Curiosity: 2205.942626953125, Exploration: 0.376783535875593, Total: 2157.3728311114537
2024-07-21 21:51:20,561 - AirSimEnvLogger - INFO - Action: [-0.64346002 -0.64346002 -0.64346002 -0.64346002], Velocity: (-0.6434600241320383, -0.6434600241320383, -0.6434600241320383), Duration: 1.0, Reward: 2157.3728311114537, Done: False
2024-07-21 21:51:20,655 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:20,656 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:51:23,550 - AirSimEnvLogger - INFO - Predictive model loss: 0.09016347676515579
2024-07-21 21:51:29,524 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -47.779837140739666, Velocity: -0.2849909687460092, Movement: 0.6806728745273417, Collision: 0, Height: -1.0, Movement Penalty: -0.15719466693537487, Smoothness: -0.0, Curiosity: 2166.737060546875, Exploration: 0.3303010494595636, Total: 2118.5213904196025
2024-07-21 21:51:29,617 - AirSimEnvLogger - INFO - Action: [-0.78597333 -0.78597333 -0.78597333 -0.78597333], Velocity: (-0.7859733346768742, -0.7859733346768742, -0.7859733346768742), Duration: 1.0, Reward: 2118.5213904196025, Done: False
2024-07-21 21:51:29,678 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:29,678 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:51:32,457 - AirSimEnvLogger - INFO - Predictive model loss: 0.6765636801719666
2024-07-21 21:51:38,377 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.1072422931034, Velocity: -0.010233481607603881, Movement: 0.057522832535228476, Collision: 0, Height: -1.0, Movement Penalty: -0.01328432913950557, Smoothness: -0.0, Curiosity: 2209.216796875, Exploration: 0.16017554289137392, Total: 2160.646782640586
2024-07-21 21:51:38,486 - AirSimEnvLogger - INFO - Action: [0.06642165 0.06642165 0.06642165 0.06642165], Velocity: (0.06642164569752784, 0.06642164569752784, 0.06642164569752784), Duration: 1.0, Reward: 2160.646782640586, Done: False
2024-07-21 21:51:38,550 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:38,550 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:51:41,487 - AirSimEnvLogger - INFO - Predictive model loss: 0.07521963864564896
2024-07-21 21:51:47,668 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.4956346144797, Velocity: -0.07583912938197543, Movement: 0.41775442831372395, Collision: 0, Height: -1.0, Movement Penalty: -0.0964762526568347, Smoothness: -0.0, Curiosity: 2256.76953125, Exploration: 0.4587324034546651, Total: 2207.8820686690124
2024-07-21 21:51:47,763 - AirSimEnvLogger - INFO - Action: [0.48238126 0.48238126 0.48238126 0.48238126], Velocity: (0.48238126328417347, 0.48238126328417347, 0.48238126328417347), Duration: 1.0, Reward: 2207.8820686690124, Done: False
2024-07-21 21:51:47,827 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:47,827 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:51:50,722 - AirSimEnvLogger - INFO - Predictive model loss: 0.17309318482875824
2024-07-21 21:51:56,990 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.93960339219105, Velocity: -0.08804273308954697, Movement: 0.6369055722229408, Collision: 0, Height: -1.0, Movement Penalty: -0.147087041428515, Smoothness: -0.0, Curiosity: 2307.1220703125, Exploration: 0.3759197624685154, Total: 2257.775747451765
2024-07-21 21:51:57,163 - AirSimEnvLogger - INFO - Action: [0.73543521 0.73543521 0.73543521 0.73543521], Velocity: (0.7354352071425749, 0.7354352071425749, 0.7354352071425749), Duration: 1.0, Reward: 2257.775747451765, Done: False
2024-07-21 21:51:57,226 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:51:57,226 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:00,124 - AirSimEnvLogger - INFO - Predictive model loss: 0.19289515912532806
2024-07-21 21:52:06,084 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.775287471261265, Velocity: -0.020274774139330102, Movement: 0.13091767555911948, Collision: 0, Height: -1.0, Movement Penalty: -0.030234142090295093, Smoothness: -0.0, Curiosity: 2280.622314453125, Exploration: 0.0910299235078272, Total: 2231.3691477903894
2024-07-21 21:52:06,209 - AirSimEnvLogger - INFO - Action: [0.15117071 0.15117071 0.15117071 0.15117071], Velocity: (0.15117071045147545, 0.15117071045147545, 0.15117071045147545), Duration: 1.0, Reward: 2231.3691477903894, Done: False
2024-07-21 21:52:06,270 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:52:06,270 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:09,066 - AirSimEnvLogger - INFO - Predictive model loss: 0.05847856029868126
2024-07-21 21:52:14,934 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.46678681154784, Velocity: -0.01956811900872004, Movement: 0.2434969479811221, Collision: 0, Height: -1.0, Movement Penalty: -0.05623321138550126, Smoothness: -0.0, Curiosity: 2242.038330078125, Exploration: 0.32989852748682774, Total: 2193.151436083023
2024-07-21 21:52:15,027 - AirSimEnvLogger - INFO - Action: [-0.28116606 -0.28116606 -0.28116606 -0.28116606], Velocity: (-0.2811660569275063, -0.2811660569275063, -0.2811660569275063), Duration: 1.0, Reward: 2193.151436083023, Done: False
2024-07-21 21:52:15,073 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:52:15,073 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:17,946 - AirSimEnvLogger - INFO - Predictive model loss: 0.057546183466911316
2024-07-21 21:52:24,032 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -48.91567815167419, Velocity: -0.02512550636400787, Movement: 0.2961739411391263, Collision: 0, Height: -1.0, Movement Penalty: -0.0683984418574508, Smoothness: -0.0, Curiosity: 2294.231201171875, Exploration: 0.04463909475737099, Total: 2244.8305425921894
2024-07-21 21:52:24,206 - AirSimEnvLogger - INFO - Action: [0.34199221 0.34199221 0.34199221 0.34199221], Velocity: (0.34199220928725393, 0.34199220928725393, 0.34199220928725393), Duration: 1.0, Reward: 2244.8305425921894, Done: False
2024-07-21 21:52:24,268 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:52:24,268 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:27,060 - AirSimEnvLogger - INFO - Predictive model loss: 0.013409975916147232
2024-07-21 21:52:33,209 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.33944123285233, Velocity: -0.07416170901457021, Movement: 0.5585217976552479, Collision: 0, Height: -1.0, Movement Penalty: -0.12898508408981244, Smoothness: -0.0, Curiosity: 2343.820556640625, Exploration: 0.3920689888871029, Total: 2294.077575613942
2024-07-21 21:52:33,349 - AirSimEnvLogger - INFO - Action: [0.64492542 0.64492542 0.64492542 0.64492542], Velocity: (0.6449254204490622, 0.6449254204490622, 0.6449254204490622), Duration: 1.0, Reward: 2294.077575613942, Done: False
2024-07-21 21:52:33,427 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:52:33,427 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:36,353 - AirSimEnvLogger - INFO - Predictive model loss: 0.01701902411878109
2024-07-21 21:52:42,372 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.770210404123716, Velocity: -0.11379414771762346, Movement: 0.6853570342472486, Collision: 0, Height: -1.0, Movement Penalty: -0.15827642728546104, Smoothness: -0.0, Curiosity: 2391.1044921875, Exploration: 0.3286092067425091, Total: 2340.9152942573073
2024-07-21 21:52:42,546 - AirSimEnvLogger - INFO - Action: [0.79138214 0.79138214 0.79138214 0.79138214], Velocity: (0.7913821364273051, 0.7913821364273051, 0.7913821364273051), Duration: 1.0, Reward: 2340.9152942573073, Done: False
2024-07-21 21:52:42,593 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:52:42,593 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:45,483 - AirSimEnvLogger - INFO - Predictive model loss: 0.010611222125589848
2024-07-21 21:52:51,405 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.108433148493475, Velocity: -0.10353461497553859, Movement: 0.6467984931236143, Collision: 0, Height: -1.0, Movement Penalty: -0.1493717136465452, Smoothness: -0.0, Curiosity: 2424.544677734375, Exploration: 0.2501826175833683, Total: 2373.999299058642
2024-07-21 21:52:51,531 - AirSimEnvLogger - INFO - Action: [0.74685857 0.74685857 0.74685857 0.74685857], Velocity: (0.746858568232726, 0.746858568232726, 0.746858568232726), Duration: 1.0, Reward: 2373.999299058642, Done: False
2024-07-21 21:52:51,577 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:52:51,577 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:52:54,609 - AirSimEnvLogger - INFO - Predictive model loss: 0.042909201234579086
2024-07-21 21:53:00,639 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.42946331950361, Velocity: -0.08147247540225075, Movement: 0.6043018382724327, Collision: 0, Height: -1.0, Movement Penalty: -0.1395575315993499, Smoothness: -0.0, Curiosity: 2455.480712890625, Exploration: 0.1994535759949192, Total: 2404.6038001376187
2024-07-21 21:53:00,781 - AirSimEnvLogger - INFO - Action: [0.69778766 0.69778766 0.69778766 0.69778766], Velocity: (0.6977876579967494, 0.6977876579967494, 0.6977876579967494), Duration: 1.0, Reward: 2404.6038001376187, Done: False
2024-07-21 21:53:00,796 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:00,796 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:03,351 - AirSimEnvLogger - INFO - Predictive model loss: 0.036856040358543396
2024-07-21 21:53:09,353 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.31541662553532, Velocity: -0.02451731726169414, Movement: 0.16724980317722002, Collision: 0, Height: -1.0, Movement Penalty: -0.03862468755451196, Smoothness: -0.0, Curiosity: 2433.918212890625, Exploration: 0.14752818877033286, Total: 2383.1383786454485
2024-07-21 21:53:09,495 - AirSimEnvLogger - INFO - Action: [0.19312344 0.19312344 0.19312344 0.19312344], Velocity: (0.1931234377725598, 0.1931234377725598, 0.1931234377725598), Duration: 1.0, Reward: 2383.1383786454485, Done: False
2024-07-21 21:53:09,589 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:09,589 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:12,520 - AirSimEnvLogger - INFO - Predictive model loss: 0.09111807495355606
2024-07-21 21:53:18,781 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.95704545159717, Velocity: -0.03350969546670478, Movement: 0.27725351544450644, Collision: 0, Height: -1.0, Movement Penalty: -0.06402895671026235, Smoothness: -0.0, Curiosity: 2388.077880859375, Exploration: 0.3456299689411533, Total: 2337.7037803448948
2024-07-21 21:53:18,891 - AirSimEnvLogger - INFO - Action: [-0.32014478 -0.32014478 -0.32014478 -0.32014478], Velocity: (-0.3201447835513117, -0.3201447835513117, -0.3201447835513117), Duration: 1.0, Reward: 2337.7037803448948, Done: False
2024-07-21 21:53:18,953 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:18,953 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:21,885 - AirSimEnvLogger - INFO - Predictive model loss: 0.14198197424411774
2024-07-21 21:53:28,173 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.38191592710128, Velocity: -0.04351880971431212, Movement: 0.2558155833321208, Collision: 0, Height: -1.0, Movement Penalty: -0.05907807835988044, Smoothness: -0.0, Curiosity: 2438.7470703125, Exploration: 0.02869610728361456, Total: 2387.873677751589
2024-07-21 21:53:28,283 - AirSimEnvLogger - INFO - Action: [0.29539039 0.29539039 0.29539039 0.29539039], Velocity: (0.2953903917994022, 0.2953903917994022, 0.2953903917994022), Duration: 1.0, Reward: 2387.873677751589, Done: False
2024-07-21 21:53:28,331 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:28,331 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:31,223 - AirSimEnvLogger - INFO - Predictive model loss: 0.09013745933771133
2024-07-21 21:53:37,311 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.990814442967654, Velocity: -0.04056539234437181, Movement: 0.23674130005442037, Collision: 0, Height: -1.0, Movement Penalty: -0.05467306132588863, Smoothness: -0.0, Curiosity: 2391.937744140625, Exploration: 0.01515893194464885, Total: 2341.452166964115
2024-07-21 21:53:37,375 - AirSimEnvLogger - INFO - Action: [-0.27336531 -0.27336531 -0.27336531 -0.27336531], Velocity: (-0.27336530662944314, -0.27336530662944314, -0.27336530662944314), Duration: 1.0, Reward: 2341.452166964115, Done: False
2024-07-21 21:53:37,421 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:37,421 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:40,311 - AirSimEnvLogger - INFO - Predictive model loss: 0.12757126986980438
2024-07-21 21:53:46,546 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.45489879268722, Velocity: -0.05521803704196886, Movement: 0.31324983213287544, Collision: 0, Height: -1.0, Movement Penalty: -0.07234194996220829, Smoothness: -0.0, Curiosity: 2447.558349609375, Exploration: 0.031828034258464186, Total: 2396.6129334623806
2024-07-21 21:53:46,687 - AirSimEnvLogger - INFO - Action: [0.36170975 0.36170975 0.36170975 0.36170975], Velocity: (0.3617097498110414, 0.3617097498110414, 0.3617097498110414), Duration: 1.0, Reward: 2396.6129334623806, Done: False
2024-07-21 21:53:46,748 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:46,748 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:49,839 - AirSimEnvLogger - INFO - Predictive model loss: 0.13426437973976135
2024-07-21 21:53:56,061 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.786157627976785, Velocity: -0.0787721744069411, Movement: 0.4726435676155145, Collision: 0, Height: -1.0, Movement Penalty: -0.10915235639742496, Smoothness: -0.0, Curiosity: 2486.409912109375, Exploration: 0.354203968675374, Total: 2435.20893809802
2024-07-21 21:53:56,093 - AirSimEnvLogger - INFO - Action: [0.54576178 0.54576178 0.54576178 0.54576178], Velocity: (0.5457617819871248, 0.5457617819871248, 0.5457617819871248), Duration: 1.0, Reward: 2435.20893809802, Done: False
2024-07-21 21:53:56,172 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:53:56,172 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:53:58,907 - AirSimEnvLogger - INFO - Predictive model loss: 0.09680384397506714
2024-07-21 21:54:04,614 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.34776829032219, Velocity: -0.003587779303280396, Movement: 0.196504392334506, Collision: 0, Height: -1.0, Movement Penalty: -0.045380745524508354, Smoothness: -0.0, Curiosity: 2429.350341796875, Exploration: 0.1294061322792731, Total: 2378.536798381211
2024-07-21 21:54:04,739 - AirSimEnvLogger - INFO - Action: [-0.22690373 -0.22690373 -0.22690373 -0.22690373], Velocity: (-0.22690372762254174, -0.22690372762254174, -0.22690372762254174), Duration: 1.0, Reward: 2378.536798381211, Done: False
2024-07-21 21:54:04,834 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:04,834 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:54:07,621 - AirSimEnvLogger - INFO - Predictive model loss: 0.03601736202836037
2024-07-21 21:54:13,396 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.42732369460889, Velocity: -0.003909899277289797, Movement: 0.03393354541737745, Collision: 0, Height: -1.0, Movement Penalty: -0.007836616632512505, Smoothness: -0.0, Curiosity: 2437.96240234375, Exploration: 0.21924093201503816, Total: 2387.085945461619
2024-07-21 21:54:13,491 - AirSimEnvLogger - INFO - Action: [-0.03918308 -0.03918308 -0.03918308 -0.03918308], Velocity: (-0.039183083162562526, -0.039183083162562526, -0.039183083162562526), Duration: 1.0, Reward: 2387.085945461619, Done: False
2024-07-21 21:54:13,600 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:13,600 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:54:16,516 - AirSimEnvLogger - INFO - Predictive model loss: 0.027859555557370186
2024-07-21 21:54:22,415 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.88359214665298, Velocity: -0.07407024344410375, Movement: 0.4157800648206833, Collision: 0, Height: -1.0, Movement Penalty: -0.09602029293916063, Smoothness: -0.0, Curiosity: 2493.74267578125, Exploration: 0.24738313887650487, Total: 2442.4187737947996
2024-07-21 21:54:22,447 - AirSimEnvLogger - INFO - Action: [0.48010146 0.48010146 0.48010146 0.48010146], Velocity: (0.4801014646958031, 0.4801014646958031, 0.4801014646958031), Duration: 1.0, Reward: 2442.4187737947996, Done: False
2024-07-21 21:54:22,493 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:22,493 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:54:25,295 - AirSimEnvLogger - INFO - Predictive model loss: 0.0326908715069294
2024-07-21 21:54:31,462 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.51561828478112, Velocity: -0.020632366038843622, Movement: 0.1445457393141568, Collision: 0, Height: -1.0, Movement Penalty: -0.03338140860129676, Smoothness: -0.0, Curiosity: 2446.842529296875, Exploration: 0.027724040833968715, Total: 2395.834770004085
2024-07-21 21:54:31,588 - AirSimEnvLogger - INFO - Action: [-0.16690704 -0.16690704 -0.16690704 -0.16690704], Velocity: (-0.1669070430064838, -0.1669070430064838, -0.1669070430064838), Duration: 1.0, Reward: 2395.834770004085, Done: False
2024-07-21 21:54:31,667 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:31,667 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:54:34,642 - AirSimEnvLogger - INFO - Predictive model loss: 0.06696944683790207
2024-07-21 21:54:40,984 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.8422303026299, Velocity: -0.033162548842376405, Movement: 0.22658257384352867, Collision: 0, Height: -1.0, Movement Penalty: -0.05232700400089582, Smoothness: -0.0, Curiosity: 2485.28955078125, Exploration: 0.06231999474727512, Total: 2433.9638958808655
2024-07-21 21:54:41,033 - AirSimEnvLogger - INFO - Action: [0.26163502 0.26163502 0.26163502 0.26163502], Velocity: (0.2616350200044791, 0.2616350200044791, 0.2616350200044791), Duration: 1.0, Reward: 2433.9638958808655, Done: False
2024-07-21 21:54:41,110 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:41,110 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:54:43,859 - AirSimEnvLogger - INFO - Predictive model loss: 0.04466930404305458
2024-07-21 21:54:50,131 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.29494143853165, Velocity: -0.08869293513664553, Movement: 0.5462163653296747, Collision: 0, Height: -1.0, Movement Penalty: -0.12614326622354666, Smoothness: -0.0, Curiosity: 2539.76513671875, Exploration: 0.3364473175286385, Total: 2488.0521075277707
2024-07-21 21:54:50,269 - AirSimEnvLogger - INFO - Action: [0.63071633 0.63071633 0.63071633 0.63071633], Velocity: (0.6307163311177333, 0.6307163311177333, 0.6307163311177333), Duration: 1.0, Reward: 2488.0521075277707, Done: False
2024-07-21 21:54:50,316 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:50,316 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:54:52,960 - AirSimEnvLogger - INFO - Predictive model loss: 0.05921291932463646
2024-07-21 21:54:58,996 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.863417377873965, Velocity: -0.0184733775289463, Movement: 0.15554879324625745, Collision: 0, Height: -1.0, Movement Penalty: -0.03592245506113927, Smoothness: -0.0, Curiosity: 2482.2578125, Exploration: 0.08333814250159706, Total: 2430.9155311606723
2024-07-21 21:54:59,090 - AirSimEnvLogger - INFO - Action: [-0.17961228 -0.17961228 -0.17961228 -0.17961228], Velocity: (-0.17961227530569635, -0.17961227530569635, -0.17961227530569635), Duration: 1.0, Reward: 2430.9155311606723, Done: False
2024-07-21 21:54:59,153 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:54:59,153 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:02,038 - AirSimEnvLogger - INFO - Predictive model loss: 0.014051711186766624
2024-07-21 21:55:08,166 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.422124490448404, Velocity: -0.08017851635586873, Movement: 0.490620172910508, Collision: 0, Height: -1.0, Movement Penalty: -0.11330387555989703, Smoothness: -0.0, Curiosity: 2430.347900390625, Exploration: 0.45792688523194713, Total: 2379.535116118572
2024-07-21 21:55:08,275 - AirSimEnvLogger - INFO - Action: [-0.56651938 -0.56651938 -0.56651938 -0.56651938], Velocity: (-0.5665193777994851, -0.5665193777994851, -0.5665193777994851), Duration: 1.0, Reward: 2379.535116118572, Done: False
2024-07-21 21:55:08,337 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:55:08,337 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:11,197 - AirSimEnvLogger - INFO - Predictive model loss: 0.006812672130763531
2024-07-21 21:55:17,637 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.05010726695879, Velocity: -0.09375347851860416, Movement: 0.5642437738840926, Collision: 0, Height: -1.0, Movement Penalty: -0.13030651789622047, Smoothness: -0.0, Curiosity: 2390.274658203125, Exploration: 0.33248838895143384, Total: 2339.8054887871685
2024-07-21 21:55:17,762 - AirSimEnvLogger - INFO - Action: [-0.65153259 -0.65153259 -0.65153259 -0.65153259], Velocity: (-0.6515325894811024, -0.6515325894811024, -0.6515325894811024), Duration: 1.0, Reward: 2339.8054887871685, Done: False
2024-07-21 21:55:17,841 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:55:17,842 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:20,669 - AirSimEnvLogger - INFO - Predictive model loss: 0.01892540045082569
2024-07-21 21:55:26,668 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.47322719023229, Velocity: -0.023074756520580633, Movement: 0.1217537996993549, Collision: 0, Height: -1.0, Movement Penalty: -0.028117835612512933, Smoothness: -0.0, Curiosity: 2443.71484375, Exploration: 0.16041142989032753, Total: 2392.7791903271423
2024-07-21 21:55:26,793 - AirSimEnvLogger - INFO - Action: [0.14058918 0.14058918 0.14058918 0.14058918], Velocity: (0.14058917806256466, 0.14058917806256466, 0.14058917806256466), Duration: 1.0, Reward: 2392.7791903271423, Done: False
2024-07-21 21:55:26,857 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:55:26,857 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:29,755 - AirSimEnvLogger - INFO - Predictive model loss: 0.1980898380279541
2024-07-21 21:55:35,572 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.9881545803019, Velocity: -0.0682707096781806, Movement: 0.37131975511268, Collision: 0, Height: -1.0, Movement Penalty: -0.0857526242278927, Smoothness: -0.0, Curiosity: 2388.769775390625, Exploration: 0.08230476123162422, Total: 2338.3028322880405
2024-07-21 21:55:35,603 - AirSimEnvLogger - INFO - Action: [-0.42876312 -0.42876312 -0.42876312 -0.42876312], Velocity: (-0.42876312113946347, -0.42876312113946347, -0.42876312113946347), Duration: 1.0, Reward: 2338.3028322880405, Done: False
2024-07-21 21:55:35,667 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:55:35,667 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:38,602 - AirSimEnvLogger - INFO - Predictive model loss: 0.03442355990409851
2024-07-21 21:55:44,376 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.226861131674035, Velocity: -0.003655698377841517, Movement: 0.03729871060223943, Collision: 0, Height: -1.0, Movement Penalty: -0.008613768242651554, Smoothness: -0.0, Curiosity: 2418.069580078125, Exploration: 0.078976451699269, Total: 2367.361432895867
2024-07-21 21:55:44,423 - AirSimEnvLogger - INFO - Action: [0.04306884 0.04306884 0.04306884 0.04306884], Velocity: (0.04306884121325777, 0.04306884121325777, 0.04306884121325777), Duration: 1.0, Reward: 2367.361432895867, Done: False
2024-07-21 21:55:44,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:55:44,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:47,358 - AirSimEnvLogger - INFO - Predictive model loss: 0.04018165171146393
2024-07-21 21:55:53,450 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.82575692728904, Velocity: -0.05883453564030824, Movement: 0.3389389870155652, Collision: 0, Height: -1.0, Movement Penalty: -0.07827460615691825, Smoothness: -0.0, Curiosity: 2373.01318359375, Exploration: 0.04890838054419736, Total: 2322.7011062955257
2024-07-21 21:55:53,497 - AirSimEnvLogger - INFO - Action: [-0.39137303 -0.39137303 -0.39137303 -0.39137303], Velocity: (-0.39137303078459124, -0.39137303078459124, -0.39137303078459124), Duration: 1.0, Reward: 2322.7011062955257, Done: False
2024-07-21 21:55:53,590 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:55:53,590 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:55:56,328 - AirSimEnvLogger - INFO - Predictive model loss: 0.007395516149699688
2024-07-21 21:56:02,500 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.011807461869935, Velocity: -0.0017007182833488554, Movement: 0.005032404493050514, Collision: 0, Height: -1.0, Movement Penalty: -0.0011621840354935186, Smoothness: -0.0, Curiosity: 2396.16259765625, Exploration: 0.06795681860198949, Total: 2345.6663736354412
2024-07-21 21:56:02,626 - AirSimEnvLogger - INFO - Action: [0.00581092 0.00581092 0.00581092 0.00581092], Velocity: (0.005810920177467593, 0.005810920177467593, 0.005810920177467593), Duration: 1.0, Reward: 2345.6663736354412, Done: False
2024-07-21 21:56:02,673 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:02,673 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:56:05,554 - AirSimEnvLogger - INFO - Predictive model loss: 0.019660137593746185
2024-07-21 21:56:11,603 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.68449859118557, Velocity: -0.03109132773029314, Movement: 0.2874955668735367, Collision: 0, Height: -1.0, Movement Penalty: -0.0663942571701042, Smoothness: -0.0, Curiosity: 2359.65283203125, Exploration: 0.043574351556377, Total: 2309.4822986589265
2024-07-21 21:56:11,604 - AirSimEnvLogger - INFO - Action: [-0.33197129 -0.33197129 -0.33197129 -0.33197129], Velocity: (-0.331971285850521, -0.331971285850521, -0.331971285850521), Duration: 1.0, Reward: 2309.4822986589265, Done: False
2024-07-21 21:56:11,665 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:11,665 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:56:14,625 - AirSimEnvLogger - INFO - Predictive model loss: 0.09322535246610641
2024-07-21 21:56:20,956 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.576700594892614, Velocity: -0.03495704789546131, Movement: 0.233930499369584, Collision: 0, Height: -1.0, Movement Penalty: -0.05402393471307716, Smoothness: -0.0, Curiosity: 2348.84326171875, Exploration: 0.16167187837050317, Total: 2298.805988253648
2024-07-21 21:56:21,001 - AirSimEnvLogger - INFO - Action: [-0.27011967 -0.27011967 -0.27011967 -0.27011967], Velocity: (-0.2701196735653858, -0.2701196735653858, -0.2701196735653858), Duration: 1.0, Reward: 2298.805988253648, Done: False
2024-07-21 21:56:21,065 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:21,065 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:56:23,867 - AirSimEnvLogger - INFO - Predictive model loss: 0.04392753541469574
2024-07-21 21:56:30,098 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.03001352141885, Velocity: -0.01404736856438874, Movement: 0.31590805472914973, Collision: 0, Height: -1.0, Movement Penalty: -0.07295584017481825, Smoothness: -0.0, Curiosity: 2404.727783203125, Exploration: 0.21566047314727907, Total: 2354.2537160612483
2024-07-21 21:56:30,208 - AirSimEnvLogger - INFO - Action: [0.3647792 0.3647792 0.3647792 0.3647792], Velocity: (0.3647792008740912, 0.3647792008740912, 0.3647792008740912), Duration: 1.0, Reward: 2354.2537160612483, Done: False
2024-07-21 21:56:30,254 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:30,254 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:56:33,260 - AirSimEnvLogger - INFO - Predictive model loss: 0.009084975346922874
2024-07-21 21:56:39,128 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.47823133267223, Velocity: -0.08970276736436168, Movement: 0.589388422537365, Collision: 0, Height: -1.0, Movement Penalty: -0.13611342576367866, Smoothness: -0.0, Curiosity: 2458.130615234375, Exploration: 0.42444728801707976, Total: 2407.2554941674834
2024-07-21 21:56:39,269 - AirSimEnvLogger - INFO - Action: [0.68056713 0.68056713 0.68056713 0.68056713], Velocity: (0.6805671288183932, 0.6805671288183932, 0.6805671288183932), Duration: 1.0, Reward: 2407.2554941674834, Done: False
2024-07-21 21:56:39,363 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:39,363 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:56:42,327 - AirSimEnvLogger - INFO - Predictive model loss: 0.040785569697618484
2024-07-21 21:56:47,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.07464040739121, Velocity: -0.00554669191802253, Movement: 0.10606120180599808, Collision: 0, Height: -1.0, Movement Penalty: -0.02449378536530729, Smoothness: -0.0, Curiosity: 2404.071044921875, Exploration: 0.08199952796025313, Total: 2353.517311412293
2024-07-21 21:56:48,054 - AirSimEnvLogger - INFO - Action: [-0.12246893 -0.12246893 -0.12246893 -0.12246893], Velocity: (-0.12246892682653643, -0.12246892682653643, -0.12246892682653643), Duration: 1.0, Reward: 2353.517311412293, Done: False
2024-07-21 21:56:48,085 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:48,085 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:56:51,039 - AirSimEnvLogger - INFO - Predictive model loss: 0.01835750602185726
2024-07-21 21:56:57,136 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -49.623687467514955, Velocity: -0.010125447013170781, Movement: 0.4837982316888562, Collision: 0, Height: -1.0, Movement Penalty: -0.11172841571961044, Smoothness: -0.0, Curiosity: 2350.851318359375, Exploration: 0.46147052954786266, Total: 2300.844624113531
2024-07-21 21:56:57,262 - AirSimEnvLogger - INFO - Action: [-0.55864208 -0.55864208 -0.55864208 -0.55864208], Velocity: (-0.5586420785980521, -0.5586420785980521, -0.5586420785980521), Duration: 1.0, Reward: 2300.844624113531, Done: False
2024-07-21 21:56:57,325 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:56:57,325 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:00,153 - AirSimEnvLogger - INFO - Predictive model loss: 0.07431136816740036
2024-07-21 21:57:06,599 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.082838503436314, Velocity: -0.007651732945217889, Movement: 0.18962313509878562, Collision: 0, Height: -1.0, Movement Penalty: -0.043791587237545865, Smoothness: -0.0, Curiosity: 2406.475341796875, Exploration: 0.03766065609674326, Total: 2355.9050515163412
2024-07-21 21:57:06,725 - AirSimEnvLogger - INFO - Action: [0.21895794 0.21895794 0.21895794 0.21895794], Velocity: (0.2189579361877293, 0.2189579361877293, 0.2189579361877293), Duration: 1.0, Reward: 2355.9050515163412, Done: False
2024-07-21 21:57:06,804 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:57:06,804 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:09,364 - AirSimEnvLogger - INFO - Predictive model loss: 0.043659910559654236
2024-07-21 21:57:14,960 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.0626168040963, Velocity: -0.00205509204883394, Movement: 0.0857037072344876, Collision: 0, Height: -1.0, Movement Penalty: -0.01979242337695212, Smoothness: -0.0, Curiosity: 2404.561767578125, Exploration: 0.241683449060204, Total: 2354.056634765432
2024-07-21 21:57:15,054 - AirSimEnvLogger - INFO - Action: [0.09896212 0.09896212 0.09896212 0.09896212], Velocity: (0.09896211688476059, 0.09896211688476059, 0.09896211688476059), Duration: 1.0, Reward: 2354.056634765432, Done: False
2024-07-21 21:57:15,070 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:57:15,070 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:17,917 - AirSimEnvLogger - INFO - Predictive model loss: 0.04597220569849014
2024-07-21 21:57:24,280 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.51210866756056, Velocity: -0.059313588441748714, Movement: 0.4662142030723418, Collision: 0, Height: -1.0, Movement Penalty: -0.10766755825753736, Smoothness: -0.0, Curiosity: 2457.820556640625, Exploration: 0.18758119932957634, Total: 2406.8570964993673
2024-07-21 21:57:24,421 - AirSimEnvLogger - INFO - Action: [0.53833779 0.53833779 0.53833779 0.53833779], Velocity: (0.5383377912876868, 0.5383377912876868, 0.5383377912876868), Duration: 1.0, Reward: 2406.8570964993673, Done: False
2024-07-21 21:57:24,515 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:57:24,515 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:27,457 - AirSimEnvLogger - INFO - Predictive model loss: 0.05462762713432312
2024-07-21 21:57:33,523 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -50.69703924497323, Velocity: -0.0703406814620661, Movement: 0.3946171855330065, Collision: 0, Height: -1.0, Movement Penalty: -0.09113293531773353, Smoothness: -0.0, Curiosity: 2476.279296875, Exploration: 0.24136352040201298, Total: 2425.1404171098884
2024-07-21 21:57:33,587 - AirSimEnvLogger - INFO - Action: [0.45566468 0.45566468 0.45566468 0.45566468], Velocity: (0.4556646765886676, 0.4556646765886676, 0.4556646765886676), Duration: 1.0, Reward: 2425.1404171098884, Done: False
2024-07-21 21:57:33,634 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:57:33,635 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:36,480 - AirSimEnvLogger - INFO - Predictive model loss: 0.007358792703598738
2024-07-21 21:57:42,641 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.13726034005173, Velocity: -0.05215605051358361, Movement: 0.6135836347720456, Collision: 0, Height: -1.0, Movement Penalty: -0.14170107068239582, Smoothness: -0.0, Curiosity: 2526.2802734375, Exploration: 0.2113224868803066, Total: 2474.7014528379063
2024-07-21 21:57:42,782 - AirSimEnvLogger - INFO - Action: [0.70850535 0.70850535 0.70850535 0.70850535], Velocity: (0.7085053534119791, 0.7085053534119791, 0.7085053534119791), Duration: 1.0, Reward: 2474.7014528379063, Done: False
2024-07-21 21:57:42,877 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:57:42,877 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:45,617 - AirSimEnvLogger - INFO - Predictive model loss: 0.01486966572701931
2024-07-21 21:57:51,577 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -51.60010943596646, Velocity: -0.11461950671250298, Movement: 0.7366560616629603, Collision: 0, Height: -1.0, Movement Penalty: -0.17012343020051188, Smoothness: -0.0, Curiosity: 2577.969970703125, Exploration: 0.31494165175155153, Total: 2525.948906029923
2024-07-21 21:57:51,734 - AirSimEnvLogger - INFO - Action: [0.85061715 0.85061715 0.85061715 0.85061715], Velocity: (0.8506171510025593, 0.8506171510025593, 0.8506171510025593), Duration: 1.0, Reward: 2525.948906029923, Done: False
2024-07-21 21:57:51,796 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:57:51,796 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:57:54,600 - AirSimEnvLogger - INFO - Predictive model loss: 0.07143136858940125
2024-07-21 21:58:00,006 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.0432993443937, Velocity: -0.04636805051706533, Movement: 0.7920236334322032, Collision: 0, Height: -1.0, Movement Penalty: -0.1829100231866512, Smoothness: -0.0, Curiosity: 2625.842041015625, Exploration: 0.29722669839427746, Total: 2573.381895303897
2024-07-21 21:58:00,100 - AirSimEnvLogger - INFO - Action: [0.91455012 0.91455012 0.91455012 0.91455012], Velocity: (0.914550115933256, 0.914550115933256, 0.914550115933256), Duration: 1.0, Reward: 2573.381895303897, Done: False
2024-07-21 21:58:00,176 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:58:00,176 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:58:03,029 - AirSimEnvLogger - INFO - Predictive model loss: 0.08367841690778732
2024-07-21 21:58:08,967 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.41605436402062, Velocity: -0.09989831115419458, Movement: 0.7140704049228549, Collision: 0, Height: -1.0, Movement Penalty: -0.16490749620102216, Smoothness: -0.0, Curiosity: 2663.068115234375, Exploration: 0.2498914986631497, Total: 2610.21706219252
2024-07-21 21:58:09,093 - AirSimEnvLogger - INFO - Action: [0.82453748 0.82453748 0.82453748 0.82453748], Velocity: (0.8245374810051107, 0.8245374810051107, 0.8245374810051107), Duration: 1.0, Reward: 2610.21706219252, Done: False
2024-07-21 21:58:09,125 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:58:09,125 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:58:12,096 - AirSimEnvLogger - INFO - Predictive model loss: 0.05740709602832794
2024-07-21 21:58:18,129 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.532705342476525, Velocity: -0.008230433517741616, Movement: 0.43917320500935875, Collision: 0, Height: -1.0, Movement Penalty: -0.10142270725320959, Smoothness: -0.0, Curiosity: 2668.23828125, Exploration: 0.16567837770299992, Total: 2615.253631786118
2024-07-21 21:58:18,254 - AirSimEnvLogger - INFO - Action: [0.50711354 0.50711354 0.50711354 0.50711354], Velocity: (0.5071135362660479, 0.5071135362660479, 0.5071135362660479), Duration: 1.0, Reward: 2615.253631786118, Done: False
2024-07-21 21:58:18,302 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:58:18,302 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:58:21,235 - AirSimEnvLogger - INFO - Predictive model loss: 0.07942721992731094
2024-07-21 21:58:27,218 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.71365820545852, Velocity: -0.06262221550002624, Movement: 0.3570917877600217, Collision: 0, Height: -1.0, Movement Penalty: -0.0824668159154613, Smoothness: -0.0, Curiosity: 2684.2587890625, Exploration: 0.13830355496009022, Total: 2631.079437895461
2024-07-21 21:58:27,359 - AirSimEnvLogger - INFO - Action: [0.41233408 0.41233408 0.41233408 0.41233408], Velocity: (0.4123340795773065, 0.4123340795773065, 0.4123340795773065), Duration: 1.0, Reward: 2631.079437895461, Done: False
2024-07-21 21:58:27,407 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:58:27,407 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:58:30,319 - AirSimEnvLogger - INFO - Predictive model loss: 0.01630614884197712
2024-07-21 21:58:36,547 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -52.35304806964963, Velocity: -0.014507266105373914, Movement: 0.18071064988242064, Collision: 0, Height: -1.0, Movement Penalty: -0.04173333694201911, Smoothness: -0.0, Curiosity: 2635.257568359375, Exploration: 0.2223582488283277, Total: 2582.4586447828583
2024-07-21 21:58:36,641 - AirSimEnvLogger - INFO - Action: [-0.20866668 -0.20866668 -0.20866668 -0.20866668], Velocity: (-0.20866668471009553, -0.20866668471009553, -0.20866668471009553), Duration: 1.0, Reward: 2582.4586447828583, Done: False
2024-07-21 21:58:36,735 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 21:58:36,735 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 21:58:43,855 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 21:58:43,899 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 22:00:15,201 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 22:00:15,217 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 22:00:15,217 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 100000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 512,
    "batch_size": 64,
    "n_epochs": 10,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 500,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 22:00:16,532 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 22:00:16,532 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 22:00:22,166 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 22:00:27,629 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 22:00:28,466 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:00:28,466 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:00:34,472 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.602435828469432, Velocity: -0.16483142363795067, Movement: 0.39914156657973066, Collision: 0, Height: -1.0, Movement Penalty: -0.09217779636383057, Smoothness: -0.0, Curiosity: 1.1335275173187256, Exploration: 0, Total: -8.975600532375253
2024-07-21 22:00:34,537 - AirSimEnvLogger - INFO - Action: [-0.46088898 -0.46088898 -0.46088898 -0.46088898], Velocity: (-0.46088898181915283, -0.46088898181915283, -0.46088898181915283), Duration: 1.0, Reward: -8.975600532375253, Done: False
2024-07-21 22:00:34,598 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:00:34,598 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:00:37,941 - AirSimEnvLogger - INFO - Predictive model loss: 0.020373482257127762
2024-07-21 22:00:44,003 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.890526870164015, Velocity: -0.004623290650055419, Movement: 0.0281128399623503, Collision: 0, Height: -1.0, Movement Penalty: -0.006492382287979126, Smoothness: -0.0, Curiosity: 0.07827060669660568, Exploration: 0.7521342793306417, Total: -10.139038101835629
2024-07-21 22:00:44,096 - AirSimEnvLogger - INFO - Action: [0.03246191 0.03246191 0.03246191 0.03246191], Velocity: (0.03246191143989563, 0.03246191143989563, 0.03246191143989563), Duration: 1.0, Reward: -10.139038101835629, Done: False
2024-07-21 22:00:44,159 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:00:44,159 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:00:46,488 - AirSimEnvLogger - INFO - Predictive model loss: 0.0020459461957216263
2024-07-21 22:00:52,095 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.361912569626176, Velocity: -0.059378023695024415, Movement: 0.37455239083152564, Collision: 0, Height: -1.0, Movement Penalty: -0.0864991694688797, Smoothness: -0.0, Curiosity: 1.2479541301727295, Exploration: 0.24337976067194395, Total: -9.554731146886427
2024-07-21 22:00:52,190 - AirSimEnvLogger - INFO - Action: [-0.43249585 -0.43249585 -0.43249585 -0.43249585], Velocity: (-0.4324958473443985, -0.4324958473443985, -0.4324958473443985), Duration: 1.0, Reward: -9.554731146886427, Done: False
2024-07-21 22:00:52,315 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:00:52,315 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:00:55,221 - AirSimEnvLogger - INFO - Predictive model loss: 0.035024553537368774
2024-07-21 22:01:01,112 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.681584346518022, Velocity: -0.0946502747630911, Movement: 0.58409095198639, Collision: 0, Height: -1.0, Movement Penalty: -0.13489002734422684, Smoothness: -0.0, Curiosity: 3.497504234313965, Exploration: 0.38033693975291555, Total: -7.591739923931215
2024-07-21 22:01:01,113 - AirSimEnvLogger - INFO - Action: [-0.67445014 -0.67445014 -0.67445014 -0.67445014], Velocity: (-0.6744501367211342, -0.6744501367211342, -0.6744501367211342), Duration: 1.0, Reward: -7.591739923931215, Done: False
2024-07-21 22:01:01,190 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:01,190 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:03,895 - AirSimEnvLogger - INFO - Predictive model loss: 0.10678425431251526
2024-07-21 22:01:09,643 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.007894615612743, Velocity: -0.01627933957209857, Movement: 0.12344064493874726, Collision: 0, Height: -1.0, Movement Penalty: -0.0285073958337307, Smoothness: -0.0, Curiosity: 0.4437010586261749, Exploration: 0.11320100552453402, Total: -10.036757267298576
2024-07-21 22:01:09,785 - AirSimEnvLogger - INFO - Action: [0.14253698 0.14253698 0.14253698 0.14253698], Velocity: (0.1425369791686535, 0.1425369791686535, 0.1425369791686535), Duration: 1.0, Reward: -10.036757267298576, Done: False
2024-07-21 22:01:09,863 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:09,863 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:12,657 - AirSimEnvLogger - INFO - Predictive model loss: 0.014916342683136463
2024-07-21 22:01:18,794 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.544859671990297, Velocity: -0.07753917496804132, Movement: 0.48234019881822654, Collision: 0, Height: -1.0, Movement Penalty: -0.11139169745147229, Smoothness: -0.0, Curiosity: 1.5764672756195068, Exploration: 0.470747560602613, Total: -8.356042595820178
2024-07-21 22:01:18,888 - AirSimEnvLogger - INFO - Action: [0.55695849 0.55695849 0.55695849 0.55695849], Velocity: (0.5569584872573614, 0.5569584872573614, 0.5569584872573614), Duration: 1.0, Reward: -8.356042595820178, Done: False
2024-07-21 22:01:18,950 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:18,950 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:21,698 - AirSimEnvLogger - INFO - Predictive model loss: 0.012140114791691303
2024-07-21 22:01:27,442 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.301668566538568, Velocity: -0.10716983044630568, Movement: 0.6724081611960012, Collision: 0, Height: -1.0, Movement Penalty: -0.15528601314872503, Smoothness: -0.0, Curiosity: 3.3248097896575928, Exploration: 0.3746992202632518, Total: -6.3849008032719805
2024-07-21 22:01:27,504 - AirSimEnvLogger - INFO - Action: [0.77643007 0.77643007 0.77643007 0.77643007], Velocity: (0.7764300657436252, 0.7764300657436252, 0.7764300657436252), Duration: 1.0, Reward: -6.3849008032719805, Done: False
2024-07-21 22:01:27,565 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:27,565 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:30,297 - AirSimEnvLogger - INFO - Predictive model loss: 0.05139388516545296
2024-07-21 22:01:36,646 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.204066618061058, Velocity: -0.10878253424410267, Movement: 0.7690942902791653, Collision: 0, Height: -1.0, Movement Penalty: -0.17761471820995212, Smoothness: -0.0, Curiosity: 5.012777805328369, Exploration: 0.31908691735919986, Total: -4.609911233630377
2024-07-21 22:01:36,647 - AirSimEnvLogger - INFO - Action: [0.88807359 0.88807359 0.88807359 0.88807359], Velocity: (0.8880735910497606, 0.8880735910497606, 0.8880735910497606), Duration: 1.0, Reward: -4.609911233630377, Done: False
2024-07-21 22:01:36,677 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:36,677 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:39,430 - AirSimEnvLogger - INFO - Predictive model loss: 0.11410753428936005
2024-07-21 22:01:45,708 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.926867408466679, Velocity: -0.009350649976284495, Movement: 0.046410650545358, Collision: 0, Height: -1.0, Movement Penalty: -0.010718080634251238, Smoothness: -0.0, Curiosity: 0.22542822360992432, Exploration: 0.19657666277819066, Total: -10.156023166656174
2024-07-21 22:01:45,850 - AirSimEnvLogger - INFO - Action: [-0.0535904 -0.0535904 -0.0535904 -0.0535904], Velocity: (-0.053590403171256185, -0.053590403171256185, -0.053590403171256185), Duration: 1.0, Reward: -10.156023166656174, Done: False
2024-07-21 22:01:45,929 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:45,929 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:48,835 - AirSimEnvLogger - INFO - Predictive model loss: 0.01862327754497528
2024-07-21 22:01:55,048 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.8850408692463, Velocity: -0.022181270120890367, Movement: 0.1268591443366485, Collision: 0, Height: -1.0, Movement Penalty: -0.029296864452771843, Smoothness: -0.0, Curiosity: 0.5282341241836548, Exploration: 0.2436684072847575, Total: -9.799869290418494
2024-07-21 22:01:55,127 - AirSimEnvLogger - INFO - Action: [0.14648432 0.14648432 0.14648432 0.14648432], Velocity: (0.1464843222638592, 0.1464843222638592, 0.1464843222638592), Duration: 1.0, Reward: -9.799869290418494, Done: False
2024-07-21 22:01:55,221 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:01:55,221 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:01:58,191 - AirSimEnvLogger - INFO - Predictive model loss: 0.03273861110210419
2024-07-21 22:02:04,043 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.837911459305682, Velocity: -0.02328696348950155, Movement: 0.17961900922035287, Collision: 0, Height: -1.0, Movement Penalty: -0.04148123332997784, Smoothness: -0.0, Curiosity: 0.8382807970046997, Exploration: 0.10688967961699643, Total: -9.472968687712163
2024-07-21 22:02:04,233 - AirSimEnvLogger - INFO - Action: [0.20740617 0.20740617 0.20740617 0.20740617], Velocity: (0.2074061666498892, 0.2074061666498892, 0.2074061666498892), Duration: 1.0, Reward: -9.472968687712163, Done: False
2024-07-21 22:02:04,328 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:02:04,328 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:02:07,047 - AirSimEnvLogger - INFO - Predictive model loss: 0.04602066054940224
2024-07-21 22:02:12,880 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.29144441371041, Velocity: -0.03929696113313081, Movement: 0.3319955471140719, Collision: 0, Height: -1.0, Movement Penalty: -0.07667108739842661, Smoothness: -0.0, Curiosity: 0.7065843939781189, Exploration: 0.1799290458678241, Total: -10.039262202399248
2024-07-21 22:02:12,973 - AirSimEnvLogger - INFO - Action: [-0.38335544 -0.38335544 -0.38335544 -0.38335544], Velocity: (-0.38335543699213304, -0.38335543699213304, -0.38335543699213304), Duration: 1.0, Reward: -10.039262202399248, Done: False
2024-07-21 22:02:13,036 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:02:13,036 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:02:15,925 - AirSimEnvLogger - INFO - Predictive model loss: 0.015384943224489689
2024-07-21 22:02:22,141 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.215299619062606, Velocity: -0.02983241841902025, Movement: 0.16558162665651238, Collision: 0, Height: -1.0, Movement Penalty: -0.038239438689197415, Smoothness: -0.0, Curiosity: 0.2579883337020874, Exploration: 0.20322064532906522, Total: -10.40949207038942
2024-07-21 22:02:22,267 - AirSimEnvLogger - INFO - Action: [-0.19119719 -0.19119719 -0.19119719 -0.19119719], Velocity: (-0.19119719344598707, -0.19119719344598707, -0.19119719344598707), Duration: 1.0, Reward: -10.40949207038942, Done: False
2024-07-21 22:02:22,378 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:02:22,378 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:02:25,064 - AirSimEnvLogger - INFO - Predictive model loss: 0.0119434529915452
2024-07-21 22:02:30,925 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.90408745789212, Velocity: -0.010830900280599895, Movement: 0.16097346144051303, Collision: 0, Height: -1.0, Movement Penalty: -0.03717522851802642, Smoothness: -0.0, Curiosity: 0.4978365898132324, Exploration: 0.16040963630791227, Total: -9.866491071413467
2024-07-21 22:02:31,020 - AirSimEnvLogger - INFO - Action: [0.18587614 0.18587614 0.18587614 0.18587614], Velocity: (0.18587614259013208, 0.18587614259013208, 0.18587614259013208), Duration: 1.0, Reward: -9.866491071413467, Done: False
2024-07-21 22:02:31,084 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:02:31,084 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:02:33,724 - AirSimEnvLogger - INFO - Predictive model loss: 0.023587100207805634
2024-07-21 22:02:39,833 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.310715155342077, Velocity: -0.05535788913224184, Movement: 0.3412339753348544, Collision: 0, Height: -1.0, Movement Penalty: -0.07880461100648972, Smoothness: -0.0, Curiosity: 0.7196792364120483, Exploration: 0.07670014991686497, Total: -10.070560222318738
2024-07-21 22:02:39,990 - AirSimEnvLogger - INFO - Action: [-0.39402306 -0.39402306 -0.39402306 -0.39402306], Velocity: (-0.3940230550324486, -0.3940230550324486, -0.3940230550324486), Duration: 1.0, Reward: -10.070560222318738, Done: False
2024-07-21 22:02:40,053 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:02:40,053 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:02:40,996 - AirSimEnvLogger - INFO - Training interrupted by user. Saving the current state...
2024-07-21 22:02:42,341 - AirSimEnvLogger - INFO - Model saved at models/saved_models\ppo_interrupted_model.pth
2024-07-21 22:02:42,341 - AirSimEnvLogger - INFO - Model saved at models/saved_models\ppo_interrupted_model.pth due to interruption.
2024-07-21 22:02:48,000 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 22:02:48,047 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 22:10:37,724 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 22:10:37,743 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 22:10:37,743 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 100000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 512,
    "batch_size": 64,
    "n_epochs": 10,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 500,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 22:10:39,136 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 22:10:39,136 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 22:10:44,786 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 22:10:49,684 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 22:10:50,267 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:10:50,267 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:10:56,118 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.515148067940927, Velocity: -0.12664537622237254, Movement: 0.2509375022711273, Collision: 0, Height: -1.0, Movement Penalty: -0.05795153379440308, Smoothness: -0.0, Curiosity: 0.7330028414726257, Exploration: 0, Total: -9.288654280669782
2024-07-21 22:10:56,229 - AirSimEnvLogger - INFO - Action: [-0.28975767 -0.28975767 -0.28975767 -0.28975767], Velocity: (-0.2897576689720154, -0.2897576689720154, -0.2897576689720154), Duration: 1.0, Reward: -9.288654280669782, Done: False
2024-07-21 22:10:56,291 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:10:56,291 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:10:59,655 - AirSimEnvLogger - INFO - Predictive model loss: 0.020281050354242325
2024-07-21 22:11:05,605 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.686906093501843, Velocity: -0.038432238080687624, Movement: 0.22690492276765428, Collision: 0, Height: -1.0, Movement Penalty: -0.05240144729614258, Smoothness: -0.0, Curiosity: 0.49370306730270386, Exploration: 0.6971946807442366, Total: -9.531125507896633
2024-07-21 22:11:05,683 - AirSimEnvLogger - INFO - Action: [0.26200724 0.26200724 0.26200724 0.26200724], Velocity: (0.2620072364807129, 0.2620072364807129, 0.2620072364807129), Duration: 1.0, Reward: -9.531125507896633, Done: False
2024-07-21 22:11:05,760 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:05,760 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:11:08,576 - AirSimEnvLogger - INFO - Predictive model loss: 0.008706546388566494
2024-07-21 22:11:14,438 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.504453115503468, Velocity: -0.08179871475332322, Movement: 0.4937773815423973, Collision: 0, Height: -1.0, Movement Penalty: -0.11403300166130066, Smoothness: -0.0, Curiosity: 2.113797426223755, Exploration: 0.2710326783440552, Total: -7.824385712150628
2024-07-21 22:11:14,595 - AirSimEnvLogger - INFO - Action: [0.57016501 0.57016501 0.57016501 0.57016501], Velocity: (0.5701650083065033, 0.5701650083065033, 0.5701650083065033), Duration: 1.0, Reward: -7.824385712150628, Done: False
2024-07-21 22:11:14,659 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:14,659 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:11:17,413 - AirSimEnvLogger - INFO - Predictive model loss: 0.040919430553913116
2024-07-21 22:11:23,129 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.51808318943721, Velocity: -0.03599673126815303, Movement: 0.46148060408539093, Collision: 0, Height: -1.0, Movement Penalty: -0.10657438039779664, Smoothness: -0.0, Curiosity: 2.330111026763916, Exploration: 0.22030390097428146, Total: -7.629581844202969
2024-07-21 22:11:23,271 - AirSimEnvLogger - INFO - Action: [0.5328719 0.5328719 0.5328719 0.5328719], Velocity: (0.5328719019889832, 0.5328719019889832, 0.5328719019889832), Duration: 1.0, Reward: -7.629581844202969, Done: False
2024-07-21 22:11:23,396 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:23,396 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:11:26,303 - AirSimEnvLogger - INFO - Predictive model loss: 0.0629449114203453
2024-07-21 22:11:32,131 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.389855354931864, Velocity: -0.09712804702392999, Movement: 0.6637474806873029, Collision: 0, Height: -1.0, Movement Penalty: -0.15328591465950014, Smoothness: -0.0, Curiosity: 4.97502326965332, Exploration: 0.23132232852610968, Total: -4.855059064453458
2024-07-21 22:11:32,258 - AirSimEnvLogger - INFO - Action: [0.76642957 0.76642957 0.76642957 0.76642957], Velocity: (0.7664295732975006, 0.7664295732975006, 0.7664295732975006), Duration: 1.0, Reward: -4.855059064453458, Done: False
2024-07-21 22:11:32,320 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:32,320 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:11:34,977 - AirSimEnvLogger - INFO - Predictive model loss: 0.14459334313869476
2024-07-21 22:11:40,779 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.349941100564484, Velocity: -0.12764219955541423, Movement: 0.7646715001512391, Collision: 0, Height: -1.0, Movement Penalty: -0.17659331858158112, Smoothness: -0.0, Curiosity: 7.599164962768555, Exploration: 0.313815538248896, Total: -2.1726054332238416
2024-07-21 22:11:40,903 - AirSimEnvLogger - INFO - Action: [0.88296659 0.88296659 0.88296659 0.88296659], Velocity: (0.8829665929079056, 0.8829665929079056, 0.8829665929079056), Duration: 1.0, Reward: -2.1726054332238416, Done: False
2024-07-21 22:11:40,997 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:40,997 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:11:43,954 - AirSimEnvLogger - INFO - Predictive model loss: 0.2513734996318817
2024-07-21 22:11:49,854 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.011386299227325, Velocity: -0.011824617795028265, Movement: 0.0506079370310204, Collision: 0, Height: -1.0, Movement Penalty: -0.011687402427196503, Smoothness: -0.0, Curiosity: 1.9034188985824585, Exploration: 0.1987671325920432, Total: -8.562192011957455
2024-07-21 22:11:50,008 - AirSimEnvLogger - INFO - Action: [-0.05843701 -0.05843701 -0.05843701 -0.05843701], Velocity: (-0.05843701213598251, -0.05843701213598251, -0.05843701213598251), Duration: 1.0, Reward: -8.562192011957455, Done: False
2024-07-21 22:11:50,070 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:50,070 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:11:52,901 - AirSimEnvLogger - INFO - Predictive model loss: 0.10278107225894928
2024-07-21 22:11:58,833 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.784100499397175, Velocity: -0.07100521901075284, Movement: 0.40770274555686825, Collision: 0, Height: -1.0, Movement Penalty: -0.09415491595864296, Smoothness: -0.0, Curiosity: 4.7221760749816895, Exploration: 0.13585547485144084, Total: -5.52777726070209
2024-07-21 22:11:58,959 - AirSimEnvLogger - INFO - Action: [0.47077458 0.47077458 0.47077458 0.47077458], Velocity: (0.4707745797932148, 0.4707745797932148, 0.4707745797932148), Duration: 1.0, Reward: -5.52777726070209, Done: False
2024-07-21 22:11:59,037 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:11:59,037 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:01,899 - AirSimEnvLogger - INFO - Predictive model loss: 0.211138054728508
2024-07-21 22:12:07,849 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.242653068622241, Velocity: -0.03981080803796807, Movement: 0.22902642250038813, Collision: 0, Height: -1.0, Movement Penalty: -0.052891386672854424, Smoothness: -0.0, Curiosity: 2.1340153217315674, Exploration: 0.07219714747403459, Total: -8.590395477961815
2024-07-21 22:12:07,957 - AirSimEnvLogger - INFO - Action: [-0.26445693 -0.26445693 -0.26445693 -0.26445693], Velocity: (-0.2644569333642721, -0.2644569333642721, -0.2644569333642721), Duration: 1.0, Reward: -8.590395477961815, Done: False
2024-07-21 22:12:08,004 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:12:08,004 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:10,854 - AirSimEnvLogger - INFO - Predictive model loss: 0.09913808107376099
2024-07-21 22:12:16,768 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.5804284038106, Velocity: -0.08975568579849022, Movement: 0.5474974193790328, Collision: 0, Height: -1.0, Movement Penalty: -0.12643911298364402, Smoothness: -0.0, Curiosity: 2.5440585613250732, Exploration: 0.44814006813121227, Total: -8.428843113173954
2024-07-21 22:12:16,879 - AirSimEnvLogger - INFO - Action: [-0.63219556 -0.63219556 -0.63219556 -0.63219556], Velocity: (-0.63219556491822, -0.63219556491822, -0.63219556491822), Duration: 1.0, Reward: -8.428843113173954, Done: False
2024-07-21 22:12:16,973 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:12:16,973 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:19,790 - AirSimEnvLogger - INFO - Predictive model loss: 0.0547037199139595
2024-07-21 22:12:25,687 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.777177263764946, Velocity: -0.10788127324237613, Movement: 0.7063630408953414, Collision: 0, Height: -1.0, Movement Penalty: -0.1631275567226112, Smoothness: -0.0, Curiosity: 3.3365509510040283, Exploration: 0.3874239088884281, Total: -7.844979893676774
2024-07-21 22:12:25,830 - AirSimEnvLogger - INFO - Action: [-0.81563778 -0.81563778 -0.81563778 -0.81563778], Velocity: (-0.8156377836130559, -0.8156377836130559, -0.8156377836130559), Duration: 1.0, Reward: -7.844979893676774, Done: False
2024-07-21 22:12:25,891 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:12:25,891 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:28,690 - AirSimEnvLogger - INFO - Predictive model loss: 0.05113179236650467
2024-07-21 22:12:34,615 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.152004024281517, Velocity: -0.0058459375044988375, Movement: 0.0121809735496243, Collision: 0, Height: -1.0, Movement Penalty: -0.0028130753431469206, Smoothness: -0.0, Curiosity: 0.6100112795829773, Exploration: 0.13531384202463004, Total: -10.01115635615795
2024-07-21 22:12:34,725 - AirSimEnvLogger - INFO - Action: [0.01406538 0.01406538 0.01406538 0.01406538], Velocity: (0.014065376715734601, 0.014065376715734601, 0.014065376715734601), Duration: 1.0, Reward: -10.01115635615795, Done: False
2024-07-21 22:12:34,788 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:12:34,788 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:37,618 - AirSimEnvLogger - INFO - Predictive model loss: 0.024130510166287422
2024-07-21 22:12:43,316 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.408764102907817, Velocity: -0.06131454713004439, Movement: 0.40101800315100833, Collision: 0, Height: -1.0, Movement Penalty: -0.09261114082764835, Smoothness: -0.0, Curiosity: 1.100728154182434, Exploration: 0.11301243483952106, Total: -9.778337593397982
2024-07-21 22:12:43,459 - AirSimEnvLogger - INFO - Action: [-0.4630557 -0.4630557 -0.4630557 -0.4630557], Velocity: (-0.4630557041382417, -0.4630557041382417, -0.4630557041382417), Duration: 1.0, Reward: -9.778337593397982, Done: False
2024-07-21 22:12:43,504 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:12:43,504 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:46,324 - AirSimEnvLogger - INFO - Predictive model loss: 0.017625123262405396
2024-07-21 22:12:51,799 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.63852380765852, Velocity: -0.10295897449685938, Movement: 0.5854722366782913, Collision: 0, Height: -1.0, Movement Penalty: -0.13520902137970553, Smoothness: -0.0, Curiosity: 2.342411756515503, Exploration: 0.3491028562381449, Total: -8.711752689212823
2024-07-21 22:12:51,892 - AirSimEnvLogger - INFO - Action: [-0.67604511 -0.67604511 -0.67604511 -0.67604511], Velocity: (-0.6760451068985276, -0.6760451068985276, -0.6760451068985276), Duration: 1.0, Reward: -8.711752689212823, Done: False
2024-07-21 22:12:51,939 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:12:51,939 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:12:54,820 - AirSimEnvLogger - INFO - Predictive model loss: 0.051197439432144165
2024-07-21 22:13:00,125 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.022940826306542, Velocity: -0.019346045316653696, Movement: 0.09780113179626414, Collision: 0, Height: -1.0, Movement Penalty: -0.022586203907849267, Smoothness: -0.0, Curiosity: 0.17093276977539062, Exploration: 0.12320062985757704, Total: -10.323207459698022
2024-07-21 22:13:00,236 - AirSimEnvLogger - INFO - Action: [0.11293102 0.11293102 0.11293102 0.11293102], Velocity: (0.11293101953924634, 0.11293101953924634, 0.11293101953924634), Duration: 1.0, Reward: -10.323207459698022, Done: False
2024-07-21 22:13:00,313 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:00,313 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:03,107 - AirSimEnvLogger - INFO - Predictive model loss: 0.003688087686896324
2024-07-21 22:13:09,055 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.377759039757573, Velocity: -0.05843554064232351, Movement: 0.38281992833903666, Collision: 0, Height: -1.0, Movement Penalty: -0.08840847547107843, Smoothness: -0.0, Curiosity: 1.0575151443481445, Exploration: 0.0806106951274777, Total: -9.798156437362413
2024-07-21 22:13:09,212 - AirSimEnvLogger - INFO - Action: [-0.44204238 -0.44204238 -0.44204238 -0.44204238], Velocity: (-0.4420423773553921, -0.4420423773553921, -0.4420423773553921), Duration: 1.0, Reward: -9.798156437362413, Done: False
2024-07-21 22:13:09,306 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:09,306 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:12,075 - AirSimEnvLogger - INFO - Predictive model loss: 0.03201833367347717
2024-07-21 22:13:17,973 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.703375956980196, Velocity: -0.12528837759961284, Movement: 0.6243245897022754, Collision: 0, Height: -1.0, Movement Penalty: -0.14418158797052458, Smoothness: -0.0, Curiosity: 3.3119490146636963, Exploration: 0.39662609592824705, Total: -7.797417129439907
2024-07-21 22:13:18,131 - AirSimEnvLogger - INFO - Action: [-0.72090794 -0.72090794 -0.72090794 -0.72090794], Velocity: (-0.7209079398526228, -0.7209079398526228, -0.7209079398526228), Duration: 1.0, Reward: -7.797417129439907, Done: False
2024-07-21 22:13:18,193 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:18,193 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:20,599 - AirSimEnvLogger - INFO - Predictive model loss: 0.10921988636255264
2024-07-21 22:13:26,402 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.931054071756039, Velocity: -0.13705514347600903, Movement: 0.7439949058529127, Collision: 0, Height: -1.0, Movement Penalty: -0.17181826366795577, Smoothness: -0.0, Curiosity: 5.715851783752441, Exploration: 0.36448160178678923, Total: -5.626826878954806
2024-07-21 22:13:26,512 - AirSimEnvLogger - INFO - Action: [-0.85909132 -0.85909132 -0.85909132 -0.85909132], Velocity: (-0.8590913183397788, -0.8590913183397788, -0.8590913183397788), Duration: 1.0, Reward: -5.626826878954806, Done: False
2024-07-21 22:13:26,574 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:26,574 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:29,371 - AirSimEnvLogger - INFO - Predictive model loss: 0.2163500040769577
2024-07-21 22:13:35,078 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.108226637547542, Velocity: -0.17930721221220156, Movement: 0.804712131733752, Collision: 0, Height: -1.0, Movement Penalty: -0.18584030635065574, Smoothness: -0.0, Curiosity: 8.261346817016602, Exploration: 0.3419179479336712, Total: -3.2664298684600186
2024-07-21 22:13:35,173 - AirSimEnvLogger - INFO - Action: [-0.92920153 -0.92920153 -0.92920153 -0.92920153], Velocity: (-0.9292015317532787, -0.9292015317532787, -0.9292015317532787), Duration: 1.0, Reward: -3.2664298684600186, Done: False
2024-07-21 22:13:35,236 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:35,236 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:38,093 - AirSimEnvLogger - INFO - Predictive model loss: 0.3527953326702118
2024-07-21 22:13:43,671 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.26184071820346, Velocity: -0.15410990529542837, Movement: 0.8323252260388698, Collision: 0, Height: -1.0, Movement Penalty: -0.19221727732274305, Smoothness: -0.0, Curiosity: 11.045238494873047, Exploration: 0.3320935627617087, Total: -0.635214801440693
2024-07-21 22:13:43,737 - AirSimEnvLogger - INFO - Action: [-0.96108639 -0.96108639 -0.96108639 -0.96108639], Velocity: (-0.9610863866137151, -0.9610863866137151, -0.9610863866137151), Duration: 1.0, Reward: -0.635214801440693, Done: False
2024-07-21 22:13:43,799 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:43,799 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:46,321 - AirSimEnvLogger - INFO - Predictive model loss: 0.5153474807739258
2024-07-21 22:13:52,084 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.411433695449752, Velocity: -0.010605351002397314, Movement: 0.05004451810118874, Collision: 0, Height: -1.0, Movement Penalty: -0.011557286398874567, Smoothness: -0.0, Curiosity: 4.007880210876465, Exploration: 0.2230399615422601, Total: -6.852087239184079
2024-07-21 22:13:52,209 - AirSimEnvLogger - INFO - Action: [-0.05778643 -0.05778643 -0.05778643 -0.05778643], Velocity: (-0.05778643199437283, -0.05778643199437283, -0.05778643199437283), Duration: 1.0, Reward: -6.852087239184079, Done: False
2024-07-21 22:13:52,272 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:52,272 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:13:54,486 - AirSimEnvLogger - INFO - Predictive model loss: 0.28568199276924133
2024-07-21 22:13:59,436 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.788889477107519, Velocity: -0.048220718956704725, Movement: 0.4554760971053092, Collision: 0, Height: -1.0, Movement Penalty: -0.10518769890927615, Smoothness: -0.0, Curiosity: 7.879108428955078, Exploration: 0.1529823399382256, Total: -3.3682443777222137
2024-07-21 22:13:59,577 - AirSimEnvLogger - INFO - Action: [-0.52593849 -0.52593849 -0.52593849 -0.52593849], Velocity: (-0.5259384945463808, -0.5259384945463808, -0.5259384945463808), Duration: 1.0, Reward: -3.3682443777222137, Done: False
2024-07-21 22:13:59,702 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:13:59,702 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:02,583 - AirSimEnvLogger - INFO - Predictive model loss: 0.46426141262054443
2024-07-21 22:14:08,241 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.104631672220012, Velocity: -0.10554184033200319, Movement: 0.6101600219435038, Collision: 0, Height: -1.0, Movement Penalty: -0.14091042116713198, Smoothness: -0.0, Curiosity: 11.731826782226562, Exploration: 0.35128420090135287, Total: 0.21240348466918607
2024-07-21 22:14:08,398 - AirSimEnvLogger - INFO - Action: [-0.70455211 -0.70455211 -0.70455211 -0.70455211], Velocity: (-0.7045521058356599, -0.7045521058356599, -0.7045521058356599), Duration: 1.0, Reward: 0.21240348466918607, Done: False
2024-07-21 22:14:08,476 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:14:08,476 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:11,019 - AirSimEnvLogger - INFO - Predictive model loss: 0.6536570191383362
2024-07-21 22:14:16,946 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.410146947608874, Velocity: -0.1369043987403301, Movement: 0.7290491433771012, Collision: 0, Height: -1.0, Movement Penalty: -0.16836668767249421, Smoothness: -0.0, Curiosity: 16.419780731201172, Exploration: 0.32982417985920154, Total: 4.589686441322428
2024-07-21 22:14:17,087 - AirSimEnvLogger - INFO - Action: [-0.84183344 -0.84183344 -0.84183344 -0.84183344], Velocity: (-0.841833438362471, -0.841833438362471, -0.841833438362471), Duration: 1.0, Reward: 4.589686441322428, Done: False
2024-07-21 22:14:17,181 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:14:17,181 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:20,159 - AirSimEnvLogger - INFO - Predictive model loss: 0.8953921794891357
2024-07-21 22:14:25,929 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.687857581567108, Velocity: -0.14243065057771293, Movement: 0.7974664263158426, Collision: 0, Height: -1.0, Movement Penalty: -0.18416698236125628, Smoothness: -0.0, Curiosity: 21.484941482543945, Exploration: 0.3398487083254522, Total: 9.38056784733751
2024-07-21 22:14:26,038 - AirSimEnvLogger - INFO - Action: [-0.92083491 -0.92083491 -0.92083491 -0.92083491], Velocity: (-0.9208349118062813, -0.9208349118062813, -0.9208349118062813), Duration: 1.0, Reward: 9.38056784733751, Done: False
2024-07-21 22:14:26,099 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:14:26,099 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:28,983 - AirSimEnvLogger - INFO - Predictive model loss: 1.1769235134124756
2024-07-21 22:14:34,891 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.94464343431861, Velocity: -0.15744404946869192, Movement: 0.8314663974256735, Collision: 0, Height: -1.0, Movement Penalty: -0.1920189393503364, Smoothness: -0.0, Curiosity: 26.795312881469727, Exploration: 0.3375201083628698, Total: 14.432950493092134
2024-07-21 22:14:35,001 - AirSimEnvLogger - INFO - Action: [-0.9600947 -0.9600947 -0.9600947 -0.9600947], Velocity: (-0.9600946967516819, -0.9600946967516819, -0.9600946967516819), Duration: 1.0, Reward: 14.432950493092134, Done: False
2024-07-21 22:14:35,048 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:14:35,048 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:37,504 - AirSimEnvLogger - INFO - Predictive model loss: 1.4891611337661743
2024-07-21 22:14:43,620 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.945125569697737, Velocity: -0.009229757343564727, Movement: 0.016249959500712608, Collision: 0, Height: -1.0, Movement Penalty: -0.003752767396822776, Smoothness: -0.0, Curiosity: 16.583576202392578, Exploration: 0.23964700138703887, Total: 5.193045077911192
2024-07-21 22:14:43,760 - AirSimEnvLogger - INFO - Action: [0.01876384 0.01876384 0.01876384 0.01876384], Velocity: (0.01876383698411388, 0.01876383698411388, 0.01876383698411388), Duration: 1.0, Reward: 5.193045077911192, Done: False
2024-07-21 22:14:43,822 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:14:43,822 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:46,573 - AirSimEnvLogger - INFO - Predictive model loss: 1.0651518106460571
2024-07-21 22:14:52,412 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.242753506739163, Velocity: -0.015492188070043806, Movement: 0.44065057766043625, Collision: 0, Height: -1.0, Movement Penalty: -0.1017638918523268, Smoothness: -0.0, Curiosity: 13.586735725402832, Exploration: 0.548303157536314, Total: 2.9793518610363137
2024-07-21 22:14:52,521 - AirSimEnvLogger - INFO - Action: [0.50881946 0.50881946 0.50881946 0.50881946], Velocity: (0.508819459261634, 0.508819459261634, 0.508819459261634), Duration: 1.0, Reward: 2.9793518610363137, Done: False
2024-07-21 22:14:52,630 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:14:52,630 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:14:55,487 - AirSimEnvLogger - INFO - Predictive model loss: 0.8036142587661743
2024-07-21 22:15:01,288 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.8442142837918, Velocity: -0.09348066370375506, Movement: 0.6205923332539137, Collision: 0, Height: -1.0, Movement Penalty: -0.14331966026446602, Smoothness: -0.0, Curiosity: 11.839177131652832, Exploration: 0.3902795982688695, Total: 1.5906021856158115
2024-07-21 22:15:01,433 - AirSimEnvLogger - INFO - Action: [0.7165983 0.7165983 0.7165983 0.7165983], Velocity: (0.71659830132233, 0.71659830132233, 0.71659830132233), Duration: 1.0, Reward: 1.5906021856158115, Done: False
2024-07-21 22:15:01,476 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:01,476 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:04,277 - AirSimEnvLogger - INFO - Predictive model loss: 0.6215266585350037
2024-07-21 22:15:10,228 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.616760700473314, Velocity: -0.06009969584326, Movement: 0.6852011548755331, Collision: 0, Height: -1.0, Movement Penalty: -0.1582404284865726, Smoothness: -0.0, Curiosity: 10.013494491577148, Exploration: 0.2902608812422054, Total: -0.025708228355496923
2024-07-21 22:15:10,307 - AirSimEnvLogger - INFO - Action: [0.79120214 0.79120214 0.79120214 0.79120214], Velocity: (0.791202142432863, 0.791202142432863, 0.791202142432863), Duration: 1.0, Reward: -0.025708228355496923, Done: False
2024-07-21 22:15:10,402 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:10,402 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:13,273 - AirSimEnvLogger - INFO - Predictive model loss: 0.47424647212028503
2024-07-21 22:15:19,035 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.523851391624829, Velocity: -0.09509559145406932, Movement: 0.6639514857933917, Collision: 0, Height: -1.0, Movement Penalty: -0.1533330276206667, Smoothness: -0.0, Curiosity: 8.132627487182617, Exploration: 0.23827925692956264, Total: -1.829642540293079
2024-07-21 22:15:19,147 - AirSimEnvLogger - INFO - Action: [0.76666514 0.76666514 0.76666514 0.76666514], Velocity: (0.7666651381033334, 0.7666651381033334, 0.7666651381033334), Duration: 1.0, Reward: -1.829642540293079, Done: False
2024-07-21 22:15:19,209 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:19,209 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:21,973 - AirSimEnvLogger - INFO - Predictive model loss: 0.35941335558891296
2024-07-21 22:15:27,907 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.37812838426633, Velocity: -0.01907824598122546, Movement: 0.09494151325494975, Collision: 0, Height: -1.0, Movement Penalty: -0.02192580329400626, Smoothness: -0.0, Curiosity: 8.065897941589355, Exploration: 0.23346366323200177, Total: -2.7581127145230466
2024-07-21 22:15:28,016 - AirSimEnvLogger - INFO - Action: [-0.10962902 -0.10962902 -0.10962902 -0.10962902], Velocity: (-0.10962901647003132, -0.10962901647003132, -0.10962901647003132), Duration: 1.0, Reward: -2.7581127145230466, Done: False
2024-07-21 22:15:28,078 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:28,078 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:30,878 - AirSimEnvLogger - INFO - Predictive model loss: 0.49954164028167725
2024-07-21 22:15:36,834 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.998462823780606, Velocity: -0.08937929944282151, Movement: 0.4775981810721586, Collision: 0, Height: -1.0, Movement Penalty: -0.11029657536259457, Smoothness: -0.0, Curiosity: 12.86332893371582, Exploration: 0.5013671052688289, Total: 1.4829580718715683
2024-07-21 22:15:36,975 - AirSimEnvLogger - INFO - Action: [-0.55148288 -0.55148288 -0.55148288 -0.55148288], Velocity: (-0.5514828768129728, -0.5514828768129728, -0.5514828768129728), Duration: 1.0, Reward: 1.4829580718715683, Done: False
2024-07-21 22:15:37,037 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:37,037 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:39,899 - AirSimEnvLogger - INFO - Predictive model loss: 0.7244760990142822
2024-07-21 22:15:45,985 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.399272607824951, Velocity: -0.10244172760471809, Movement: 0.6430992155278372, Collision: 0, Height: -1.0, Movement Penalty: -0.1485174020802536, Smoothness: -0.0, Curiosity: 17.979455947875977, Exploration: 0.39268685596419706, Total: 6.17603233329609
2024-07-21 22:15:46,095 - AirSimEnvLogger - INFO - Action: [-0.74258701 -0.74258701 -0.74258701 -0.74258701], Velocity: (-0.742587010401268, -0.742587010401268, -0.742587010401268), Duration: 1.0, Reward: 6.17603233329609, Done: False
2024-07-21 22:15:46,188 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:46,188 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:48,985 - AirSimEnvLogger - INFO - Predictive model loss: 0.9738790392875671
2024-07-21 22:15:54,727 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.744001466094732, Velocity: -0.14195956066127702, Movement: 0.754503128316073, Collision: 0, Height: -1.0, Movement Penalty: -0.17424503369507982, Smoothness: -0.0, Curiosity: 23.700103759765625, Exploration: 0.3473406584949639, Total: 11.540302610175317
2024-07-21 22:15:54,868 - AirSimEnvLogger - INFO - Action: [-0.87122517 -0.87122517 -0.87122517 -0.87122517], Velocity: (-0.871225168475399, -0.871225168475399, -0.871225168475399), Duration: 1.0, Reward: 11.540302610175317, Done: False
2024-07-21 22:15:54,932 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:15:54,932 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:15:57,032 - AirSimEnvLogger - INFO - Predictive model loss: 1.2659659385681152
2024-07-21 22:16:50,004 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 22:16:50,020 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 22:16:50,020 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 100000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 512,
    "batch_size": 64,
    "n_epochs": 10,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 500,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 22:16:51,320 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 22:16:51,320 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 22:16:57,072 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 22:17:02,320 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 22:17:02,898 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:02,898 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:08,702 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.275072484966518, Velocity: -0.9003249514156568, Movement: 0.965704711378736, Collision: 0, Height: -1.0, Movement Penalty: -0.24581400519632843, Smoothness: -0.0, Curiosity: 7.591336727142334, Exploration: 0, Total: -3.2614765959708922
2024-07-21 22:17:08,814 - AirSimEnvLogger - INFO - Action: [-0.82659666 -0.0358204  -1.74522125 -1.52056245], Velocity: (-0.826596655629299, -0.03582040041668533, -1.7452212542061243), Duration: 1.0, Reward: -3.2614765959708922, Done: False
2024-07-21 22:17:08,875 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:08,875 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:12,102 - AirSimEnvLogger - INFO - Predictive model loss: 0.06581628322601318
2024-07-21 22:17:18,031 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.55545037022399, Velocity: -0.12152168166523457, Movement: 0.71743058846834, Collision: 0, Height: -1.0, Movement Penalty: -0.14350775278808392, Smoothness: -0.0, Curiosity: 3.329871892929077, Exploration: 0.9799837463753268, Total: -7.483647977803586
2024-07-21 22:17:18,185 - AirSimEnvLogger - INFO - Action: [ 1.04250948  0.74132819 -0.64994852  0.02491814], Velocity: (1.0425094833533768, 0.7413281927569373, -0.6499485247480166), Duration: 1.0, Reward: -7.483647977803586, Done: False
2024-07-21 22:17:18,248 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:18,248 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:20,959 - AirSimEnvLogger - INFO - Predictive model loss: 0.06605880707502365
2024-07-21 22:17:27,148 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.434480960009703, Velocity: -0.039398099971373494, Movement: 0.2644576439677666, Collision: 0, Height: -1.0, Movement Penalty: -0.12155945529944817, Smoothness: -0.0, Curiosity: 2.013638734817505, Exploration: 0.3070058330543387, Total: -8.877926351081072
2024-07-21 22:17:27,257 - AirSimEnvLogger - INFO - Action: [-0.32624131  0.11560457 -0.39994196 -1.09449474], Velocity: (-0.32624131130871614, 0.11560457324710405, -0.39994195985399217), Duration: 1.0, Reward: -8.877926351081072, Done: False
2024-07-21 22:17:27,336 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:27,336 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:30,198 - AirSimEnvLogger - INFO - Predictive model loss: 0.017544828355312347
2024-07-21 22:17:36,166 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.702287540714881, Velocity: -0.019544966422225977, Movement: 0.38776349715009184, Collision: 0, Height: -1.0, Movement Penalty: -0.13660365239387945, Smoothness: -0.0, Curiosity: 2.6770036220550537, Exploration: 0.23772268717533704, Total: -8.48657713384765
2024-07-21 22:17:36,260 - AirSimEnvLogger - INFO - Action: [-0.05087055 -0.16096774 -0.75693044 -1.12455043], Velocity: (-0.05087055466206025, -0.1609677404616734, -0.7569304407191686), Duration: 1.0, Reward: -8.48657713384765, Done: False
2024-07-21 22:17:36,337 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:36,337 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:39,219 - AirSimEnvLogger - INFO - Predictive model loss: 0.02760288678109646
2024-07-21 22:17:44,897 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.131697018068392, Velocity: -0.059423370037944935, Movement: 0.5792938118828295, Collision: 0, Height: -1.0, Movement Penalty: -0.11899025479444886, Smoothness: -0.0, Curiosity: 2.4081830978393555, Exploration: 0.26677033469411326, Total: -8.146493073986814
2024-07-21 22:17:45,007 - AirSimEnvLogger - INFO - Action: [-1.15144276  0.11886706  0.04873892 -0.27118774], Velocity: (-1.1514427561799798, 0.11886706414926085, 0.04873891919342421), Duration: 1.0, Reward: -8.146493073986814, Done: False
2024-07-21 22:17:45,037 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:45,037 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:47,799 - AirSimEnvLogger - INFO - Predictive model loss: 0.056823018938302994
2024-07-21 22:17:53,725 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.532330355738011, Velocity: -0.04353440834068056, Movement: 0.4733077272561546, Collision: 0, Height: -1.0, Movement Penalty: -0.1120444353515854, Smoothness: -0.0, Curiosity: 2.0136165618896484, Exploration: 0.20049231797211423, Total: -8.966713137408776
2024-07-21 22:17:53,817 - AirSimEnvLogger - INFO - Action: [-0.01615241 -0.73216833 -0.59979118 -0.59942867], Velocity: (-0.01615241312463578, -0.7321683261779826, -0.5997911806733474), Duration: 1.0, Reward: -8.966713137408776, Done: False
2024-07-21 22:17:53,943 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:17:53,943 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:17:56,720 - AirSimEnvLogger - INFO - Predictive model loss: 0.02867313101887703
2024-07-21 22:18:02,671 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.369651292468127, Velocity: -0.13147006065951752, Movement: 0.9528967240207734, Collision: 0, Height: -1.0, Movement Penalty: -0.19360549896309664, Smoothness: -0.0, Curiosity: 7.873788356781006, Exploration: 0.41238964474597545, Total: -3.8775575315801394
2024-07-21 22:18:02,798 - AirSimEnvLogger - INFO - Action: [-1.28320473 -0.10005601 -1.40549745 -0.34096958], Velocity: (-1.2832047290594, -0.10005601484057136, -1.4054974506600513), Duration: 1.0, Reward: -3.8775575315801394, Done: False
2024-07-21 22:18:02,862 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:02,862 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:05,705 - AirSimEnvLogger - INFO - Predictive model loss: 0.21099677681922913
2024-07-21 22:18:11,373 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.229356181916753, Velocity: -0.33455695337402186, Movement: 1.491996553402683, Collision: 0, Height: -1.0, Movement Penalty: -0.34888053652153395, Smoothness: -0.0, Curiosity: 16.329479217529297, Exploration: 0.8291530977223199, Total: 6.791811801966885
2024-07-21 22:18:11,515 - AirSimEnvLogger - INFO - Action: [1.58633167 1.79202709 1.78224735 1.80763603], Velocity: (1.5863316673243157, 1.7920270931778992, 1.782247345354276), Duration: 1.0, Reward: 6.791811801966885, Done: False
2024-07-21 22:18:11,578 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:11,578 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:14,445 - AirSimEnvLogger - INFO - Predictive model loss: 0.2988080084323883
2024-07-21 22:18:20,462 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.315207603500252, Velocity: -0.17926383051142095, Movement: 0.9927491921685635, Collision: 0, Height: -1.0, Movement Penalty: -0.27013264948903737, Smoothness: -0.0, Curiosity: 9.681465148925781, Exploration: 0.3323832210655487, Total: -1.0713021346214573
2024-07-21 22:18:20,462 - AirSimEnvLogger - INFO - Action: [-1.78079167 -0.7965958  -0.36935078 -1.83165526], Velocity: (-1.780791668497249, -0.7965957992815063, -0.3693507820259545), Duration: 1.0, Reward: -1.0713021346214573, Done: False
2024-07-21 22:18:20,525 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:20,525 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:23,392 - AirSimEnvLogger - INFO - Predictive model loss: 0.12038270384073257
2024-07-21 22:18:28,546 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.398368915825303, Velocity: -0.16468140656866023, Movement: 0.9640164487185439, Collision: 0, Height: -1.0, Movement Penalty: -0.21739967523660006, Smoothness: -0.0, Curiosity: 8.436418533325195, Exploration: 1.060712889692529, Total: -2.2081920933253945
2024-07-21 22:18:28,623 - AirSimEnvLogger - INFO - Action: [-1.47206572 -1.22769397 -0.20760755 -1.00446554], Velocity: (-1.4720657160456965, -1.2276939712696313, -0.2076075483953428), Duration: 1.0, Reward: -2.2081920933253945, Done: False
2024-07-21 22:18:28,715 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:28,715 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:31,565 - AirSimEnvLogger - INFO - Predictive model loss: 0.20979025959968567
2024-07-21 22:18:37,535 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.258358743078912, Velocity: -0.23639027303113327, Movement: 1.2189044200851533, Collision: 0, Height: -1.0, Movement Penalty: -0.26294037554286326, Smoothness: -0.0, Curiosity: 9.655634880065918, Exploration: 0.6675252220024812, Total: 0.06634434178495316
2024-07-21 22:18:37,598 - AirSimEnvLogger - INFO - Action: [1.50055725 1.6016316  1.06113906 0.98531831], Velocity: (1.5005572490496861, 1.6016315994123074, 1.0611390593606982), Duration: 1.0, Reward: 0.06634434178495316, Done: False
2024-07-21 22:18:37,661 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:37,661 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:40,482 - AirSimEnvLogger - INFO - Predictive model loss: 0.18913093209266663
2024-07-21 22:18:46,308 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.286730571502424, Velocity: -0.2107327295956233, Movement: 1.428110261421669, Collision: 0, Height: -1.0, Movement Penalty: -0.31203107801175206, Smoothness: -0.0, Curiosity: 13.981486320495605, Exploration: 0.3143022023849158, Total: 2.2898918801753076
2024-07-21 22:18:46,433 - AirSimEnvLogger - INFO - Action: [-1.3348945  -1.83126057 -1.73854453 -1.25632149], Velocity: (-1.334894500346922, -1.831260566893651, -1.7385445304033362), Duration: 1.0, Reward: 2.2898918801753076, Done: False
2024-07-21 22:18:46,496 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:46,496 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:49,260 - AirSimEnvLogger - INFO - Predictive model loss: 0.2014220654964447
2024-07-21 22:18:55,038 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.941743296474202, Velocity: -0.04161291796576201, Movement: 0.26844193278387085, Collision: 0, Height: -1.0, Movement Penalty: -0.0832497909453005, Smoothness: -0.0, Curiosity: 1.1385875940322876, Exploration: 0.5225034192833394, Total: -9.191184232686231
2024-07-21 22:18:55,194 - AirSimEnvLogger - INFO - Action: [ 0.35741361 -0.26831457  0.29750141 -0.63624562], Velocity: (0.3574136104141563, -0.26831456767398265, 0.29750141003296515), Duration: 1.0, Reward: -9.191184232686231, Done: False
2024-07-21 22:18:55,255 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:18:55,255 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:18:58,044 - AirSimEnvLogger - INFO - Predictive model loss: 0.02195524051785469
2024-07-21 22:19:03,851 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.647216460532194, Velocity: -0.11952862684822967, Movement: 1.2354914720799592, Collision: 0, Height: -1.0, Movement Penalty: -0.2996439795335836, Smoothness: -0.0, Curiosity: 13.773811340332031, Exploration: 0.2761140567307191, Total: 1.7012950664874829
2024-07-21 22:19:03,960 - AirSimEnvLogger - INFO - Action: [-0.13786694 -1.6324591  -1.84981802 -1.69496157], Velocity: (-0.13786693973264486, -1.6324590983272278, -1.8498180206567967), Duration: 1.0, Reward: 1.7012950664874829, Done: False
2024-07-21 22:19:04,052 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:04,052 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:19:06,760 - AirSimEnvLogger - INFO - Predictive model loss: 0.2422613650560379
2024-07-21 22:19:12,902 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.898684928239716, Velocity: -0.13904476449562128, Movement: 1.0542539603936736, Collision: 0, Height: -1.0, Movement Penalty: -0.21179243762023386, Smoothness: -0.0, Curiosity: 11.568414688110352, Exploration: 0.6797022410754856, Total: -0.6461438654865674
2024-07-21 22:19:13,012 - AirSimEnvLogger - INFO - Action: [-1.08095042 -0.31473078 -1.78277771 -0.19949439], Velocity: (-1.080950419844059, -0.3147307756228428, -1.7827777148982367), Duration: 1.0, Reward: -0.6461438654865674, Done: False
2024-07-21 22:19:13,075 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:13,075 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:19:15,825 - AirSimEnvLogger - INFO - Predictive model loss: 0.3661719262599945
2024-07-21 22:19:21,776 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.027406932232083, Velocity: -0.0772372043444515, Movement: 0.5909599160372692, Collision: 0, Height: -1.0, Movement Penalty: -0.1244172355337522, Smoothness: -0.0, Curiosity: 7.45062255859375, Exploration: 0.46395953409501944, Total: -3.9572716307525813
2024-07-21 22:19:21,886 - AirSimEnvLogger - INFO - Action: [-0.98794833 -0.2503206  -0.59852501 -0.38862625], Velocity: (-0.987948330115056, -0.2503206002834881, -0.598525006618513), Duration: 1.0, Reward: -3.9572716307525813, Done: False
2024-07-21 22:19:21,948 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:21,948 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:19:24,570 - AirSimEnvLogger - INFO - Predictive model loss: 0.31295856833457947
2024-07-21 22:19:30,517 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.120721806591463, Velocity: -0.15629478654258727, Movement: 1.193383555503262, Collision: 0, Height: -1.0, Movement Penalty: -0.24415466716321388, Smoothness: -0.0, Curiosity: 17.520063400268555, Exploration: 0.3174465840628666, Total: 5.001721193546142
2024-07-21 22:19:30,626 - AirSimEnvLogger - INFO - Action: [-0.86069326 -1.0894739  -1.94136833 -0.51428874], Velocity: (-0.8606932594273091, -1.0894738974029796, -1.941368327298142), Duration: 1.0, Reward: 5.001721193546142, Done: False
2024-07-21 22:19:30,687 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:30,687 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:19:33,608 - AirSimEnvLogger - INFO - Predictive model loss: 0.6224122047424316
2024-07-21 22:19:39,524 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.44810250683721, Velocity: -0.04816511458545451, Movement: 0.1797384148576949, Collision: 0, Height: -1.0, Movement Penalty: -0.17106795977487, Smoothness: -0.0, Curiosity: 8.10455322265625, Exploration: 0.18466520515718068, Total: -2.8662634002607126
2024-07-21 22:19:39,650 - AirSimEnvLogger - INFO - Action: [-0.18066501  0.25927545  0.17134756  1.67248351], Velocity: (-0.18066501324034445, 0.2592754463189746, 0.17134756207542035), Duration: 1.0, Reward: -2.8662634002607126, Done: False
2024-07-21 22:19:39,666 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:39,666 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:19:42,555 - AirSimEnvLogger - INFO - Predictive model loss: 0.2813422977924347
2024-07-21 22:19:48,569 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.388250665438832, Velocity: -0.09036351874864675, Movement: 0.6890458499197297, Collision: 0, Height: -1.0, Movement Penalty: -0.19877807708797954, Smoothness: -0.0, Curiosity: 13.296762466430664, Exploration: 0.29109936341758574, Total: 1.4635056831477828
2024-07-21 22:19:48,726 - AirSimEnvLogger - INFO - Action: [-0.72698141 -0.13948717 -1.16240186 -1.43252772], Velocity: (-0.7269814071013472, -0.13948717345183304, -1.1624018648203274), Duration: 1.0, Reward: 1.4635056831477828, Done: False
2024-07-21 22:19:48,852 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:48,852 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:19:51,678 - AirSimEnvLogger - INFO - Predictive model loss: 0.4713957607746124
2024-07-21 22:19:57,625 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.259659439354424, Velocity: -0.06364132030622417, Movement: 0.29293838692500923, Collision: 0, Height: -1.0, Movement Penalty: -0.060653465689048794, Smoothness: -0.0, Curiosity: 4.4000630378723145, Exploration: 0.10252143008517896, Total: -6.331695963268165
2024-07-21 22:19:57,704 - AirSimEnvLogger - INFO - Action: [0.54624923 0.16445478 0.13348408 0.15694807], Velocity: (0.546249230020164, 0.16445477878972836, 0.13348407610087842), Duration: 1.0, Reward: -6.331695963268165, Done: False
2024-07-21 22:19:57,767 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:19:57,767 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:00,562 - AirSimEnvLogger - INFO - Predictive model loss: 0.20245261490345
2024-07-21 22:20:06,631 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -8.854435301409062, Velocity: -0.16504975698583493, Movement: 1.1704865176775336, Collision: 0, Height: -1.0, Movement Penalty: -0.23542489630868257, Smoothness: -0.0, Curiosity: 7.432158470153809, Exploration: 0.9143013243948088, Total: -1.6823368380225159
2024-07-21 22:20:06,789 - AirSimEnvLogger - INFO - Action: [1.54307158 1.04632057 1.41573236 0.24966663], Velocity: (1.5430715785232145, 1.0463205664267121, 1.4157323645682032), Duration: 1.0, Reward: -1.6823368380225159, Done: False
2024-07-21 22:20:06,853 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:06,853 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:09,570 - AirSimEnvLogger - INFO - Predictive model loss: 0.15130682289600372
2024-07-21 22:20:14,852 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.49146823073006, Velocity: -0.1209625797864895, Movement: 0.6916902618325028, Collision: 0, Height: -1.0, Movement Penalty: -0.14523699674216559, Smoothness: -0.0, Curiosity: 5.7406744956970215, Exploration: 0.24051577618545902, Total: -5.183353226203565
2024-07-21 22:20:14,993 - AirSimEnvLogger - INFO - Action: [ 0.18189059 -1.2660899  -0.52694767 -0.44230855], Velocity: (0.18189058603375297, -1.2660899023319392, -0.5269476702492042), Duration: 1.0, Reward: -5.183353226203565, Done: False
2024-07-21 22:20:15,056 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:15,056 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:17,930 - AirSimEnvLogger - INFO - Predictive model loss: 0.16186396777629852
2024-07-21 22:20:24,037 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.356266415299746, Velocity: -0.1764882845877003, Movement: 0.8042156513470118, Collision: 0, Height: -1.0, Movement Penalty: -0.18501390579646168, Smoothness: -0.0, Curiosity: 11.602699279785156, Exploration: 0.7359338479259379, Total: 0.9181320593399568
2024-07-21 22:20:24,162 - AirSimEnvLogger - INFO - Action: [-1.53587805 -0.45541668  0.14396356 -0.91431027], Velocity: (-1.5358780534737755, -0.4554166810664444, 0.1439635611883241), Duration: 1.0, Reward: 0.9181320593399568, Done: False
2024-07-21 22:20:24,209 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:24,209 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:27,059 - AirSimEnvLogger - INFO - Predictive model loss: 0.38024795055389404
2024-07-21 22:20:32,935 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.985014544422755, Velocity: -0.034048045608231436, Movement: 0.48326219230123507, Collision: 0, Height: -1.0, Movement Penalty: -0.1601324139847837, Smoothness: -0.0, Curiosity: 9.925472259521484, Exploration: 0.29599458073201207, Total: -1.5072778359639494
2024-07-21 22:20:33,060 - AirSimEnvLogger - INFO - Action: [-0.35968699 -0.42096389 -0.79220203 -1.2767418 ], Velocity: (-0.3596869888703138, -0.4209638915499778, -0.7922020311004567), Duration: 1.0, Reward: -1.5072778359639494, Done: False
2024-07-21 22:20:33,154 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:33,154 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:35,888 - AirSimEnvLogger - INFO - Predictive model loss: 0.2890574634075165
2024-07-21 22:20:42,096 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.694899023648247, Velocity: -0.055045992322921206, Movement: 0.44150216625386385, Collision: 0, Height: -1.0, Movement Penalty: -0.09302944032884358, Smoothness: -0.0, Curiosity: 5.021407604217529, Exploration: 0.324211099171608, Total: -5.089131882742422
2024-07-21 22:20:42,221 - AirSimEnvLogger - INFO - Action: [ 0.47774767 -0.03346079  0.74184512  0.29283276], Velocity: (0.4777476744092528, -0.033460790081825564, 0.7418451228868923), Duration: 1.0, Reward: -5.089131882742422, Done: False
2024-07-21 22:20:42,317 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:42,317 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:45,074 - AirSimEnvLogger - INFO - Predictive model loss: 0.18688267469406128
2024-07-21 22:20:50,915 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -8.613260131296203, Velocity: -0.17932561935112487, Movement: 1.3128796741642186, Collision: 0, Height: -1.0, Movement Penalty: -0.2810488341761845, Smoothness: -0.0, Curiosity: 11.458775520324707, Exploration: 0.8645253144120559, Total: 2.5697023867030633
2024-07-21 22:20:51,024 - AirSimEnvLogger - INFO - Action: [0.87919823 1.81395658 1.6826123  1.00211405], Velocity: (0.8791982329077945, 1.8139565778439974, 1.6826123012394991), Duration: 1.0, Reward: 2.5697023867030633, Done: False
2024-07-21 22:20:51,087 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:51,087 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:20:53,705 - AirSimEnvLogger - INFO - Predictive model loss: 0.24841560423374176
2024-07-21 22:20:58,872 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.165098393722273, Velocity: -0.37052660223690775, Movement: 1.3681306344060928, Collision: 0, Height: -1.0, Movement Penalty: -0.29817170853980474, Smoothness: -0.0, Curiosity: 15.516331672668457, Exploration: 0.4691397583864492, Total: 3.9645351976983276
2024-07-21 22:20:58,874 - AirSimEnvLogger - INFO - Action: [-0.26609261 -1.93545763 -1.91580902 -1.18469872], Velocity: (-0.2660926075901848, -1.9354576337614102, -1.91580902057126), Duration: 1.0, Reward: 3.9645351976983276, Done: False
2024-07-21 22:20:58,888 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:20:58,888 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:01,755 - AirSimEnvLogger - INFO - Predictive model loss: 0.19819334149360657
2024-07-21 22:21:07,788 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.445786986150216, Velocity: -0.08411653758619643, Movement: 0.467532475354182, Collision: 0, Height: -1.0, Movement Penalty: -0.09836099251438286, Smoothness: -0.0, Curiosity: 2.779693603515625, Exploration: 0.584619417533878, Total: -8.023768520068025
2024-07-21 22:21:07,804 - AirSimEnvLogger - INFO - Action: [ 0.51414122  0.73603503 -0.26126174 -0.30519178], Velocity: (0.5141412179718672, 0.7360350339936643, -0.26126174377353006), Duration: 1.0, Reward: -8.023768520068025, Done: False
2024-07-21 22:21:07,867 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:07,867 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:10,761 - AirSimEnvLogger - INFO - Predictive model loss: 0.017846105620265007
2024-07-21 22:21:16,565 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.113946206605624, Velocity: -0.05884458188022144, Movement: 0.7255777540870976, Collision: 0, Height: -1.0, Movement Penalty: -0.16234049075238938, Smoothness: -0.0, Curiosity: 4.709251880645752, Exploration: 0.7597090491390659, Total: -4.7154350626499095
2024-07-21 22:21:16,659 - AirSimEnvLogger - INFO - Action: [0.32462279 0.73231808 1.21003412 0.72773016], Velocity: (0.32462279224422863, 0.7323180849836723, 1.210034121033665), Duration: 1.0, Reward: -4.7154350626499095, Done: False
2024-07-21 22:21:16,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:16,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:19,440 - AirSimEnvLogger - INFO - Predictive model loss: 0.1319407969713211
2024-07-21 22:21:25,038 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.068157173471814, Velocity: -0.12037674082329505, Movement: 0.875744481141963, Collision: 0, Height: -1.0, Movement Penalty: -0.18557003210923295, Smoothness: -0.0, Curiosity: 9.186239242553711, Exploration: 0.31911454373338777, Total: -2.2907400486364953
2024-07-21 22:21:25,167 - AirSimEnvLogger - INFO - Action: [-1.2278545  -0.10352481 -1.24473673  0.61311508], Velocity: (-1.2278544994830825, -0.10352480798225527, -1.2447367300894638), Duration: 1.0, Reward: -2.2907400486364953, Done: False
2024-07-21 22:21:25,227 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:25,227 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:28,024 - AirSimEnvLogger - INFO - Predictive model loss: 0.07563941925764084
2024-07-21 22:21:33,825 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.510554864434983, Velocity: -0.04045355003976502, Movement: 0.39058065929579716, Collision: 0, Height: -1.0, Movement Penalty: -0.11031615367150749, Smoothness: -0.0, Curiosity: 6.589705944061279, Exploration: 0.5171482116592704, Total: -4.30642697123039
2024-07-21 22:21:33,934 - AirSimEnvLogger - INFO - Action: [-0.67536772 -0.36909751 -0.13363559 -0.77894311], Velocity: (-0.6753677212702169, -0.36909751409820535, -0.13363559337451258), Duration: 1.0, Reward: -4.30642697123039, Done: False
2024-07-21 22:21:33,997 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:33,997 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:36,740 - AirSimEnvLogger - INFO - Predictive model loss: 0.04964909330010414
2024-07-21 22:21:42,345 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.356852736115126, Velocity: -0.13918893191769946, Movement: 1.0818904194085779, Collision: 0, Height: -1.0, Movement Penalty: -0.2164549629843284, Smoothness: -0.0, Curiosity: 6.566694736480713, Exploration: 0.6983673499620843, Total: -3.1002152251098676
2024-07-21 22:21:42,439 - AirSimEnvLogger - INFO - Action: [1.27153602 1.49384542 0.91300029 0.05768519], Velocity: (1.2715360189548661, 1.493845419346095, 0.9130002924596325), Duration: 1.0, Reward: -3.1002152251098676, Done: False
2024-07-21 22:21:42,454 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:42,455 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:45,164 - AirSimEnvLogger - INFO - Predictive model loss: 0.22014473378658295
2024-07-21 22:21:51,091 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.12514616695057, Velocity: -0.0636814618553787, Movement: 0.8065417443514425, Collision: 0, Height: -1.0, Movement Penalty: -0.1754903508242355, Smoothness: -0.0, Curiosity: 6.907476425170898, Exploration: 0.32274613816906034, Total: -4.624635607389243
2024-07-21 22:21:51,169 - AirSimEnvLogger - INFO - Action: [-0.05606487 -0.90123374 -1.33666481 -0.69112082], Velocity: (-0.05606486500116881, -0.9012337401083992, -1.3366648114349753), Duration: 1.0, Reward: -4.624635607389243, Done: False
2024-07-21 22:21:51,232 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:51,232 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:21:54,004 - AirSimEnvLogger - INFO - Predictive model loss: 0.049337711185216904
2024-07-21 22:21:59,773 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.569053608707874, Velocity: -0.04242009105728916, Movement: 0.44788227604285724, Collision: 0, Height: -1.0, Movement Penalty: -0.09292015305329603, Smoothness: -0.0, Curiosity: 2.0887279510498047, Exploration: 0.2688894382811536, Total: -7.906479653839781
2024-07-21 22:21:59,866 - AirSimEnvLogger - INFO - Action: [ 0.45767124  0.06249871  0.76747969 -0.247025  ], Velocity: (0.4576712445221649, 0.062498711413083496, 0.7674796907948493), Duration: 1.0, Reward: -7.906479653839781, Done: False
2024-07-21 22:21:59,914 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:21:59,914 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:02,656 - AirSimEnvLogger - INFO - Predictive model loss: 0.07961197197437286
2024-07-21 22:22:08,510 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.292064715518787, Velocity: -0.1871539345811491, Movement: 1.005319083202947, Collision: 0, Height: -1.0, Movement Penalty: -0.21227431140860498, Smoothness: -0.0, Curiosity: 6.962878704071045, Exploration: 0.7758777394213691, Total: -2.634842008894832
2024-07-21 22:22:08,635 - AirSimEnvLogger - INFO - Action: [0.73802519 1.63929634 0.90038446 0.68071469], Velocity: (0.7380251857096949, 1.6392963367534659, 0.9003844633115861), Duration: 1.0, Reward: -2.634842008894832, Done: False
2024-07-21 22:22:08,729 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:22:08,729 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:11,635 - AirSimEnvLogger - INFO - Predictive model loss: 0.27207136154174805
2024-07-21 22:22:17,652 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.877553475552599, Velocity: -0.17335236996646133, Movement: 0.7929192599716735, Collision: 0, Height: -1.0, Movement Penalty: -0.15987511873526256, Smoothness: -0.0, Curiosity: 5.742539405822754, Exploration: 0.5250843142817035, Total: -4.5005087774132955
2024-07-21 22:22:17,776 - AirSimEnvLogger - INFO - Action: [ 0.60080803  1.45676295  0.17819999 -0.20278449], Velocity: (0.6008080254977561, 1.4567629498808885, 0.17819998790102298), Duration: 1.0, Reward: -4.5005087774132955, Done: False
2024-07-21 22:22:17,839 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:22:17,839 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:20,555 - AirSimEnvLogger - INFO - Predictive model loss: 0.26947319507598877
2024-07-21 22:22:26,327 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.839167553847473, Velocity: -0.11626900654832072, Movement: 0.7433219640227345, Collision: 0, Height: -1.0, Movement Penalty: -0.18138558182297476, Smoothness: -0.0, Curiosity: 7.370341777801514, Exploration: 0.5682275798169395, Total: -3.8363880492912004
2024-07-21 22:22:26,436 - AirSimEnvLogger - INFO - Action: [-1.13310374 -0.46371886 -0.84329764 -1.03921257], Velocity: (-1.133103735264769, -0.46371885918197386, -0.8432976423318512), Duration: 1.0, Reward: -3.8363880492912004, Done: False
2024-07-21 22:22:26,497 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:22:26,497 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:29,343 - AirSimEnvLogger - INFO - Predictive model loss: 0.0641423910856247
2024-07-21 22:22:35,055 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.656963428049703, Velocity: -0.07057715492871139, Movement: 0.4876449494730209, Collision: 0, Height: -1.0, Movement Penalty: -0.1250587720297889, Smoothness: -0.0, Curiosity: 4.415828704833984, Exploration: 0.5637778167207945, Total: -6.612782633951478
2024-07-21 22:22:35,087 - AirSimEnvLogger - INFO - Action: [-0.37697609 -0.74080461 -0.51018422 -0.78280218], Velocity: (-0.3769760889268978, -0.7408046137573536, -0.5101842212371703), Duration: 1.0, Reward: -6.612782633951478, Done: False
2024-07-21 22:22:35,148 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:22:35,148 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:37,854 - AirSimEnvLogger - INFO - Predictive model loss: 0.02344520017504692
2024-07-21 22:22:43,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.187130890613055, Velocity: -0.15028977618538453, Movement: 0.8277101650352982, Collision: 0, Height: -1.0, Movement Penalty: -0.16867776587680428, Smoothness: -0.0, Curiosity: 8.36534309387207, Exploration: 0.28785569595837374, Total: -3.239069424122558
2024-07-21 22:22:43,649 - AirSimEnvLogger - INFO - Action: [-0.86309135 -0.91200848 -1.07876332 -0.32373199], Velocity: (-0.8630913531835942, -0.9120084844505854, -1.0787633241637857), Duration: 1.0, Reward: -3.239069424122558, Done: False
2024-07-21 22:22:43,711 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:22:43,711 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:46,543 - AirSimEnvLogger - INFO - Predictive model loss: 0.09478357434272766
2024-07-21 22:22:52,491 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.82966686247806, Velocity: -0.17809483942157006, Movement: 0.7800429411166266, Collision: 0, Height: -1.0, Movement Penalty: -0.21725017903376279, Smoothness: -0.0, Curiosity: 4.911103248596191, Exploration: 0.4176578265758308, Total: -5.339730875472138
2024-07-21 22:22:52,600 - AirSimEnvLogger - INFO - Action: [ 1.45540838 -0.3980809   0.3964669   1.51191801], Velocity: (1.4554083811301217, -0.39808090254120043, 0.396466895354064), Duration: 1.0, Reward: -5.339730875472138, Done: False
2024-07-21 22:22:52,631 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:22:52,631 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:22:55,495 - AirSimEnvLogger - INFO - Predictive model loss: 0.04545118287205696
2024-07-21 22:23:01,317 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.405713760201849, Velocity: -0.15602301860613563, Movement: 1.0401817828442281, Collision: 0, Height: -1.0, Movement Penalty: -0.2671944270508074, Smoothness: -0.0, Curiosity: 13.308830261230469, Exploration: 0.11561665311308272, Total: 1.4261342654568039
2024-07-21 22:23:01,426 - AirSimEnvLogger - INFO - Action: [-1.33731688 -0.75158649 -1.40520955 -1.67671513], Velocity: (-1.337316875726971, -0.7515864870979188, -1.4052095543909635), Duration: 1.0, Reward: 1.4261342654568039, Done: False
2024-07-21 22:23:01,487 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:01,487 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:04,406 - AirSimEnvLogger - INFO - Predictive model loss: 0.15433363616466522
2024-07-21 22:23:10,105 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.758562622811892, Velocity: -0.13826322220479081, Movement: 1.0914800480291187, Collision: 0, Height: -1.0, Movement Penalty: -0.21829618190540967, Smoothness: -0.0, Curiosity: 15.413954734802246, Exploration: 0.8776366623213333, Total: 4.387081337875154
2024-07-21 22:23:10,106 - AirSimEnvLogger - INFO - Action: [-1.96510926 -0.94909842 -0.05359622 -0.00274271], Velocity: (-1.9651092611618206, -0.9490984237982818, -0.053596218357726766), Duration: 1.0, Reward: 4.387081337875154, Done: False
2024-07-21 22:23:10,198 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:10,199 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:13,031 - AirSimEnvLogger - INFO - Predictive model loss: 0.30252793431282043
2024-07-21 22:23:18,691 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.691063759365386, Velocity: -0.1654822666001245, Movement: 0.9001391001436359, Collision: 0, Height: -1.0, Movement Penalty: -0.2249616496272871, Smoothness: -0.0, Curiosity: 14.239587783813477, Exploration: 0.5422188055969461, Total: 3.1702247722818426
2024-07-21 22:23:18,783 - AirSimEnvLogger - INFO - Action: [-0.07412118 -1.78617071 -0.21237196 -1.34898954], Velocity: (-0.07412118034630977, -1.7861707080249474, -0.21237196342103593), Duration: 1.0, Reward: 3.1702247722818426, Done: False
2024-07-21 22:23:18,861 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:18,861 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:21,823 - AirSimEnvLogger - INFO - Predictive model loss: 0.3571651875972748
2024-07-21 22:23:27,686 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.604007042645316, Velocity: -0.05345780286003646, Movement: 0.8986675361869786, Collision: 0, Height: -1.0, Movement Penalty: -0.21251445994862506, Smoothness: -0.0, Curiosity: 18.47942352294922, Exploration: 0.4265504020948204, Total: 6.487733517591574
2024-07-21 22:23:27,797 - AirSimEnvLogger - INFO - Action: [-0.75900981 -1.14291985 -1.16105628 -1.13394277], Velocity: (-0.7590098136978556, -1.1429198455487557, -1.1610562827643853), Duration: 1.0, Reward: 6.487733517591574, Done: False
2024-07-21 22:23:27,890 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:27,890 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:30,740 - AirSimEnvLogger - INFO - Predictive model loss: 0.5509493947029114
2024-07-21 22:23:36,576 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.089567818399047, Velocity: -0.049903515318844585, Movement: 0.36722467879829873, Collision: 0, Height: -1.0, Movement Penalty: -0.07851919695716456, Smoothness: -0.0, Curiosity: 9.168828964233398, Exploration: 0.3075625399134359, Total: -1.3428379649642623
2024-07-21 22:23:36,685 - AirSimEnvLogger - INFO - Action: [-0.14128672  0.35880818  0.62506849  0.2776879 ], Velocity: (-0.14128671954570238, 0.3588081787220896, 0.6250684863566598), Duration: 1.0, Reward: -1.3428379649642623, Done: False
2024-07-21 22:23:36,732 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:36,732 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:39,566 - AirSimEnvLogger - INFO - Predictive model loss: 0.26668816804885864
2024-07-21 22:23:45,404 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.567541088340212, Velocity: -0.10391062165943715, Movement: 0.7742315085014809, Collision: 0, Height: -1.0, Movement Penalty: -0.16975355075506943, Smoothness: -0.0, Curiosity: 17.11175537109375, Exploration: 0.2585955011900591, Total: 5.116815821673981
2024-07-21 22:23:45,513 - AirSimEnvLogger - INFO - Action: [-1.01581194  0.04986171 -1.16763762  0.69562137], Velocity: (-1.0158119427991499, 0.04986171446992782, -1.167637624146572), Duration: 1.0, Reward: 5.116815821673981, Done: False
2024-07-21 22:23:45,575 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:45,575 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:48,406 - AirSimEnvLogger - INFO - Predictive model loss: 0.4049984812736511
2024-07-21 22:23:54,427 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.449009767145657, Velocity: -0.12714371772591473, Movement: 0.6589701269629737, Collision: 0, Height: -1.0, Movement Penalty: -0.19621302554347364, Smoothness: -0.0, Curiosity: 20.95294761657715, Exploration: 0.4867385803889708, Total: 9.09732265615144
2024-07-21 22:23:54,505 - AirSimEnvLogger - INFO - Action: [-0.58051403 -1.00641261 -0.62217653 -1.45361227], Velocity: (-0.5805140334877057, -1.0064126077718984, -0.6221765286154668), Duration: 1.0, Reward: 9.09732265615144, Done: False
2024-07-21 22:23:54,537 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:23:54,537 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:23:57,154 - AirSimEnvLogger - INFO - Predictive model loss: 0.5453742146492004
2024-07-21 22:24:02,905 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.82781120744258, Velocity: -0.11415830738109108, Movement: 0.9245963733176983, Collision: 0, Height: -1.0, Movement Penalty: -0.2542900153678688, Smoothness: -0.0, Curiosity: 31.703832626342773, Exploration: 0.4138577418242207, Total: 19.462091353362197
2024-07-21 22:24:02,983 - AirSimEnvLogger - INFO - Action: [-1.67241164 -0.35020127 -0.70704469 -1.74551636], Velocity: (-1.672411639702223, -0.3502012726441095, -0.7070446875805874), Duration: 1.0, Reward: 19.462091353362197, Done: False
2024-07-21 22:24:03,059 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:03,060 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:05,916 - AirSimEnvLogger - INFO - Predictive model loss: 0.7961412668228149
2024-07-21 22:24:11,794 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.979290965485953, Velocity: -0.09167339481391354, Movement: 0.7151000970645064, Collision: 0, Height: -1.0, Movement Penalty: -0.14885573463563853, Smoothness: -0.0, Curiosity: 13.117779731750488, Exploration: 0.457806079098866, Total: 2.7603029712470946
2024-07-21 22:24:11,903 - AirSimEnvLogger - INFO - Action: [1.13507864 0.18826939 0.84948438 0.41271101], Velocity: (1.1350786417945253, 0.18826939455739944, 0.8494843773145873), Duration: 1.0, Reward: 2.7603029712470946, Done: False
2024-07-21 22:24:11,967 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:11,967 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:14,859 - AirSimEnvLogger - INFO - Predictive model loss: 0.28520652651786804
2024-07-21 22:24:20,607 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.521614019094814, Velocity: -0.04428153614428963, Movement: 0.23306924794828274, Collision: 0, Height: -1.0, Movement Penalty: -0.06049694532589233, Smoothness: -0.0, Curiosity: 15.287745475769043, Exploration: 0.43647994411198915, Total: 4.364474912255372
2024-07-21 22:24:20,734 - AirSimEnvLogger - INFO - Action: [-0.31794264  0.29295516  0.17428381 -0.3856202 ], Velocity: (-0.3179426428223422, 0.29295516114552234, 0.1742838110383691), Duration: 1.0, Reward: 4.364474912255372, Done: False
2024-07-21 22:24:20,796 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:20,796 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:23,539 - AirSimEnvLogger - INFO - Predictive model loss: 0.25793877243995667
2024-07-21 22:24:29,398 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.240787145212042, Velocity: -0.07070074763053145, Movement: 0.8789135834761442, Collision: 0, Height: -1.0, Movement Penalty: -0.22227857321611533, Smoothness: -0.0, Curiosity: 17.140470504760742, Exploration: 0.2927596947474405, Total: 7.471856629656161
2024-07-21 22:24:29,553 - AirSimEnvLogger - INFO - Action: [0.43789753 0.61734652 1.5865325  1.36044848], Velocity: (0.43789752919031444, 0.6173465249007701, 1.5865324991935932), Duration: 1.0, Reward: 7.471856629656161, Done: False
2024-07-21 22:24:29,617 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:29,617 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:32,420 - AirSimEnvLogger - INFO - Predictive model loss: 0.297004371881485
2024-07-21 22:24:37,441 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.847276318802308, Velocity: -0.02776409670937415, Movement: 0.4668234169860917, Collision: 0, Height: -1.0, Movement Penalty: -0.12604063891689285, Smoothness: -0.0, Curiosity: 13.370132446289062, Exploration: 0.25541744308456066, Total: 3.081160688644872
2024-07-21 22:24:37,553 - AirSimEnvLogger - INFO - Action: [-0.01269112  0.66060088  0.6596528   0.84671592], Velocity: (-0.012691119173549259, 0.6606008814228086, 0.6596528037868201), Duration: 1.0, Reward: 3.081160688644872, Done: False
2024-07-21 22:24:37,613 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:37,613 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:40,844 - AirSimEnvLogger - INFO - Predictive model loss: 0.13993969559669495
2024-07-21 22:24:46,876 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -8.856344573878268, Velocity: -0.1226903426890168, Movement: 1.2230488418859211, Collision: 0, Height: -1.0, Movement Penalty: -0.3086909109312221, Smoothness: -0.0, Curiosity: 21.98638916015625, Exploration: 0.35799287441634786, Total: 12.716995295523258
2024-07-21 22:24:47,019 - AirSimEnvLogger - INFO - Action: [0.08466914 1.5073066  1.9246433  1.88298008], Velocity: (0.08466914301876316, 1.507306595441466, 1.924643302566243), Duration: 1.0, Reward: 12.716995295523258, Done: False
2024-07-21 22:24:47,096 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:47,096 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:49,901 - AirSimEnvLogger - INFO - Predictive model loss: 0.3275010287761688
2024-07-21 22:24:55,910 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.178308775142849, Velocity: -0.11502668145624105, Movement: 0.73232011416596, Collision: 0, Height: -1.0, Movement Penalty: -0.20874755722466012, Smoothness: -0.0, Curiosity: 11.445871353149414, Exploration: 0.4140018125515149, Total: 0.8494313641186921
2024-07-21 22:24:56,019 - AirSimEnvLogger - INFO - Action: [ 0.82068827  1.21245103 -0.04005317  1.48740824], Velocity: (0.8206882704874174, 1.2124510319690984, -0.040053167285723834), Duration: 1.0, Reward: 0.8494313641186921, Done: False
2024-07-21 22:24:56,114 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:24:56,114 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:24:58,738 - AirSimEnvLogger - INFO - Predictive model loss: 0.18019230663776398
2024-07-21 22:25:04,607 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.882389468101884, Velocity: -0.09940273368991819, Movement: 0.8876533688278074, Collision: 0, Height: -1.0, Movement Penalty: -0.22496432639588654, Smoothness: -0.0, Curiosity: 20.47733497619629, Exploration: 0.932606683618855, Total: 8.3112940803957
2024-07-21 22:25:04,749 - AirSimEnvLogger - INFO - Action: [-0.86860658 -0.18452023 -1.5372667  -1.38173109], Velocity: (-0.8686065815913531, -0.18452023316377741, -1.5372666986358585), Duration: 1.0, Reward: 8.3112940803957, Done: False
2024-07-21 22:25:04,843 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:04,843 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:25:07,640 - AirSimEnvLogger - INFO - Predictive model loss: 0.05730817839503288
2024-07-21 22:25:13,708 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.413563912650398, Velocity: -0.23272650386488225, Movement: 1.0024167268718964, Collision: 0, Height: -1.0, Movement Penalty: -0.2629800771544274, Smoothness: -0.0, Curiosity: 13.061140060424805, Exploration: 0.31692019928212983, Total: 2.2060434464076604
2024-07-21 22:25:13,787 - AirSimEnvLogger - INFO - Action: [1.12721352 1.65507144 0.09739298 1.7019092 ], Velocity: (1.1272135206770955, 1.655071437457799, 0.09739298211054925), Duration: 1.0, Reward: 2.2060434464076604, Done: False
2024-07-21 22:25:13,834 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:13,834 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:25:16,826 - AirSimEnvLogger - INFO - Predictive model loss: 0.39043042063713074
2024-07-21 22:25:22,671 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.702191454051645, Velocity: -0.055880700537160495, Movement: 0.30952450701231465, Collision: 0, Height: -1.0, Movement Penalty: -0.06292127767490581, Smoothness: -0.0, Curiosity: 9.13234806060791, Exploration: 0.38671915518453365, Total: -1.974613265660737
2024-07-21 22:25:22,734 - AirSimEnvLogger - INFO - Action: [-0.1869335  -0.50493364 -0.30548251 -0.11263675], Velocity: (-0.18693350217843174, -0.5049336415051251, -0.3054825121091971), Duration: 1.0, Reward: -1.974613265660737, Done: False
2024-07-21 22:25:22,798 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:22,798 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:25:25,696 - AirSimEnvLogger - INFO - Predictive model loss: 0.13938219845294952
2024-07-21 22:25:31,416 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.307928866038026, Velocity: -0.08686326002556981, Movement: 0.8396522223737254, Collision: 0, Height: -1.0, Movement Penalty: -0.22709636929533153, Smoothness: -0.0, Curiosity: 14.660639762878418, Exploration: 0.4270236528750103, Total: 4.9462431374837434
2024-07-21 22:25:31,588 - AirSimEnvLogger - INFO - Action: [0.02987847 0.84066919 1.45342561 1.52879452], Velocity: (0.029878470202012952, 0.840669189772705, 1.4534256116271775), Duration: 1.0, Reward: 4.9462431374837434, Done: False
2024-07-21 22:25:31,651 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:31,651 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:25:34,603 - AirSimEnvLogger - INFO - Predictive model loss: 0.2452690750360489
2024-07-21 22:25:40,630 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.207820794207755, Velocity: -0.09694000281849302, Movement: 0.467351955133413, Collision: 0, Height: -1.0, Movement Penalty: -0.09660184603563897, Smoothness: -0.0, Curiosity: 9.330080032348633, Exploration: 0.4095058940822062, Total: -1.2761200558012058
2024-07-21 22:25:40,673 - AirSimEnvLogger - INFO - Action: [ 0.21168661  0.90887809  0.05292248 -0.24396776], Velocity: (0.21168661285684798, 0.90887809338137, 0.05292248271514999), Duration: 1.0, Reward: -1.2761200558012058, Done: False
2024-07-21 22:25:40,755 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:40,755 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:25:43,607 - AirSimEnvLogger - INFO - Predictive model loss: 0.17778778076171875
2024-07-21 22:25:49,550 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.38020277090265, Velocity: -0.17074558357191263, Movement: 1.0358020120811793, Collision: 0, Height: -1.0, Movement Penalty: -0.222038219389378, Smoothness: -0.0, Curiosity: 14.177461624145508, Exploration: 0.3789376948714302, Total: 4.40133313670277
2024-07-21 22:25:49,677 - AirSimEnvLogger - INFO - Action: [0.72769494 1.49238703 1.2388641  0.79909565], Velocity: (0.7276949352745319, 1.4923870290762806, 1.2388641045515627), Duration: 1.0, Reward: 4.40133313670277, Done: False
2024-07-21 22:25:49,755 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:49,755 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:25:52,786 - AirSimEnvLogger - INFO - Predictive model loss: 0.39440464973449707
2024-07-21 22:25:58,072 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.760964466699287, Velocity: -0.11084976197972175, Movement: 0.9758111983233969, Collision: 0, Height: -1.0, Movement Penalty: -0.19530752607464752, Smoothness: -0.0, Curiosity: 20.15230369567871, Exploration: 0.37290614010502043, Total: 9.004982469733559
2024-07-21 22:25:58,197 - AirSimEnvLogger - INFO - Action: [-1.93827295 -0.18185207 -0.13732361 -0.07531929], Velocity: (-1.9382729502383638, -0.18185206998164283, -0.13732361090153566), Duration: 1.0, Reward: 9.004982469733559, Done: False
2024-07-21 22:25:58,228 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:25:58,228 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:00,943 - AirSimEnvLogger - INFO - Predictive model loss: 0.1879960000514984
2024-07-21 22:26:07,178 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.660003992389328, Velocity: -0.1257134979856166, Movement: 0.6466794139622435, Collision: 0, Height: -1.0, Movement Penalty: -0.15348511926475297, Smoothness: -0.0, Curiosity: 12.78088092803955, Exploration: 0.6993343394789707, Total: 1.7829450422541617
2024-07-21 22:26:07,256 - AirSimEnvLogger - INFO - Action: [-0.16309801 -1.26805149 -0.19550325 -0.82643277], Velocity: (-0.16309800662230778, -1.268051487971804, -0.19550325280804448), Duration: 1.0, Reward: 1.7829450422541617, Done: False
2024-07-21 22:26:07,318 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:26:07,318 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:10,150 - AirSimEnvLogger - INFO - Predictive model loss: 0.03265243023633957
2024-07-21 22:26:15,876 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -8.961170622644705, Velocity: -0.19915620701937808, Movement: 1.4244592763842625, Collision: 0, Height: -1.0, Movement Penalty: -0.3408322353998851, Smoothness: -0.0, Curiosity: 21.90227508544922, Exploration: 0.7860296889928592, Total: 12.63098385156479
2024-07-21 22:26:15,969 - AirSimEnvLogger - INFO - Action: [1.32798646 1.57929829 1.96433342 1.87091538], Velocity: (1.3279864559797403, 1.5792982940250442, 1.9643334216809176), Duration: 1.0, Reward: 12.63098385156479, Done: False
2024-07-21 22:26:16,047 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:26:16,047 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:18,847 - AirSimEnvLogger - INFO - Predictive model loss: 0.3851523995399475
2024-07-21 22:26:24,729 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.355754266914401, Velocity: -0.044238086922722575, Movement: 0.352869723010752, Collision: 0, Height: -1.0, Movement Penalty: -0.07058445598994302, Smoothness: -0.0, Curiosity: 8.123266220092773, Exploration: 0.5354921290602584, Total: -2.5996391326035067
2024-07-21 22:26:24,855 - AirSimEnvLogger - INFO - Action: [ 0.58184158  0.34539723 -0.20057243 -0.01218101], Velocity: (0.5818415763599916, 0.34539723061313166, -0.20057242775226736), Duration: 1.0, Reward: -2.5996391326035067, Done: False
2024-07-21 22:26:24,903 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:26:24,903 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:27,681 - AirSimEnvLogger - INFO - Predictive model loss: 0.1660555601119995
2024-07-21 22:26:33,609 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.574336015674408, Velocity: -0.010888972963276638, Movement: 0.13142985583190375, Collision: 0, Height: -1.0, Movement Penalty: -0.026321071222356066, Smoothness: -0.0, Curiosity: 8.30272102355957, Exploration: 0.5718753634327689, Total: -2.635932911616339
2024-07-21 22:26:33,735 - AirSimEnvLogger - INFO - Action: [-0.04463307 -0.08884014 -0.24333217 -0.01358864], Velocity: (-0.04463306744914375, -0.08884014265042106, -0.243332172882899), Duration: 1.0, Reward: -2.635932911616339, Done: False
2024-07-21 22:26:33,787 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:26:33,787 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:36,671 - AirSimEnvLogger - INFO - Predictive model loss: 0.1313123106956482
2024-07-21 22:26:42,444 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.40569908440937, Velocity: -0.07043369064006258, Movement: 0.4038837894529882, Collision: 0, Height: -1.0, Movement Penalty: -0.09428272295568377, Smoothness: -0.0, Curiosity: 12.801467895507812, Exploration: 0.23484778108527324, Total: 1.9521428007296273
2024-07-21 22:26:42,521 - AirSimEnvLogger - INFO - Action: [-0.60425555  0.47016961  0.25749607 -0.48624554], Velocity: (-0.6042555539484762, 0.4701696113204952, 0.2574960652996032), Duration: 1.0, Reward: 1.9521428007296273, Done: False
2024-07-21 22:26:42,584 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:26:42,584 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:45,239 - AirSimEnvLogger - INFO - Predictive model loss: 0.21630676090717316
2024-07-21 22:26:51,055 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.624912134951845, Velocity: -0.052002626915398135, Movement: 0.5348841561393142, Collision: 0, Height: -1.0, Movement Penalty: -0.10700605757074569, Smoothness: -0.0, Curiosity: 8.584268569946289, Exploration: 0.2913689294995642, Total: -1.4574482208380666
2024-07-21 22:26:51,148 - AirSimEnvLogger - INFO - Action: [ 0.59493851 -0.21183117  0.86346973  0.02500787], Velocity: (0.5949385113070285, -0.21183116706128047, 0.8634697252253245), Duration: 1.0, Reward: -1.4574482208380666, Done: False
2024-07-21 22:26:51,210 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:26:51,210 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:26:54,156 - AirSimEnvLogger - INFO - Predictive model loss: 0.1026291474699974
2024-07-21 22:27:00,292 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.025661910172115, Velocity: -0.11220233535082172, Movement: 1.08407581492604, Collision: 0, Height: -1.0, Movement Penalty: -0.2380011268059204, Smoothness: -0.0, Curiosity: 19.828645706176758, Exploration: 0.5033971244127262, Total: 10.440314951771173
2024-07-21 22:27:00,385 - AirSimEnvLogger - INFO - Action: [0.04595736 1.17933068 1.81877667 0.98161711], Velocity: (0.04595736125100314, 1.1793306799931953, 1.8187766652901243), Duration: 1.0, Reward: 10.440314951771173, Done: False
2024-07-21 22:27:00,448 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:00,448 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:02,981 - AirSimEnvLogger - INFO - Predictive model loss: 0.38669684529304504
2024-07-21 22:27:09,032 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.083860652756295, Velocity: -0.04294803080850475, Movement: 0.1937160131333363, Collision: 0, Height: -1.0, Movement Penalty: -0.10280554945032226, Smoothness: -0.0, Curiosity: 9.917016983032227, Exploration: 0.1931028785084364, Total: -0.6510073436344728
2024-07-21 22:27:09,188 - AirSimEnvLogger - INFO - Action: [ 0.35452277 -0.00797683  0.15605625  0.95225759], Velocity: (0.3545227655397025, -0.00797683420388684, 0.15605625206080176), Duration: 1.0, Reward: -0.6510073436344728, Done: False
2024-07-21 22:27:09,280 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:09,280 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:11,963 - AirSimEnvLogger - INFO - Predictive model loss: 0.16425134241580963
2024-07-21 22:27:17,581 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.710746532893364, Velocity: -0.13264463921396188, Movement: 1.0039429207024784, Collision: 0, Height: -1.0, Movement Penalty: -0.23465175059420892, Smoothness: -0.0, Curiosity: 18.109832763671875, Exploration: 0.6515524974988596, Total: 7.058904974883093
2024-07-21 22:27:17,691 - AirSimEnvLogger - INFO - Action: [ 1.18478464  1.60530361 -0.22559086  1.21430591], Velocity: (1.1847846360530172, 1.605303609945584, -0.225590864582073), Duration: 1.0, Reward: 7.058904974883093, Done: False
2024-07-21 22:27:17,754 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:17,754 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:20,358 - AirSimEnvLogger - INFO - Predictive model loss: 0.5121214985847473
2024-07-21 22:27:26,241 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.59217558174957, Velocity: -0.20716301486874766, Movement: 1.3583318902122077, Collision: 0, Height: -1.0, Movement Penalty: -0.2825528626445757, Smoothness: -0.0, Curiosity: 29.165321350097656, Exploration: 0.7666706451401075, Total: 19.277653748550858
2024-07-21 22:27:26,304 - AirSimEnvLogger - INFO - Action: [0.59529545 1.89857397 1.84967627 0.77675603], Velocity: (0.5952954505318206, 1.8985739714259802, 1.849676268288528), Duration: 1.0, Reward: 19.277653748550858, Done: False
2024-07-21 22:27:26,396 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:26,396 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:29,142 - AirSimEnvLogger - INFO - Predictive model loss: 0.8758265972137451
2024-07-21 22:27:34,921 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.033329944423588, Velocity: -0.15060246029897592, Movement: 0.9063395941773903, Collision: 0, Height: -1.0, Movement Penalty: -0.1949538731691055, Smoothness: -0.0, Curiosity: 17.583309173583984, Exploration: 0.38646880245431486, Total: 6.153217414295273
2024-07-21 22:27:35,045 - AirSimEnvLogger - INFO - Action: [-1.21809881 -1.03656104 -0.85298437 -0.71756214], Velocity: (-1.2180988062000255, -1.0365610440087398, -0.8529843728182422), Duration: 1.0, Reward: 6.153217414295273, Done: False
2024-07-21 22:27:35,122 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:35,122 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:38,107 - AirSimEnvLogger - INFO - Predictive model loss: 0.36663052439689636
2024-07-21 22:27:43,938 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.642341330031517, Velocity: -0.07630224755906628, Movement: 1.035816737296087, Collision: 0, Height: -1.0, Movement Penalty: -0.23463583548623587, Smoothness: -0.0, Curiosity: 17.23394012451172, Exploration: 1.0960331581934069, Total: 5.363752621587112
2024-07-21 22:27:44,048 - AirSimEnvLogger - INFO - Action: [-0.57367063 -1.44205673 -1.37223892 -1.10169518], Velocity: (-0.5736706342814895, -1.4420567311084744, -1.3722389153054424), Duration: 1.0, Reward: 5.363752621587112, Done: False
2024-07-21 22:27:44,110 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:44,110 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:47,164 - AirSimEnvLogger - INFO - Predictive model loss: 0.33011168241500854
2024-07-21 22:27:53,417 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.527835066782483, Velocity: -0.041055320474932294, Movement: 0.7417991391015818, Collision: 0, Height: -1.0, Movement Penalty: -0.2180875560891875, Smoothness: -0.0, Curiosity: 22.779876708984375, Exploration: 0.282774000833979, Total: 10.807782231775843
2024-07-21 22:27:53,542 - AirSimEnvLogger - INFO - Action: [-1.32346761 -0.06351398 -0.66743038 -1.59848502], Velocity: (-1.32346761152836, -0.06351397833092931, -0.6674303760538536), Duration: 1.0, Reward: 10.807782231775843, Done: False
2024-07-21 22:27:53,605 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:27:53,605 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:27:56,630 - AirSimEnvLogger - INFO - Predictive model loss: 0.4388004541397095
2024-07-21 22:28:02,900 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.163789101997416, Velocity: -0.17576081986844122, Movement: 0.9589761313776635, Collision: 0, Height: -1.0, Movement Penalty: -0.2560615570451934, Smoothness: -0.0, Curiosity: 25.408428192138672, Exploration: 0.54855511885377, Total: 13.859456565361056
2024-07-21 22:28:03,007 - AirSimEnvLogger - INFO - Action: [-1.55839088 -1.10169967 -0.19030653 -1.69652917], Velocity: (-1.5583908832842364, -1.101699668857603, -0.19030653363430217), Duration: 1.0, Reward: 13.859456565361056, Done: False
2024-07-21 22:28:03,055 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:03,055 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:28:06,018 - AirSimEnvLogger - INFO - Predictive model loss: 0.3567502796649933
2024-07-21 22:28:12,416 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.211983470245396, Velocity: -0.1737637680410622, Movement: 1.3159461920598465, Collision: 0, Height: -1.0, Movement Penalty: -0.27599196132101955, Smoothness: -0.0, Curiosity: 19.092975616455078, Exploration: 0.7942829410466039, Total: 9.592537332074164
2024-07-21 22:28:12,509 - AirSimEnvLogger - INFO - Action: [1.84048573 0.61230518 1.77891882 0.83084219], Velocity: (1.840485727342986, 0.6123051753277995, 1.7789188236962346), Duration: 1.0, Reward: 9.592537332074164, Done: False
2024-07-21 22:28:12,541 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:12,541 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:28:15,373 - AirSimEnvLogger - INFO - Predictive model loss: 0.20396646857261658
2024-07-21 22:28:21,140 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.094205671277772, Velocity: -0.06573464071991521, Movement: 0.40659975178917424, Collision: 0, Height: -1.0, Movement Penalty: -0.08136294267377732, Smoothness: -0.0, Curiosity: 11.852904319763184, Exploration: 0.4482159879739431, Total: 1.371457355561024
2024-07-21 22:28:21,267 - AirSimEnvLogger - INFO - Action: [-0.28333587 -0.69393883  0.31537773  0.02644639], Velocity: (-0.28333587099038526, -0.6939388323398668, 0.3153777319358082), Duration: 1.0, Reward: 1.371457355561024, Done: False
2024-07-21 22:28:21,329 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:21,329 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:28:24,200 - AirSimEnvLogger - INFO - Predictive model loss: 0.07567180693149567
2024-07-21 22:28:30,265 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.62403930807096, Velocity: -0.1065474140064259, Movement: 0.8223434162383283, Collision: 0, Height: -1.0, Movement Penalty: -0.2318857978698834, Smoothness: -0.0, Curiosity: 21.073448181152344, Exploration: 0.25809971816385097, Total: 10.997302246196853
2024-07-21 22:28:30,312 - AirSimEnvLogger - INFO - Action: [0.16529274 0.99466135 1.29935441 1.63465824], Velocity: (0.16529274323370524, 0.9946613538498119, 1.2993544078172181), Duration: 1.0, Reward: 10.997302246196853, Done: False
2024-07-21 22:28:30,374 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:30,374 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:28:33,323 - AirSimEnvLogger - INFO - Predictive model loss: 0.19800817966461182
2024-07-21 22:28:39,402 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.72497624282909, Velocity: -0.1112065770567543, Movement: 0.513780620874222, Collision: 0, Height: -1.0, Movement Penalty: -0.17571274846267915, Smoothness: -0.0, Curiosity: 18.269563674926758, Exploration: 0.2203940985353404, Total: 7.068230329746172
2024-07-21 22:28:39,465 - AirSimEnvLogger - INFO - Action: [-0.89662607 -0.46281837 -0.19427546 -1.42534729], Velocity: (-0.8966260664003536, -0.462818374311174, -0.19427546163886822), Duration: 1.0, Reward: 7.068230329746172, Done: False
2024-07-21 22:28:39,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:39,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:28:42,376 - AirSimEnvLogger - INFO - Predictive model loss: 0.13935486972332
2024-07-21 22:28:48,350 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.282595822032242, Velocity: -0.08532478735618843, Movement: 0.48793584411781116, Collision: 0, Height: -1.0, Movement Penalty: -0.12694084328094266, Smoothness: -0.0, Curiosity: 15.921242713928223, Exploration: 0.2245940293906322, Total: 5.186611636456229
2024-07-21 22:28:48,429 - AirSimEnvLogger - INFO - Action: [0.52784017 0.64551016 0.50697825 0.81183263], Velocity: (0.5278401662242198, 0.6455101574232183, 0.5069782514898284), Duration: 1.0, Reward: 5.186611636456229, Done: False
2024-07-21 22:28:48,492 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:48,492 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:28:51,415 - AirSimEnvLogger - INFO - Predictive model loss: 0.14774399995803833
2024-07-21 22:28:57,309 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.981290662894192, Velocity: -0.19581137758901035, Movement: 1.4452818138952372, Collision: 0, Height: -1.0, Movement Penalty: -0.3189276881412875, Smoothness: -0.0, Curiosity: 27.156728744506836, Exploration: 0.4844938247216488, Total: 14.81016613341441
2024-07-21 22:28:57,434 - AirSimEnvLogger - INFO - Action: [-1.36113133 -1.8337967  -1.77196763 -1.34763828], Velocity: (-1.3611313251996826, -1.8337967001795816, -1.7719676250642569), Duration: 1.0, Reward: 14.81016613341441, Done: False
2024-07-21 22:28:57,497 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:28:57,497 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:00,346 - AirSimEnvLogger - INFO - Predictive model loss: 0.38916903734207153
2024-07-21 22:29:06,297 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.499418494154119, Velocity: -0.13456829798914777, Movement: 0.6410674319302688, Collision: 0, Height: -1.0, Movement Penalty: -0.203024599136399, Smoothness: -0.0, Curiosity: 17.523807525634766, Exploration: 0.09821984621586498, Total: 6.5217599072134185
2024-07-21 22:29:06,407 - AirSimEnvLogger - INFO - Action: [0.83708426 0.85299928 0.46427577 1.57417565], Velocity: (0.8370842552500808, 0.8529992773383821, 0.4642757710663943), Duration: 1.0, Reward: 6.5217599072134185, Done: False
2024-07-21 22:29:06,470 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:29:06,471 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:09,193 - AirSimEnvLogger - INFO - Predictive model loss: 0.15680480003356934
2024-07-21 22:29:15,503 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.919319336378056, Velocity: -0.09506438387381509, Movement: 0.7594776009370301, Collision: 0, Height: -1.0, Movement Penalty: -0.16162771611484028, Smoothness: -0.0, Curiosity: 17.204187393188477, Exploration: 0.8591085428894886, Total: 5.9984695893613855
2024-07-21 22:29:15,676 - AirSimEnvLogger - INFO - Action: [ 1.0584998   1.03937639 -0.32634308  0.55238298], Velocity: (1.0584997994630378, 1.039376385727558, -0.3263430842363506), Duration: 1.0, Reward: 5.9984695893613855, Done: False
2024-07-21 22:29:15,725 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:29:15,725 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:17,992 - AirSimEnvLogger - INFO - Predictive model loss: 0.3158417344093323
2024-07-21 22:29:23,940 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.919574398131886, Velocity: -0.27966003669983275, Movement: 0.975588665379295, Collision: 0, Height: -1.0, Movement Penalty: -0.24277298540033865, Smoothness: -0.0, Curiosity: 25.604175567626953, Exploration: 0.4657460467847059, Total: 15.278952677038498
2024-07-21 22:29:24,064 - AirSimEnvLogger - INFO - Action: [0.60386537 1.3345275  1.28898252 1.44456889], Velocity: (0.6038653733291544, 1.3345274967650604, 1.2889825240623858), Duration: 1.0, Reward: 15.278952677038498, Done: False
2024-07-21 22:29:24,157 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:29:24,158 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:26,688 - AirSimEnvLogger - INFO - Predictive model loss: 0.361316055059433
2024-07-21 22:29:32,737 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.523801980393854, Velocity: -0.12940613484638358, Movement: 0.8263626521861718, Collision: 0, Height: -1.0, Movement Penalty: -0.17036426295957302, Smoothness: -0.0, Curiosity: 22.003936767578125, Exploration: 0.34411994040607136, Total: 10.076850399819307
2024-07-21 22:29:32,784 - AirSimEnvLogger - INFO - Action: [-1.21365833 -0.33423976 -1.07089597 -0.41339724], Velocity: (-1.2136583284292919, -0.33423975620978874, -1.0708959701650267), Duration: 1.0, Reward: 10.076850399819307, Done: False
2024-07-21 22:29:32,847 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:29:32,847 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:35,786 - AirSimEnvLogger - INFO - Predictive model loss: 0.27055278420448303
2024-07-21 22:29:41,840 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.310132774420056, Velocity: -0.06909560868317051, Movement: 0.4307774672166125, Collision: 0, Height: -1.0, Movement Penalty: -0.14043173744442325, Smoothness: -0.0, Curiosity: 16.56120491027832, Exploration: 0.2934133735721049, Total: 5.801740627599645
2024-07-21 22:29:41,934 - AirSimEnvLogger - INFO - Action: [ 0.63098738 -0.03771178  0.58541409 -1.10897718], Velocity: (0.6309873774265893, -0.037711782736740806, 0.5854140893567308), Duration: 1.0, Reward: 5.801740627599645, Done: False
2024-07-21 22:29:41,996 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:29:41,996 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:45,010 - AirSimEnvLogger - INFO - Predictive model loss: 0.2065555304288864
2024-07-21 22:29:51,590 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.349102446911877, Velocity: -0.08321204262079346, Movement: 1.303557722659914, Collision: 0, Height: -1.0, Movement Penalty: -0.3100120338203153, Smoothness: -0.0, Curiosity: 31.524538040161133, Exploration: 0.9605005641984522, Total: 21.91552158301505
2024-07-21 22:29:51,715 - AirSimEnvLogger - INFO - Action: [1.27370586 1.14150937 1.96765868 1.67740727], Velocity: (1.2737058577398255, 1.1415093695920107, 1.9676586828812845), Duration: 1.0, Reward: 21.91552158301505, Done: False
2024-07-21 22:29:51,761 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:29:51,762 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:29:54,734 - AirSimEnvLogger - INFO - Predictive model loss: 0.5315795540809631
2024-07-21 22:30:00,758 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.110692531874488, Velocity: -0.17677937439361707, Movement: 1.022499665117845, Collision: 0, Height: -1.0, Movement Penalty: -0.21903313436870075, Smoothness: -0.0, Curiosity: 27.870193481445312, Exploration: 0.5751171157811117, Total: 17.407733334693265
2024-07-21 22:30:00,869 - AirSimEnvLogger - INFO - Action: [1.39916675 1.18731692 0.90257032 0.78455665], Velocity: (1.3991667501354288, 1.1873169244067792, 0.9025703224664046), Duration: 1.0, Reward: 17.407733334693265, Done: False
2024-07-21 22:30:00,963 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:00,963 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:03,781 - AirSimEnvLogger - INFO - Predictive model loss: 0.6760810017585754
2024-07-21 22:30:09,002 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.499343950400084, Velocity: -0.07804806668304065, Movement: 0.7799102535754896, Collision: 0, Height: -1.0, Movement Penalty: -0.15753949610934556, Smoothness: -0.0, Curiosity: 31.178359985351562, Exploration: 0.36680245739810563, Total: 20.285993480930635
2024-07-21 22:30:09,143 - AirSimEnvLogger - INFO - Action: [-0.13045507  1.2825636   0.87809572 -0.22097346], Velocity: (-0.13045507455214111, 1.282563601530664, 0.87809572147974), Duration: 1.0, Reward: 20.285993480930635, Done: False
2024-07-21 22:30:09,206 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:09,206 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:11,925 - AirSimEnvLogger - INFO - Predictive model loss: 0.6165374517440796
2024-07-21 22:30:17,831 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.585721289297709, Velocity: -0.19234255268142733, Movement: 1.342730878892347, Collision: 0, Height: -1.0, Movement Penalty: -0.3095169508884884, Smoothness: -0.0, Curiosity: 34.93082046508789, Exploration: 0.32725184411351493, Total: 24.934356692268832
2024-07-21 22:30:17,925 - AirSimEnvLogger - INFO - Action: [1.96815261 0.18307642 1.81784575 1.53895076], Velocity: (1.9681526094642032, 0.1830764167095249, 1.8178457536403099), Duration: 1.0, Reward: 24.934356692268832, Done: False
2024-07-21 22:30:18,003 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:18,003 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:20,822 - AirSimEnvLogger - INFO - Predictive model loss: 0.872480571269989
2024-07-21 22:30:26,804 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.009690374876689, Velocity: -0.15522531863765585, Movement: 0.8649238195451129, Collision: 0, Height: -1.0, Movement Penalty: -0.21084358656712202, Smoothness: -0.0, Curiosity: 19.84296417236328, Exploration: 0.6195012540602014, Total: 8.475904095509428
2024-07-21 22:30:26,899 - AirSimEnvLogger - INFO - Action: [ 0.24999415 -1.45603678 -0.89990704 -1.20545798], Velocity: (0.2499941452639718, -1.4560367806059675, -0.8999070370445905), Duration: 1.0, Reward: 8.475904095509428, Done: False
2024-07-21 22:30:26,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:26,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:29,868 - AirSimEnvLogger - INFO - Predictive model loss: 0.49591484665870667
2024-07-21 22:30:35,721 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.79946716365988, Velocity: -0.10269193105754226, Movement: 0.7577869816956692, Collision: 0, Height: -1.0, Movement Penalty: -0.1581620928907742, Smoothness: -0.0, Curiosity: 28.97713851928711, Exploration: 0.37823749325589967, Total: 17.78140591696234
2024-07-21 22:30:35,832 - AirSimEnvLogger - INFO - Action: [1.01922251 0.94387584 0.60601017 0.45228346], Velocity: (1.019222506273313, 0.9438758363407498, 0.6060101705315899), Duration: 1.0, Reward: 17.78140591696234, Done: False
2024-07-21 22:30:35,910 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:35,910 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:38,841 - AirSimEnvLogger - INFO - Predictive model loss: 0.8057762384414673
2024-07-21 22:30:44,667 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.390782476490786, Velocity: -0.04692158628534817, Movement: 0.4504398681024506, Collision: 0, Height: -1.0, Movement Penalty: -0.19468768946845647, Smoothness: -0.0, Curiosity: 26.795366287231445, Exploration: 0.28606964104754445, Total: 14.931405406353175
2024-07-21 22:30:44,673 - AirSimEnvLogger - INFO - Action: [-7.26247809e-01  4.15438451e-04 -5.33055575e-01 -1.72590421e+00], Velocity: (-0.7262478094246994, 0.0004154384509440501, -0.5330555747962227), Duration: 1.0, Reward: 14.931405406353175, Done: False
2024-07-21 22:30:44,699 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:44,699 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:47,548 - AirSimEnvLogger - INFO - Predictive model loss: 0.47915297746658325
2024-07-21 22:30:53,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.436573875923434, Velocity: -0.013646532680459665, Movement: 0.42508248051855735, Collision: 0, Height: -1.0, Movement Penalty: -0.12636296919282294, Smoothness: -0.0, Curiosity: 24.133609771728516, Exploration: 0.25243390518494696, Total: 13.25006110340576
2024-07-21 22:30:53,407 - AirSimEnvLogger - INFO - Action: [-3.62985281e-02 -9.14186463e-04  8.49389217e-01  9.34868727e-01], Velocity: (-0.03629852806538758, -0.0009141864632113972, 0.8493892170840383), Duration: 1.0, Reward: 13.25006110340576, Done: False
2024-07-21 22:30:53,503 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:30:53,503 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:30:56,219 - AirSimEnvLogger - INFO - Predictive model loss: 0.3612995445728302
2024-07-21 22:31:02,017 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.896435386354261, Velocity: -0.042142142810549864, Movement: 0.20206549637425977, Collision: 0, Height: -1.0, Movement Penalty: -0.0418907728106781, Smoothness: -0.0, Curiosity: 20.25600814819336, Exploration: 0.20543469011955526, Total: 8.9099523093726
2024-07-21 22:31:02,127 - AirSimEnvLogger - INFO - Action: [ 0.21534659 -0.33000329 -0.08969691  0.11028067], Velocity: (0.21534658758694625, -0.33000328952239255, -0.08969690863659463), Duration: 1.0, Reward: 8.9099523093726, Done: False
2024-07-21 22:31:02,189 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:02,189 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:04,886 - AirSimEnvLogger - INFO - Predictive model loss: 0.3332579731941223
2024-07-21 22:31:10,333 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.829033416454292, Velocity: -0.14342924859685666, Movement: 0.7561380313979105, Collision: 0, Height: -1.0, Movement Penalty: -0.1631385110612268, Smoothness: -0.0, Curiosity: 22.1125431060791, Exploration: 0.38904600034541603, Total: 10.882937413709664
2024-07-21 22:31:10,443 - AirSimEnvLogger - INFO - Action: [-1.42396987 -0.5005542   0.09345685 -0.61191379], Velocity: (-1.4239698722179968, -0.5005542039792326, 0.0934568456529965), Duration: 1.0, Reward: 10.882937413709664, Done: False
2024-07-21 22:31:10,553 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:10,553 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:13,254 - AirSimEnvLogger - INFO - Predictive model loss: 0.10588303208351135
2024-07-21 22:31:19,050 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.549033476122652, Velocity: -0.087437091898915, Movement: 0.6697973236832072, Collision: 0, Height: -1.0, Movement Penalty: -0.18176965190533217, Smoothness: -0.0, Curiosity: 25.715320587158203, Exploration: 0.49170006684283946, Total: 13.773521216582495
2024-07-21 22:31:19,142 - AirSimEnvLogger - INFO - Action: [-1.13387218 -0.03003438 -0.71270305 -1.22861988], Velocity: (-1.1338721805585823, -0.03003437691675548, -0.7127030472863829), Duration: 1.0, Reward: 13.773521216582495, Done: False
2024-07-21 22:31:19,188 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:19,188 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:21,946 - AirSimEnvLogger - INFO - Predictive model loss: 0.223258838057518
2024-07-21 22:31:27,815 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.107261713228773, Velocity: -0.13901732864413854, Movement: 0.6655400382531137, Collision: 0, Height: -1.0, Movement Penalty: -0.16334418105600573, Smoothness: -0.0, Curiosity: 16.998897552490234, Exploration: 0.2931003334377187, Total: 5.456650697915157
2024-07-21 22:31:27,908 - AirSimEnvLogger - INFO - Action: [-0.39026797 -1.18730396 -0.45801135 -0.94676184], Velocity: (-0.39026796572336386, -1.1873039561744854, -0.4580113542854449), Duration: 1.0, Reward: 5.456650697915157, Done: False
2024-07-21 22:31:28,001 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:28,001 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:30,777 - AirSimEnvLogger - INFO - Predictive model loss: 0.11946478486061096
2024-07-21 22:31:36,831 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.201058443809064, Velocity: -0.1696059337416049, Movement: 0.7963694623856661, Collision: 0, Height: -1.0, Movement Penalty: -0.16602491386352525, Smoothness: -0.0, Curiosity: 29.900171279907227, Exploration: 0.3929443459227884, Total: 18.301008710088475
2024-07-21 22:31:36,958 - AirSimEnvLogger - INFO - Action: [0.55761652 1.48244663 0.1680271  0.46862556], Velocity: (0.5576165170405374, 1.4824466253004143, 0.16802709763824697), Duration: 1.0, Reward: 18.301008710088475, Done: False
2024-07-21 22:31:37,020 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:37,021 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:39,827 - AirSimEnvLogger - INFO - Predictive model loss: 0.19423076510429382
2024-07-21 22:31:45,735 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.817288997780363, Velocity: -0.050994025297709474, Movement: 0.40957229915504645, Collision: 0, Height: -1.0, Movement Penalty: -0.09090597888109923, Smoothness: -0.0, Curiosity: 19.753986358642578, Exploration: 0.2580931108803961, Total: 8.502846506276093
2024-07-21 22:31:45,862 - AirSimEnvLogger - INFO - Action: [-0.59356246 -0.55887431  0.07963029 -0.3941977 ], Velocity: (-0.5935624601707357, -0.5588743106281449, 0.07963029408826183), Duration: 1.0, Reward: 8.502846506276093, Done: False
2024-07-21 22:31:45,956 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:45,956 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:48,593 - AirSimEnvLogger - INFO - Predictive model loss: 0.05934998020529747
2024-07-21 22:31:54,430 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.66005895022876, Velocity: -0.15989746577948868, Movement: 0.9311078224053437, Collision: 0, Height: -1.0, Movement Penalty: -0.2201853483949456, Smoothness: -0.0, Curiosity: 28.62897491455078, Exploration: 0.29168691345408987, Total: 17.540276628777786
2024-07-21 22:31:54,557 - AirSimEnvLogger - INFO - Action: [1.63091537 0.69242819 0.57315387 1.17486665], Velocity: (1.6309153741803666, 0.6924281879284446, 0.5731538664284925), Duration: 1.0, Reward: 17.540276628777786, Done: False
2024-07-21 22:31:54,635 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:31:54,635 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:31:57,247 - AirSimEnvLogger - INFO - Predictive model loss: 0.1345916986465454
2024-07-21 22:32:03,297 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.426084871330449, Velocity: -0.009377645973398145, Movement: 0.3545866344742577, Collision: 0, Height: -1.0, Movement Penalty: -0.08375498568666431, Smoothness: -0.0, Curiosity: 22.98262596130371, Exploration: 0.3433131160644451, Total: 12.142329978053807
2024-07-21 22:32:03,344 - AirSimEnvLogger - INFO - Action: [-0.03638489  0.09299784  0.70210702  0.44560413], Velocity: (-0.03638489143479373, 0.09299784140352862, 0.7021070193073744), Duration: 1.0, Reward: 12.142329978053807, Done: False
2024-07-21 22:32:03,406 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:03,406 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:06,197 - AirSimEnvLogger - INFO - Predictive model loss: 0.030797502025961876
2024-07-21 22:32:12,157 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.324492477612775, Velocity: -0.14448031996501642, Movement: 1.050285353824501, Collision: 0, Height: -1.0, Movement Penalty: -0.24586960740776176, Smoothness: -0.0, Curiosity: 36.16667938232422, Exploration: 0.2912553195276436, Total: 24.41883274203785
2024-07-21 22:32:12,298 - AirSimEnvLogger - INFO - Action: [ 1.77973365  1.11431858 -0.0569171   1.27780636], Velocity: (1.7797336461761786, 1.1143185766552628, -0.0569170995612176), Duration: 1.0, Reward: 24.41883274203785, Done: False
2024-07-21 22:32:12,344 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:12,344 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:14,913 - AirSimEnvLogger - INFO - Predictive model loss: 0.2676226496696472
2024-07-21 22:32:20,993 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.345659076868845, Velocity: -0.20788976428673317, Movement: 1.1233583931676059, Collision: 0, Height: -1.0, Movement Penalty: -0.24466251990177457, Smoothness: -0.0, Curiosity: 21.27873992919922, Exploration: 0.5271216410704467, Total: 9.568468768440482
2024-07-21 22:32:21,118 - AirSimEnvLogger - INFO - Action: [-1.20380685 -1.70009092 -0.8415915  -0.96862715], Velocity: (-1.2038068514439748, -1.700090920059901, -0.8415914958855304), Duration: 1.0, Reward: 9.568468768440482, Done: False
2024-07-21 22:32:21,213 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:21,213 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:24,021 - AirSimEnvLogger - INFO - Predictive model loss: 0.1712670922279358
2024-07-21 22:32:30,231 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.545624172674817, Velocity: -0.13590430705113576, Movement: 0.6912301344151901, Collision: 0, Height: -1.0, Movement Penalty: -0.166094295040956, Smoothness: -0.0, Curiosity: 26.965673446655273, Exploration: 0.7331397985486264, Total: 15.088806068239176
2024-07-21 22:32:30,357 - AirSimEnvLogger - INFO - Action: [-1.27665318  0.30968538 -0.43063675 -0.92061669], Velocity: (-1.276653181984714, 0.3096853835850366, -0.4306367506572806), Duration: 1.0, Reward: 15.088806068239176, Done: False
2024-07-21 22:32:30,420 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:30,420 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:33,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.14145442843437195
2024-07-21 22:32:38,532 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.516783026352178, Velocity: -0.09213671132273248, Movement: 0.8958247508854048, Collision: 0, Height: -1.0, Movement Penalty: -0.20816634887825605, Smoothness: -0.0, Curiosity: 35.95415496826172, Exploration: 0.7746195716711876, Total: 25.12765306294647
2024-07-21 22:32:38,657 - AirSimEnvLogger - INFO - Action: [0.13178754 1.15740087 1.36127265 1.05986553], Velocity: (0.13178754174742258, 1.1574008668692601, 1.3612726451398511), Duration: 1.0, Reward: 25.12765306294647, Done: False
2024-07-21 22:32:38,717 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:38,718 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:41,316 - AirSimEnvLogger - INFO - Predictive model loss: 0.05707533657550812
2024-07-21 22:32:47,409 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.023440370771262, Velocity: -0.10068740985190651, Movement: 1.0129841993432849, Collision: 0, Height: -1.0, Movement Penalty: -0.20918377003769206, Smoothness: -0.0, Curiosity: 36.866668701171875, Exploration: 0.7763312463824119, Total: 26.54894167897259
2024-07-21 22:32:47,535 - AirSimEnvLogger - INFO - Action: [0.85274497 0.68528985 1.70521312 0.5208042 ], Velocity: (0.8527449668513365, 0.6852898530494467, 1.7052131219568434), Duration: 1.0, Reward: 26.54894167897259, Done: False
2024-07-21 22:32:47,627 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:47,627 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:50,607 - AirSimEnvLogger - INFO - Predictive model loss: 0.09267907589673996
2024-07-21 22:32:56,642 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.362550409954506, Velocity: -0.11159998315132102, Movement: 0.9194845237716489, Collision: 0, Height: -1.0, Movement Penalty: -0.18481032425549848, Smoothness: -0.0, Curiosity: 38.531768798828125, Exploration: 0.32853726403162686, Total: 27.769944632486045
2024-07-21 22:32:56,735 - AirSimEnvLogger - INFO - Action: [1.04532872 0.75304942 1.31225439 0.18351686], Velocity: (1.0453287235813984, 0.753049422917601, 1.3122543900179784), Duration: 1.0, Reward: 27.769944632486045, Done: False
2024-07-21 22:32:56,798 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:32:56,798 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:32:59,743 - AirSimEnvLogger - INFO - Predictive model loss: 0.10812219977378845
2024-07-21 22:33:05,891 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.849737099503152, Velocity: -0.17303590720737805, Movement: 1.0881115343604342, Collision: 0, Height: -1.0, Movement Penalty: -0.2404630435613156, Smoothness: -0.0, Curiosity: 32.93168640136719, Exploration: 0.7596004844194698, Total: 20.77145791558958
2024-07-21 22:33:06,001 - AirSimEnvLogger - INFO - Action: [-1.43010427 -0.84796057 -1.40417645 -1.0228884 ], Velocity: (-1.4301042730217188, -0.8479605662578444, -1.4041764458890378), Duration: 1.0, Reward: 20.77145791558958, Done: False
2024-07-21 22:33:06,063 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:06,063 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:33:08,508 - AirSimEnvLogger - INFO - Predictive model loss: 0.12626302242279053
2024-07-21 22:33:14,811 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.871711938704262, Velocity: -0.12507859734758858, Movement: 0.7483826955873649, Collision: 0, Height: -1.0, Movement Penalty: -0.201207299396287, Smoothness: -0.0, Curiosity: 31.0301513671875, Exploration: 0.8713265236073746, Total: 18.85050659686226
2024-07-21 22:33:14,905 - AirSimEnvLogger - INFO - Action: [-1.17721048 -0.51907786 -0.76487927 -1.34466765], Velocity: (-1.1772104758383852, -0.5190778635434252, -0.7648792737249439), Duration: 1.0, Reward: 18.85050659686226, Done: False
2024-07-21 22:33:14,968 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:14,968 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:33:17,863 - AirSimEnvLogger - INFO - Predictive model loss: 0.1741056591272354
2024-07-21 22:33:23,718 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.442622249993995, Velocity: -0.10767969995023079, Movement: 0.9244005252797003, Collision: 0, Height: -1.0, Movement Penalty: -0.22062911469360313, Smoothness: -0.0, Curiosity: 31.413387298583984, Exploration: 0.3036277603071066, Total: 18.54893297965796
2024-07-21 22:33:23,858 - AirSimEnvLogger - INFO - Action: [-0.70346524 -0.58983446 -1.60477328 -1.20401632], Velocity: (-0.7034652444779852, -0.5898344622596041, -1.6047732804019375), Duration: 1.0, Reward: 18.54893297965796, Done: False
2024-07-21 22:33:23,937 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:23,938 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:33:26,904 - AirSimEnvLogger - INFO - Predictive model loss: 0.36324644088745117
2024-07-21 22:33:32,575 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.401631689814504, Velocity: -0.17526219521070582, Movement: 0.9077171948694939, Collision: 0, Height: -1.0, Movement Penalty: -0.19149027365560672, Smoothness: -0.0, Curiosity: 26.147937774658203, Exploration: 0.4273742678224849, Total: 14.358411217375727
2024-07-21 22:33:32,655 - AirSimEnvLogger - INFO - Action: [-1.45229022 -1.0687476  -0.21079303 -0.60913912], Velocity: (-1.4522902179675752, -1.0687476047667135, -0.21079303486613687), Duration: 1.0, Reward: 14.358411217375727, Done: False
2024-07-21 22:33:32,716 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:32,716 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:33:35,496 - AirSimEnvLogger - INFO - Predictive model loss: 0.4875887334346771
2024-07-21 22:33:41,148 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.16103048682466, Velocity: -0.09730242509184442, Movement: 0.8069004745479978, Collision: 0, Height: -1.0, Movement Penalty: -0.25306430928045376, Smoothness: -0.0, Curiosity: 33.94068908691406, Exploration: 0.38944592683960216, Total: 21.345934832549815
2024-07-21 22:33:41,149 - AirSimEnvLogger - INFO - Action: [-1.07831338 -0.21099188 -1.18197977 -1.94930782], Velocity: (-1.0783133829165967, -0.21099187831456012, -1.1819797708977067), Duration: 1.0, Reward: 21.345934832549815, Done: False
2024-07-21 22:33:41,205 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:41,205 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:33:44,028 - AirSimEnvLogger - INFO - Predictive model loss: 0.48270484805107117
2024-07-21 22:33:49,944 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.01033766861, Velocity: -0.09478405606425806, Movement: 0.788385826444418, Collision: 0, Height: -1.0, Movement Penalty: -0.19978012717309043, Smoothness: -0.0, Curiosity: 22.526796340942383, Exploration: 0.503032319154238, Total: 11.133161652247104
2024-07-21 22:33:50,071 - AirSimEnvLogger - INFO - Action: [ 1.5651272  -0.19094644  0.01118657  1.22678485], Velocity: (1.5651272035284296, -0.19094643981668025, 0.01118656562151732), Duration: 1.0, Reward: 11.133161652247104, Done: False
2024-07-21 22:33:50,118 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:50,118 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:33:52,811 - AirSimEnvLogger - INFO - Predictive model loss: 0.18336652219295502
2024-07-21 22:33:58,647 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.11626117632557, Velocity: -0.04478545943843968, Movement: 0.4114439673266617, Collision: 0, Height: -1.0, Movement Penalty: -0.08230514372418309, Smoothness: -0.0, Curiosity: 27.195377349853516, Exploration: 0.5417109444895929, Total: 15.715680728380347
2024-07-21 22:33:58,805 - AirSimEnvLogger - INFO - Action: [0.07343169 0.79770349 0.18820597 0.01640474], Velocity: (0.07343168712066417, 0.797703487298189, 0.18820596876011497), Duration: 1.0, Reward: 15.715680728380347, Done: False
2024-07-21 22:33:58,868 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:33:58,868 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:01,739 - AirSimEnvLogger - INFO - Predictive model loss: 0.09930596500635147
2024-07-21 22:34:07,691 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.055062238166707, Velocity: -0.11133197554376588, Movement: 1.1178664882292684, Collision: 0, Height: -1.0, Movement Penalty: -0.2285447526210608, Smoothness: -0.0, Curiosity: 23.59726333618164, Exploration: 0.5163797182451156, Total: 11.1920641676985
2024-07-21 22:34:07,879 - AirSimEnvLogger - INFO - Action: [-0.36886256 -1.4927786  -1.62297702 -0.47409751], Velocity: (-0.3688625577486284, -1.4927785998905065, -1.6229770199310358), Duration: 1.0, Reward: 11.1920641676985, Done: False
2024-07-21 22:34:07,942 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:34:07,942 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:10,795 - AirSimEnvLogger - INFO - Predictive model loss: 0.2875993847846985
2024-07-21 22:34:16,907 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.746943644110626, Velocity: -0.15273749637921707, Movement: 0.6631689078759649, Collision: 0, Height: -1.0, Movement Penalty: -0.13263465562712837, Smoothness: -0.0, Curiosity: 20.259756088256836, Exploration: 0.35754077738931905, Total: 9.1062993925969
2024-07-21 22:34:17,046 - AirSimEnvLogger - INFO - Action: [ 1.24712052  0.15708619  0.42330406 -0.00481517], Velocity: (1.2471205230751206, 0.15708619417220504, 0.42330406331493053), Duration: 1.0, Reward: 9.1062993925969, Done: False
2024-07-21 22:34:17,139 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:34:17,139 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:20,112 - AirSimEnvLogger - INFO - Predictive model loss: 0.007719500456005335
2024-07-21 22:34:26,269 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.926686398676795, Velocity: -0.15076802207250986, Movement: 0.7459161057257175, Collision: 0, Height: -1.0, Movement Penalty: -0.15213515183633897, Smoothness: -0.0, Curiosity: 26.498685836791992, Exploration: 0.8125343516256457, Total: 15.272166215665276
2024-07-21 22:34:26,348 - AirSimEnvLogger - INFO - Action: [1.13500391 0.93747394 0.24181004 0.29824   ], Velocity: (1.1350039089066115, 0.9374739353495325, 0.24181003790383038), Duration: 1.0, Reward: 15.272166215665276, Done: False
2024-07-21 22:34:26,395 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:34:26,395 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:29,368 - AirSimEnvLogger - INFO - Predictive model loss: 0.09868986904621124
2024-07-21 22:34:35,236 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.96135064153967, Velocity: -0.09049675181981504, Movement: 0.8458787819823932, Collision: 0, Height: -1.0, Movement Penalty: -0.23606313989678174, Smoothness: -0.0, Curiosity: 25.76964569091797, Exploration: 0.4575399193326069, Total: 13.404871015171961
2024-07-21 22:34:35,314 - AirSimEnvLogger - INFO - Action: [-0.70943913 -0.62110157 -1.40462544 -1.64637084], Velocity: (-0.7094391257288819, -0.6211015726872131, -1.4046254370908207), Duration: 1.0, Reward: 13.404871015171961, Done: False
2024-07-21 22:34:35,409 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:34:35,409 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:38,298 - AirSimEnvLogger - INFO - Predictive model loss: 0.1622016578912735
2024-07-21 22:34:44,141 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.818149344693396, Velocity: -0.12220312317736357, Movement: 0.5713546887739519, Collision: 0, Height: -1.0, Movement Penalty: -0.1159416353729153, Smoothness: -0.0, Curiosity: 29.7391357421875, Exploration: 0.4223087667430909, Total: 17.52791594026917
2024-07-21 22:34:44,172 - AirSimEnvLogger - INFO - Action: [-0.50598617  0.89844362 -0.49250562 -0.19611619], Velocity: (-0.5059861726264818, 0.8984436162240617, -0.4925056173415212), Duration: 1.0, Reward: 17.52791594026917, Done: False
2024-07-21 22:34:44,234 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:34:44,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:47,215 - AirSimEnvLogger - INFO - Predictive model loss: 0.23592066764831543
2024-07-21 22:34:53,184 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.7308303172983, Velocity: -0.03695339145805887, Movement: 0.574458362936716, Collision: 0, Height: -1.0, Movement Penalty: -0.16059198875081587, Smoothness: -0.0, Curiosity: 22.818130493164062, Exploration: 0.10036944888425556, Total: 10.606817986399067
2024-07-21 22:34:53,295 - AirSimEnvLogger - INFO - Action: [-0.02305122 -0.59743253 -0.98109768 -1.1220379 ], Velocity: (-0.023051218524700046, -0.5974325334075821, -0.9810976772689108), Duration: 1.0, Reward: 10.606817986399067, Done: False
2024-07-21 22:34:53,388 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:34:53,388 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:34:56,182 - AirSimEnvLogger - INFO - Predictive model loss: 0.12322092801332474
2024-07-21 22:35:02,238 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.827937079165272, Velocity: -0.17631024156856798, Movement: 1.0953812087455372, Collision: 0, Height: -1.0, Movement Penalty: -0.23296546729710374, Smoothness: -0.0, Curiosity: 27.883438110351562, Exploration: 0.5723115269419629, Total: 15.706372293801907
2024-07-21 22:35:02,333 - AirSimEnvLogger - INFO - Action: [-1.79764    -0.87422177 -0.8964746  -0.79237045], Velocity: (-1.797639999313116, -0.8742217680684057, -0.8964745969609107), Duration: 1.0, Reward: 15.706372293801907, Done: False
2024-07-21 22:35:02,395 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:02,395 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:05,113 - AirSimEnvLogger - INFO - Predictive model loss: 0.3348293900489807
2024-07-21 22:35:10,974 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.264487692782577, Velocity: -0.24230814288095415, Movement: 1.1278116149610202, Collision: 0, Height: -1.0, Movement Penalty: -0.22795682803842127, Smoothness: -0.0, Curiosity: 24.117294311523438, Exploration: 0.5820495334204253, Total: 13.506362409214795
2024-07-21 22:35:11,082 - AirSimEnvLogger - INFO - Action: [1.98430375 0.21716996 1.05033899 0.32953815], Velocity: (1.9843037517271505, 0.2171699586132061, 1.0503389859095165), Duration: 1.0, Reward: 13.506362409214795, Done: False
2024-07-21 22:35:11,128 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:11,128 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:13,879 - AirSimEnvLogger - INFO - Predictive model loss: 0.10833603888750076
2024-07-21 22:35:19,840 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.599508855548645, Velocity: -0.03345957876714535, Movement: 0.5148030449251592, Collision: 0, Height: -1.0, Movement Penalty: -0.2023072924279684, Smoothness: -0.0, Curiosity: 23.383033752441406, Exploration: 0.5449960070661272, Total: 11.376446800716796
2024-07-21 22:35:19,855 - AirSimEnvLogger - INFO - Action: [ 0.10303137 -0.08474821 -1.02092653 -1.74147505], Velocity: (0.1030313665290663, -0.08474821282030964, -1.0209265292819942), Duration: 1.0, Reward: 11.376446800716796, Done: False
2024-07-21 22:35:19,902 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:19,902 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:22,825 - AirSimEnvLogger - INFO - Predictive model loss: 0.0637798085808754
2024-07-21 22:35:29,014 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.947681022827386, Velocity: -0.07702940344736199, Movement: 0.5360964044609977, Collision: 0, Height: -1.0, Movement Penalty: -0.16149658187412447, Smoothness: -0.0, Curiosity: 22.175094604492188, Exploration: 0.22041081977821778, Total: 10.764710335556533
2024-07-21 22:35:29,092 - AirSimEnvLogger - INFO - Action: [1.02959047 0.28346264 0.09586357 1.20769084], Velocity: (1.0295904654518593, 0.2834626417260775, 0.09586356815983765), Duration: 1.0, Reward: 10.764710335556533, Done: False
2024-07-21 22:35:29,156 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:29,156 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:31,966 - AirSimEnvLogger - INFO - Predictive model loss: 0.07078449428081512
2024-07-21 22:35:38,056 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.302469635216783, Velocity: -0.05002922479620026, Movement: 0.5115474981895264, Collision: 0, Height: -1.0, Movement Penalty: -0.16367067945907723, Smoothness: -0.0, Curiosity: 23.008358001708984, Exploration: 0.17046296175846556, Total: 11.229873235234024
2024-07-21 22:35:38,231 - AirSimEnvLogger - INFO - Action: [-0.90032293 -0.04599225 -0.48376307 -1.27753112], Velocity: (-0.9003229317780623, -0.045992246010758686, -0.4837630653918241), Duration: 1.0, Reward: 11.229873235234024, Done: False
2024-07-21 22:35:38,293 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:38,293 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:41,239 - AirSimEnvLogger - INFO - Predictive model loss: 0.04089362174272537
2024-07-21 22:35:47,248 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.359992604726475, Velocity: -0.2116844809028842, Movement: 1.380596371749876, Collision: 0, Height: -1.0, Movement Penalty: -0.28805982375183803, Smoothness: -0.0, Curiosity: 39.82201385498047, Exploration: 0.5412506688679529, Total: 29.1145940361724
2024-07-21 22:35:47,341 - AirSimEnvLogger - INFO - Action: [1.77271218 1.40520265 1.5833769  0.82076844], Velocity: (1.7727121776521775, 1.405202648439941, 1.5833769035756258), Duration: 1.0, Reward: 29.1145940361724, Done: False
2024-07-21 22:35:47,404 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:47,404 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:50,364 - AirSimEnvLogger - INFO - Predictive model loss: 0.4116237461566925
2024-07-21 22:35:55,803 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.500865399418728, Velocity: -0.12002155041006402, Movement: 0.8109049207089134, Collision: 0, Height: -1.0, Movement Penalty: -0.19329131398613952, Smoothness: -0.0, Curiosity: 23.846981048583984, Exploration: 0.18829277144160553, Total: 11.894301863461996
2024-07-21 22:35:55,929 - AirSimEnvLogger - INFO - Action: [-0.90220112 -0.86022973 -1.03745126 -1.05161117], Velocity: (-0.902201119346, -0.86022972684627, -1.037451261033757), Duration: 1.0, Reward: 11.894301863461996, Done: False
2024-07-21 22:35:55,991 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:35:55,991 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:35:58,947 - AirSimEnvLogger - INFO - Predictive model loss: 0.026382330805063248
2024-07-21 22:36:05,126 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.550703681989157, Velocity: -0.2144015283451525, Movement: 1.305241940759197, Collision: 0, Height: -1.0, Movement Penalty: -0.3002171589179702, Smoothness: -0.0, Curiosity: 46.66982650756836, Exploration: 0.1639474178627784, Total: 35.66801587110043
2024-07-21 22:36:05,236 - AirSimEnvLogger - INFO - Action: [1.04596744 1.51729036 1.84889377 1.48270299], Velocity: (1.0459674351386359, 1.5172903611441062, 1.848893772036683), Duration: 1.0, Reward: 35.66801587110043, Done: False
2024-07-21 22:36:05,313 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:05,313 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:36:08,245 - AirSimEnvLogger - INFO - Predictive model loss: 0.46605998277664185
2024-07-21 22:36:14,516 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.18327556433816, Velocity: -0.12135049951148326, Movement: 0.6010722307974863, Collision: 0, Height: -1.0, Movement Penalty: -0.12684406632822515, Smoothness: -0.0, Curiosity: 31.953874588012695, Exploration: 0.6638996311216397, Total: 20.4318889680289
2024-07-21 22:36:14,641 - AirSimEnvLogger - INFO - Action: [1.1956736  0.11393425 0.0503481  0.40471028], Velocity: (1.1956736022137748, 0.11393424599129798, 0.05034809929874684), Duration: 1.0, Reward: 20.4318889680289, Done: False
2024-07-21 22:36:14,688 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:14,688 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:36:17,546 - AirSimEnvLogger - INFO - Predictive model loss: 0.31244587898254395
2024-07-21 22:36:23,196 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.411169085736619, Velocity: -0.03617059125066488, Movement: 0.4130759942285433, Collision: 0, Height: -1.0, Movement Penalty: -0.13829891554677873, Smoothness: -0.0, Curiosity: 26.63085174560547, Exploration: 0.608556899362792, Total: 14.844714869015831
2024-07-21 22:36:23,323 - AirSimEnvLogger - INFO - Action: [-0.42178437 -0.56106076 -0.43570159 -1.10911311], Velocity: (-0.4217843687149996, -0.5610607627052964, -0.43570158926935143), Duration: 1.0, Reward: 14.844714869015831, Done: False
2024-07-21 22:36:23,386 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:23,386 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:36:26,130 - AirSimEnvLogger - INFO - Predictive model loss: 0.0964077040553093
2024-07-21 22:36:31,866 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.916457315709746, Velocity: -0.13702249827817506, Movement: 1.085262623344838, Collision: 0, Height: -1.0, Movement Penalty: -0.243604659670604, Smoothness: -0.0, Curiosity: 48.37053298950195, Exploration: 0.4007758898052992, Total: 37.062686316052584
2024-07-21 22:36:31,914 - AirSimEnvLogger - INFO - Action: [0.79207322 1.4202038  1.43764427 1.10595803], Velocity: (0.7920732229525165, 1.4202037962931404, 1.437644265106443), Duration: 1.0, Reward: 37.062686316052584, Done: False
2024-07-21 22:36:31,930 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:31,930 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:36:34,601 - AirSimEnvLogger - INFO - Predictive model loss: 0.4827336072921753
2024-07-21 22:36:40,370 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.601450634846174, Velocity: -0.11897546573578, Movement: 1.2184232129196688, Collision: 0, Height: -1.0, Movement Penalty: -0.2507576908006469, Smoothness: -0.0, Curiosity: 54.85197830200195, Exploration: 0.8907628186426716, Total: 43.98870597327845
2024-07-21 22:36:40,479 - AirSimEnvLogger - INFO - Action: [0.87737981 1.21463545 1.92174033 0.59137251], Velocity: (0.8773798144134923, 1.2146354535632227, 1.921740325676954), Duration: 1.0, Reward: 43.98870597327845, Done: False
2024-07-21 22:36:40,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:40,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:36:43,202 - AirSimEnvLogger - INFO - Predictive model loss: 0.7043647766113281
2024-07-21 22:36:49,181 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.869829995629228, Velocity: -0.12067856205977502, Movement: 0.6711581509693937, Collision: 0, Height: -1.0, Movement Penalty: -0.13428189988850542, Smoothness: -0.0, Curiosity: 31.149715423583984, Exploration: 0.38558786476592266, Total: 19.883323971836404
2024-07-21 22:36:49,273 - AirSimEnvLogger - INFO - Action: [-0.12603765 -1.29695248  0.322245   -0.03673975], Velocity: (-0.1260376516464432, -1.2969524754796862, 0.3222450017691283), Duration: 1.0, Reward: 19.883323971836404, Done: False
2024-07-21 22:36:49,334 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:49,334 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:36:52,150 - AirSimEnvLogger - INFO - Predictive model loss: 0.2950431704521179
2024-07-21 22:36:58,178 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.669724659192713, Velocity: -0.058903182868859025, Movement: 0.7581086503521967, Collision: 0, Height: -1.0, Movement Penalty: -0.20152151552116956, Smoothness: -0.0, Curiosity: 43.750526428222656, Exploration: 0.23452365738295067, Total: 32.63422634522986
2024-07-21 22:36:58,209 - AirSimEnvLogger - INFO - Action: [0.46354914 0.33257677 1.40478816 1.32747023], Velocity: (0.4635491438364294, 0.33257677421602083, 1.4047881631961083), Duration: 1.0, Reward: 32.63422634522986, Done: False
2024-07-21 22:36:58,272 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:36:58,272 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:00,960 - AirSimEnvLogger - INFO - Predictive model loss: 0.47761616110801697
2024-07-21 22:37:06,727 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.279720845780318, Velocity: -0.04820151017118007, Movement: 0.6127223085349258, Collision: 0, Height: -1.0, Movement Penalty: -0.12618656456957228, Smoothness: -0.0, Curiosity: 45.4199104309082, Exploration: 0.46722075443370803, Total: 32.76551804854063
2024-07-21 22:37:06,853 - AirSimEnvLogger - INFO - Action: [-0.37348279  0.90479591 -0.73727165 -0.30098239], Velocity: (-0.37348279216673475, 0.9047959052357932, -0.7372716482609085), Duration: 1.0, Reward: 32.76551804854063, Done: False
2024-07-21 22:37:06,900 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:37:06,900 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:09,770 - AirSimEnvLogger - INFO - Predictive model loss: 0.21824687719345093
2024-07-21 22:37:15,708 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.25479062623118, Velocity: -0.044305859621530067, Movement: 0.3317864054040382, Collision: 0, Height: -1.0, Movement Penalty: -0.07791077523270508, Smoothness: -0.0, Curiosity: 43.47718048095703, Exploration: 0.5151101874795448, Total: 30.843929321024206
2024-07-21 22:37:15,801 - AirSimEnvLogger - INFO - Action: [ 0.42442322  0.25199372 -0.44350081  0.40826464], Velocity: (0.42442322075340977, 0.2519937213234551, -0.4435008109830132), Duration: 1.0, Reward: 30.843929321024206, Done: False
2024-07-21 22:37:15,864 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:37:15,864 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:18,657 - AirSimEnvLogger - INFO - Predictive model loss: 0.3108890652656555
2024-07-21 22:37:24,254 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.848629461312422, Velocity: -0.18616756352104286, Movement: 1.1526061511567494, Collision: 0, Height: -1.0, Movement Penalty: -0.23876873147116143, Smoothness: -0.0, Curiosity: 34.68486785888672, Exploration: 0.5301370780715849, Total: 22.48153366460502
2024-07-21 22:37:24,331 - AirSimEnvLogger - INFO - Action: [-1.84575868 -1.21539965 -0.65573039 -0.62213098], Velocity: (-1.8457586833025879, -1.215399647492968, -0.655730385619401), Duration: 1.0, Reward: 22.48153366460502, Done: False
2024-07-21 22:37:24,394 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:37:24,394 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:26,610 - AirSimEnvLogger - INFO - Predictive model loss: 0.04855192452669144
2024-07-21 22:37:32,773 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.589206439661078, Velocity: -0.06666067694223111, Movement: 0.5214409755996235, Collision: 0, Height: -1.0, Movement Penalty: -0.15699692260718676, Smoothness: -0.0, Curiosity: 45.47726058959961, Exploration: 0.329067550938407, Total: 33.451576894240496
2024-07-21 22:37:32,916 - AirSimEnvLogger - INFO - Action: [0.19267987 0.83802956 0.59007092 1.17354191], Velocity: (0.19267987451845148, 0.8380295600393415, 0.5900709165794991), Duration: 1.0, Reward: 33.451576894240496, Done: False
2024-07-21 22:37:32,995 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:37:32,995 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:35,789 - AirSimEnvLogger - INFO - Predictive model loss: 0.1255245804786682
2024-07-21 22:37:41,801 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.364857687854412, Velocity: -0.152643489935432, Movement: 1.0099705109439585, Collision: 0, Height: -1.0, Movement Penalty: -0.22008462137244356, Smoothness: -0.0, Curiosity: 57.2364387512207, Exploration: 0.9145219574017742, Total: 45.59801032542109
2024-07-21 22:37:41,958 - AirSimEnvLogger - INFO - Action: [1.09419129 1.12934679 1.26786552 0.87382053], Velocity: (1.0941912913357243, 1.1293467915867483, 1.2678655189740673), Duration: 1.0, Reward: 45.59801032542109, Done: False
2024-07-21 22:37:42,022 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:37:42,022 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:44,930 - AirSimEnvLogger - INFO - Predictive model loss: 0.3330986499786377
2024-07-21 22:37:51,180 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.55575191446506, Velocity: -0.18872963803494558, Movement: 1.1603119158384978, Collision: 0, Height: -1.0, Movement Penalty: -0.23206243463472842, Smoothness: -0.0, Curiosity: 37.480018615722656, Exploration: 0.4371316039777321, Total: 25.552346457269007
2024-07-21 22:37:51,306 - AirSimEnvLogger - INFO - Action: [-1.82580166e+00 -1.40028749e+00 -3.01559638e-01  1.54554603e-03], Velocity: (-1.8258016568154614, -1.400287492941248, -0.30155963793162166), Duration: 1.0, Reward: 25.552346457269007, Done: False
2024-07-21 22:37:51,370 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:37:51,370 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:37:54,259 - AirSimEnvLogger - INFO - Predictive model loss: 0.06400404125452042
2024-07-21 22:38:00,366 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.125361454255488, Velocity: -0.250185279939279, Movement: 1.282592675724977, Collision: 0, Height: -1.0, Movement Penalty: -0.26523673177915325, Smoothness: -0.0, Curiosity: 68.50751495361328, Exploration: 0.24375870025238028, Total: 55.96014408113383
2024-07-21 22:38:00,491 - AirSimEnvLogger - INFO - Action: [1.5417062  1.87358104 0.83247339 0.67444533], Velocity: (1.5417062010038072, 1.8735810431332909, 0.8324733941042222), Duration: 1.0, Reward: 55.96014408113383, Done: False
2024-07-21 22:38:00,554 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:00,554 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:03,024 - AirSimEnvLogger - INFO - Predictive model loss: 0.32865622639656067
2024-07-21 22:38:09,145 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.852816135559644, Velocity: -0.16392095073859259, Movement: 0.8238442370904697, Collision: 0, Height: -1.0, Movement Penalty: -0.20283730009792397, Smoothness: -0.0, Curiosity: 39.70263671875, Exploration: 0.21785723092411313, Total: 27.397455194372746
2024-07-21 22:38:09,223 - AirSimEnvLogger - INFO - Action: [-0.89925516 -1.30248068 -0.45799732 -1.18297072], Velocity: (-0.8992551620943374, -1.302480676537372, -0.45799732383732206), Duration: 1.0, Reward: 27.397455194372746, Done: False
2024-07-21 22:38:09,317 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:09,317 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:12,140 - AirSimEnvLogger - INFO - Predictive model loss: 0.06542535871267319
2024-07-21 22:38:18,285 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.125426925563266, Velocity: -0.01426891039847591, Movement: 0.13310612377312062, Collision: 0, Height: -1.0, Movement Penalty: -0.08179438621987596, Smoothness: -0.0, Curiosity: 44.062110900878906, Exploration: 0.5109646031383723, Total: 31.53051660721592
2024-07-21 22:38:18,349 - AirSimEnvLogger - INFO - Action: [ 0.12414662  0.06737719 -0.22564772 -0.77341011], Velocity: (0.1241466212795459, 0.06737718952011762, -0.22564771547893936), Duration: 1.0, Reward: 31.53051660721592, Done: False
2024-07-21 22:38:18,396 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:18,396 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:21,093 - AirSimEnvLogger - INFO - Predictive model loss: 0.0565502867102623
2024-07-21 22:38:26,890 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.304325874938991, Velocity: -0.10899034999462216, Movement: 0.809159173140638, Collision: 0, Height: -1.0, Movement Penalty: -0.2324517533146996, Smoothness: -0.0, Curiosity: 57.92204284667969, Exploration: 0.5961503825574767, Total: 45.240988932311794
2024-07-21 22:38:27,015 - AirSimEnvLogger - INFO - Action: [1.48181178 0.64790467 0.05837512 1.66866039], Velocity: (1.4818117791998588, 0.6479046742149039, 0.05837511512272564), Duration: 1.0, Reward: 45.240988932311794, Done: False
2024-07-21 22:38:27,093 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:27,093 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:30,016 - AirSimEnvLogger - INFO - Predictive model loss: 0.22087828814983368
2024-07-21 22:38:36,185 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.12592235018601, Velocity: -0.15983879599781334, Movement: 0.8554304091688479, Collision: 0, Height: -1.0, Movement Penalty: -0.17393859949236673, Smoothness: -0.0, Curiosity: 58.43102264404297, Exploration: 0.5718524403996101, Total: 45.953433433086545
2024-07-21 22:38:36,310 - AirSimEnvLogger - INFO - Action: [1.615415   0.34496771 0.4455069  0.31371787], Velocity: (1.6154149996628318, 0.34496771392866443, 0.44550689661810483), Duration: 1.0, Reward: 45.953433433086545, Done: False
2024-07-21 22:38:36,373 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:36,373 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:39,304 - AirSimEnvLogger - INFO - Predictive model loss: 0.3459986746311188
2024-07-21 22:38:45,322 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.60674351298656, Velocity: -0.15343246894859805, Movement: 0.7805957903921886, Collision: 0, Height: -1.0, Movement Penalty: -0.20277507712106832, Smoothness: -0.0, Curiosity: 39.06779861450195, Exploration: 0.42430916654799417, Total: 27.051198835020944
2024-07-21 22:38:45,416 - AirSimEnvLogger - INFO - Action: [-0.39338653 -1.50577879 -0.12327378 -1.29400697], Velocity: (-0.39338652566047383, -1.5057787909761668, -0.1232737846711851), Duration: 1.0, Reward: 27.051198835020944, Done: False
2024-07-21 22:38:45,494 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:45,494 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:48,531 - AirSimEnvLogger - INFO - Predictive model loss: 0.11567668616771698
2024-07-21 22:38:54,456 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.876165654516782, Velocity: -0.034318495152734205, Movement: 0.20435484272171286, Collision: 0, Height: -1.0, Movement Penalty: -0.07996902254191954, Smoothness: -0.0, Curiosity: 40.584434509277344, Exploration: 0.4396214613568669, Total: 28.29457510806745
2024-07-21 22:38:54,549 - AirSimEnvLogger - INFO - Action: [-0.31613024 -0.22913816 -0.12083452 -0.68735788], Velocity: (-0.3161302425304555, -0.22913815813873883, -0.12083451997540129), Duration: 1.0, Reward: 28.29457510806745, Done: False
2024-07-21 22:38:54,612 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:38:54,612 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:38:57,504 - AirSimEnvLogger - INFO - Predictive model loss: 0.07036834210157394
2024-07-21 22:39:03,516 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.392256142278141, Velocity: -0.0426717927573316, Movement: 0.5793601254620084, Collision: 0, Height: -1.0, Movement Penalty: -0.11735104807439883, Smoothness: -0.0, Curiosity: 42.4727668762207, Exploration: 0.330483017374716, Total: 29.6746895421905
2024-07-21 22:39:03,673 - AirSimEnvLogger - INFO - Action: [-0.83640486  0.06266922 -0.79945738  0.18572622], Velocity: (-0.8364048575048075, 0.06266921562909333, -0.7994573807628604), Duration: 1.0, Reward: 29.6746895421905, Done: False
2024-07-21 22:39:03,752 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:03,752 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:39:06,638 - AirSimEnvLogger - INFO - Predictive model loss: 0.05769581347703934
2024-07-21 22:39:12,809 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.730042044666355, Velocity: -0.05484522640268661, Movement: 0.7564436473782338, Collision: 0, Height: -1.0, Movement Penalty: -0.15272939385977038, Smoothness: -0.0, Curiosity: 40.22602462768555, Exploration: 0.3651565625319634, Total: 27.104021483464344
2024-07-21 22:39:12,855 - AirSimEnvLogger - INFO - Action: [ 0.21295196 -0.54629895 -1.39464579  0.20928165], Velocity: (0.2129519550980674, -0.546298950981569, -1.3946457928850216), Duration: 1.0, Reward: 27.104021483464344, Done: False
2024-07-21 22:39:12,903 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:12,904 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:39:15,848 - AirSimEnvLogger - INFO - Predictive model loss: 0.16581128537654877
2024-07-21 22:39:22,022 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.547556782236311, Velocity: -0.03888996515452314, Movement: 0.3646171987080046, Collision: 0, Height: -1.0, Movement Penalty: -0.11724493726899794, Smoothness: -0.0, Curiosity: 42.35509490966797, Exploration: 0.3582097348508935, Total: 30.37846130911653
2024-07-21 22:39:22,146 - AirSimEnvLogger - INFO - Action: [ 0.1700418   0.18149649  0.68551267 -0.9180712 ], Velocity: (0.1700417955831668, 0.18149649009465807, 0.6855126681636614), Duration: 1.0, Reward: 30.37846130911653, Done: False
2024-07-21 22:39:22,208 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:22,208 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:39:25,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.036331892013549805
2024-07-21 22:39:30,964 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.618860819738119, Velocity: -0.1464077074418866, Movement: 0.8576349404728868, Collision: 0, Height: -1.0, Movement Penalty: -0.17154501033990402, Smoothness: -0.0, Curiosity: 56.42798614501953, Exploration: 0.7627822628777572, Total: 44.504220861495355
2024-07-21 22:39:31,044 - AirSimEnvLogger - INFO - Action: [0.87860887 1.0864972  0.99484725 0.02486549], Velocity: (0.878608873868856, 1.0864971980573657, 0.9948472495072409), Duration: 1.0, Reward: 44.504220861495355, Done: False
2024-07-21 22:39:31,074 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:31,074 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:39:33,306 - AirSimEnvLogger - INFO - Predictive model loss: 0.13409392535686493
2024-07-21 22:39:39,314 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.088359164256541, Velocity: -0.13032710972153508, Movement: 1.2387525187597825, Collision: 0, Height: -1.0, Movement Penalty: -0.2567190087238873, Smoothness: -0.0, Curiosity: 66.22803497314453, Exploration: 0.627592575971809, Total: 54.816055238653774
2024-07-21 22:39:39,393 - AirSimEnvLogger - INFO - Action: [1.3544637  0.65240091 1.96922126 0.67263194], Velocity: (1.3544636958528011, 0.6524009060695732, 1.9692212585972133), Duration: 1.0, Reward: 54.816055238653774, Done: False
2024-07-21 22:39:39,472 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:39,472 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:39:41,954 - AirSimEnvLogger - INFO - Predictive model loss: 0.3535973131656647
2024-07-21 22:39:48,147 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.647259894476578, Velocity: -0.11851547470234787, Movement: 0.7140242243354084, Collision: 0, Height: -1.0, Movement Penalty: -0.15123247318878652, Smoothness: -0.0, Curiosity: 49.065834045410156, Exploration: 0.29546407822916243, Total: 36.99902649626862
2024-07-21 22:39:48,273 - AirSimEnvLogger - INFO - Action: [-1.39111681 -0.09963458  0.30690282 -0.49779888], Velocity: (-1.3911168115439627, -0.09963458279618709, 0.3069028157282281), Duration: 1.0, Reward: 36.99902649626862, Done: False
2024-07-21 22:39:48,352 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:48,352 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:39:51,303 - AirSimEnvLogger - INFO - Predictive model loss: 0.03735268861055374
2024-07-21 22:39:57,412 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.407931527523852, Velocity: -0.10273608678136419, Movement: 0.826151604445667, Collision: 0, Height: -1.0, Movement Penalty: -0.18293406470376522, Smoothness: -0.0, Curiosity: 41.55852127075195, Exploration: 0.9360198722305283, Total: 28.8797948974335
2024-07-21 22:39:57,553 - AirSimEnvLogger - INFO - Action: [-0.27623775 -1.23217427 -1.06561962 -0.78509955], Velocity: (-0.276237745764355, -1.2321742670113331, -1.0656196214610125), Duration: 1.0, Reward: 28.8797948974335, Done: False
2024-07-21 22:39:57,616 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:39:57,616 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:00,344 - AirSimEnvLogger - INFO - Predictive model loss: 0.08172052353620529
2024-07-21 22:40:06,197 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.130986685064384, Velocity: -0.14627312546976617, Movement: 1.2209867592395633, Collision: 0, Height: -1.0, Movement Penalty: -0.2797995538280838, Smoothness: -0.0, Curiosity: 75.36193084716797, Exploration: 0.6057627740888922, Total: 62.88668065697655
2024-07-21 22:40:06,307 - AirSimEnvLogger - INFO - Action: [1.3861333  1.39865616 1.44417107 1.36584932], Velocity: (1.3861333046200819, 1.3986561584541044, 1.444171069227861), Duration: 1.0, Reward: 62.88668065697655, Done: False
2024-07-21 22:40:06,387 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:40:06,387 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:09,321 - AirSimEnvLogger - INFO - Predictive model loss: 0.16238155961036682
2024-07-21 22:40:15,482 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.649864391385046, Velocity: -0.1009721027878636, Movement: 0.8973947624224068, Collision: 0, Height: -1.0, Movement Penalty: -0.23254799652668845, Smoothness: -0.0, Curiosity: 80.20233154296875, Exploration: 0.913907344059464, Total: 67.26192989891439
2024-07-21 22:40:15,620 - AirSimEnvLogger - INFO - Action: [0.74179933 1.50332242 0.64111225 1.47871148], Velocity: (0.7417993263313445, 1.5033224159058343, 0.6411122458525347), Duration: 1.0, Reward: 67.26192989891439, Done: False
2024-07-21 22:40:15,698 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:40:15,698 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:18,486 - AirSimEnvLogger - INFO - Predictive model loss: 0.17160601913928986
2024-07-21 22:40:24,602 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.432174987873092, Velocity: -0.13269708563329055, Movement: 1.011922818975737, Collision: 0, Height: -1.0, Movement Penalty: -0.2720197964664385, Smoothness: -0.0, Curiosity: 92.30756378173828, Exploration: 0.43079237071794535, Total: 78.46686062699037
2024-07-21 22:40:24,679 - AirSimEnvLogger - INFO - Action: [1.70043846 1.0960822  0.0553535  1.8175604 ], Velocity: (1.700438461427323, 1.096082202669891, 0.055353501643586656), Duration: 1.0, Reward: 78.46686062699037, Done: False
2024-07-21 22:40:24,741 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:40:24,741 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:27,605 - AirSimEnvLogger - INFO - Predictive model loss: 0.2399517297744751
2024-07-21 22:40:33,045 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.118922647729493, Velocity: -0.04762312682478277, Movement: 0.8762236190113266, Collision: 0, Height: -1.0, Movement Penalty: -0.1805220067649026, Smoothness: -0.0, Curiosity: 95.4741439819336, Exploration: 0.3150151440720441, Total: 81.95532280793732
2024-07-21 22:40:33,187 - AirSimEnvLogger - INFO - Action: [0.55250644 1.24897127 1.0981251  0.43329917], Velocity: (0.5525064371800517, 1.2489712659794552, 1.098125100198292), Duration: 1.0, Reward: 81.95532280793732, Done: False
2024-07-21 22:40:33,234 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:40:33,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:36,183 - AirSimEnvLogger - INFO - Predictive model loss: 0.2083873152732849
2024-07-21 22:40:42,480 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.55036024385839, Velocity: -0.11570635833488788, Movement: 0.6291349708456542, Collision: 0, Height: -1.0, Movement Penalty: -0.13136366370687758, Smoothness: -0.0, Curiosity: 96.78463745117188, Exploration: 0.1925774276246663, Total: 82.78939644389862
2024-07-21 22:40:42,619 - AirSimEnvLogger - INFO - Action: [1.04778928 0.56540439 0.4070611  0.37735655], Velocity: (1.047789279867345, 0.565404393968866, 0.40706110406131724), Duration: 1.0, Reward: 82.78939644389862, Done: False
2024-07-21 22:40:42,713 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:40:42,713 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:45,619 - AirSimEnvLogger - INFO - Predictive model loss: 0.15695376694202423
2024-07-21 22:40:51,653 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.347919767712897, Velocity: -0.07525673003122539, Movement: 0.460927191322824, Collision: 0, Height: -1.0, Movement Penalty: -0.09663219274512576, Smoothness: -0.0, Curiosity: 92.45924377441406, Exploration: 0.2504731611606117, Total: 78.67762087117762
2024-07-21 22:40:51,669 - AirSimEnvLogger - INFO - Action: [ 0.68144473 -0.03473512  0.61987261  0.28976295], Velocity: (0.6814447340690581, -0.034735120518142315, 0.6198726067629781), Duration: 1.0, Reward: 78.67762087117762, Done: False
2024-07-21 22:40:51,733 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:40:51,733 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:40:54,711 - AirSimEnvLogger - INFO - Predictive model loss: 0.14236290752887726
2024-07-21 22:41:00,576 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.17665971743918, Velocity: -0.11367370768491354, Movement: 0.8496117893585727, Collision: 0, Height: -1.0, Movement Penalty: -0.22361239392463295, Smoothness: -0.0, Curiosity: 90.0040512084961, Exploration: 0.5400586716114179, Total: 75.44737706830693
2024-07-21 22:41:00,701 - AirSimEnvLogger - INFO - Action: [-1.28595234  0.09737649 -1.10643805 -1.45357817], Velocity: (-1.2859523415006575, 0.09737648762126394, -1.1064380531759677), Duration: 1.0, Reward: 75.44737706830693, Done: False
2024-07-21 22:41:00,748 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:00,748 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:03,645 - AirSimEnvLogger - INFO - Predictive model loss: 0.10008549690246582
2024-07-21 22:41:09,751 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.442054545942362, Velocity: -0.1727560552306543, Movement: 1.0151416220753728, Collision: 0, Height: -1.0, Movement Penalty: -0.21282275412506757, Smoothness: -0.0, Curiosity: 67.51162719726562, Exploration: 0.7403635929765003, Total: 53.75828912221281
2024-07-21 22:41:09,874 - AirSimEnvLogger - INFO - Action: [-0.92571538 -1.51230449 -0.98895713 -0.63820249], Velocity: (-0.9257153786066652, -1.5123044917742088, -0.9889571342833556), Duration: 1.0, Reward: 53.75828912221281, Done: False
2024-07-21 22:41:09,937 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:09,937 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:12,709 - AirSimEnvLogger - INFO - Predictive model loss: 0.43916577100753784
2024-07-21 22:41:18,939 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.55725650976659, Velocity: -0.009408247762723743, Movement: 0.24053394341628073, Collision: 0, Height: -1.0, Movement Penalty: -0.16432476962246145, Smoothness: -0.0, Curiosity: 76.75686645507812, Exploration: 0.24178604451425578, Total: 62.70579227804059
2024-07-21 22:41:19,032 - AirSimEnvLogger - INFO - Action: [ 0.00346384 -0.08366722 -0.47372366 -1.57125322], Velocity: (0.003463840420992792, -0.08366721824839374, -0.473723664325163), Duration: 1.0, Reward: 62.70579227804059, Done: False
2024-07-21 22:41:19,094 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:19,094 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:21,973 - AirSimEnvLogger - INFO - Predictive model loss: 0.3159610331058502
2024-07-21 22:41:27,606 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.836944586819476, Velocity: -0.09244454675127953, Movement: 0.9071469761301107, Collision: 0, Height: -1.0, Movement Penalty: -0.18309820000149993, Smoothness: -0.0, Curiosity: 56.39900207519531, Exploration: 0.21856953422010458, Total: 43.138535503228795
2024-07-21 22:41:27,761 - AirSimEnvLogger - INFO - Action: [-0.5701842  -1.58452165 -0.67516195 -0.24664253], Velocity: (-0.5701842011107183, -1.5845216492291627, -0.6751619547455567), Duration: 1.0, Reward: 43.138535503228795, Done: False
2024-07-21 22:41:27,823 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:27,823 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:30,501 - AirSimEnvLogger - INFO - Predictive model loss: 0.4393574297428131
2024-07-21 22:41:36,145 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.171872809107358, Velocity: -0.24861553105637793, Movement: 1.2969637782853294, Collision: 0, Height: -1.0, Movement Penalty: -0.2899032277392652, Smoothness: -0.0, Curiosity: 104.84695434570312, Exploration: 0.6301783222993774, Total: 91.3317843127093
2024-07-21 22:41:36,270 - AirSimEnvLogger - INFO - Action: [1.11220204 1.72766495 1.58323739 1.29457637], Velocity: (1.1122020351862907, 1.7276649497809855, 1.583237386800339), Duration: 1.0, Reward: 91.3317843127093, Done: False
2024-07-21 22:41:36,334 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:36,334 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:39,219 - AirSimEnvLogger - INFO - Predictive model loss: 0.07682527601718903
2024-07-21 22:41:45,065 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.250224547283636, Velocity: -0.025294555192448588, Movement: 0.11871895132267596, Collision: 0, Height: -1.0, Movement Penalty: -0.050647341877654066, Smoothness: -0.0, Curiosity: 80.61866760253906, Exploration: 0.5822259031292656, Total: 66.99112253970226
2024-07-21 22:41:45,206 - AirSimEnvLogger - INFO - Action: [ 0.23203386  0.04941893  0.00973729 -0.44736849], Velocity: (0.2320338609204493, 0.04941892623473909, 0.009737285493330194), Duration: 1.0, Reward: 66.99112253970226, Done: False
2024-07-21 22:41:45,284 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:45,284 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:48,054 - AirSimEnvLogger - INFO - Predictive model loss: 0.02313983254134655
2024-07-21 22:41:53,906 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.751428680513168, Velocity: -0.11191596358170283, Movement: 0.6395572392030744, Collision: 0, Height: -1.0, Movement Penalty: -0.12961220925150815, Smoothness: -0.0, Curiosity: 96.7029800415039, Exploration: 0.2992169922998734, Total: 82.53391158172421
2024-07-21 22:41:54,018 - AirSimEnvLogger - INFO - Action: [1.04601775 0.70080777 0.2254976  0.20928122], Velocity: (1.0460177541993387, 0.700807774998244, 0.22549760369018657), Duration: 1.0, Reward: 82.53391158172421, Done: False
2024-07-21 22:41:54,112 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:41:54,112 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:41:56,751 - AirSimEnvLogger - INFO - Predictive model loss: 0.12933559715747833
2024-07-21 22:42:02,567 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.515320557979408, Velocity: -0.11254942069455147, Movement: 0.7194153937822293, Collision: 0, Height: -1.0, Movement Penalty: -0.16608651626424453, Smoothness: -0.0, Curiosity: 75.09989166259766, Exploration: 0.29704094395061054, Total: 61.159310476654824
2024-07-21 22:42:02,676 - AirSimEnvLogger - INFO - Action: [-1.01313373 -0.70139414 -0.74285957 -0.82960174], Velocity: (-1.013133728090136, -0.7013941427599097, -0.7428595700074365), Duration: 1.0, Reward: 61.159310476654824, Done: False
2024-07-21 22:42:02,770 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:02,770 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:05,647 - AirSimEnvLogger - INFO - Predictive model loss: 0.14516909420490265
2024-07-21 22:42:11,577 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.299529630521413, Velocity: -0.12876375166518933, Movement: 0.6852503676426732, Collision: 0, Height: -1.0, Movement Penalty: -0.15130413955368782, Smoothness: -0.0, Curiosity: 66.80689239501953, Exploration: 0.6394762795540987, Total: 53.16184891532217
2024-07-21 22:42:11,602 - AirSimEnvLogger - INFO - Action: [-0.71228576 -0.9265074  -0.71589476 -0.64110997], Velocity: (-0.7122857561563902, -0.9265073975096774, -0.7158947613680655), Duration: 1.0, Reward: 53.16184891532217, Done: False
2024-07-21 22:42:11,618 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:11,618 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:14,017 - AirSimEnvLogger - INFO - Predictive model loss: 0.08769393712282181
2024-07-21 22:42:20,103 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.970152977583663, Velocity: -0.11514701334918485, Movement: 1.023989641463176, Collision: 0, Height: -1.0, Movement Penalty: -0.23839308737137796, Smoothness: -0.0, Curiosity: 69.95621490478516, Exploration: 0.3768642584634085, Total: 55.585388011432315
2024-07-21 22:42:20,181 - AirSimEnvLogger - INFO - Action: [-0.78351873 -0.58527684 -1.79938005 -1.22020788], Velocity: (-0.7835187251329927, -0.5852768405957884, -1.7993800517188456), Duration: 1.0, Reward: 55.585388011432315, Done: False
2024-07-21 22:42:20,291 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:20,291 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:23,222 - AirSimEnvLogger - INFO - Predictive model loss: 0.2271987795829773
2024-07-21 22:42:29,091 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.78283889647332, Velocity: -0.0967638177860522, Movement: 0.4763292176310744, Collision: 0, Height: -1.0, Movement Penalty: -0.0973666800836262, Smoothness: -0.0, Curiosity: 55.83821105957031, Exploration: 0.19621177753687627, Total: 42.6088272405784
2024-07-21 22:42:29,262 - AirSimEnvLogger - INFO - Action: [-0.44004231 -0.79257377 -0.29282706 -0.20116895], Velocity: (-0.4400423051672624, -0.7925737685838719, -0.2928270569670206), Duration: 1.0, Reward: 42.6088272405784, Done: False
2024-07-21 22:42:29,326 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:29,326 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:32,166 - AirSimEnvLogger - INFO - Predictive model loss: 0.051213376224040985
2024-07-21 22:42:38,247 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.98419710597009, Velocity: -0.12360809518698312, Movement: 1.1233291919806256, Collision: 0, Height: -1.0, Movement Penalty: -0.28905370740491765, Smoothness: -0.0, Curiosity: 45.96435546875, Exploration: 0.651566138969841, Total: 33.63039699839911
2024-07-21 22:42:38,340 - AirSimEnvLogger - INFO - Action: [-1.3369331  -1.77811572 -0.31366904 -1.81871677], Velocity: (-1.3369331042905501, -1.7781157170714996, -0.31366903828513504), Duration: 1.0, Reward: 33.63039699839911, Done: False
2024-07-21 22:42:38,387 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:38,387 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:41,203 - AirSimEnvLogger - INFO - Predictive model loss: 0.14952930808067322
2024-07-21 22:42:47,180 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.319184436908625, Velocity: -0.10423842060973851, Movement: 0.7566631951782721, Collision: 0, Height: -1.0, Movement Penalty: -0.15291802255792147, Smoothness: -0.0, Curiosity: 66.93476867675781, Exploration: 0.3903418741197361, Total: 54.22441286488175
2024-07-21 22:42:47,276 - AirSimEnvLogger - INFO - Action: [0.75290833 1.06155397 0.77226226 0.21962559], Velocity: (0.7529083277098074, 1.0615539660598445, 0.7722622552976439), Duration: 1.0, Reward: 54.22441286488175, Done: False
2024-07-21 22:42:47,337 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:47,337 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:50,147 - AirSimEnvLogger - INFO - Predictive model loss: 0.08195477724075317
2024-07-21 22:42:55,678 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.61614935007275, Velocity: -0.03962636415311694, Movement: 0.6883929640721126, Collision: 0, Height: -1.0, Movement Penalty: -0.1377441216083709, Smoothness: -0.0, Curiosity: 50.116607666015625, Exploration: 0.2603830853590958, Total: 37.08388674332607
2024-07-21 22:42:55,773 - AirSimEnvLogger - INFO - Action: [-0.99180028 -0.52052231 -0.80057993 -0.04248308], Velocity: (-0.9918002774427168, -0.5205223144199025, -0.8005799284215498), Duration: 1.0, Reward: 37.08388674332607, Done: False
2024-07-21 22:42:55,818 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:42:55,818 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:42:58,716 - AirSimEnvLogger - INFO - Predictive model loss: 0.17587849497795105
2024-07-21 22:43:05,056 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.380931356317513, Velocity: -0.10078804995062896, Movement: 0.8594461817142125, Collision: 0, Height: -1.0, Movement Penalty: -0.21737545499294333, Smoothness: -0.0, Curiosity: 58.286319732666016, Exploration: 0.7214025816014618, Total: 44.5728669030653
2024-07-21 22:43:05,182 - AirSimEnvLogger - INFO - Action: [-0.82904706  0.13397292 -1.49977438 -1.33064567], Velocity: (-0.8290470585567731, 0.13397292467632171, -1.4997743787665503), Duration: 1.0, Reward: 44.5728669030653, Done: False
2024-07-21 22:43:05,213 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:43:05,213 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:43:08,093 - AirSimEnvLogger - INFO - Predictive model loss: 0.24805298447608948
2024-07-21 22:43:14,472 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.203579566761938, Velocity: -0.26604807052338214, Movement: 1.0855420602445127, Collision: 0, Height: -1.0, Movement Penalty: -0.2364703387666566, Smoothness: -0.0, Curiosity: 83.39569091796875, Exploration: 0.7019935635287183, Total: 69.86070578281698
2024-07-21 22:43:14,550 - AirSimEnvLogger - INFO - Action: [1.6968126  1.28728955 0.42109245 0.93713172], Velocity: (1.6968126046203695, 1.2872895531321618, 0.42109244761434717), Duration: 1.0, Reward: 69.86070578281698, Done: False
2024-07-21 22:43:14,643 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:43:14,643 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:43:17,482 - AirSimEnvLogger - INFO - Predictive model loss: 0.12099578976631165
2024-07-21 22:43:23,372 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.62675575668131, Velocity: -0.23792324790271052, Movement: 1.260935118420591, Collision: 0, Height: -1.0, Movement Penalty: -0.28634419070988687, Smoothness: -0.0, Curiosity: 49.881004333496094, Exploration: 0.314373498916914, Total: 36.83612097799934
2024-07-21 22:43:23,450 - AirSimEnvLogger - INFO - Action: [-1.25665216 -1.87822932 -1.11933439 -1.35627065], Velocity: (-1.256652155287646, -1.878229320352349, -1.1193343880382962), Duration: 1.0, Reward: 36.83612097799934, Done: False
2024-07-21 22:43:23,498 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:43:23,498 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:43:26,088 - AirSimEnvLogger - INFO - Predictive model loss: 0.10161568969488144
2024-07-21 22:43:32,154 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.089066439885718, Velocity: -0.12908356116450273, Movement: 0.6893551399066975, Collision: 0, Height: -1.0, Movement Penalty: -0.13816143760953423, Smoothness: -0.0, Curiosity: 53.048851013183594, Exploration: 0.5443074583528621, Total: 40.59949593338475
2024-07-21 22:43:32,264 - AirSimEnvLogger - INFO - Action: [ 1.12846427 -0.68515423  0.39745955 -0.08953351], Velocity: (1.1284642744423607, -0.6851542330225953, 0.39745955006972133), Duration: 1.0, Reward: 40.59949593338475, Done: False
2024-07-21 22:43:32,310 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:43:32,310 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:43:35,132 - AirSimEnvLogger - INFO - Predictive model loss: 0.04323931038379669
2024-07-21 22:43:41,390 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.798639263623613, Velocity: -0.051936930006165, Movement: 0.8805905711901378, Collision: 0, Height: -1.0, Movement Penalty: -0.21533198130116352, Smoothness: -0.0, Curiosity: 69.59378051757812, Exploration: 0.8889934591820107, Total: 57.510032745881794
2024-07-21 22:43:41,500 - AirSimEnvLogger - INFO - Action: [0.63260312 0.80630299 1.43228761 1.23896215], Velocity: (0.632603119395067, 0.8063029867813647, 1.4322876118695482), Duration: 1.0, Reward: 57.510032745881794, Done: False
2024-07-21 22:43:41,578 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:43:41,579 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:43:44,428 - AirSimEnvLogger - INFO - Predictive model loss: 0.19045673310756683
2024-07-21 22:43:50,573 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.488365283255373, Velocity: -0.0546792546639954, Movement: 0.3250564099366553, Collision: 0, Height: -1.0, Movement Penalty: -0.06520073449764223, Smoothness: -0.0, Curiosity: 64.94020080566406, Exploration: 0.30791506742582725, Total: 52.03009559259254
2024-07-21 22:43:50,681 - AirSimEnvLogger - INFO - Action: [0.32660915 0.52096024 0.21112453 0.04966789], Velocity: (0.326609151692163, 0.5209602413570917, 0.2111245308336147), Duration: 1.0, Reward: 52.03009559259254, Done: False
2024-07-21 22:43:50,758 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:43:50,758 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:43:53,694 - AirSimEnvLogger - INFO - Predictive model loss: 0.08855060487985611
2024-07-21 22:44:00,014 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.101807412835377, Velocity: -0.10331243791300711, Movement: 0.4864072422606294, Collision: 0, Height: -1.0, Movement Penalty: -0.10292957306449182, Smoothness: -0.0, Curiosity: 50.53868103027344, Exploration: 0.4586635926738448, Total: 38.04866722734599
2024-07-21 22:44:00,093 - AirSimEnvLogger - INFO - Action: [-0.53909843 -0.8092266   0.02988679  0.3362762 ], Velocity: (-0.539098430737752, -0.8092265957133269, 0.029886787393019087), Duration: 1.0, Reward: 38.04866722734599, Done: False
2024-07-21 22:44:00,156 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:44:00,156 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:44:03,103 - AirSimEnvLogger - INFO - Predictive model loss: 0.014300472103059292
2024-07-21 22:44:08,755 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.330484763536626, Velocity: -0.08376179640845889, Movement: 0.8860324029625212, Collision: 0, Height: -1.0, Movement Penalty: -0.23612724763022946, Smoothness: -0.0, Curiosity: 49.48765563964844, Exploration: 0.6060702158760419, Total: 36.794171758722094
2024-07-21 22:44:08,881 - AirSimEnvLogger - INFO - Action: [-1.57670956 -0.56159024 -0.58207992 -1.5605749 ], Velocity: (-1.5767095624203926, -0.5615902375943211, -0.5820799233859604), Duration: 1.0, Reward: 36.794171758722094, Done: False
2024-07-21 22:44:08,959 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:44:08,959 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:44:11,790 - AirSimEnvLogger - INFO - Predictive model loss: 0.058039385825395584
2024-07-21 22:44:18,000 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.821351907857679, Velocity: -0.11488529102263118, Movement: 0.6826816399730566, Collision: 0, Height: -1.0, Movement Penalty: -0.15161285549703854, Smoothness: -0.0, Curiosity: 41.64706802368164, Exploration: 0.2730663029443145, Total: 29.3968018382466
2024-07-21 22:44:18,108 - AirSimEnvLogger - INFO - Action: [ 0.47163221 -1.28117988  0.01892254 -0.65911221], Velocity: (0.4716322076946571, -1.2811798797602816, 0.018922541787585345), Duration: 1.0, Reward: 29.3968018382466, Done: False
2024-07-21 22:44:18,170 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:44:18,170 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:44:21,140 - AirSimEnvLogger - INFO - Predictive model loss: 0.08842262625694275
2024-07-21 22:44:28,288 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 22:44:28,378 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 22:46:13,716 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 22:46:13,737 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 22:46:13,739 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 10000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 256,
    "batch_size": 32,
    "n_epochs": 3,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 1000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 200,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.05,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 22:46:15,162 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 22:46:15,162 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 22:46:21,720 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 22:46:27,170 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 22:46:27,968 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:46:27,969 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:46:34,250 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.479495855995308, Velocity: -0.10607215937102532, Movement: 0.17225139398034808, Collision: 0, Height: -1.0, Movement Penalty: -0.039779755473136905, Smoothness: -0.0, Curiosity: 0.4613220989704132, Exploration: 0, Total: -9.524555655541318
2024-07-21 22:46:34,456 - AirSimEnvLogger - INFO - Action: [-0.19889878 -0.19889878 -0.19889878 -0.19889878], Velocity: (-0.1988987773656845, -0.1988987773656845, -0.1988987773656845), Duration: 1.0, Reward: -9.524555655541318, Done: False
2024-07-21 22:46:34,517 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:46:34,517 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:46:37,956 - AirSimEnvLogger - INFO - Predictive model loss: 0.026872960850596428
2024-07-21 22:46:44,004 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.310398846201666, Velocity: -0.04957703865994522, Movement: 0.390074013551537, Collision: 0, Height: -1.0, Movement Penalty: -0.09008373469114304, Smoothness: -0.0, Curiosity: 1.324838399887085, Exploration: 0.8911283896031176, Total: -9.275990126020215
2024-07-21 22:46:44,097 - AirSimEnvLogger - INFO - Action: [-0.45041867 -0.45041867 -0.45041867 -0.45041867], Velocity: (-0.4504186734557152, -0.4504186734557152, -0.4504186734557152), Duration: 1.0, Reward: -9.275990126020215, Done: False
2024-07-21 22:46:44,161 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:46:44,161 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:46:46,859 - AirSimEnvLogger - INFO - Predictive model loss: 0.033707890659570694
2024-07-21 22:46:52,973 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.091767599670636, Velocity: -0.005855432218059796, Movement: 0.033342758604474367, Collision: 0, Height: -1.0, Movement Penalty: -0.007700180262327195, Smoothness: -0.0, Curiosity: 0.23392826318740845, Exploration: 0.19166764169447448, Total: -10.31352342604184
2024-07-21 22:46:53,081 - AirSimEnvLogger - INFO - Action: [-0.0385009 -0.0385009 -0.0385009 -0.0385009], Velocity: (-0.03850090131163597, -0.03850090131163597, -0.03850090131163597), Duration: 1.0, Reward: -10.31352342604184, Done: False
2024-07-21 22:46:53,143 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:46:53,143 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:46:55,985 - AirSimEnvLogger - INFO - Predictive model loss: 0.012217435985803604
2024-07-21 22:47:02,194 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.466816444062555, Velocity: -0.08140555519431011, Movement: 0.4328458414201942, Collision: 0, Height: -1.0, Movement Penalty: -0.09996146522462368, Smoothness: -0.0, Curiosity: 2.124819278717041, Exploration: 0.1054509745424453, Total: -8.815266311533668
2024-07-21 22:47:02,226 - AirSimEnvLogger - INFO - Action: [-0.49980733 -0.49980733 -0.49980733 -0.49980733], Velocity: (-0.4998073261231184, -0.4998073261231184, -0.4998073261231184), Duration: 1.0, Reward: -8.815266311533668, Done: False
2024-07-21 22:47:02,320 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:02,320 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:05,038 - AirSimEnvLogger - INFO - Predictive model loss: 0.06567171961069107
2024-07-21 22:47:10,882 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.071554813349492, Velocity: -0.004575870921638326, Movement: 0.025616552229688484, Collision: 0, Height: -1.0, Movement Penalty: -0.005915889330208302, Smoothness: -0.0, Curiosity: 0.43494126200675964, Exploration: 0.061933955084310946, Total: -10.122197956118454
2024-07-21 22:47:11,007 - AirSimEnvLogger - INFO - Action: [0.02957945 0.02957945 0.02957945 0.02957945], Velocity: (0.029579446651041508, 0.029579446651041508, 0.029579446651041508), Duration: 1.0, Reward: -10.122197956118454, Done: False
2024-07-21 22:47:11,038 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:11,038 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:13,833 - AirSimEnvLogger - INFO - Predictive model loss: 0.019802020862698555
2024-07-21 22:47:19,662 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.602140530775939, Velocity: -0.05849394795991048, Movement: 0.43304795171337157, Collision: 0, Height: -1.0, Movement Penalty: -0.10000814059749247, Smoothness: -0.0, Curiosity: 1.2484418153762817, Exploration: 0.36719047176271014, Total: -8.7644716587491
2024-07-21 22:47:19,772 - AirSimEnvLogger - INFO - Action: [0.5000407 0.5000407 0.5000407 0.5000407], Velocity: (0.5000407029874623, 0.5000407029874623, 0.5000407029874623), Duration: 1.0, Reward: -8.7644716587491, Done: False
2024-07-21 22:47:19,850 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:19,850 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:22,542 - AirSimEnvLogger - INFO - Predictive model loss: 0.012923435308039188
2024-07-21 22:47:28,615 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.555548185675832, Velocity: -0.060532406401812296, Movement: 0.4028688523582584, Collision: 0, Height: -1.0, Movement Penalty: -0.09303857614286243, Smoothness: -0.0, Curiosity: 1.1605559587478638, Exploration: 0.270827313186657, Total: -8.828872834276494
2024-07-21 22:47:28,741 - AirSimEnvLogger - INFO - Action: [0.46519288 0.46519288 0.46519288 0.46519288], Velocity: (0.4651928807143122, 0.4651928807143122, 0.4651928807143122), Duration: 1.0, Reward: -8.828872834276494, Done: False
2024-07-21 22:47:28,820 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:28,820 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:31,673 - AirSimEnvLogger - INFO - Predictive model loss: 0.019612792879343033
2024-07-21 22:47:37,728 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.150610817393286, Velocity: -0.04162346108331927, Movement: 0.22242475746534943, Collision: 0, Height: -1.0, Movement Penalty: -0.05136679743882269, Smoothness: -0.0, Curiosity: 0.4975065588951111, Exploration: 0.19781002789390129, Total: -10.106314230865173
2024-07-21 22:47:37,855 - AirSimEnvLogger - INFO - Action: [-0.25683399 -0.25683399 -0.25683399 -0.25683399], Velocity: (-0.25683398719411343, -0.25683398719411343, -0.25683398719411343), Duration: 1.0, Reward: -10.106314230865173, Done: False
2024-07-21 22:47:37,950 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:37,950 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:40,949 - AirSimEnvLogger - INFO - Predictive model loss: 0.00788271427154541
2024-07-21 22:47:46,802 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.578634374959117, Velocity: -0.08808116388364101, Movement: 0.5368722443368787, Collision: 0, Height: -1.0, Movement Penalty: -0.1239853339153342, Smoothness: -0.0, Curiosity: 2.810974597930908, Exploration: 0.45015999529508705, Total: -8.159761647249207
2024-07-21 22:47:46,849 - AirSimEnvLogger - INFO - Action: [-0.61992667 -0.61992667 -0.61992667 -0.61992667], Velocity: (-0.619926669576671, -0.619926669576671, -0.619926669576671), Duration: 1.0, Reward: -8.159761647249207, Done: False
2024-07-21 22:47:46,925 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:46,925 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:49,607 - AirSimEnvLogger - INFO - Predictive model loss: 0.06334039568901062
2024-07-21 22:47:55,561 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.86524752278085, Velocity: -0.08904921076605582, Movement: 0.700803610663239, Collision: 0, Height: -1.0, Movement Penalty: -0.1618436613061931, Smoothness: -0.0, Curiosity: 5.594906806945801, Exploration: 0.38467397535521436, Total: -5.673579947740199
2024-07-21 22:47:55,624 - AirSimEnvLogger - INFO - Action: [-0.80921831 -0.80921831 -0.80921831 -0.80921831], Velocity: (-0.8092183065309655, -0.8092183065309655, -0.8092183065309655), Duration: 1.0, Reward: -5.673579947740199, Done: False
2024-07-21 22:47:55,671 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:47:55,671 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:47:58,443 - AirSimEnvLogger - INFO - Predictive model loss: 0.15542738139629364
2024-07-21 22:48:04,534 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.071917954937348, Velocity: -0.13389832242096658, Movement: 0.7812558723616447, Collision: 0, Height: -1.0, Movement Penalty: -0.18042331528558864, Smoothness: -0.0, Curiosity: 8.447127342224121, Exploration: 0.3518114957018654, Total: -3.038099636456057
2024-07-21 22:48:04,644 - AirSimEnvLogger - INFO - Action: [-0.90211658 -0.90211658 -0.90211658 -0.90211658], Velocity: (-0.9021165764279431, -0.9021165764279431, -0.9021165764279431), Duration: 1.0, Reward: -3.038099636456057, Done: False
2024-07-21 22:48:04,708 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:04,709 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:48:07,497 - AirSimEnvLogger - INFO - Predictive model loss: 0.2737588882446289
2024-07-21 22:48:13,337 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.207152748305784, Velocity: -0.11596730047457295, Movement: 0.7917498451250722, Collision: 0, Height: -1.0, Movement Penalty: -0.182846794485522, Smoothness: -0.0, Curiosity: 10.941118240356445, Exploration: 0.3226128710460432, Total: -0.6840086965814565
2024-07-21 22:48:13,400 - AirSimEnvLogger - INFO - Action: [-0.91423397 -0.91423397 -0.91423397 -0.91423397], Velocity: (-0.9142339724276098, -0.9142339724276098, -0.9142339724276098), Duration: 1.0, Reward: -0.6840086965814565, Done: False
2024-07-21 22:48:13,460 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:13,460 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:48:16,158 - AirSimEnvLogger - INFO - Predictive model loss: 0.3986892104148865
2024-07-21 22:48:22,045 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.383185875218102, Velocity: -0.13910530446124325, Movement: 0.82868803906325, Collision: 0, Height: -1.0, Movement Penalty: -0.19137730497095617, Smoothness: -0.0, Curiosity: 14.500734329223633, Exploration: 0.317950349399437, Total: 2.6970941769046557
2024-07-21 22:48:22,215 - AirSimEnvLogger - INFO - Action: [-0.95688652 -0.95688652 -0.95688652 -0.95688652], Velocity: (-0.9568865248547809, -0.9568865248547809, -0.9568865248547809), Duration: 1.0, Reward: 2.6970941769046557, Done: False
2024-07-21 22:48:22,261 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:22,261 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:48:24,990 - AirSimEnvLogger - INFO - Predictive model loss: 0.5642736554145813
2024-07-21 22:48:30,704 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.57353480462058, Velocity: -0.016549665558917046, Movement: 0.09028971375613, Collision: 0, Height: -1.0, Movement Penalty: -0.020851516216862365, Smoothness: -0.0, Curiosity: 7.19881534576416, Exploration: 0.20754204257887718, Total: -3.826424953801743
2024-07-21 22:48:30,845 - AirSimEnvLogger - INFO - Action: [-0.10425758 -0.10425758 -0.10425758 -0.10425758], Velocity: (-0.10425758108431182, -0.10425758108431182, -0.10425758108431182), Duration: 1.0, Reward: -3.826424953801743, Done: False
2024-07-21 22:48:30,909 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:30,909 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:48:33,624 - AirSimEnvLogger - INFO - Predictive model loss: 0.33014410734176636
2024-07-21 22:48:39,657 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.96209723306584, Velocity: -0.06459208919569252, Movement: 0.4759052609848039, Collision: 0, Height: -1.0, Movement Penalty: -0.10990561221533426, Smoothness: -0.0, Curiosity: 11.885964393615723, Exploration: 0.14424914472877023, Total: 0.46225918534813426
2024-07-21 22:48:39,783 - AirSimEnvLogger - INFO - Action: [-0.54952806 -0.54952806 -0.54952806 -0.54952806], Velocity: (-0.5495280610766713, -0.5495280610766713, -0.5495280610766713), Duration: 1.0, Reward: 0.46225918534813426, Done: False
2024-07-21 22:48:39,860 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:39,860 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:48:42,636 - AirSimEnvLogger - INFO - Predictive model loss: 0.49822768568992615
2024-07-21 22:48:48,735 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.248891691207373, Velocity: -0.027962572517105343, Movement: 0.1773649539586751, Collision: 0, Height: -1.0, Movement Penalty: -0.04096068156513866, Smoothness: -0.0, Curiosity: 7.273608207702637, Exploration: 0.1021618463070738, Total: -3.450231763334174
2024-07-21 22:48:48,830 - AirSimEnvLogger - INFO - Action: [0.20480341 0.20480341 0.20480341 0.20480341], Velocity: (0.20480340782569328, 0.20480340782569328, 0.20480340782569328), Duration: 1.0, Reward: -3.450231763334174, Done: False
2024-07-21 22:48:48,894 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:48,894 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:48:51,612 - AirSimEnvLogger - INFO - Predictive model loss: 0.28159573674201965
2024-07-21 22:48:57,599 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.7157957328179, Velocity: -0.045022186572669584, Movement: 0.5179475779382343, Collision: 0, Height: -1.0, Movement Penalty: -0.11961486941950171, Smoothness: -0.0, Curiosity: 6.264320373535156, Exploration: 0.4518283778248462, Total: -3.839351824838695
2024-07-21 22:48:57,724 - AirSimEnvLogger - INFO - Action: [0.59807435 0.59807435 0.59807435 0.59807435], Velocity: (0.5980743470975085, 0.5980743470975085, 0.5980743470975085), Duration: 1.0, Reward: -3.839351824838695, Done: False
2024-07-21 22:48:57,786 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:48:57,786 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:00,672 - AirSimEnvLogger - INFO - Predictive model loss: 0.16289186477661133
2024-07-21 22:49:06,559 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.988702480762074, Velocity: -0.02042839574406736, Movement: 0.1728752371923811, Collision: 0, Height: -1.0, Movement Penalty: -0.03992382589169666, Smoothness: -0.0, Curiosity: 5.324416160583496, Exploration: 0.1312424338955477, Total: -5.131902779695923
2024-07-21 22:49:06,670 - AirSimEnvLogger - INFO - Action: [0.19961913 0.19961913 0.19961913 0.19961913], Velocity: (0.19961912945848326, 0.19961912945848326, 0.19961912945848326), Duration: 1.0, Reward: -5.131902779695923, Done: False
2024-07-21 22:49:06,748 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:06,748 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:09,544 - AirSimEnvLogger - INFO - Predictive model loss: 0.15821494162082672
2024-07-21 22:49:15,348 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.625338090631764, Velocity: -0.07055073129697258, Movement: 0.5063044749809202, Collision: 0, Height: -1.0, Movement Penalty: -0.11692600996885859, Smoothness: -0.0, Curiosity: 4.737442970275879, Exploration: 0.11559659759555413, Total: -5.355943354525705
2024-07-21 22:49:15,473 - AirSimEnvLogger - INFO - Action: [0.58463005 0.58463005 0.58463005 0.58463005], Velocity: (0.5846300498442929, 0.5846300498442929, 0.5846300498442929), Duration: 1.0, Reward: -5.355943354525705, Done: False
2024-07-21 22:49:15,552 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:15,552 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:18,098 - AirSimEnvLogger - INFO - Predictive model loss: 0.09927575290203094
2024-07-21 22:49:24,069 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.999744740740885, Velocity: -0.011149037151088012, Movement: 0.09518854303263993, Collision: 0, Height: -1.0, Movement Penalty: -0.02198285237746518, Smoothness: -0.0, Curiosity: 3.949666738510132, Exploration: 0.09016077537324632, Total: -6.52812095777418
2024-07-21 22:49:24,195 - AirSimEnvLogger - INFO - Action: [0.10991426 0.10991426 0.10991426 0.10991426], Velocity: (0.10991426188732589, 0.10991426188732589, 0.10991426188732589), Duration: 1.0, Reward: -6.52812095777418, Done: False
2024-07-21 22:49:24,258 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:24,258 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:26,978 - AirSimEnvLogger - INFO - Predictive model loss: 0.0999206006526947
2024-07-21 22:49:32,953 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.615629334055749, Velocity: -0.08403345012083345, Movement: 0.4774743886777494, Collision: 0, Height: -1.0, Movement Penalty: -0.11026798673370025, Smoothness: -0.0, Curiosity: 3.5622670650482178, Exploration: 0.09448154374004533, Total: -6.52832243791137
2024-07-21 22:49:33,061 - AirSimEnvLogger - INFO - Action: [0.55133993 0.55133993 0.55133993 0.55133993], Velocity: (0.5513399336685012, 0.5513399336685012, 0.5513399336685012), Duration: 1.0, Reward: -6.52832243791137, Done: False
2024-07-21 22:49:33,124 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:33,124 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:35,807 - AirSimEnvLogger - INFO - Predictive model loss: 0.08856379240751266
2024-07-21 22:49:41,750 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.235649422084157, Velocity: -0.022300034305558998, Movement: 0.14781025787447757, Collision: 0, Height: -1.0, Movement Penalty: -0.034135316869127054, Smoothness: -0.0, Curiosity: 3.8872885704040527, Exploration: 0.0756099765364263, Total: -6.829574782839418
2024-07-21 22:49:41,906 - AirSimEnvLogger - INFO - Action: [-0.17067658 -0.17067658 -0.17067658 -0.17067658], Velocity: (-0.17067658434563526, -0.17067658434563526, -0.17067658434563526), Duration: 1.0, Reward: -6.829574782839418, Done: False
2024-07-21 22:49:41,970 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:41,970 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:44,685 - AirSimEnvLogger - INFO - Predictive model loss: 0.08614703267812729
2024-07-21 22:49:50,753 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.77197148038088, Velocity: -0.09139439166972735, Movement: 0.5052022946354868, Collision: 0, Height: -1.0, Movement Penalty: -0.11667147232120599, Smoothness: -0.0, Curiosity: 7.857875347137451, Exploration: 0.440682520402901, Total: -3.309486007629369
2024-07-21 22:49:50,832 - AirSimEnvLogger - INFO - Action: [-0.58335736 -0.58335736 -0.58335736 -0.58335736], Velocity: (-0.5833573616060299, -0.5833573616060299, -0.5833573616060299), Duration: 1.0, Reward: -3.309486007629369, Done: False
2024-07-21 22:49:50,895 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:50,895 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:49:53,646 - AirSimEnvLogger - INFO - Predictive model loss: 0.15218959748744965
2024-07-21 22:49:59,501 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.082149941389348, Velocity: -0.07783666472747056, Movement: 0.6318132621303739, Collision: 0, Height: -1.0, Movement Penalty: -0.1459110227874188, Smoothness: -0.0, Curiosity: 11.561485290527344, Exploration: 0.3690461534298223, Total: 0.07193064325865092
2024-07-21 22:49:59,626 - AirSimEnvLogger - INFO - Action: [-0.72955511 -0.72955511 -0.72955511 -0.72955511], Velocity: (-0.7295551139370939, -0.7295551139370939, -0.7295551139370939), Duration: 1.0, Reward: 0.07193064325865092, Done: False
2024-07-21 22:49:59,690 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:49:59,690 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:02,571 - AirSimEnvLogger - INFO - Predictive model loss: 0.23715363442897797
2024-07-21 22:50:08,488 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.77183127160284, Velocity: -0.05250644459319585, Movement: 0.27993801988873857, Collision: 0, Height: -1.0, Movement Penalty: -0.06464891645566961, Smoothness: -0.0, Curiosity: 9.000575065612793, Exploration: 0.12587665231043169, Total: -2.2406883558613795
2024-07-21 22:50:08,645 - AirSimEnvLogger - INFO - Action: [-0.32324458 -0.32324458 -0.32324458 -0.32324458], Velocity: (-0.32324458227834807, -0.32324458227834807, -0.32324458227834807), Duration: 1.0, Reward: -2.2406883558613795, Done: False
2024-07-21 22:50:08,707 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:08,707 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:11,443 - AirSimEnvLogger - INFO - Predictive model loss: 0.17544789612293243
2024-07-21 22:50:16,654 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.125979392676072, Velocity: -0.041490710294663787, Movement: 0.5588449337645752, Collision: 0, Height: -1.0, Movement Penalty: -0.12905970917769444, Smoothness: -0.0, Curiosity: 13.928112030029297, Exploration: 0.14101997089455082, Total: 2.344126595767698
2024-07-21 22:50:16,795 - AirSimEnvLogger - INFO - Action: [-0.64529855 -0.64529855 -0.64529855 -0.64529855], Velocity: (-0.6452985458884721, -0.6452985458884721, -0.6452985458884721), Duration: 1.0, Reward: 2.344126595767698, Done: False
2024-07-21 22:50:16,857 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:16,857 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:19,707 - AirSimEnvLogger - INFO - Predictive model loss: 0.2739231586456299
2024-07-21 22:50:25,504 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.531402681185613, Velocity: -0.003312921548013833, Movement: 0.0038956681331862694, Collision: 0, Height: -1.0, Movement Penalty: -0.0008996660181474159, Smoothness: -0.0, Curiosity: 9.283988952636719, Exploration: 0.09903957344391513, Total: -1.7248703582820215
2024-07-21 22:50:25,662 - AirSimEnvLogger - INFO - Action: [0.00449833 0.00449833 0.00449833 0.00449833], Velocity: (0.004498330090737079, 0.004498330090737079, 0.004498330090737079), Duration: 1.0, Reward: -1.7248703582820215, Done: False
2024-07-21 22:50:25,788 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:25,788 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:28,532 - AirSimEnvLogger - INFO - Predictive model loss: 0.14541128277778625
2024-07-21 22:50:34,313 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.012562788827944, Velocity: -0.07655329184728894, Movement: 0.43071821951406397, Collision: 0, Height: -1.0, Movement Penalty: -0.09947011199252848, Smoothness: -0.0, Curiosity: 14.217287063598633, Exploration: 0.07277670960541152, Total: 2.724373083530909
2024-07-21 22:50:34,314 - AirSimEnvLogger - INFO - Action: [-0.49735056 -0.49735056 -0.49735056 -0.49735056], Velocity: (-0.4973505599626423, -0.4973505599626423, -0.4973505599626423), Duration: 1.0, Reward: 2.724373083530909, Done: False
2024-07-21 22:50:34,361 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:34,361 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:37,042 - AirSimEnvLogger - INFO - Predictive model loss: 0.21653024852275848
2024-07-21 22:50:42,720 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.382125934723886, Velocity: -0.07066199546119281, Movement: 0.5972231319008148, Collision: 0, Height: -1.0, Movement Penalty: -0.13792277438768272, Smoothness: -0.0, Curiosity: 19.16160011291504, Exploration: 0.36010269439546744, Total: 7.369881449628263
2024-07-21 22:50:42,829 - AirSimEnvLogger - INFO - Action: [-0.68961387 -0.68961387 -0.68961387 -0.68961387], Velocity: (-0.6896138719384135, -0.6896138719384135, -0.6896138719384135), Duration: 1.0, Reward: 7.369881449628263, Done: False
2024-07-21 22:50:42,939 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:42,939 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:45,353 - AirSimEnvLogger - INFO - Predictive model loss: 0.28217780590057373
2024-07-21 22:50:51,315 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.051531033656133, Velocity: -0.027554817386649062, Movement: 0.23299109993368913, Collision: 0, Height: -1.0, Movement Penalty: -0.05380698970620097, Smoothness: -0.0, Curiosity: 16.22771644592285, Exploration: 0.11365201654068828, Total: 4.705285153470029
2024-07-21 22:50:51,440 - AirSimEnvLogger - INFO - Action: [-0.26903495 -0.26903495 -0.26903495 -0.26903495], Velocity: (-0.26903494853100485, -0.26903494853100485, -0.26903494853100485), Duration: 1.0, Reward: 4.705285153470029, Done: False
2024-07-21 22:50:51,535 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:51,535 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:50:54,384 - AirSimEnvLogger - INFO - Predictive model loss: 0.1866310089826584
2024-07-21 22:50:59,599 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.480988144561918, Velocity: -0.06012720368311508, Movement: 0.5446631506537915, Collision: 0, Height: -1.0, Movement Penalty: -0.12578456665905446, Smoothness: -0.0, Curiosity: 22.52495765686035, Exploration: 0.13231606645284585, Total: 10.581750044976282
2024-07-21 22:50:59,692 - AirSimEnvLogger - INFO - Action: [-0.62892283 -0.62892283 -0.62892283 -0.62892283], Velocity: (-0.6289228332952723, -0.6289228332952723, -0.6289228332952723), Duration: 1.0, Reward: 10.581750044976282, Done: False
2024-07-21 22:50:59,754 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:50:59,754 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:02,512 - AirSimEnvLogger - INFO - Predictive model loss: 0.23597514629364014
2024-07-21 22:51:08,207 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.886717993841161, Velocity: -0.12952659184220014, Movement: 0.702620825764692, Collision: 0, Height: -1.0, Movement Penalty: -0.16226332915739283, Smoothness: -0.0, Curiosity: 29.236284255981445, Exploration: 0.36338167788486725, Total: 16.937426639897943
2024-07-21 22:51:08,302 - AirSimEnvLogger - INFO - Action: [-0.81131665 -0.81131665 -0.81131665 -0.81131665], Velocity: (-0.8113166457869642, -0.8113166457869642, -0.8113166457869642), Duration: 1.0, Reward: 16.937426639897943, Done: False
2024-07-21 22:51:08,349 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:51:08,349 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:10,990 - AirSimEnvLogger - INFO - Predictive model loss: 0.2768685817718506
2024-07-21 22:51:16,825 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.192372311121156, Velocity: -0.1046170762380602, Movement: 0.7575383319576058, Collision: 0, Height: -1.0, Movement Penalty: -0.17494598395087355, Smoothness: -0.0, Curiosity: 35.30205154418945, Exploration: 0.3335054033435416, Total: 22.694506142712132
2024-07-21 22:51:16,886 - AirSimEnvLogger - INFO - Action: [-0.87472992 -0.87472992 -0.87472992 -0.87472992], Velocity: (-0.8747299197543676, -0.8747299197543676, -0.8747299197543676), Duration: 1.0, Reward: 22.694506142712132, Done: False
2024-07-21 22:51:16,949 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:51:16,949 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:19,712 - AirSimEnvLogger - INFO - Predictive model loss: 0.2853711247444153
2024-07-21 22:51:25,606 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.514404579034114, Velocity: -0.10750856476966082, Movement: 0.8115430777453001, Collision: 0, Height: -1.0, Movement Penalty: -0.1874178457580906, Smoothness: -0.0, Curiosity: 42.4284782409668, Exploration: 0.3245334372175153, Total: 29.497872604021044
2024-07-21 22:51:25,764 - AirSimEnvLogger - INFO - Action: [-0.93708923 -0.93708923 -0.93708923 -0.93708923], Velocity: (-0.9370892287904529, -0.9370892287904529, -0.9370892287904529), Duration: 1.0, Reward: 29.497872604021044, Done: False
2024-07-21 22:51:25,828 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:51:25,828 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:28,684 - AirSimEnvLogger - INFO - Predictive model loss: 0.2892318367958069
2024-07-21 22:51:34,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.768300093067515, Velocity: -0.14592794071782017, Movement: 0.7995014842233727, Collision: 0, Height: -1.0, Movement Penalty: -0.18463695885354786, Smoothness: -0.0, Curiosity: 48.8041877746582, Exploration: 0.31877031020556756, Total: 35.61422378723068
2024-07-21 22:51:34,736 - AirSimEnvLogger - INFO - Action: [-0.92318479 -0.92318479 -0.92318479 -0.92318479], Velocity: (-0.9231847942677393, -0.9231847942677393, -0.9231847942677393), Duration: 1.0, Reward: 35.61422378723068, Done: False
2024-07-21 22:51:34,830 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:51:34,830 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:37,763 - AirSimEnvLogger - INFO - Predictive model loss: 0.26718825101852417
2024-07-21 22:51:43,784 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.08754357085922, Velocity: -0.13705658081396238, Movement: 0.8314087255744821, Collision: 0, Height: -1.0, Movement Penalty: -0.19200562060681237, Smoothness: -0.0, Curiosity: 56.822837829589844, Exploration: 0.31705769929930727, Total: 43.314906282765094
2024-07-21 22:51:43,815 - AirSimEnvLogger - INFO - Action: [-0.9600281 -0.9600281 -0.9600281 -0.9600281], Velocity: (-0.9600281030340618, -0.9600281030340618, -0.9600281030340618), Duration: 1.0, Reward: 43.314906282765094, Done: False
2024-07-21 22:51:43,909 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:51:43,909 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:46,610 - AirSimEnvLogger - INFO - Predictive model loss: 0.2587071657180786
2024-07-21 22:51:52,514 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.743744534155208, Velocity: -0.07633500314297019, Movement: 0.42373574069081493, Collision: 0, Height: -1.0, Movement Penalty: -0.09785757758124299, Smoothness: -0.0, Curiosity: 52.945072174072266, Exploration: 0.18970506353624422, Total: 39.74772051912219
2024-07-21 22:51:52,608 - AirSimEnvLogger - INFO - Action: [-0.48928789 -0.48928789 -0.48928789 -0.48928789], Velocity: (-0.4892878879062149, -0.4892878879062149, -0.4892878879062149), Duration: 1.0, Reward: 39.74772051912219, Done: False
2024-07-21 22:51:52,671 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:51:52,671 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:51:55,371 - AirSimEnvLogger - INFO - Predictive model loss: 0.2760690152645111
2024-07-21 22:52:00,895 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.431594520692084, Velocity: -0.017871882431319493, Movement: 0.16566238429268565, Collision: 0, Height: -1.0, Movement Penalty: -0.03825808886639092, Smoothness: -0.0, Curiosity: 50.6282844543457, Exploration: 0.22082106977676028, Total: 37.749755280826925
2024-07-21 22:52:01,020 - AirSimEnvLogger - INFO - Action: [-0.19129044 -0.19129044 -0.19129044 -0.19129044], Velocity: (-0.19129044433195458, -0.19129044433195458, -0.19129044433195458), Duration: 1.0, Reward: 37.749755280826925, Done: False
2024-07-21 22:52:01,115 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:01,115 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:03,718 - AirSimEnvLogger - INFO - Predictive model loss: 0.30335289239883423
2024-07-21 22:52:09,432 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.367222712799961, Velocity: -0.013000528077179952, Movement: 0.11791500938162087, Collision: 0, Height: -1.0, Movement Penalty: -0.027231304963190425, Smoothness: -0.0, Curiosity: 51.045753479003906, Exploration: 0.12969949681520385, Total: 38.209954046495554
2024-07-21 22:52:09,587 - AirSimEnvLogger - INFO - Action: [-0.13615652 -0.13615652 -0.13615652 -0.13615652], Velocity: (-0.1361565248159521, -0.1361565248159521, -0.1361565248159521), Duration: 1.0, Reward: 38.209954046495554, Done: False
2024-07-21 22:52:09,618 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:09,618 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:12,386 - AirSimEnvLogger - INFO - Predictive model loss: 0.22235462069511414
2024-07-21 22:52:18,047 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.970376969482707, Velocity: -0.05740723412130556, Movement: 0.49164301868595034, Collision: 0, Height: -1.0, Movement Penalty: -0.11354009167341346, Smoothness: -0.0, Curiosity: 60.8853759765625, Exploration: 0.19564786604359943, Total: 47.46631726963702
2024-07-21 22:52:18,156 - AirSimEnvLogger - INFO - Action: [-0.56770046 -0.56770046 -0.56770046 -0.56770046], Velocity: (-0.5677004583670673, -0.5677004583670673, -0.5677004583670673), Duration: 1.0, Reward: 47.46631726963702, Done: False
2024-07-21 22:52:18,217 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:18,218 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:20,952 - AirSimEnvLogger - INFO - Predictive model loss: 0.1272580325603485
2024-07-21 22:52:26,808 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.4192564179836, Velocity: -0.11101132274069737, Movement: 0.6303520986526928, Collision: 0, Height: -1.0, Movement Penalty: -0.1455735815365511, Smoothness: -0.0, Curiosity: 69.80403137207031, Exploration: 0.358297766672292, Total: 55.97154481119037
2024-07-21 22:52:26,903 - AirSimEnvLogger - INFO - Action: [-0.72786791 -0.72786791 -0.72786791 -0.72786791], Velocity: (-0.7278679076827554, -0.7278679076827554, -0.7278679076827554), Duration: 1.0, Reward: 55.97154481119037, Done: False
2024-07-21 22:52:26,996 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:26,996 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:29,682 - AirSimEnvLogger - INFO - Predictive model loss: 0.18338356912136078
2024-07-21 22:52:35,687 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.799852463899919, Velocity: -0.13606423764912876, Movement: 0.6976079077818468, Collision: 0, Height: -1.0, Movement Penalty: -0.16110564533866434, Smoothness: -0.0, Curiosity: 78.68810272216797, Exploration: 0.3090653444854713, Total: 64.46284114815492
2024-07-21 22:52:35,781 - AirSimEnvLogger - INFO - Action: [-0.80552823 -0.80552823 -0.80552823 -0.80552823], Velocity: (-0.8055282266933217, -0.8055282266933217, -0.8055282266933217), Duration: 1.0, Reward: 64.46284114815492, Done: False
2024-07-21 22:52:35,860 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:35,860 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:38,217 - AirSimEnvLogger - INFO - Predictive model loss: 0.2437385618686676
2024-07-21 22:52:44,099 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.200301674179828, Velocity: -0.10853497313082403, Movement: 0.7662115713717075, Collision: 0, Height: -1.0, Movement Penalty: -0.1769489827950646, Smoothness: -0.0, Curiosity: 88.8747787475586, Exploration: 0.31093340694416977, Total: 74.25393338825735
2024-07-21 22:52:44,208 - AirSimEnvLogger - INFO - Action: [-0.88474491 -0.88474491 -0.88474491 -0.88474491], Velocity: (-0.884744913975323, -0.884744913975323, -0.884744913975323), Duration: 1.0, Reward: 74.25393338825735, Done: False
2024-07-21 22:52:44,254 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:44,254 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:47,086 - AirSimEnvLogger - INFO - Predictive model loss: 0.31153973937034607
2024-07-21 22:52:53,264 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.601428865994261, Velocity: -0.08747631121625665, Movement: 0.8107892246814451, Collision: 0, Height: -1.0, Movement Penalty: -0.18724375085035214, Smoothness: -0.0, Curiosity: 99.77857971191406, Exploration: 0.3267003067507565, Total: 84.76343290138108
2024-07-21 22:52:53,343 - AirSimEnvLogger - INFO - Action: [-0.93621875 -0.93621875 -0.93621875 -0.93621875], Velocity: (-0.9362187542517606, -0.9362187542517606, -0.9362187542517606), Duration: 1.0, Reward: 84.76343290138108, Done: False
2024-07-21 22:52:53,421 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:52:53,421 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:52:56,147 - AirSimEnvLogger - INFO - Predictive model loss: 0.2942356467247009
2024-07-21 22:53:01,373 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.666164756117242, Velocity: -0.008286643276199832, Movement: 0.023261654495467302, Collision: 0, Height: -1.0, Movement Penalty: -0.005372048993901646, Smoothness: -0.0, Curiosity: 84.79300689697266, Exploration: 0.21496490543156055, Total: 70.67602601190947
2024-07-21 22:53:01,374 - AirSimEnvLogger - INFO - Action: [-0.02686024 -0.02686024 -0.02686024 -0.02686024], Velocity: (-0.02686024496950823, -0.02686024496950823, -0.02686024496950823), Duration: 1.0, Reward: 70.67602601190947, Done: False
2024-07-21 22:53:01,478 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:01,478 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:04,451 - AirSimEnvLogger - INFO - Predictive model loss: 0.05619818717241287
2024-07-21 22:53:09,940 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.120804998699981, Velocity: -0.057775782607429, Movement: 0.36123941886289374, Collision: 0, Height: -1.0, Movement Penalty: -0.08342467028895828, Smoothness: -0.0, Curiosity: 94.2914810180664, Exploration: 0.1722394006736326, Total: 79.71337468675695
2024-07-21 22:53:10,051 - AirSimEnvLogger - INFO - Action: [-0.41712335 -0.41712335 -0.41712335 -0.41712335], Velocity: (-0.41712335144479135, -0.41712335144479135, -0.41712335144479135), Duration: 1.0, Reward: 79.71337468675695, Done: False
2024-07-21 22:53:10,112 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:10,112 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:12,584 - AirSimEnvLogger - INFO - Predictive model loss: 0.04328841343522072
2024-07-21 22:53:18,554 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.23063421854288, Velocity: -0.05561019851958726, Movement: 0.3200845764919006, Collision: 0, Height: -1.0, Movement Penalty: -0.07392036656041848, Smoothness: -0.0, Curiosity: 97.3816909790039, Exploration: 0.1979269139210141, Total: 82.69887058823956
2024-07-21 22:53:18,664 - AirSimEnvLogger - INFO - Action: [-0.36960183 -0.36960183 -0.36960183 -0.36960183], Velocity: (-0.36960183280209236, -0.36960183280209236, -0.36960183280209236), Duration: 1.0, Reward: 82.69887058823956, Done: False
2024-07-21 22:53:18,773 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:18,773 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:21,726 - AirSimEnvLogger - INFO - Predictive model loss: 0.07107064872980118
2024-07-21 22:53:27,313 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.891238914917556, Velocity: -0.004036667201560602, Movement: 0.0228784087215363, Collision: 0, Height: -1.0, Movement Penalty: -0.0052835421736037065, Smoothness: -0.0, Curiosity: 92.94293975830078, Exploration: 0.08982560268795868, Total: 78.5725182714155
2024-07-21 22:53:27,422 - AirSimEnvLogger - INFO - Action: [-0.02641771 -0.02641771 -0.02641771 -0.02641771], Velocity: (-0.02641771086801853, -0.02641771086801853, -0.02641771086801853), Duration: 1.0, Reward: 78.5725182714155, Done: False
2024-07-21 22:53:27,484 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:27,484 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:30,419 - AirSimEnvLogger - INFO - Predictive model loss: 0.20517970621585846
2024-07-21 22:53:36,383 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.52798703295012, Velocity: -0.07794669677354937, Movement: 0.4388756199783563, Collision: 0, Height: -1.0, Movement Penalty: -0.10135398293410716, Smoothness: -0.0, Curiosity: 105.2581558227539, Exploration: 0.11195064111359987, Total: 90.25888836291247
2024-07-21 22:53:36,477 - AirSimEnvLogger - INFO - Action: [-0.50676991 -0.50676991 -0.50676991 -0.50676991], Velocity: (-0.5067699146705358, -0.5067699146705358, -0.5067699146705358), Duration: 1.0, Reward: 90.25888836291247, Done: False
2024-07-21 22:53:36,540 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:36,540 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:39,254 - AirSimEnvLogger - INFO - Predictive model loss: 0.05524163320660591
2024-07-21 22:53:45,189 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.107312687160956, Velocity: -0.038301942844936133, Movement: 0.6498048762753372, Collision: 0, Height: -1.0, Movement Penalty: -0.15006600809531898, Smoothness: -0.0, Curiosity: 118.52405548095703, Exploration: 0.3844765683033376, Total: 103.01728188885224
2024-07-21 22:53:45,283 - AirSimEnvLogger - INFO - Action: [-0.75033004 -0.75033004 -0.75033004 -0.75033004], Velocity: (-0.7503300404765948, -0.7503300404765948, -0.7503300404765948), Duration: 1.0, Reward: 103.01728188885224, Done: False
2024-07-21 22:53:45,362 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:45,362 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:48,071 - AirSimEnvLogger - INFO - Predictive model loss: 0.02533353492617607
2024-07-21 22:53:53,825 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.563181223287565, Velocity: -0.09045712708364219, Movement: 0.7281800654628968, Collision: 0, Height: -1.0, Movement Penalty: -0.16816598272540914, Smoothness: -0.0, Curiosity: 130.6830291748047, Exploration: 0.3437658974635834, Total: 114.7077306130275
2024-07-21 22:53:53,904 - AirSimEnvLogger - INFO - Action: [-0.84082991 -0.84082991 -0.84082991 -0.84082991], Velocity: (-0.8408299136270456, -0.8408299136270456, -0.8408299136270456), Duration: 1.0, Reward: 114.7077306130275, Done: False
2024-07-21 22:53:53,965 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:53:53,965 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:53:56,787 - AirSimEnvLogger - INFO - Predictive model loss: 0.10060285031795502
2024-07-21 22:54:02,826 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.022690588989867, Velocity: -0.14974200306714638, Movement: 0.7955017154841527, Collision: 0, Height: -1.0, Movement Penalty: -0.18371325183023388, Smoothness: -0.0, Curiosity: 144.10211181640625, Exploration: 0.3281832959851515, Total: 127.65944279943892
2024-07-21 22:54:02,951 - AirSimEnvLogger - INFO - Action: [-0.91856626 -0.91856626 -0.91856626 -0.91856626], Velocity: (-0.9185662591511693, -0.9185662591511693, -0.9185662591511693), Duration: 1.0, Reward: 127.65944279943892, Done: False
2024-07-21 22:54:03,045 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:03,045 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:06,014 - AirSimEnvLogger - INFO - Predictive model loss: 0.16439762711524963
2024-07-21 22:54:12,044 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.72920290699165, Velocity: -0.05464407328863599, Movement: 0.3839871521402493, Collision: 0, Height: -1.0, Movement Penalty: -0.0886780342614123, Smoothness: -0.0, Curiosity: 139.77012634277344, Exploration: 0.17434045819983154, Total: 123.58497651800782
2024-07-21 22:54:12,154 - AirSimEnvLogger - INFO - Action: [-0.44339017 -0.44339017 -0.44339017 -0.44339017], Velocity: (-0.44339017130706143, -0.44339017130706143, -0.44339017130706143), Duration: 1.0, Reward: 123.58497651800782, Done: False
2024-07-21 22:54:12,232 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:12,232 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:15,040 - AirSimEnvLogger - INFO - Predictive model loss: 0.06841134279966354
2024-07-21 22:54:20,835 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.868835812613547, Velocity: -0.026903589209255115, Movement: 0.22150861496730764, Collision: 0, Height: -1.0, Movement Penalty: -0.051155223391678495, Smoothness: -0.0, Curiosity: 125.96603393554688, Exploration: 0.3696354983573035, Total: 110.68495752303416
2024-07-21 22:54:20,945 - AirSimEnvLogger - INFO - Action: [0.25577612 0.25577612 0.25577612 0.25577612], Velocity: (0.2557761169583925, 0.2557761169583925, 0.2557761169583925), Duration: 1.0, Reward: 110.68495752303416, Done: False
2024-07-21 22:54:21,006 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:21,006 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:23,757 - AirSimEnvLogger - INFO - Predictive model loss: 0.08214552700519562
2024-07-21 22:54:29,574 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.199175487813633, Velocity: -0.07239928827235753, Movement: 0.5432649616536999, Collision: 0, Height: -1.0, Movement Penalty: -0.12546166874082215, Smoothness: -0.0, Curiosity: 116.01326751708984, Exploration: 0.4637595084274025, Total: 101.42684304764839
2024-07-21 22:54:29,733 - AirSimEnvLogger - INFO - Action: [0.62730834 0.62730834 0.62730834 0.62730834], Velocity: (0.6273083437041107, 0.6273083437041107, 0.6273083437041107), Duration: 1.0, Reward: 101.42684304764839, Done: False
2024-07-21 22:54:29,826 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:29,826 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:32,639 - AirSimEnvLogger - INFO - Predictive model loss: 0.32492929697036743
2024-07-21 22:54:38,873 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.998745049783162, Velocity: -0.025381774868225845, Movement: 0.14518492344651815, Collision: 0, Height: -1.0, Movement Penalty: -0.03352902185364899, Smoothness: -0.0, Curiosity: 126.08460998535156, Exploration: 0.058755608584704855, Total: 110.60040192641175
2024-07-21 22:54:39,032 - AirSimEnvLogger - INFO - Action: [-0.16764511 -0.16764511 -0.16764511 -0.16764511], Velocity: (-0.16764510926824494, -0.16764510926824494, -0.16764510926824494), Duration: 1.0, Reward: 110.60040192641175, Done: False
2024-07-21 22:54:39,110 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:39,110 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:41,858 - AirSimEnvLogger - INFO - Predictive model loss: 0.06395266950130463
2024-07-21 22:54:48,082 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.710653245427661, Velocity: -0.08072786232525239, Movement: 0.5053582692853165, Collision: 0, Height: -1.0, Movement Penalty: -0.11670749312363236, Smoothness: -0.0, Curiosity: 140.5575408935547, Exploration: 0.4594502281478165, Total: 124.45688482550663
2024-07-21 22:54:48,145 - AirSimEnvLogger - INFO - Action: [-0.58353747 -0.58353747 -0.58353747 -0.58353747], Velocity: (-0.5835374656181618, -0.5835374656181618, -0.5835374656181618), Duration: 1.0, Reward: 124.45688482550663, Done: False
2024-07-21 22:54:48,239 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:48,239 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:50,981 - AirSimEnvLogger - INFO - Predictive model loss: 0.03466442972421646
2024-07-21 22:54:56,012 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.948515122946427, Velocity: -0.01633455810529577, Movement: 0.1365700889877622, Collision: 0, Height: -1.0, Movement Penalty: -0.03153951105613426, Smoothness: -0.0, Curiosity: 126.6100845336914, Exploration: 0.03587881583124696, Total: 111.17153813950586
2024-07-21 22:54:56,015 - AirSimEnvLogger - INFO - Action: [0.15769756 0.15769756 0.15769756 0.15769756], Velocity: (0.1576975552806713, 0.1576975552806713, 0.1576975552806713), Duration: 1.0, Reward: 111.17153813950586, Done: False
2024-07-21 22:54:56,091 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:54:56,091 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:54:58,955 - AirSimEnvLogger - INFO - Predictive model loss: 0.023740999400615692
2024-07-21 22:55:04,336 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.360885499817233, Velocity: -0.07126217371444055, Movement: 0.435045126613512, Collision: 0, Height: -1.0, Movement Penalty: -0.10046936838397841, Smoothness: -0.0, Curiosity: 117.75304412841797, Exploration: 0.4108470394171851, Total: 102.99019886382915
2024-07-21 22:55:04,382 - AirSimEnvLogger - INFO - Action: [0.50234684 0.50234684 0.50234684 0.50234684], Velocity: (0.502346841919892, 0.502346841919892, 0.502346841919892), Duration: 1.0, Reward: 102.99019886382915, Done: False
2024-07-21 22:55:04,445 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:04,445 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:55:07,006 - AirSimEnvLogger - INFO - Predictive model loss: 0.058424849063158035
2024-07-21 22:55:13,247 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.809609119891187, Velocity: -0.09883431731422743, Movement: 0.6492006782328098, Collision: 0, Height: -1.0, Movement Penalty: -0.14992647453432015, Smoothness: -0.0, Curiosity: 108.71002197265625, Exploration: 0.3470787472669197, Total: 94.48628239059046
2024-07-21 22:55:13,387 - AirSimEnvLogger - INFO - Action: [0.74963237 0.74963237 0.74963237 0.74963237], Velocity: (0.7496323726716007, 0.7496323726716007, 0.7496323726716007), Duration: 1.0, Reward: 94.48628239059046, Done: False
2024-07-21 22:55:13,466 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:13,466 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:55:16,451 - AirSimEnvLogger - INFO - Predictive model loss: 0.09675468504428864
2024-07-21 22:55:22,478 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.897189779025464, Velocity: -0.04394961728841976, Movement: 0.3870247611263259, Collision: 0, Height: -1.0, Movement Penalty: -0.08937954000773396, Smoothness: -0.0, Curiosity: 105.62920379638672, Exploration: 0.171295355776959, Total: 91.27651068401494
2024-07-21 22:55:22,589 - AirSimEnvLogger - INFO - Action: [0.4468977 0.4468977 0.4468977 0.4468977], Velocity: (0.44689770003866974, 0.44689770003866974, 0.44689770003866974), Duration: 1.0, Reward: 91.27651068401494, Done: False
2024-07-21 22:55:22,650 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:22,650 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:55:25,570 - AirSimEnvLogger - INFO - Predictive model loss: 0.030617421492934227
2024-07-21 22:55:31,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.74867493312337, Velocity: -0.03149538023517056, Movement: 0.23819528631857087, Collision: 0, Height: -1.0, Movement Penalty: -0.055008845070290746, Smoothness: -0.0, Curiosity: 116.7033920288086, Exploration: 0.29980479669900933, Total: 101.52636557845193
2024-07-21 22:55:31,739 - AirSimEnvLogger - INFO - Action: [-0.27504423 -0.27504423 -0.27504423 -0.27504423], Velocity: (-0.27504422535145373, -0.27504422535145373, -0.27504422535145373), Duration: 1.0, Reward: 101.52636557845193, Done: False
2024-07-21 22:55:31,817 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:31,818 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:55:34,623 - AirSimEnvLogger - INFO - Predictive model loss: 0.07264871895313263
2024-07-21 22:55:40,570 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.046946641982263, Velocity: -0.057162910954739736, Movement: 0.3124073669923176, Collision: 0, Height: -1.0, Movement Penalty: -0.07214739097193472, Smoothness: -0.0, Curiosity: 105.22039794921875, Exploration: 0.06571776133270726, Total: 90.6905134371405
2024-07-21 22:55:40,664 - AirSimEnvLogger - INFO - Action: [0.36073695 0.36073695 0.36073695 0.36073695], Velocity: (0.36073695485967355, 0.36073695485967355, 0.36073695485967355), Duration: 1.0, Reward: 90.6905134371405, Done: False
2024-07-21 22:55:40,726 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:40,726 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:55:43,552 - AirSimEnvLogger - INFO - Predictive model loss: 0.019557341933250427
2024-07-21 22:55:49,775 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.704931950053936, Velocity: -0.055809754617840136, Movement: 0.4166831638272018, Collision: 0, Height: -1.0, Movement Penalty: -0.09622885472096797, Smoothness: -0.0, Curiosity: 99.46562194824219, Exploration: 0.3207822360075684, Total: 85.33911015258353
2024-07-21 22:55:49,853 - AirSimEnvLogger - INFO - Action: [0.48114427 0.48114427 0.48114427 0.48114427], Velocity: (0.4811442736048398, 0.4811442736048398, 0.4811442736048398), Duration: 1.0, Reward: 85.33911015258353, Done: False
2024-07-21 22:55:49,931 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:49,931 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:55:52,628 - AirSimEnvLogger - INFO - Predictive model loss: 0.009816611185669899
2024-07-21 22:55:57,623 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.177558309833573, Velocity: -0.03191535486629676, Movement: 0.6410219598046532, Collision: 0, Height: -1.0, Movement Penalty: -0.14803768041987117, Smoothness: -0.0, Curiosity: 91.33905792236328, Exploration: 0.27414760298979546, Total: 77.73708625989345
2024-07-21 22:55:57,748 - AirSimEnvLogger - INFO - Action: [0.7401884 0.7401884 0.7401884 0.7401884], Velocity: (0.7401884020993559, 0.7401884020993559, 0.7401884020993559), Duration: 1.0, Reward: 77.73708625989345, Done: False
2024-07-21 22:55:57,811 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:55:57,811 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:00,513 - AirSimEnvLogger - INFO - Predictive model loss: 0.011559371836483479
2024-07-21 22:56:06,466 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.025744403581248, Velocity: -0.016901817741888297, Movement: 0.09084754582251385, Collision: 0, Height: -1.0, Movement Penalty: -0.020980342014338096, Smoothness: -0.0, Curiosity: 98.96854400634766, Exploration: 0.13118272500064762, Total: 84.47350993315035
2024-07-21 22:56:06,623 - AirSimEnvLogger - INFO - Action: [-0.10490171 -0.10490171 -0.10490171 -0.10490171], Velocity: (-0.10490171007169047, -0.10490171007169047, -0.10490171007169047), Duration: 1.0, Reward: 84.47350993315035, Done: False
2024-07-21 22:56:06,653 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:06,653 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:09,576 - AirSimEnvLogger - INFO - Predictive model loss: 0.04283909499645233
2024-07-21 22:56:15,661 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.68603423989514, Velocity: -0.07668722189259715, Movement: 0.43358798810077476, Collision: 0, Height: -1.0, Movement Penalty: -0.10013285665894822, Smoothness: -0.0, Curiosity: 110.37432861328125, Exploration: 0.45758946631971703, Total: 95.29650711845503
2024-07-21 22:56:15,662 - AirSimEnvLogger - INFO - Action: [-0.50066428 -0.50066428 -0.50066428 -0.50066428], Velocity: (-0.5006642832947411, -0.5006642832947411, -0.5006642832947411), Duration: 1.0, Reward: 95.29650711845503, Done: False
2024-07-21 22:56:15,724 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:15,724 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:18,459 - AirSimEnvLogger - INFO - Predictive model loss: 0.1612566113471985
2024-07-21 22:56:24,447 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.640644504450805, Velocity: -0.03528567480583234, Movement: 0.2556240217284969, Collision: 0, Height: -1.0, Movement Penalty: -0.059033839102512975, Smoothness: -0.0, Curiosity: 110.27259826660156, Exploration: 0.1893831725059276, Total: 95.17825376783728
2024-07-21 22:56:24,588 - AirSimEnvLogger - INFO - Action: [-0.2951692 -0.2951692 -0.2951692 -0.2951692], Velocity: (-0.2951691955125649, -0.2951691955125649, -0.2951691955125649), Duration: 1.0, Reward: 95.17825376783728, Done: False
2024-07-21 22:56:24,650 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:24,650 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:27,655 - AirSimEnvLogger - INFO - Predictive model loss: 0.12339434027671814
2024-07-21 22:56:33,426 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.22275915105764, Velocity: -0.10088773879305174, Movement: 0.5517647025746037, Collision: 0, Height: -1.0, Movement Penalty: -0.1274245998243125, Smoothness: -0.0, Curiosity: 123.74647521972656, Exploration: 0.18318517421802347, Total: 108.06929464330804
2024-07-21 22:56:33,514 - AirSimEnvLogger - INFO - Action: [-0.637123 -0.637123 -0.637123 -0.637123], Velocity: (-0.6371229991215626, -0.6371229991215626, -0.6371229991215626), Duration: 1.0, Reward: 108.06929464330804, Done: False
2024-07-21 22:56:33,575 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:33,575 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:36,327 - AirSimEnvLogger - INFO - Predictive model loss: 0.20226630568504333
2024-07-21 22:56:41,999 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.590298654055461, Velocity: -0.056434888631395105, Movement: 0.6046725321751104, Collision: 0, Height: -1.0, Movement Penalty: -0.13964313968914907, Smoothness: -0.0, Curiosity: 133.47879028320312, Exploration: 0.31135414300565717, Total: 117.46929217783577
2024-07-21 22:56:42,110 - AirSimEnvLogger - INFO - Action: [-0.6982157 -0.6982157 -0.6982157 -0.6982157], Velocity: (-0.6982156984457453, -0.6982156984457453, -0.6982156984457453), Duration: 1.0, Reward: 117.46929217783577, Done: False
2024-07-21 22:56:42,174 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:42,174 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:45,018 - AirSimEnvLogger - INFO - Predictive model loss: 0.2529407739639282
2024-07-21 22:56:50,911 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.07709839947487, Velocity: -0.143283638908955, Movement: 0.7227455363592472, Collision: 0, Height: -1.0, Movement Penalty: -0.16691093198904475, Smoothness: -0.0, Curiosity: 147.22909545898438, Exploration: 0.2949800491707545, Total: 130.72324301602364
2024-07-21 22:56:51,006 - AirSimEnvLogger - INFO - Action: [-0.83455466 -0.83455466 -0.83455466 -0.83455466], Velocity: (-0.8345546599452237, -0.8345546599452237, -0.8345546599452237), Duration: 1.0, Reward: 130.72324301602364, Done: False
2024-07-21 22:56:51,038 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:51,038 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:56:53,817 - AirSimEnvLogger - INFO - Predictive model loss: 0.23206128180027008
2024-07-21 22:56:59,569 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.507238132770112, Velocity: -0.12840604723089277, Movement: 0.7665185694856281, Collision: 0, Height: -1.0, Movement Penalty: -0.17701988097254973, Smoothness: -0.0, Curiosity: 160.3723907470703, Exploration: 0.3239347439133386, Total: 143.4456196599189
2024-07-21 22:56:59,633 - AirSimEnvLogger - INFO - Action: [-0.8850994 -0.8850994 -0.8850994 -0.8850994], Velocity: (-0.8850994048627485, -0.8850994048627485, -0.8850994048627485), Duration: 1.0, Reward: 143.4456196599189, Done: False
2024-07-21 22:56:59,710 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:56:59,710 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:02,595 - AirSimEnvLogger - INFO - Predictive model loss: 0.22981856763362885
2024-07-21 22:57:08,575 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.47338693290667, Velocity: -0.07576809284513104, Movement: 0.5234880375956367, Collision: 0, Height: -1.0, Movement Penalty: -0.1208943837693559, Smoothness: -0.0, Curiosity: 162.00794982910156, Exploration: 0.2048965167166479, Total: 145.08695341913392
2024-07-21 22:57:08,606 - AirSimEnvLogger - INFO - Action: [-0.60447192 -0.60447192 -0.60447192 -0.60447192], Velocity: (-0.6044719188467795, -0.6044719188467795, -0.6044719188467795), Duration: 1.0, Reward: 145.08695341913392, Done: False
2024-07-21 22:57:08,667 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:57:08,667 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:11,395 - AirSimEnvLogger - INFO - Predictive model loss: 0.07419973611831665
2024-07-21 22:57:17,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.453436358356583, Velocity: -0.06472564006692857, Movement: 0.3795781963980275, Collision: 0, Height: -1.0, Movement Penalty: -0.08765982954756552, Smoothness: -0.0, Curiosity: 164.31761169433594, Exploration: 0.14722867257951, Total: 147.4008763993879
2024-07-21 22:57:17,253 - AirSimEnvLogger - INFO - Action: [-0.43829915 -0.43829915 -0.43829915 -0.43829915], Velocity: (-0.43829914773782763, -0.43829914773782763, -0.43829914773782763), Duration: 1.0, Reward: 147.4008763993879, Done: False
2024-07-21 22:57:17,331 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:57:17,331 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:19,978 - AirSimEnvLogger - INFO - Predictive model loss: 0.05977154150605202
2024-07-21 22:57:25,532 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.647636673723467, Velocity: -0.04269124596376663, Movement: 0.2108480038146735, Collision: 0, Height: -1.0, Movement Penalty: -0.0486932607041988, Smoothness: -0.0, Curiosity: 149.90908813476562, Exploration: 0.27201740222616627, Total: 133.82491842913973
2024-07-21 22:57:25,641 - AirSimEnvLogger - INFO - Action: [0.2434663 0.2434663 0.2434663 0.2434663], Velocity: (0.24346630352099397, 0.24346630352099397, 0.24346630352099397), Duration: 1.0, Reward: 133.82491842913973, Done: False
2024-07-21 22:57:25,783 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:57:25,783 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:28,513 - AirSimEnvLogger - INFO - Predictive model loss: 0.4763507843017578
2024-07-21 22:57:34,487 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.707692566713098, Velocity: -0.00708016567447823, Movement: 0.07266096169457674, Collision: 0, Height: -1.0, Movement Penalty: -0.01678033031757639, Smoothness: -0.0, Curiosity: 150.9659881591797, Exploration: 0.22948437058189702, Total: 134.81215135061143
2024-07-21 22:57:34,565 - AirSimEnvLogger - INFO - Action: [0.08390165 0.08390165 0.08390165 0.08390165], Velocity: (0.08390165158788193, 0.08390165158788193, 0.08390165158788193), Duration: 1.0, Reward: 134.81215135061143, Done: False
2024-07-21 22:57:34,643 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:57:34,643 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:37,299 - AirSimEnvLogger - INFO - Predictive model loss: 0.43868863582611084
2024-07-21 22:57:43,006 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.420579376211432, Velocity: -0.038919632663536236, Movement: 0.38851145372856943, Collision: 0, Height: -1.0, Movement Penalty: -0.08972287695737696, Smoothness: -0.0, Curiosity: 165.79971313476562, Exploration: 0.23566625369245323, Total: 148.9389751986804
2024-07-21 22:57:43,131 - AirSimEnvLogger - INFO - Action: [-0.44861438 -0.44861438 -0.44861438 -0.44861438], Velocity: (-0.4486143847868848, -0.4486143847868848, -0.4486143847868848), Duration: 1.0, Reward: 148.9389751986804, Done: False
2024-07-21 22:57:43,194 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:57:43,194 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:45,881 - AirSimEnvLogger - INFO - Predictive model loss: 0.05760948359966278
2024-07-21 22:57:51,904 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.469537475290927, Velocity: -0.04392234767228739, Movement: 0.2764356828654544, Collision: 0, Height: -1.0, Movement Penalty: -0.06384008636639525, Smoothness: -0.0, Curiosity: 167.62960815429688, Exploration: 0.23119308992133059, Total: 150.7156338073386
2024-07-21 22:57:51,936 - AirSimEnvLogger - INFO - Action: [-0.31920043 -0.31920043 -0.31920043 -0.31920043], Velocity: (-0.3192004318319762, -0.3192004318319762, -0.3192004318319762), Duration: 1.0, Reward: 150.7156338073386, Done: False
2024-07-21 22:57:51,998 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:57:51,998 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:57:54,702 - AirSimEnvLogger - INFO - Predictive model loss: 0.02379385195672512
2024-07-21 22:58:00,952 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.95518123721121, Velocity: -0.06705861482349204, Movement: 0.49275502820654404, Collision: 0, Height: -1.0, Movement Penalty: -0.11379689927183595, Smoothness: -0.0, Curiosity: 180.82156372070312, Exploration: 0.17277807581251045, Total: 163.41150283375944
2024-07-21 22:58:01,030 - AirSimEnvLogger - INFO - Action: [-0.5689845 -0.5689845 -0.5689845 -0.5689845], Velocity: (-0.5689844963591797, -0.5689844963591797, -0.5689844963591797), Duration: 1.0, Reward: 163.41150283375944, Done: False
2024-07-21 22:58:01,109 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:01,109 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:03,746 - AirSimEnvLogger - INFO - Predictive model loss: 0.12950573861598969
2024-07-21 22:58:09,550 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.3970651304298, Velocity: -0.0033499966213544267, Movement: 0.0075420773060073535, Collision: 0, Height: -1.0, Movement Penalty: -0.0017417681451489253, Smoothness: -0.0, Curiosity: 169.7086639404297, Exploration: 0.08029911479537456, Total: 152.82991761349095
2024-07-21 22:58:09,644 - AirSimEnvLogger - INFO - Action: [0.00870884 0.00870884 0.00870884 0.00870884], Velocity: (0.008708840725744627, 0.008708840725744627, 0.008708840725744627), Duration: 1.0, Reward: 152.82991761349095, Done: False
2024-07-21 22:58:09,676 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:09,676 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:12,331 - AirSimEnvLogger - INFO - Predictive model loss: 0.0864863395690918
2024-07-21 22:58:18,365 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.01344345148057, Velocity: -0.07315554374600798, Movement: 0.40403361275802024, Collision: 0, Height: -1.0, Movement Penalty: -0.09330756603500001, Smoothness: -0.0, Curiosity: 184.96749877929688, Exploration: 0.06473054996099036, Total: 167.47153872270135
2024-07-21 22:58:18,412 - AirSimEnvLogger - INFO - Action: [-0.46653783 -0.46653783 -0.46653783 -0.46653783], Velocity: (-0.466537830175, -0.466537830175, -0.466537830175), Duration: 1.0, Reward: 167.47153872270135, Done: False
2024-07-21 22:58:18,458 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:18,458 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:21,206 - AirSimEnvLogger - INFO - Predictive model loss: 0.2405160814523697
2024-07-21 22:58:27,006 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.212610640783662, Velocity: -0.03748112898948217, Movement: 0.23077669484981128, Collision: 0, Height: -1.0, Movement Penalty: -0.05329559475769227, Smoothness: -0.0, Curiosity: 168.79534912109375, Exploration: 0.07486106871365109, Total: 152.1018693531154
2024-07-21 22:58:27,117 - AirSimEnvLogger - INFO - Action: [0.26647797 0.26647797 0.26647797 0.26647797], Velocity: (0.26647797378846133, 0.26647797378846133, 0.26647797378846133), Duration: 1.0, Reward: 152.1018693531154, Done: False
2024-07-21 22:58:27,196 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:27,196 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:30,006 - AirSimEnvLogger - INFO - Predictive model loss: 0.06669948995113373
2024-07-21 22:58:35,838 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.881377618340824, Velocity: -0.036424862465673176, Movement: 0.289132346137369, Collision: 0, Height: -1.0, Movement Penalty: -0.0667722551495352, Smoothness: -0.0, Curiosity: 183.0598907470703, Exploration: 0.04913915034983941, Total: 165.69326504794785
2024-07-21 22:58:35,885 - AirSimEnvLogger - INFO - Action: [-0.33386128 -0.33386128 -0.33386128 -0.33386128], Velocity: (-0.33386127574767605, -0.33386127574767605, -0.33386127574767605), Duration: 1.0, Reward: 165.69326504794785, Done: False
2024-07-21 22:58:35,948 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:35,948 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:38,868 - AirSimEnvLogger - INFO - Predictive model loss: 0.11646977066993713
2024-07-21 22:58:44,672 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.24360528558502, Velocity: -0.0494973497299976, Movement: 0.3982840110827229, Collision: 0, Height: -1.0, Movement Penalty: -0.09197975240501358, Smoothness: -0.0, Curiosity: 192.4850616455078, Exploration: 0.31303821602206133, Total: 174.81827529998392
2024-07-21 22:58:44,704 - AirSimEnvLogger - INFO - Action: [-0.45989876 -0.45989876 -0.45989876 -0.45989876], Velocity: (-0.45989876202506785, -0.45989876202506785, -0.45989876202506785), Duration: 1.0, Reward: 174.81827529998392, Done: False
2024-07-21 22:58:44,766 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:44,766 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:47,510 - AirSimEnvLogger - INFO - Predictive model loss: 0.13797834515571594
2024-07-21 22:58:53,411 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.280871777330038, Velocity: -0.05249280904833814, Movement: 0.2913873060280188, Collision: 0, Height: -1.0, Movement Penalty: -0.06729301582948662, Smoothness: -0.0, Curiosity: 194.82672119140625, Exploration: 0.1305678748329962, Total: 177.07777845931216
2024-07-21 22:58:53,505 - AirSimEnvLogger - INFO - Action: [-0.33646508 -0.33646508 -0.33646508 -0.33646508], Velocity: (-0.33646507914743307, -0.33646507914743307, -0.33646507914743307), Duration: 1.0, Reward: 177.07777845931216, Done: False
2024-07-21 22:58:53,584 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:58:53,584 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:58:56,418 - AirSimEnvLogger - INFO - Predictive model loss: 0.06608100235462189
2024-07-21 22:59:02,420 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.670793472140193, Velocity: -0.029139807161904875, Movement: 0.16860768148088476, Collision: 0, Height: -1.0, Movement Penalty: -0.038938276116171004, Smoothness: -0.0, Curiosity: 182.80386352539062, Exploration: 0.18841620651394578, Total: 165.6776277373817
2024-07-21 22:59:02,576 - AirSimEnvLogger - INFO - Action: [0.19469138 0.19469138 0.19469138 0.19469138], Velocity: (0.194691380580855, 0.194691380580855, 0.194691380580855), Duration: 1.0, Reward: 165.6776277373817, Done: False
2024-07-21 22:59:02,627 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:02,628 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:05,244 - AirSimEnvLogger - INFO - Predictive model loss: 0.10834825038909912
2024-07-21 22:59:11,087 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.376144419816104, Velocity: -0.05907313639378432, Movement: 0.348694923984906, Collision: 0, Height: -1.0, Movement Penalty: -0.08052764329109663, Smoothness: -0.0, Curiosity: 199.03585815429688, Exploration: 0.04563793229770969, Total: 181.1728566129822
2024-07-21 22:59:11,228 - AirSimEnvLogger - INFO - Action: [-0.40263822 -0.40263822 -0.40263822 -0.40263822], Velocity: (-0.40263821645548314, -0.40263821645548314, -0.40263821645548314), Duration: 1.0, Reward: 181.1728566129822, Done: False
2024-07-21 22:59:11,354 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:11,354 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:14,089 - AirSimEnvLogger - INFO - Predictive model loss: 0.03050435706973076
2024-07-21 22:59:19,981 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.979697038208226, Velocity: -0.09426229279716293, Movement: 0.5825695573608298, Collision: 0, Height: -1.0, Movement Penalty: -0.1345386763055825, Smoothness: -0.0, Curiosity: 215.81166076660156, Exploration: 0.39754806036598683, Total: 197.4282639528755
2024-07-21 22:59:20,045 - AirSimEnvLogger - INFO - Action: [-0.67269338 -0.67269338 -0.67269338 -0.67269338], Velocity: (-0.6726933815279125, -0.6726933815279125, -0.6726933815279125), Duration: 1.0, Reward: 197.4282639528755, Done: False
2024-07-21 22:59:20,109 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:20,109 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:22,860 - AirSimEnvLogger - INFO - Predictive model loss: 0.05530745908617973
2024-07-21 22:59:28,789 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.563370980310566, Velocity: -0.011382051544487628, Movement: 0.13531608642189658, Collision: 0, Height: -1.0, Movement Penalty: -0.031249911568547464, Smoothness: -0.0, Curiosity: 206.71725463867188, Exploration: 0.09110201823984314, Total: 188.67701821371682
2024-07-21 22:59:28,899 - AirSimEnvLogger - INFO - Action: [-0.15624956 -0.15624956 -0.15624956 -0.15624956], Velocity: (-0.1562495578427373, -0.1562495578427373, -0.1562495578427373), Duration: 1.0, Reward: 188.67701821371682, Done: False
2024-07-21 22:59:28,962 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:28,962 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:31,878 - AirSimEnvLogger - INFO - Predictive model loss: 0.03670412674546242
2024-07-21 22:59:37,804 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.934884244253237, Velocity: -0.03935140493211596, Movement: 0.2733870368873409, Collision: 0, Height: -1.0, Movement Penalty: -0.06313603173594416, Smoothness: -0.0, Curiosity: 194.46493530273438, Exploration: 0.32977268967154355, Total: 177.10866980590865
2024-07-21 22:59:37,850 - AirSimEnvLogger - INFO - Action: [0.31568016 0.31568016 0.31568016 0.31568016], Velocity: (0.3156801586797208, 0.3156801586797208, 0.3156801586797208), Duration: 1.0, Reward: 177.10866980590865, Done: False
2024-07-21 22:59:37,927 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:37,927 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:40,769 - AirSimEnvLogger - INFO - Predictive model loss: 0.2994949221611023
2024-07-21 22:59:46,706 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.316863543815952, Velocity: -0.09412857416880176, Movement: 0.5586653257885729, Collision: 0, Height: -1.0, Movement Penalty: -0.129018230492377, Smoothness: -0.0, Curiosity: 182.02830505371094, Exploration: 0.3783244078414811, Total: 165.30274729664586
2024-07-21 22:59:46,752 - AirSimEnvLogger - INFO - Action: [0.64509115 0.64509115 0.64509115 0.64509115], Velocity: (0.645091152461885, 0.645091152461885, 0.645091152461885), Duration: 1.0, Reward: 165.30274729664586, Done: False
2024-07-21 22:59:46,784 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:46,784 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:49,485 - AirSimEnvLogger - INFO - Predictive model loss: 0.6178324818611145
2024-07-21 22:59:55,259 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.986831161746757, Velocity: -0.08610568327959552, Movement: 0.5846891495009798, Collision: 0, Height: -1.0, Movement Penalty: -0.1350281751426576, Smoothness: -0.0, Curiosity: 173.12376403808594, Exploration: 0.2885044070380213, Total: 156.70902071498878
2024-07-21 22:59:55,397 - AirSimEnvLogger - INFO - Action: [0.67514088 0.67514088 0.67514088 0.67514088], Velocity: (0.6751408757132881, 0.6751408757132881, 0.6751408757132881), Duration: 1.0, Reward: 156.70902071498878, Done: False
2024-07-21 22:59:55,461 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 22:59:55,461 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 22:59:58,315 - AirSimEnvLogger - INFO - Predictive model loss: 0.5464079976081848
2024-07-21 23:00:04,070 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.510714843743669, Velocity: -0.11138454583397635, Movement: 0.72013649136156, Collision: 0, Height: -1.0, Movement Penalty: -0.1663083988563477, Smoothness: -0.0, Curiosity: 161.7847442626953, Exploration: 0.25859207716250954, Total: 145.84003205147806
2024-07-21 23:00:04,196 - AirSimEnvLogger - INFO - Action: [0.83154199 0.83154199 0.83154199 0.83154199], Velocity: (0.8315419942817386, 0.8315419942817386, 0.8315419942817386), Duration: 1.0, Reward: 145.84003205147806, Done: False
2024-07-21 23:00:04,260 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:04,260 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:07,145 - AirSimEnvLogger - INFO - Predictive model loss: 0.3665201961994171
2024-07-21 23:00:13,227 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.335249488965818, Velocity: -0.008480750852363396, Movement: 0.03154830898094267, Collision: 0, Height: -1.0, Movement Penalty: -0.007285769873049897, Smoothness: -0.0, Curiosity: 171.24464416503906, Exploration: 0.1727247554906348, Total: 154.44904717307165
2024-07-21 23:00:13,352 - AirSimEnvLogger - INFO - Action: [-0.03642885 -0.03642885 -0.03642885 -0.03642885], Velocity: (-0.03642884936524948, -0.03642884936524948, -0.03642884936524948), Duration: 1.0, Reward: 154.44904717307165, Done: False
2024-07-21 23:00:13,382 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:13,382 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:16,266 - AirSimEnvLogger - INFO - Predictive model loss: 0.04411724954843521
2024-07-21 23:00:22,060 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.987356778723434, Velocity: -0.05697905760431218, Movement: 0.38556954507804664, Collision: 0, Height: -1.0, Movement Penalty: -0.08904347225685272, Smoothness: -0.0, Curiosity: 184.94973754882812, Exploration: 0.4598909364346499, Total: 167.57191577990673
2024-07-21 23:00:22,186 - AirSimEnvLogger - INFO - Action: [-0.44521736 -0.44521736 -0.44521736 -0.44521736], Velocity: (-0.44521736128426354, -0.44521736128426354, -0.44521736128426354), Duration: 1.0, Reward: 167.57191577990673, Done: False
2024-07-21 23:00:22,249 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:22,249 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:25,092 - AirSimEnvLogger - INFO - Predictive model loss: 0.4039447009563446
2024-07-21 23:00:30,592 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.550225751008693, Velocity: -0.051816441889542646, Movement: 0.583684982633148, Collision: 0, Height: -1.0, Movement Penalty: -0.134796272738076, Smoothness: -0.0, Curiosity: 200.26161193847656, Exploration: 0.3663187393215336, Total: 182.30477561452247
2024-07-21 23:00:30,671 - AirSimEnvLogger - INFO - Action: [-0.67398136 -0.67398136 -0.67398136 -0.67398136], Velocity: (-0.6739813636903801, -0.6739813636903801, -0.6739813636903801), Duration: 1.0, Reward: 182.30477561452247, Done: False
2024-07-21 23:00:30,733 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:30,733 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:33,532 - AirSimEnvLogger - INFO - Predictive model loss: 1.022408366203308
2024-07-21 23:00:39,209 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.09565668677351, Velocity: -0.07843254882692209, Movement: 0.7179349769042793, Collision: 0, Height: -1.0, Movement Penalty: -0.16579998087053338, Smoothness: -0.0, Curiosity: 217.1987762451172, Exploration: 0.34372525468648185, Total: 198.6919440183702
2024-07-21 23:00:39,272 - AirSimEnvLogger - INFO - Action: [-0.8289999 -0.8289999 -0.8289999 -0.8289999], Velocity: (-0.8289999043526668, -0.8289999043526668, -0.8289999043526668), Duration: 1.0, Reward: 198.6919440183702, Done: False
2024-07-21 23:00:39,288 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:39,288 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:42,035 - AirSimEnvLogger - INFO - Predictive model loss: 1.6884905099868774
2024-07-21 23:00:47,827 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.58327558782346, Velocity: -0.025215574458680418, Movement: 0.18195060287987724, Collision: 0, Height: -1.0, Movement Penalty: -0.04201969182076473, Smoothness: -0.0, Curiosity: 205.93728637695312, Exploration: 0.12314183507061678, Total: 187.88427509224283
2024-07-21 23:00:47,969 - AirSimEnvLogger - INFO - Action: [-0.21009846 -0.21009846 -0.21009846 -0.21009846], Velocity: (-0.21009845910382363, -0.21009845910382363, -0.21009845910382363), Duration: 1.0, Reward: 187.88427509224283, Done: False
2024-07-21 23:00:48,063 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:48,063 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:50,873 - AirSimEnvLogger - INFO - Predictive model loss: 1.0109668970108032
2024-07-21 23:00:56,522 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.77701030496779, Velocity: -0.03777218591197924, Movement: 0.34093016997308945, Collision: 0, Height: -1.0, Movement Penalty: -0.07873445016353123, Smoothness: -0.0, Curiosity: 190.43678283691406, Exploration: 0.4159212842944787, Total: 173.26002020745727
2024-07-21 23:00:56,631 - AirSimEnvLogger - INFO - Action: [0.39367225 0.39367225 0.39367225 0.39367225], Velocity: (0.3936722508176561, 0.3936722508176561, 0.3936722508176561), Duration: 1.0, Reward: 173.26002020745727, Done: False
2024-07-21 23:00:56,678 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:00:56,678 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:00:59,524 - AirSimEnvLogger - INFO - Predictive model loss: 0.3163595497608185
2024-07-21 23:01:05,212 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.51925223123824, Velocity: -0.026014614445856024, Movement: 0.2613149261150765, Collision: 0, Height: -1.0, Movement Penalty: -0.060348097174322636, Smoothness: -0.0, Curiosity: 206.0065460205078, Exploration: 0.0337089427182226, Total: 187.9988554257191
2024-07-21 23:01:05,322 - AirSimEnvLogger - INFO - Action: [-0.30174049 -0.30174049 -0.30174049 -0.30174049], Velocity: (-0.3017404858716132, -0.3017404858716132, -0.3017404858716132), Duration: 1.0, Reward: 187.9988554257191, Done: False
2024-07-21 23:01:05,417 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:05,417 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:08,132 - AirSimEnvLogger - INFO - Predictive model loss: 0.43910089135169983
2024-07-21 23:01:13,890 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.87139140983797, Velocity: -0.04368812251385445, Movement: 0.2497012160163416, Collision: 0, Height: -1.0, Movement Penalty: -0.057666025713604685, Smoothness: -0.0, Curiosity: 191.87754821777344, Exploration: 0.029576156525370795, Total: 174.51471566907043
2024-07-21 23:01:14,016 - AirSimEnvLogger - INFO - Action: [0.28833013 0.28833013 0.28833013 0.28833013], Velocity: (0.2883301285680234, 0.2883301285680234, 0.2883301285680234), Duration: 1.0, Reward: 174.51471566907043, Done: False
2024-07-21 23:01:14,080 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:14,080 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:16,883 - AirSimEnvLogger - INFO - Predictive model loss: 0.05538194999098778
2024-07-21 23:01:22,774 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.900313658134305, Velocity: -0.0130521637503059, Movement: 0.13017516307823476, Collision: 0, Height: -1.0, Movement Penalty: -0.030062666178008915, Smoothness: -0.0, Curiosity: 191.73614501953125, Exploration: 0.18737979360969545, Total: 174.38081668729407
2024-07-21 23:01:22,915 - AirSimEnvLogger - INFO - Action: [0.15031333 0.15031333 0.15031333 0.15031333], Velocity: (0.15031333089004456, 0.15031333089004456, 0.15031333089004456), Duration: 1.0, Reward: 174.38081668729407, Done: False
2024-07-21 23:01:22,979 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:22,979 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:25,807 - AirSimEnvLogger - INFO - Predictive model loss: 0.08804842084646225
2024-07-21 23:01:31,724 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.293649090295848, Velocity: -0.0817393185730896, Movement: 0.48132829681884787, Collision: 0, Height: -1.0, Movement Penalty: -0.11115800869477838, Smoothness: -0.0, Curiosity: 179.16310119628906, Exploration: 0.1752955887526241, Total: 162.41340311675626
2024-07-21 23:01:31,741 - AirSimEnvLogger - INFO - Action: [0.55579004 0.55579004 0.55579004 0.55579004], Velocity: (0.5557900434738918, 0.5557900434738918, 0.5557900434738918), Duration: 1.0, Reward: 162.41340311675626, Done: False
2024-07-21 23:01:31,834 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:31,834 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:34,463 - AirSimEnvLogger - INFO - Predictive model loss: 0.4828515946865082
2024-07-21 23:01:40,107 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.829302762057942, Velocity: -0.0707605710658377, Movement: 0.6221115266924704, Collision: 0, Height: -1.0, Movement Penalty: -0.14367050296074674, Smoothness: -0.0, Curiosity: 168.70651245117188, Exploration: 0.3256137089644829, Total: 152.46028514732575
2024-07-21 23:01:40,247 - AirSimEnvLogger - INFO - Action: [0.71835251 0.71835251 0.71835251 0.71835251], Velocity: (0.7183525148037336, 0.7183525148037336, 0.7183525148037336), Duration: 1.0, Reward: 152.46028514732575, Done: False
2024-07-21 23:01:40,310 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:40,310 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:43,106 - AirSimEnvLogger - INFO - Predictive model loss: 0.9404626488685608
2024-07-21 23:01:48,682 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.957642906051724, Velocity: -0.04126169694733353, Movement: 0.34059597567271593, Collision: 0, Height: -1.0, Movement Penalty: -0.07865727129581832, Smoothness: -0.0, Curiosity: 166.62274169921875, Exploration: 0.13552136228865186, Total: 150.20049733774493
2024-07-21 23:01:48,745 - AirSimEnvLogger - INFO - Action: [0.39328636 0.39328636 0.39328636 0.39328636], Velocity: (0.39328635647909155, 0.39328635647909155, 0.39328635647909155), Duration: 1.0, Reward: 150.20049733774493, Done: False
2024-07-21 23:01:48,776 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:48,776 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:51,463 - AirSimEnvLogger - INFO - Predictive model loss: 0.6744146347045898
2024-07-21 23:01:57,134 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.425915097744934, Velocity: -0.07499787407731236, Movement: 0.6027260513879096, Collision: 0, Height: -1.0, Movement Penalty: -0.13919361920656392, Smoothness: -0.0, Curiosity: 155.3011016845703, Exploration: 0.1568705315028022, Total: 139.41855185925434
2024-07-21 23:01:57,260 - AirSimEnvLogger - INFO - Action: [0.6959681 0.6959681 0.6959681 0.6959681], Velocity: (0.6959680960328196, 0.6959680960328196, 0.6959680960328196), Duration: 1.0, Reward: 139.41855185925434, Done: False
2024-07-21 23:01:57,339 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:01:57,339 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:01:59,953 - AirSimEnvLogger - INFO - Predictive model loss: 0.6902804970741272
2024-07-21 23:02:05,602 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.090872789573472, Velocity: -0.05106899378362401, Movement: 0.6325627104476342, Collision: 0, Height: -1.0, Movement Penalty: -0.1460841004625044, Smoothness: -0.0, Curiosity: 146.459228515625, Exploration: 0.2781594881720557, Total: 130.94274223818414
2024-07-21 23:02:05,634 - AirSimEnvLogger - INFO - Action: [0.7304205 0.7304205 0.7304205 0.7304205], Velocity: (0.7304205023125219, 0.7304205023125219, 0.7304205023125219), Duration: 1.0, Reward: 130.94274223818414, Done: False
2024-07-21 23:02:05,745 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:05,745 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:02:08,522 - AirSimEnvLogger - INFO - Predictive model loss: 0.4995502829551697
2024-07-21 23:02:14,606 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.975481340084212, Velocity: -0.012100440468230219, Movement: 0.11618049305261245, Collision: 0, Height: -1.0, Movement Penalty: -0.0268307355754037, Smoothness: -0.0, Curiosity: 157.60997009277344, Exploration: 0.20792178585418733, Total: 141.18395062062854
2024-07-21 23:02:14,733 - AirSimEnvLogger - INFO - Action: [-0.13415368 -0.13415368 -0.13415368 -0.13415368], Velocity: (-0.13415367787701848, -0.13415367787701848, -0.13415367787701848), Duration: 1.0, Reward: 141.18395062062854, Done: False
2024-07-21 23:02:14,828 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:14,828 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:02:17,663 - AirSimEnvLogger - INFO - Predictive model loss: 0.04826662689447403
2024-07-21 23:02:23,589 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.409955937677658, Velocity: -0.0504621920638963, Movement: 0.3175630127258298, Collision: 0, Height: -1.0, Movement Penalty: -0.07333803635277057, Smoothness: -0.0, Curiosity: 146.45147705078125, Exploration: 0.12645076038025616, Total: 130.5733483723899
2024-07-21 23:02:23,697 - AirSimEnvLogger - INFO - Action: [0.36669018 0.36669018 0.36669018 0.36669018], Velocity: (0.3666901817638528, 0.3666901817638528, 0.3666901817638528), Duration: 1.0, Reward: 130.5733483723899, Done: False
2024-07-21 23:02:23,775 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:23,775 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:02:26,501 - AirSimEnvLogger - INFO - Predictive model loss: 0.0544096902012825
2024-07-21 23:02:32,032 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.146954175556274, Velocity: -0.026493646187072215, Movement: 0.25588443810824085, Collision: 0, Height: -1.0, Movement Penalty: -0.0590939796892916, Smoothness: -0.0, Curiosity: 159.57415771484375, Exploration: 0.061925159907529614, Total: 142.94507379293802
2024-07-21 23:02:32,142 - AirSimEnvLogger - INFO - Action: [-0.2954699 -0.2954699 -0.2954699 -0.2954699], Velocity: (-0.295469898446458, -0.295469898446458, -0.295469898446458), Duration: 1.0, Reward: 142.94507379293802, Done: False
2024-07-21 23:02:32,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:32,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:02:34,967 - AirSimEnvLogger - INFO - Predictive model loss: 0.2467426359653473
2024-07-21 23:02:40,781 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.91713157231246, Velocity: -0.0014567099390329931, Movement: 0.007155271985324745, Collision: 0, Height: -1.0, Movement Penalty: -0.0016524392827408918, Smoothness: -0.0, Curiosity: 154.961181640625, Exploration: 0.1542405342477622, Total: 138.5795552386322
2024-07-21 23:02:40,893 - AirSimEnvLogger - INFO - Action: [-0.0082622 -0.0082622 -0.0082622 -0.0082622], Velocity: (-0.008262196413704459, -0.008262196413704459, -0.008262196413704459), Duration: 1.0, Reward: 138.5795552386322, Done: False
2024-07-21 23:02:40,955 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:40,955 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:02:43,669 - AirSimEnvLogger - INFO - Predictive model loss: 0.3402748107910156
2024-07-21 23:02:49,513 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.449931964520612, Velocity: -0.03865846030905898, Movement: 0.3450399793813265, Collision: 0, Height: -1.0, Movement Penalty: -0.07968356999079673, Smoothness: -0.0, Curiosity: 167.03648376464844, Exploration: 0.09319018174169136, Total: 150.1125835080155
2024-07-21 23:02:49,669 - AirSimEnvLogger - INFO - Action: [-0.39841785 -0.39841785 -0.39841785 -0.39841785], Velocity: (-0.3984178499539836, -0.3984178499539836, -0.3984178499539836), Duration: 1.0, Reward: 150.1125835080155, Done: False
2024-07-21 23:02:49,731 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:49,731 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:02:52,579 - AirSimEnvLogger - INFO - Predictive model loss: 0.6595311760902405
2024-07-21 23:02:58,595 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.67121485300943, Velocity: -0.02770081728390426, Movement: 0.25846444146874187, Collision: 0, Height: -1.0, Movement Penalty: -0.05968980594316976, Smoothness: -0.0, Curiosity: 152.17832946777344, Exploration: 0.08116860979825312, Total: 136.02935343212326
2024-07-21 23:02:58,688 - AirSimEnvLogger - INFO - Action: [0.29844903 0.29844903 0.29844903 0.29844903], Velocity: (0.2984490297158488, 0.2984490297158488, 0.2984490297158488), Duration: 1.0, Reward: 136.02935343212326, Done: False
2024-07-21 23:02:58,766 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:02:58,766 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:01,638 - AirSimEnvLogger - INFO - Predictive model loss: 0.4189596176147461
2024-07-21 23:03:07,600 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.039539223405058, Velocity: -0.0915678485711115, Movement: 0.5528427291884763, Collision: 0, Height: -1.0, Movement Penalty: -0.1276735594065977, Smoothness: -0.0, Curiosity: 141.31973266601562, Exploration: 0.42772185213902886, Total: 125.88297388612853
2024-07-21 23:03:07,710 - AirSimEnvLogger - INFO - Action: [0.6383678 0.6383678 0.6383678 0.6383678], Velocity: (0.6383677970329884, 0.6383677970329884, 0.6383677970329884), Duration: 1.0, Reward: 125.88297388612853, Done: False
2024-07-21 23:03:07,774 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:03:07,774 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:10,365 - AirSimEnvLogger - INFO - Predictive model loss: 0.2199440598487854
2024-07-21 23:03:16,297 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.531681026533077, Velocity: -0.0913101558831067, Movement: 0.70287843614346, Collision: 0, Height: -1.0, Movement Penalty: -0.16232282172600393, Smoothness: -0.0, Curiosity: 131.26902770996094, Exploration: 0.34780864235950404, Total: 116.32545322577933
2024-07-21 23:03:16,389 - AirSimEnvLogger - INFO - Action: [0.81161411 0.81161411 0.81161411 0.81161411], Velocity: (0.8116141086300196, 0.8116141086300196, 0.8116141086300196), Duration: 1.0, Reward: 116.32545322577933, Done: False
2024-07-21 23:03:16,453 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:03:16,453 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:19,117 - AirSimEnvLogger - INFO - Predictive model loss: 0.0901116132736206
2024-07-21 23:03:24,751 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.190688841154412, Velocity: -0.010287122577519833, Movement: 0.0692366242164334, Collision: 0, Height: -1.0, Movement Penalty: -0.015989513451655512, Smoothness: -0.0, Curiosity: 136.80792236328125, Exploration: 0.11937272134067672, Total: 121.14535890644191
2024-07-21 23:03:24,814 - AirSimEnvLogger - INFO - Action: [0.07994757 0.07994757 0.07994757 0.07994757], Velocity: (0.07994756725827756, 0.07994756725827756, 0.07994756725827756), Duration: 1.0, Reward: 121.14535890644191, Done: False
2024-07-21 23:03:24,875 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:03:24,875 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:27,639 - AirSimEnvLogger - INFO - Predictive model loss: 0.10315185785293579
2024-07-21 23:03:33,650 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.952906817411717, Velocity: -0.04019231888630924, Movement: 0.3897302983795464, Collision: 0, Height: -1.0, Movement Penalty: -0.09000435707231372, Smoothness: -0.0, Curiosity: 150.783203125, Exploration: 0.43855970182632215, Total: 134.4367058703567
2024-07-21 23:03:33,775 - AirSimEnvLogger - INFO - Action: [-0.45002179 -0.45002179 -0.45002179 -0.45002179], Velocity: (-0.45002178536156856, -0.45002178536156856, -0.45002178536156856), Duration: 1.0, Reward: 134.4367058703567, Done: False
2024-07-21 23:03:33,806 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:03:33,806 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:36,517 - AirSimEnvLogger - INFO - Predictive model loss: 0.19882842898368835
2024-07-21 23:03:42,387 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.133467451881597, Velocity: -0.03717780845288303, Movement: 0.35355706061659997, Collision: 0, Height: -1.0, Movement Penalty: -0.08165050564835474, Smoothness: -0.0, Curiosity: 155.41607666015625, Exploration: 0.28434532863384887, Total: 138.85296358867728
2024-07-21 23:03:42,513 - AirSimEnvLogger - INFO - Action: [-0.40825253 -0.40825253 -0.40825253 -0.40825253], Velocity: (-0.40825252824177366, -0.40825252824177366, -0.40825252824177366), Duration: 1.0, Reward: 138.85296358867728, Done: False
2024-07-21 23:03:42,608 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:03:42,608 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:45,341 - AirSimEnvLogger - INFO - Predictive model loss: 0.1834668517112732
2024-07-21 23:03:51,003 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.34198015319662, Velocity: -0.029399212063261945, Movement: 0.24063179717715352, Collision: 0, Height: -1.0, Movement Penalty: -0.0555715331503252, Smoothness: -0.0, Curiosity: 141.2711639404297, Exploration: 0.19278166322246817, Total: 125.47648633359755
2024-07-21 23:03:51,161 - AirSimEnvLogger - INFO - Action: [0.27785767 0.27785767 0.27785767 0.27785767], Velocity: (0.277857665751626, 0.277857665751626, 0.277857665751626), Duration: 1.0, Reward: 125.47648633359755, Done: False
2024-07-21 23:03:51,224 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:03:51,224 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:03:54,009 - AirSimEnvLogger - INFO - Predictive model loss: 0.03053440898656845
2024-07-21 23:04:00,134 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.94033237626589, Velocity: -0.02747041427711855, Movement: 0.2398781003129034, Collision: 0, Height: -1.0, Movement Penalty: -0.05539747431534034, Smoothness: -0.0, Curiosity: 152.7266082763672, Exploration: 0.06494829524524505, Total: 136.30435116346612
2024-07-21 23:04:00,261 - AirSimEnvLogger - INFO - Action: [-0.27698737 -0.27698737 -0.27698737 -0.27698737], Velocity: (-0.27698737157670167, -0.27698737157670167, -0.27698737157670167), Duration: 1.0, Reward: 136.30435116346612, Done: False
2024-07-21 23:04:00,339 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:00,339 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:03,024 - AirSimEnvLogger - INFO - Predictive model loss: 0.05788160115480423
2024-07-21 23:04:08,883 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.592275003145197, Velocity: -0.08760313500655936, Movement: 0.5448529159991045, Collision: 0, Height: -1.0, Movement Penalty: -0.1258283910883342, Smoothness: -0.0, Curiosity: 168.26295471191406, Exploration: 0.3807292034668715, Total: 151.2628523247613
2024-07-21 23:04:08,946 - AirSimEnvLogger - INFO - Action: [-0.62914196 -0.62914196 -0.62914196 -0.62914196], Velocity: (-0.6291419554416711, -0.6291419554416711, -0.6291419554416711), Duration: 1.0, Reward: 151.2628523247613, Done: False
2024-07-21 23:04:09,040 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:09,040 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:11,708 - AirSimEnvLogger - INFO - Predictive model loss: 0.10481012612581253
2024-07-21 23:04:17,545 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.735194073192206, Velocity: -0.017234900547547546, Movement: 0.16053635399718277, Collision: 0, Height: -1.0, Movement Penalty: -0.03707428287799781, Smoothness: -0.0, Curiosity: 151.32723999023438, Exploration: 0.0834016301025661, Total: 135.11344275003162
2024-07-21 23:04:17,655 - AirSimEnvLogger - INFO - Action: [0.18537141 0.18537141 0.18537141 0.18537141], Velocity: (0.18537141438998905, 0.18537141438998905, 0.18537141438998905), Duration: 1.0, Reward: 135.11344275003162, Done: False
2024-07-21 23:04:17,686 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:17,686 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:20,593 - AirSimEnvLogger - INFO - Predictive model loss: 0.04628036543726921
2024-07-21 23:04:26,600 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.421642554512445, Velocity: -0.06096817646448789, Movement: 0.3526093085654115, Collision: 0, Height: -1.0, Movement Penalty: -0.08143163168760326, Smoothness: -0.0, Curiosity: 166.27322387695312, Exploration: 0.07076612317875855, Total: 149.3704102004807
2024-07-21 23:04:26,757 - AirSimEnvLogger - INFO - Action: [-0.40715816 -0.40715816 -0.40715816 -0.40715816], Velocity: (-0.4071581584380163, -0.4071581584380163, -0.4071581584380163), Duration: 1.0, Reward: 149.3704102004807, Done: False
2024-07-21 23:04:26,820 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:26,820 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:29,481 - AirSimEnvLogger - INFO - Predictive model loss: 0.034133534878492355
2024-07-21 23:04:35,146 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.301012526033574, Velocity: -0.02734195319664257, Movement: 0.14984326696938668, Collision: 0, Height: -1.0, Movement Penalty: -0.034604820208411345, Smoothness: -0.0, Curiosity: 164.31509399414062, Exploration: 0.17514803012563146, Total: 147.55530696698779
2024-07-21 23:04:35,240 - AirSimEnvLogger - INFO - Action: [-0.1730241 -0.1730241 -0.1730241 -0.1730241], Velocity: (-0.17302410104205673, -0.17302410104205673, -0.17302410104205673), Duration: 1.0, Reward: 147.55530696698779, Done: False
2024-07-21 23:04:35,303 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:35,303 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:38,032 - AirSimEnvLogger - INFO - Predictive model loss: 0.03338051214814186
2024-07-21 23:04:43,797 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.10193154080396, Velocity: -0.001570953835965805, Movement: 0.008115451826647009, Collision: 0, Height: -1.0, Movement Penalty: -0.0018741833186840364, Smoothness: -0.0, Curiosity: 161.22235107421875, Exploration: 0.1065985323828974, Total: 144.64497917207564
2024-07-21 23:04:43,906 - AirSimEnvLogger - INFO - Action: [0.00937092 0.00937092 0.00937092 0.00937092], Velocity: (0.009370916593420181, 0.009370916593420181, 0.009370916593420181), Duration: 1.0, Reward: 144.64497917207564, Done: False
2024-07-21 23:04:43,968 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:43,968 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:46,696 - AirSimEnvLogger - INFO - Predictive model loss: 0.05720379203557968
2024-07-21 23:04:52,388 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.45467794999111, Velocity: -0.039663042751625424, Movement: 0.2250280537548135, Collision: 0, Height: -1.0, Movement Penalty: -0.05196800296422366, Smoothness: -0.0, Curiosity: 169.19631958007812, Exploration: 0.055821210354952, Total: 152.25603413023705
2024-07-21 23:04:52,546 - AirSimEnvLogger - INFO - Action: [-0.25984001 -0.25984001 -0.25984001 -0.25984001], Velocity: (-0.2598400148211183, -0.2598400148211183, -0.2598400148211183), Duration: 1.0, Reward: 152.25603413023705, Done: False
2024-07-21 23:04:52,623 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:04:52,623 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:04:55,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.028515111654996872
2024-07-21 23:05:01,062 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.706154444605403, Velocity: -0.03403947463753146, Movement: 0.32021474395416955, Collision: 0, Height: -1.0, Movement Penalty: -0.07395042744817075, Smoothness: -0.0, Curiosity: 155.0828094482422, Exploration: 0.12191516638609509, Total: 138.90914639487133
2024-07-21 23:05:01,170 - AirSimEnvLogger - INFO - Action: [0.36975214 0.36975214 0.36975214 0.36975214], Velocity: (0.36975213724085376, 0.36975213724085376, 0.36975213724085376), Duration: 1.0, Reward: 138.90914639487133, Done: False
2024-07-21 23:05:01,296 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:01,296 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:04,155 - AirSimEnvLogger - INFO - Predictive model loss: 0.1616918295621872
2024-07-21 23:05:10,080 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.444234589559056, Velocity: -0.026498555489350005, Movement: 0.2673915027957398, Collision: 0, Height: -1.0, Movement Penalty: -0.06175142244725558, Smoothness: -0.0, Curiosity: 169.02297973632812, Exploration: 0.005705079328831874, Total: 152.08396655863353
2024-07-21 23:05:10,205 - AirSimEnvLogger - INFO - Action: [-0.30875711 -0.30875711 -0.30875711 -0.30875711], Velocity: (-0.3087571122362779, -0.3087571122362779, -0.3087571122362779), Duration: 1.0, Reward: 152.08396655863353, Done: False
2024-07-21 23:05:10,269 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:10,269 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:12,992 - AirSimEnvLogger - INFO - Predictive model loss: 0.024536747485399246
2024-07-21 23:05:18,635 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.508292894187505, Velocity: -0.01981702366482824, Movement: 0.1984473060364978, Collision: 0, Height: -1.0, Movement Penalty: -0.04582944222405122, Smoothness: -0.0, Curiosity: 170.76828002929688, Exploration: 0.24293010620526026, Total: 153.81874725890316
2024-07-21 23:05:18,778 - AirSimEnvLogger - INFO - Action: [-0.22914721 -0.22914721 -0.22914721 -0.22914721], Velocity: (-0.2291472111202561, -0.2291472111202561, -0.2291472111202561), Duration: 1.0, Reward: 153.81874725890316, Done: False
2024-07-21 23:05:18,825 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:18,825 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:21,666 - AirSimEnvLogger - INFO - Predictive model loss: 0.017951207235455513
2024-07-21 23:05:27,639 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.153175140705514, Velocity: -0.05469310300107633, Movement: 0.532085085030782, Collision: 0, Height: -1.0, Movement Penalty: -0.12287978682972277, Smoothness: -0.0, Curiosity: 187.48867797851562, Exploration: 0.2170170670606935, Total: 169.89299947142342
2024-07-21 23:05:27,748 - AirSimEnvLogger - INFO - Action: [-0.61439893 -0.61439893 -0.61439893 -0.61439893], Velocity: (-0.6143989341486138, -0.6143989341486138, -0.6143989341486138), Duration: 1.0, Reward: 169.89299947142342, Done: False
2024-07-21 23:05:27,810 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:27,810 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:30,638 - AirSimEnvLogger - INFO - Predictive model loss: 0.0729123204946518
2024-07-21 23:05:36,238 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.916672677512818, Velocity: -0.018180612012108563, Movement: 0.20873989383030683, Collision: 0, Height: -1.0, Movement Penalty: -0.048206413557416616, Smoothness: -0.0, Curiosity: 183.10374450683594, Exploration: 0.13991711067213586, Total: 165.72255508193405
2024-07-21 23:05:36,348 - AirSimEnvLogger - INFO - Action: [-0.24103207 -0.24103207 -0.24103207 -0.24103207], Velocity: (-0.24103206778708308, -0.24103206778708308, -0.24103206778708308), Duration: 1.0, Reward: 165.72255508193405, Done: False
2024-07-21 23:05:36,426 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:36,426 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:39,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.05303896218538284
2024-07-21 23:05:44,928 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.480183319939275, Velocity: -0.09245688509547254, Movement: 0.5078163430073614, Collision: 0, Height: -1.0, Movement Penalty: -0.11727516093367658, Smoothness: -0.0, Curiosity: 199.01007080078125, Exploration: 0.1268715310413422, Total: 181.06227895202613
2024-07-21 23:05:45,007 - AirSimEnvLogger - INFO - Action: [-0.5863758 -0.5863758 -0.5863758 -0.5863758], Velocity: (-0.5863758046683829, -0.5863758046683829, -0.5863758046683829), Duration: 1.0, Reward: 181.06227895202613, Done: False
2024-07-21 23:05:45,070 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:45,071 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:47,823 - AirSimEnvLogger - INFO - Predictive model loss: 0.18472148478031158
2024-07-21 23:05:53,562 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.051834885771612, Velocity: -0.050098621419878565, Movement: 0.6830786545852647, Collision: 0, Height: -1.0, Movement Penalty: -0.15775025804099602, Smoothness: -0.0, Curiosity: 216.33973693847656, Exploration: 0.3531836805008109, Total: 197.8808803196996
2024-07-21 23:05:53,626 - AirSimEnvLogger - INFO - Action: [-0.78875129 -0.78875129 -0.78875129 -0.78875129], Velocity: (-0.78875129020498, -0.78875129020498, -0.78875129020498), Duration: 1.0, Reward: 197.8808803196996, Done: False
2024-07-21 23:05:53,689 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:05:53,689 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:05:56,394 - AirSimEnvLogger - INFO - Predictive model loss: 0.4332939386367798
2024-07-21 23:06:01,728 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.572863955224463, Velocity: -0.09596978938396725, Movement: 0.7742284030081948, Collision: 0, Height: -1.0, Movement Penalty: -0.17880039075641416, Smoothness: -0.0, Curiosity: 233.6719512939453, Exploration: 0.34804562790077437, Total: 214.68853263524258
2024-07-21 23:06:01,823 - AirSimEnvLogger - INFO - Action: [-0.89400195 -0.89400195 -0.89400195 -0.89400195], Velocity: (-0.8940019537820707, -0.8940019537820707, -0.8940019537820707), Duration: 1.0, Reward: 214.68853263524258, Done: False
2024-07-21 23:06:01,901 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:01,901 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:04,619 - AirSimEnvLogger - INFO - Predictive model loss: 0.6215009689331055
2024-07-21 23:06:10,492 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.0615820087136, Velocity: -0.13436281847973613, Movement: 0.8201248386308544, Collision: 0, Height: -1.0, Movement Penalty: -0.18939971854104887, Smoothness: -0.0, Curiosity: 251.2031707763672, Exploration: 0.33735034060929386, Total: 231.72586068228355
2024-07-21 23:06:10,602 - AirSimEnvLogger - INFO - Action: [-0.94699859 -0.94699859 -0.94699859 -0.94699859], Velocity: (-0.9469985927052443, -0.9469985927052443, -0.9469985927052443), Duration: 1.0, Reward: 231.72586068228355, Done: False
2024-07-21 23:06:10,726 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:10,726 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:13,539 - AirSimEnvLogger - INFO - Predictive model loss: 0.7780500650405884
2024-07-21 23:06:19,466 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.47147222009324, Velocity: -0.037733396951420375, Movement: 0.20772488902035002, Collision: 0, Height: -1.0, Movement Penalty: -0.04797200823731369, Smoothness: -0.0, Curiosity: 237.2985382080078, Exploration: 0.1727318389087286, Total: 218.36811645151263
2024-07-21 23:06:19,575 - AirSimEnvLogger - INFO - Action: [-0.23986004 -0.23986004 -0.23986004 -0.23986004], Velocity: (-0.23986004118656845, -0.23986004118656845, -0.23986004118656845), Duration: 1.0, Reward: 218.36811645151263, Done: False
2024-07-21 23:06:19,668 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:19,668 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:22,199 - AirSimEnvLogger - INFO - Predictive model loss: 0.2045818269252777
2024-07-21 23:06:27,994 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.281052832650698, Velocity: -0.00440758248856718, Movement: 0.06210249205760236, Collision: 0, Height: -1.0, Movement Penalty: -0.014341956202721329, Smoothness: -0.0, Curiosity: 234.88584899902344, Exploration: 0.27767851090996165, Total: 216.16974483641988
2024-07-21 23:06:28,135 - AirSimEnvLogger - INFO - Action: [-0.07170978 -0.07170978 -0.07170978 -0.07170978], Velocity: (-0.07170978101360664, -0.07170978101360664, -0.07170978101360664), Duration: 1.0, Reward: 216.16974483641988, Done: False
2024-07-21 23:06:28,197 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:28,197 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:30,849 - AirSimEnvLogger - INFO - Predictive model loss: 0.03828739374876022
2024-07-21 23:06:36,918 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.89450498000661, Velocity: -0.06957497014646714, Movement: 0.4344968678339607, Collision: 0, Height: -1.0, Movement Penalty: -0.10034275344239459, Smoothness: -0.0, Curiosity: 252.56358337402344, Exploration: 0.14114103858345428, Total: 233.20524152065195
2024-07-21 23:06:37,045 - AirSimEnvLogger - INFO - Action: [-0.50171377 -0.50171377 -0.50171377 -0.50171377], Velocity: (-0.5017137672119729, -0.5017137672119729, -0.5017137672119729), Duration: 1.0, Reward: 233.20524152065195, Done: False
2024-07-21 23:06:37,107 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:37,107 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:39,784 - AirSimEnvLogger - INFO - Predictive model loss: 0.04205794259905815
2024-07-21 23:06:45,837 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.628032056686884, Velocity: -0.016808965175885364, Movement: 0.11561207036852965, Collision: 0, Height: -1.0, Movement Penalty: -0.026699463979536223, Smoothness: -0.0, Curiosity: 246.50527954101562, Exploration: 0.11379359868776477, Total: 227.40457507337115
2024-07-21 23:06:45,915 - AirSimEnvLogger - INFO - Action: [-0.13349732 -0.13349732 -0.13349732 -0.13349732], Velocity: (-0.1334973198976811, -0.1334973198976811, -0.1334973198976811), Duration: 1.0, Reward: 227.40457507337115, Done: False
2024-07-21 23:06:45,979 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:45,979 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:48,775 - AirSimEnvLogger - INFO - Predictive model loss: 0.07048644125461578
2024-07-21 23:06:54,706 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.708513593802675, Velocity: -0.02342887316777103, Movement: 0.14493656964387328, Collision: 0, Height: -1.0, Movement Penalty: -0.033471666999724474, Smoothness: -0.0, Curiosity: 250.00161743164062, Exploration: 0.08341276342277858, Total: 230.8135011723587
2024-07-21 23:06:54,832 - AirSimEnvLogger - INFO - Action: [-0.16735833 -0.16735833 -0.16735833 -0.16735833], Velocity: (-0.16735833499862235, -0.16735833499862235, -0.16735833499862235), Duration: 1.0, Reward: 230.8135011723587, Done: False
2024-07-21 23:06:54,894 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:06:54,894 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:06:57,476 - AirSimEnvLogger - INFO - Predictive model loss: 0.16735413670539856
2024-07-21 23:07:03,268 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.274977266641255, Velocity: -0.05719727477229288, Movement: 0.4478415102582399, Collision: 0, Height: -1.0, Movement Penalty: -0.10342456660075335, Smoothness: -0.0, Curiosity: 266.8382568359375, Exploration: 0.19787514902288403, Total: 247.11405665423004
2024-07-21 23:07:03,394 - AirSimEnvLogger - INFO - Action: [-0.51712283 -0.51712283 -0.51712283 -0.51712283], Velocity: (-0.5171228330037667, -0.5171228330037667, -0.5171228330037667), Duration: 1.0, Reward: 247.11405665423004, Done: False
2024-07-21 23:07:03,489 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:03,489 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:06,252 - AirSimEnvLogger - INFO - Predictive model loss: 0.06659925729036331
2024-07-21 23:07:12,265 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.963887766640994, Velocity: -0.013945328820258777, Movement: 0.09782032771469923, Collision: 0, Height: -1.0, Movement Penalty: -0.022590637015319603, Smoothness: -0.0, Curiosity: 259.4880065917969, Exploration: 0.09007753550458111, Total: 240.0458416528123
2024-07-21 23:07:12,359 - AirSimEnvLogger - INFO - Action: [-0.11295319 -0.11295319 -0.11295319 -0.11295319], Velocity: (-0.11295318507659802, -0.11295318507659802, -0.11295318507659802), Duration: 1.0, Reward: 240.0458416528123, Done: False
2024-07-21 23:07:12,422 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:12,422 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:15,187 - AirSimEnvLogger - INFO - Predictive model loss: 0.2094719558954239
2024-07-21 23:07:21,073 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.223788512048607, Velocity: -0.045257103632279784, Movement: 0.3789086205142996, Collision: 0, Height: -1.0, Movement Penalty: -0.08750519762088026, Smoothness: -0.0, Curiosity: 242.82029724121094, Exploration: 0.32979483394870857, Total: 224.17713043866888
2024-07-21 23:07:21,182 - AirSimEnvLogger - INFO - Action: [0.43752599 0.43752599 0.43752599 0.43752599], Velocity: (0.43752598810440124, 0.43752598810440124, 0.43752598810440124), Duration: 1.0, Reward: 224.17713043866888, Done: False
2024-07-21 23:07:21,243 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:21,243 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:24,025 - AirSimEnvLogger - INFO - Predictive model loss: 0.7028597593307495
2024-07-21 23:07:29,582 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.797765591562353, Velocity: -0.0772981279347613, Movement: 0.5095758314469166, Collision: 0, Height: -1.0, Movement Penalty: -0.1176814973833619, Smoothness: -0.0, Curiosity: 232.1916961669922, Exploration: 0.35396361423190464, Total: 213.9801122616206
2024-07-21 23:07:29,740 - AirSimEnvLogger - INFO - Action: [0.58840749 0.58840749 0.58840749 0.58840749], Velocity: (0.5884074869168094, 0.5884074869168094, 0.5884074869168094), Duration: 1.0, Reward: 213.9801122616206, Done: False
2024-07-21 23:07:29,817 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:29,817 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:32,575 - AirSimEnvLogger - INFO - Predictive model loss: 0.942156970500946
2024-07-21 23:07:38,383 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.954888224082957, Velocity: -0.039422257606476344, Movement: 0.23333702539443701, Collision: 0, Height: -1.0, Movement Penalty: -0.05388687776935391, Smoothness: -0.0, Curiosity: 232.2158660888672, Exploration: 0.09841027586808808, Total: 213.7853937471438
2024-07-21 23:07:38,461 - AirSimEnvLogger - INFO - Action: [0.26943439 0.26943439 0.26943439 0.26943439], Velocity: (0.2694343888467695, 0.2694343888467695, 0.2694343888467695), Duration: 1.0, Reward: 213.7853937471438, Done: False
2024-07-21 23:07:38,524 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:38,524 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:41,250 - AirSimEnvLogger - INFO - Predictive model loss: 0.4061667025089264
2024-07-21 23:07:47,309 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.385932330832446, Velocity: -0.0843319417837014, Movement: 0.544009123197353, Collision: 0, Height: -1.0, Movement Penalty: -0.12563352548784162, Smoothness: -0.0, Curiosity: 218.40313720703125, Exploration: 0.14091195396808667, Total: 200.5545259459368
2024-07-21 23:07:47,404 - AirSimEnvLogger - INFO - Action: [0.62816763 0.62816763 0.62816763 0.62816763], Velocity: (0.6281676274392081, 0.6281676274392081, 0.6281676274392081), Duration: 1.0, Reward: 200.5545259459368, Done: False
2024-07-21 23:07:47,467 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:47,467 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:50,141 - AirSimEnvLogger - INFO - Predictive model loss: 0.39478281140327454
2024-07-21 23:07:55,944 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.036904707705176, Velocity: -0.006830655250575258, Movement: 0.05481152284013654, Collision: 0, Height: -1.0, Movement Penalty: -0.012658178986578462, Smoothness: -0.0, Curiosity: 229.2509307861328, Exploration: 0.08867207777194189, Total: 210.73508211449445
2024-07-21 23:07:56,086 - AirSimEnvLogger - INFO - Action: [-0.06329089 -0.06329089 -0.06329089 -0.06329089], Velocity: (-0.0632908949328923, -0.0632908949328923, -0.0632908949328923), Duration: 1.0, Reward: 210.73508211449445, Done: False
2024-07-21 23:07:56,149 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:07:56,149 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:07:58,755 - AirSimEnvLogger - INFO - Predictive model loss: 0.027624323964118958
2024-07-21 23:08:04,480 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.399581065762487, Velocity: -0.031640240266911764, Movement: 0.39619690092528026, Collision: 0, Height: -1.0, Movement Penalty: -0.09149775496052243, Smoothness: -0.0, Curiosity: 214.30313110351562, Exploration: 0.06674127166687215, Total: 196.4254551948591
2024-07-21 23:08:04,574 - AirSimEnvLogger - INFO - Action: [0.45748877 0.45748877 0.45748877 0.45748877], Velocity: (0.4574887748026121, 0.4574887748026121, 0.4574887748026121), Duration: 1.0, Reward: 196.4254551948591, Done: False
2024-07-21 23:08:04,669 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:04,669 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:07,529 - AirSimEnvLogger - INFO - Predictive model loss: 0.04743225872516632
2024-07-21 23:08:13,195 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.323841701059095, Velocity: -0.03732053971488804, Movement: 0.30180591572246485, Collision: 0, Height: -1.0, Movement Penalty: -0.06969909067415464, Smoothness: -0.0, Curiosity: 210.6333770751953, Exploration: 0.21733897758992926, Total: 192.86319456787447
2024-07-21 23:08:13,304 - AirSimEnvLogger - INFO - Action: [0.34849545 0.34849545 0.34849545 0.34849545], Velocity: (0.3484954533707732, 0.3484954533707732, 0.3484954533707732), Duration: 1.0, Reward: 192.86319456787447, Done: False
2024-07-21 23:08:13,414 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:13,414 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:16,285 - AirSimEnvLogger - INFO - Predictive model loss: 0.09046058356761932
2024-07-21 23:08:22,112 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.108282030807043, Velocity: -0.04321796302211514, Movement: 0.2819811607618183, Collision: 0, Height: -1.0, Movement Penalty: -0.06512075962889557, Smoothness: -0.0, Curiosity: 227.05604553222656, Exploration: 0.22878292837830275, Total: 208.50297876133652
2024-07-21 23:08:22,237 - AirSimEnvLogger - INFO - Action: [-0.3256038 -0.3256038 -0.3256038 -0.3256038], Velocity: (-0.3256037981444779, -0.3256037981444779, -0.3256037981444779), Duration: 1.0, Reward: 208.50297876133652, Done: False
2024-07-21 23:08:22,253 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:22,253 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:25,052 - AirSimEnvLogger - INFO - Predictive model loss: 0.4919886291027069
2024-07-21 23:08:30,216 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.48423724225049, Velocity: -0.024368797489017796, Movement: 0.22538104192654673, Collision: 0, Height: -1.0, Movement Penalty: -0.05204952209061203, Smoothness: -0.0, Curiosity: 212.5243377685547, Exploration: 0.05795113458055185, Total: 194.55652099233322
2024-07-21 23:08:30,311 - AirSimEnvLogger - INFO - Action: [0.26024761 0.26024761 0.26024761 0.26024761], Velocity: (0.26024761045306016, 0.26024761045306016, 0.26024761045306016), Duration: 1.0, Reward: 194.55652099233322, Done: False
2024-07-21 23:08:30,403 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:30,403 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:32,996 - AirSimEnvLogger - INFO - Predictive model loss: 0.3270854651927948
2024-07-21 23:08:38,670 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.179924009566747, Velocity: -0.025496137747191696, Movement: 0.29997760806563806, Collision: 0, Height: -1.0, Movement Penalty: -0.06927686110702248, Smoothness: -0.0, Curiosity: 228.9620819091797, Exploration: 0.022228868083596275, Total: 210.2920793600731
2024-07-21 23:08:38,779 - AirSimEnvLogger - INFO - Action: [-0.34638431 -0.34638431 -0.34638431 -0.34638431], Velocity: (-0.3463843055351124, -0.3463843055351124, -0.3463843055351124), Duration: 1.0, Reward: 210.2920793600731, Done: False
2024-07-21 23:08:38,826 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:38,826 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:41,659 - AirSimEnvLogger - INFO - Predictive model loss: 0.7270212769508362
2024-07-21 23:08:47,525 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.831334526384502, Velocity: -0.006524533496861643, Movement: 0.033789297998571374, Collision: 0, Height: -1.0, Movement Penalty: -0.007803304118081467, Smoothness: -0.0, Curiosity: 220.73556518554688, Exploration: 0.10484640638372271, Total: 202.4285217289417
2024-07-21 23:08:47,604 - AirSimEnvLogger - INFO - Action: [0.03901652 0.03901652 0.03901652 0.03901652], Velocity: (0.03901652059040733, 0.03901652059040733, 0.03901652059040733), Duration: 1.0, Reward: 202.4285217289417, Done: False
2024-07-21 23:08:47,652 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:47,652 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:50,575 - AirSimEnvLogger - INFO - Predictive model loss: 0.4975290298461914
2024-07-21 23:08:56,529 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.21472951610687, Velocity: -0.023878660777192702, Movement: 0.22981279407867009, Collision: 0, Height: -1.0, Movement Penalty: -0.053072991409816084, Smoothness: -0.0, Curiosity: 230.75787353515625, Exploration: 0.023505301883455097, Total: 212.05179966787094
2024-07-21 23:08:56,622 - AirSimEnvLogger - INFO - Action: [-0.26536496 -0.26536496 -0.26536496 -0.26536496], Velocity: (-0.2653649570490804, -0.2653649570490804, -0.2653649570490804), Duration: 1.0, Reward: 212.05179966787094, Done: False
2024-07-21 23:08:56,764 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:08:56,764 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:08:59,387 - AirSimEnvLogger - INFO - Predictive model loss: 0.5976564288139343
2024-07-21 23:09:05,125 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.70158826090301, Velocity: -0.08203544006610579, Movement: 0.4433262880510373, Collision: 0, Height: -1.0, Movement Penalty: -0.10238182069804158, Smoothness: -0.0, Curiosity: 244.7985382080078, Exploration: 0.2643318653608555, Total: 225.6604175021093
2024-07-21 23:09:05,234 - AirSimEnvLogger - INFO - Action: [-0.5119091 -0.5119091 -0.5119091 -0.5119091], Velocity: (-0.5119091034902079, -0.5119091034902079, -0.5119091034902079), Duration: 1.0, Reward: 225.6604175021093, Done: False
2024-07-21 23:09:05,312 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:05,312 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:07,930 - AirSimEnvLogger - INFO - Predictive model loss: 0.7427089214324951
2024-07-21 23:09:13,726 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.966485527361073, Velocity: -0.025452667399848704, Movement: 0.16028917265134826, Collision: 0, Height: -1.0, Movement Penalty: -0.03701719879137533, Smoothness: -0.0, Curiosity: 227.53175354003906, Exploration: 0.09648201952577794, Total: 209.08884549520445
2024-07-21 23:09:13,774 - AirSimEnvLogger - INFO - Action: [0.18508599 0.18508599 0.18508599 0.18508599], Velocity: (0.18508599395687664, 0.18508599395687664, 0.18508599395687664), Duration: 1.0, Reward: 209.08884549520445, Done: False
2024-07-21 23:09:13,803 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:13,803 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:16,515 - AirSimEnvLogger - INFO - Predictive model loss: 0.17165710031986237
2024-07-21 23:09:22,079 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.663125933198113, Velocity: -0.0459591657136933, Movement: 0.3527101351991063, Collision: 0, Height: -1.0, Movement Penalty: -0.08145491660124533, Smoothness: -0.0, Curiosity: 245.68731689453125, Exploration: 0.04453833440115178, Total: 226.53849082230127
2024-07-21 23:09:22,173 - AirSimEnvLogger - INFO - Action: [-0.40727458 -0.40727458 -0.40727458 -0.40727458], Velocity: (-0.40727458300622665, -0.40727458300622665, -0.40727458300622665), Duration: 1.0, Reward: 226.53849082230127, Done: False
2024-07-21 23:09:22,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:22,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:24,998 - AirSimEnvLogger - INFO - Predictive model loss: 0.27939409017562866
2024-07-21 23:09:30,741 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.302840322360268, Velocity: -0.1089382210377539, Movement: 0.6083729138728702, Collision: 0, Height: -1.0, Movement Penalty: -0.14049770623687144, Smoothness: -0.0, Curiosity: 265.0989685058594, Exploration: 0.40539280788475346, Total: 245.3933980620326
2024-07-21 23:09:30,742 - AirSimEnvLogger - INFO - Action: [-0.70248853 -0.70248853 -0.70248853 -0.70248853], Velocity: (-0.7024885311843572, -0.7024885311843572, -0.7024885311843572), Duration: 1.0, Reward: 245.3933980620326, Done: False
2024-07-21 23:09:30,773 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:30,773 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:33,490 - AirSimEnvLogger - INFO - Predictive model loss: 0.3659803569316864
2024-07-21 23:09:39,205 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.796292181326642, Velocity: -0.07566481941977513, Movement: 0.6952165149685741, Collision: 0, Height: -1.0, Movement Penalty: -0.1605533768248719, Smoothness: -0.0, Curiosity: 282.3004455566406, Exploration: 0.346046190099012, Total: 262.09323114077796
2024-07-21 23:09:39,299 - AirSimEnvLogger - INFO - Action: [-0.80276688 -0.80276688 -0.80276688 -0.80276688], Velocity: (-0.8027668841243594, -0.8027668841243594, -0.8027668841243594), Duration: 1.0, Reward: 262.09323114077796, Done: False
2024-07-21 23:09:39,346 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:39,346 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:42,045 - AirSimEnvLogger - INFO - Predictive model loss: 0.38679012656211853
2024-07-21 23:09:47,953 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.256389537366495, Velocity: -0.08950950040961761, Movement: 0.7422272908431026, Collision: 0, Height: -1.0, Movement Penalty: -0.17141005046726077, Smoothness: -0.0, Curiosity: 299.77264404296875, Exploration: 0.3064309401192269, Total: 279.0959894672731
2024-07-21 23:09:48,093 - AirSimEnvLogger - INFO - Action: [-0.85705025 -0.85705025 -0.85705025 -0.85705025], Velocity: (-0.8570502523363038, -0.8570502523363038, -0.8570502523363038), Duration: 1.0, Reward: 279.0959894672731, Done: False
2024-07-21 23:09:48,157 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:48,157 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:50,931 - AirSimEnvLogger - INFO - Predictive model loss: 0.33877137303352356
2024-07-21 23:09:56,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.37811014500267, Velocity: -0.056339064142150196, Movement: 0.5743565108479532, Collision: 0, Height: -1.0, Movement Penalty: -0.13264195445955204, Smoothness: -0.0, Curiosity: 306.5675354003906, Exploration: 0.22198140711962336, Total: 285.7489360069002
2024-07-21 23:09:57,005 - AirSimEnvLogger - INFO - Action: [-0.66320977 -0.66320977 -0.66320977 -0.66320977], Velocity: (-0.6632097722977601, -0.6632097722977601, -0.6632097722977601), Duration: 1.0, Reward: 285.7489360069002, Done: False
2024-07-21 23:09:57,053 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:09:57,053 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:09:59,687 - AirSimEnvLogger - INFO - Predictive model loss: 0.16490915417671204
2024-07-21 23:10:05,145 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.50700889613515, Velocity: -0.023440876516268565, Movement: 0.1265163201354112, Collision: 0, Height: -1.0, Movement Penalty: -0.029217692594824207, Smoothness: -0.0, Curiosity: 284.2516174316406, Exploration: 0.28366849396488675, Total: 264.3106116399873
2024-07-21 23:10:05,271 - AirSimEnvLogger - INFO - Action: [0.14608846 0.14608846 0.14608846 0.14608846], Velocity: (0.14608846297412104, 0.14608846297412104, 0.14608846297412104), Duration: 1.0, Reward: 264.3106116399873, Done: False
2024-07-21 23:10:05,319 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:05,319 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:10:07,969 - AirSimEnvLogger - INFO - Predictive model loss: 0.2085113525390625
2024-07-21 23:10:13,838 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.817913224282815, Velocity: -0.06886876810132951, Movement: 0.488874201024971, Collision: 0, Height: -1.0, Movement Penalty: -0.1129006606246521, Smoothness: -0.0, Curiosity: 268.17413330078125, Exploration: 0.4900730292920938, Total: 248.97404205425664
2024-07-21 23:10:13,980 - AirSimEnvLogger - INFO - Action: [0.5645033 0.5645033 0.5645033 0.5645033], Velocity: (0.5645033031232605, 0.5645033031232605, 0.5645033031232605), Duration: 1.0, Reward: 248.97404205425664, Done: False
2024-07-21 23:10:14,075 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:14,075 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:10:16,696 - AirSimEnvLogger - INFO - Predictive model loss: 1.099334478378296
2024-07-21 23:10:22,669 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.256848862624057, Velocity: -0.11203369049404041, Movement: 0.6755021349536275, Collision: 0, Height: -1.0, Movement Penalty: -0.15600053578145753, Smoothness: -0.0, Curiosity: 253.58453369140625, Exploration: 0.37799753559630633, Total: 234.9199909239227
2024-07-21 23:10:22,796 - AirSimEnvLogger - INFO - Action: [0.78000268 0.78000268 0.78000268 0.78000268], Velocity: (0.7800026789072876, 0.7800026789072876, 0.7800026789072876), Duration: 1.0, Reward: 234.9199909239227, Done: False
2024-07-21 23:10:22,874 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:22,874 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:10:25,614 - AirSimEnvLogger - INFO - Predictive model loss: 2.122504949569702
2024-07-21 23:10:31,449 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.359054912611104, Velocity: -0.042215101640765154, Movement: 0.37571700048751316, Collision: 0, Height: -1.0, Movement Penalty: -0.0867681245482338, Smoothness: -0.0, Curiosity: 250.81597900390625, Exploration: 0.15738614990968625, Total: 231.99811771340444
2024-07-21 23:10:31,544 - AirSimEnvLogger - INFO - Action: [0.43384062 0.43384062 0.43384062 0.43384062], Velocity: (0.43384062274116897, 0.43384062274116897, 0.43384062274116897), Duration: 1.0, Reward: 231.99811771340444, Done: False
2024-07-21 23:10:31,607 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:31,607 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:10:34,384 - AirSimEnvLogger - INFO - Predictive model loss: 1.4405159950256348
2024-07-21 23:10:40,330 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.864319485886153, Velocity: -0.08985512368068059, Movement: 0.5950779196940423, Collision: 0, Height: -1.0, Movement Penalty: -0.13742735884966314, Smoothness: -0.0, Curiosity: 237.22265625, Exploration: 0.1522548030727345, Total: 218.89896708578485
2024-07-21 23:10:40,440 - AirSimEnvLogger - INFO - Action: [0.68713679 0.68713679 0.68713679 0.68713679], Velocity: (0.6871367942483158, 0.6871367942483158, 0.6871367942483158), Duration: 1.0, Reward: 218.89896708578485, Done: False
2024-07-21 23:10:40,503 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:40,503 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:10:43,266 - AirSimEnvLogger - INFO - Predictive model loss: 1.3519352674484253
2024-07-21 23:10:49,187 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.904842433555686, Velocity: -0.06774390255058493, Movement: 0.3691952192396271, Collision: 0, Height: -1.0, Movement Penalty: -0.08526198368460865, Smoothness: -0.0, Curiosity: 233.8389129638672, Exploration: 0.16389137283343985, Total: 215.47404749465937
2024-07-21 23:10:49,296 - AirSimEnvLogger - INFO - Action: [0.42630992 0.42630992 0.42630992 0.42630992], Velocity: (0.4263099184230432, 0.4263099184230432, 0.4263099184230432), Duration: 1.0, Reward: 215.47404749465937, Done: False
2024-07-21 23:10:49,405 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:49,405 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:10:52,029 - AirSimEnvLogger - INFO - Predictive model loss: 0.5244677662849426
2024-07-21 23:10:58,134 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.551566339337047, Velocity: -0.02608260846007776, Movement: 0.14352067077697428, Collision: 0, Height: -1.0, Movement Penalty: -0.0331446791629447, Smoothness: -0.0, Curiosity: 246.29690551757812, Exploration: 0.24461095491721618, Total: 227.30251199135336
2024-07-21 23:10:58,198 - AirSimEnvLogger - INFO - Action: [-0.1657234 -0.1657234 -0.1657234 -0.1657234], Velocity: (-0.1657233958147235, -0.1657233958147235, -0.1657233958147235), Duration: 1.0, Reward: 227.30251199135336, Done: False
2024-07-21 23:10:58,243 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:10:58,243 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:00,839 - AirSimEnvLogger - INFO - Predictive model loss: 0.05877934768795967
2024-07-21 23:11:06,707 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.25809102550696, Velocity: -0.06968397614418874, Movement: 0.5047602615444081, Collision: 0, Height: -1.0, Movement Penalty: -0.11656938915155597, Smoothness: -0.0, Curiosity: 265.69482421875, Exploration: 0.418987606937552, Total: 246.0385136872647
2024-07-21 23:11:06,850 - AirSimEnvLogger - INFO - Action: [-0.58284695 -0.58284695 -0.58284695 -0.58284695], Velocity: (-0.5828469457577798, -0.5828469457577798, -0.5828469457577798), Duration: 1.0, Reward: 246.0385136872647, Done: False
2024-07-21 23:11:06,913 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:06,913 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:09,611 - AirSimEnvLogger - INFO - Predictive model loss: 0.4920285642147064
2024-07-21 23:11:15,377 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.78121635140861, Velocity: -0.11661612568841306, Movement: 0.6360179413055649, Collision: 0, Height: -1.0, Movement Penalty: -0.14688205184887979, Smoothness: -0.0, Curiosity: 283.017822265625, Exploration: 0.3712265434949812, Total: 262.82592789250975
2024-07-21 23:11:15,455 - AirSimEnvLogger - INFO - Action: [-0.73441026 -0.73441026 -0.73441026 -0.73441026], Velocity: (-0.734410259244399, -0.734410259244399, -0.734410259244399), Duration: 1.0, Reward: 262.82592789250975, Done: False
2024-07-21 23:11:15,517 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:15,517 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:18,175 - AirSimEnvLogger - INFO - Predictive model loss: 1.290808916091919
2024-07-21 23:11:24,112 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.29715273043338, Velocity: -0.08026737108642851, Movement: 0.7297610664843172, Collision: 0, Height: -1.0, Movement Penalty: -0.16853109927153156, Smoothness: -0.0, Curiosity: 301.9162292480469, Exploration: 0.3227600652315518, Total: 281.2031855951801
2024-07-21 23:11:24,254 - AirSimEnvLogger - INFO - Action: [-0.8426555 -0.8426555 -0.8426555 -0.8426555], Velocity: (-0.8426554963576578, -0.8426554963576578, -0.8426554963576578), Duration: 1.0, Reward: 281.2031855951801, Done: False
2024-07-21 23:11:24,317 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:24,317 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:26,929 - AirSimEnvLogger - INFO - Predictive model loss: 2.2240304946899414
2024-07-21 23:11:32,978 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.674719566517272, Velocity: -0.019605346893300075, Movement: 0.11501851568123876, Collision: 0, Height: -1.0, Movement Penalty: -0.026562388396141758, Smoothness: -0.0, Curiosity: 285.6087951660156, Exploration: 0.14539968650857746, Total: 265.46837839070326
2024-07-21 23:11:33,118 - AirSimEnvLogger - INFO - Action: [-0.13281194 -0.13281194 -0.13281194 -0.13281194], Velocity: (-0.13281194198070878, -0.13281194198070878, -0.13281194198070878), Duration: 1.0, Reward: 265.46837839070326, Done: False
2024-07-21 23:11:33,181 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:33,181 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:35,781 - AirSimEnvLogger - INFO - Predictive model loss: 1.8226191997528076
2024-07-21 23:11:41,860 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.30747383382268, Velocity: -0.022224568562720298, Movement: 0.11921521822782023, Collision: 0, Height: -1.0, Movement Penalty: -0.027531575334132797, Smoothness: -0.0, Curiosity: 277.7091064453125, Exploration: 0.32943058666676084, Total: 257.9781035324517
2024-07-21 23:11:41,968 - AirSimEnvLogger - INFO - Action: [0.13765788 0.13765788 0.13765788 0.13765788], Velocity: (0.13765787667066398, 0.13765787667066398, 0.13765787667066398), Duration: 1.0, Reward: 257.9781035324517, Done: False
2024-07-21 23:11:41,999 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:41,999 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:44,568 - AirSimEnvLogger - INFO - Predictive model loss: 1.4605631828308105
2024-07-21 23:11:50,290 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.40165406918069, Velocity: -0.0006020372928698565, Movement: 0.0030426495093911115, Collision: 0, Height: -1.0, Movement Penalty: -0.0007026698053186564, Smoothness: -0.0, Curiosity: 280.2509460449219, Exploration: 0.09399592763232055, Total: 260.370925471396
2024-07-21 23:11:50,416 - AirSimEnvLogger - INFO - Action: [0.00351335 0.00351335 0.00351335 0.00351335], Velocity: (0.003513349026593282, 0.003513349026593282, 0.003513349026593282), Duration: 1.0, Reward: 260.370925471396, Done: False
2024-07-21 23:11:50,477 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:50,477 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:11:53,047 - AirSimEnvLogger - INFO - Predictive model loss: 1.2477993965148926
2024-07-21 23:11:58,946 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.854219993723305, Velocity: -0.05254752973647608, Movement: 0.3536552535781651, Collision: 0, Height: -1.0, Movement Penalty: -0.08167318234147158, Smoothness: -0.0, Curiosity: 266.396728515625, Exploration: 0.13748734989009667, Total: 247.07755100373296
2024-07-21 23:11:59,009 - AirSimEnvLogger - INFO - Action: [0.40836591 0.40836591 0.40836591 0.40836591], Velocity: (0.4083659117073579, 0.4083659117073579, 0.4083659117073579), Duration: 1.0, Reward: 247.07755100373296, Done: False
2024-07-21 23:11:59,072 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:11:59,072 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:01,842 - AirSimEnvLogger - INFO - Predictive model loss: 0.56094890832901
2024-07-21 23:12:07,854 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.51473516484775, Velocity: -0.0299775296311804, Movement: 0.19439121310824473, Collision: 0, Height: -1.0, Movement Penalty: -0.044892727686457204, Smoothness: -0.0, Curiosity: 281.94903564453125, Exploration: 0.04759884932655985, Total: 261.9470188680574
2024-07-21 23:12:07,964 - AirSimEnvLogger - INFO - Action: [-0.22446364 -0.22446364 -0.22446364 -0.22446364], Velocity: (-0.224463638432286, -0.224463638432286, -0.224463638432286), Duration: 1.0, Reward: 261.9470188680574, Done: False
2024-07-21 23:12:08,026 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:08,026 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:10,575 - AirSimEnvLogger - INFO - Predictive model loss: 0.531193733215332
2024-07-21 23:12:16,367 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.95899168985493, Velocity: -0.045608820212460086, Movement: 0.38030212891927245, Collision: 0, Height: -1.0, Movement Penalty: -0.0878270146019719, Smoothness: -0.0, Curiosity: 295.0451354980469, Exploration: 0.3392994104280658, Total: 274.6689505813169
2024-07-21 23:12:16,479 - AirSimEnvLogger - INFO - Action: [-0.43913507 -0.43913507 -0.43913507 -0.43913507], Velocity: (-0.43913507300985943, -0.43913507300985943, -0.43913507300985943), Duration: 1.0, Reward: 274.6689505813169, Done: False
2024-07-21 23:12:16,542 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:16,542 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:19,402 - AirSimEnvLogger - INFO - Predictive model loss: 0.4375718832015991
2024-07-21 23:12:25,448 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.446421867208443, Velocity: -0.01049701476261076, Movement: 0.07392580511734251, Collision: 0, Height: -1.0, Movement Penalty: -0.017072433393823007, Smoothness: -0.0, Curiosity: 281.8009948730469, Exploration: 0.04341393552507941, Total: 261.8653219055525
2024-07-21 23:12:25,480 - AirSimEnvLogger - INFO - Action: [0.08536217 0.08536217 0.08536217 0.08536217], Velocity: (0.08536216696911503, 0.08536216696911503, 0.08536216696911503), Duration: 1.0, Reward: 261.8653219055525, Done: False
2024-07-21 23:12:25,589 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:25,589 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:28,291 - AirSimEnvLogger - INFO - Predictive model loss: 0.08336897194385529
2024-07-21 23:12:34,295 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.39724874007347, Velocity: -0.004960919383832674, Movement: 0.058725324055414474, Collision: 0, Height: -1.0, Movement Penalty: -0.013562032660656621, Smoothness: -0.0, Curiosity: 280.9412536621094, Exploration: 0.1788655382720402, Total: 261.0860884329375
2024-07-21 23:12:34,436 - AirSimEnvLogger - INFO - Action: [0.06781016 0.06781016 0.06781016 0.06781016], Velocity: (0.0678101633032831, 0.0678101633032831, 0.0678101633032831), Duration: 1.0, Reward: 261.0860884329375, Done: False
2024-07-21 23:12:34,468 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:34,468 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:36,632 - AirSimEnvLogger - INFO - Predictive model loss: 0.0760236456990242
2024-07-21 23:12:42,456 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.788529489521675, Velocity: -0.03426318446275714, Movement: 0.2069922793080828, Collision: 0, Height: -1.0, Movement Penalty: -0.04780281927147833, Smoothness: -0.0, Curiosity: 291.677734375, Exploration: 0.10477977343657703, Total: 271.41495542438986
2024-07-21 23:12:42,610 - AirSimEnvLogger - INFO - Action: [-0.2390141 -0.2390141 -0.2390141 -0.2390141], Velocity: (-0.23901409635739163, -0.23901409635739163, -0.23901409635739163), Duration: 1.0, Reward: 271.41495542438986, Done: False
2024-07-21 23:12:42,689 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:42,689 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:45,377 - AirSimEnvLogger - INFO - Predictive model loss: 0.1326577514410019
2024-07-21 23:12:51,200 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.06554703270541, Velocity: -0.04244423098396124, Movement: 0.3265161483065474, Collision: 0, Height: -1.0, Movement Penalty: -0.07540567444781798, Smoothness: -0.0, Curiosity: 273.1401672363281, Exploration: 0.10283337071438371, Total: 253.60203687932764
2024-07-21 23:12:51,309 - AirSimEnvLogger - INFO - Action: [0.37702837 0.37702837 0.37702837 0.37702837], Velocity: (0.37702837223908986, 0.37702837223908986, 0.37702837223908986), Duration: 1.0, Reward: 253.60203687932764, Done: False
2024-07-21 23:12:51,371 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:51,371 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:12:54,110 - AirSimEnvLogger - INFO - Predictive model loss: 0.6017627716064453
2024-07-21 23:12:59,739 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.590761961300476, Velocity: -0.013490086225362637, Movement: 0.13438870726480354, Collision: 0, Height: -1.0, Movement Penalty: -0.031035742526152058, Smoothness: -0.0, Curiosity: 285.86083984375, Exploration: 0.05981056646810015, Total: 265.78578185186865
2024-07-21 23:12:59,881 - AirSimEnvLogger - INFO - Action: [-0.15517871 -0.15517871 -0.15517871 -0.15517871], Velocity: (-0.15517871263076027, -0.15517871263076027, -0.15517871263076027), Duration: 1.0, Reward: 265.78578185186865, Done: False
2024-07-21 23:12:59,992 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:12:59,992 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:02,770 - AirSimEnvLogger - INFO - Predictive model loss: 0.4734130799770355
2024-07-21 23:13:08,692 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.909867167161796, Velocity: -0.022816447283582526, Movement: 0.3584640408264103, Collision: 0, Height: -1.0, Movement Penalty: -0.0827837241863716, Smoothness: -0.0, Curiosity: 268.30999755859375, Exploration: 0.04842457953193979, Total: 248.91777950361845
2024-07-21 23:13:08,817 - AirSimEnvLogger - INFO - Action: [0.41391862 0.41391862 0.41391862 0.41391862], Velocity: (0.413918620931858, 0.413918620931858, 0.413918620931858), Duration: 1.0, Reward: 248.91777950361845, Done: False
2024-07-21 23:13:08,895 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:13:08,895 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:11,573 - AirSimEnvLogger - INFO - Predictive model loss: 0.9017431139945984
2024-07-21 23:13:17,384 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.959685025769065, Velocity: -0.028782637145096875, Movement: 0.188608822075426, Collision: 0, Height: -1.0, Movement Penalty: -0.043557341678714175, Smoothness: -0.0, Curiosity: 267.8464050292969, Exploration: 0.19064878651795156, Total: 248.43231752496362
2024-07-21 23:13:17,527 - AirSimEnvLogger - INFO - Action: [0.21778671 0.21778671 0.21778671 0.21778671], Velocity: (0.21778670839357087, 0.21778670839357087, 0.21778670839357087), Duration: 1.0, Reward: 248.43231752496362, Done: False
2024-07-21 23:13:17,606 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:13:17,606 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:20,365 - AirSimEnvLogger - INFO - Predictive model loss: 0.6992478370666504
2024-07-21 23:13:26,416 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.68100819132337, Velocity: -0.05354753374348091, Movement: 0.31850074098919445, Collision: 0, Height: -1.0, Movement Penalty: -0.07355459541888268, Smoothness: -0.0, Curiosity: 286.081787109375, Exploration: 0.24508515060186237, Total: 265.95960655534475
2024-07-21 23:13:26,447 - AirSimEnvLogger - INFO - Action: [-0.36777298 -0.36777298 -0.36777298 -0.36777298], Velocity: (-0.36777297709441337, -0.36777297709441337, -0.36777297709441337), Duration: 1.0, Reward: 265.95960655534475, Done: False
2024-07-21 23:13:26,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:13:26,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:29,334 - AirSimEnvLogger - INFO - Predictive model loss: 0.15806612372398376
2024-07-21 23:13:35,593 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.11946607534806, Velocity: -0.07344055166257024, Movement: 0.4565494195992445, Collision: 0, Height: -1.0, Movement Penalty: -0.10543557212159649, Smoothness: -0.0, Curiosity: 299.8604736328125, Exploration: 0.35357504140207324, Total: 279.3261848945038
2024-07-21 23:13:35,734 - AirSimEnvLogger - INFO - Action: [-0.52717786 -0.52717786 -0.52717786 -0.52717786], Velocity: (-0.5271778606079824, -0.5271778606079824, -0.5271778606079824), Duration: 1.0, Reward: 279.3261848945038, Done: False
2024-07-21 23:13:35,813 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:13:35,813 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:38,532 - AirSimEnvLogger - INFO - Predictive model loss: 0.03010651282966137
2024-07-21 23:13:44,261 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.38792042927533, Velocity: -0.04405990563097736, Movement: 0.4581840211724221, Collision: 0, Height: -1.0, Movement Penalty: -0.10581306718491323, Smoothness: -0.0, Curiosity: 309.9654846191406, Exploration: 0.21576667175544725, Total: 289.1340237631776
2024-07-21 23:13:44,433 - AirSimEnvLogger - INFO - Action: [-0.52906534 -0.52906534 -0.52906534 -0.52906534], Velocity: (-0.5290653359245662, -0.5290653359245662, -0.5290653359245662), Duration: 1.0, Reward: 289.1340237631776, Done: False
2024-07-21 23:13:44,496 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:13:44,496 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:47,185 - AirSimEnvLogger - INFO - Predictive model loss: 0.14379416406154633
2024-07-21 23:13:53,006 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.076450804323898, Velocity: -0.020406889994163922, Movement: 0.11902406112915019, Collision: 0, Height: -1.0, Movement Penalty: -0.027487429493182936, Smoothness: -0.0, Curiosity: 302.7109069824219, Exploration: 0.09188452846535652, Total: 282.15646858445706
2024-07-21 23:13:53,133 - AirSimEnvLogger - INFO - Action: [-0.13743715 -0.13743715 -0.13743715 -0.13743715], Velocity: (-0.13743714746591468, -0.13743714746591468, -0.13743714746591468), Duration: 1.0, Reward: 282.15646858445706, Done: False
2024-07-21 23:13:53,212 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:13:53,212 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:13:55,850 - AirSimEnvLogger - INFO - Predictive model loss: 0.1861559897661209
2024-07-21 23:14:01,828 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.695211757715185, Velocity: -0.08865156764558756, Movement: 0.4784759908221825, Collision: 0, Height: -1.0, Movement Penalty: -0.110499296840784, Smoothness: -0.0, Curiosity: 323.47930908203125, Exploration: 0.11413764620767018, Total: 302.313220816474
2024-07-21 23:14:01,907 - AirSimEnvLogger - INFO - Action: [-0.55249648 -0.55249648 -0.55249648 -0.55249648], Velocity: (-0.55249648420392, -0.55249648420392, -0.55249648420392), Duration: 1.0, Reward: 302.313220816474, Done: False
2024-07-21 23:14:01,954 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:01,954 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:04,754 - AirSimEnvLogger - INFO - Predictive model loss: 0.5394012331962585
2024-07-21 23:14:09,802 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.29100592067128, Velocity: -0.06491167293980603, Movement: 0.6682273727520579, Collision: 0, Height: -1.0, Movement Penalty: -0.15432050141531084, Smoothness: -0.0, Curiosity: 345.1521911621094, Exploration: 0.36773432589151867, Total: 323.4556645505768
2024-07-21 23:14:09,897 - AirSimEnvLogger - INFO - Action: [-0.77160251 -0.77160251 -0.77160251 -0.77160251], Velocity: (-0.7716025070765541, -0.7716025070765541, -0.7716025070765541), Duration: 1.0, Reward: 323.4556645505768, Done: False
2024-07-21 23:14:09,960 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:09,960 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:12,629 - AirSimEnvLogger - INFO - Predictive model loss: 1.026004672050476
2024-07-21 23:14:18,201 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.830345164866873, Velocity: -0.06997015786277798, Movement: 0.7611015058873508, Collision: 0, Height: -1.0, Movement Penalty: -0.1757688637218766, Smoothness: -0.0, Curiosity: 366.5011291503906, Exploration: 0.3484596426331958, Total: 344.2626024665064
2024-07-21 23:14:18,327 - AirSimEnvLogger - INFO - Action: [-0.87884432 -0.87884432 -0.87884432 -0.87884432], Velocity: (-0.8788443186093831, -0.8788443186093831, -0.8788443186093831), Duration: 1.0, Reward: 344.2626024665064, Done: False
2024-07-21 23:14:18,374 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:18,374 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:21,077 - AirSimEnvLogger - INFO - Predictive model loss: 1.4471207857131958
2024-07-21 23:14:27,055 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.182636170225972, Velocity: -0.11694030329558737, Movement: 0.714884519383105, Collision: 0, Height: -1.0, Movement Penalty: -0.16509550788213279, Smoothness: -0.0, Curiosity: 382.1505126953125, Exploration: 0.2909085250779943, Total: 359.54062753429747
2024-07-21 23:14:27,164 - AirSimEnvLogger - INFO - Action: [-0.82547754 -0.82547754 -0.82547754 -0.82547754], Velocity: (-0.8254775394106638, -0.8254775394106638, -0.8254775394106638), Duration: 1.0, Reward: 359.54062753429747, Done: False
2024-07-21 23:14:27,243 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:27,243 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:29,911 - AirSimEnvLogger - INFO - Predictive model loss: 1.5585753917694092
2024-07-21 23:14:35,591 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.700808706397982, Velocity: -0.10720564650703182, Movement: 0.7902905804433996, Collision: 0, Height: -1.0, Movement Penalty: -0.18250979174280896, Smoothness: -0.0, Curiosity: 404.7948913574219, Exploration: 0.29163921029607204, Total: 381.66982489013196
2024-07-21 23:14:35,717 - AirSimEnvLogger - INFO - Action: [-0.91254896 -0.91254896 -0.91254896 -0.91254896], Velocity: (-0.9125489587140447, -0.9125489587140447, -0.9125489587140447), Duration: 1.0, Reward: 381.66982489013196, Done: False
2024-07-21 23:14:35,764 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:35,764 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:38,638 - AirSimEnvLogger - INFO - Predictive model loss: 1.7615455389022827
2024-07-21 23:14:44,469 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.694403006359344, Velocity: -0.05473577822009229, Movement: 0.5228085773289343, Collision: 0, Height: -1.0, Movement Penalty: -0.12073746914220221, Smoothness: -0.0, Curiosity: 407.51641845703125, Exploration: 0.21125537879898065, Total: 384.37795507622866
2024-07-21 23:14:44,625 - AirSimEnvLogger - INFO - Action: [-0.60368735 -0.60368735 -0.60368735 -0.60368735], Velocity: (-0.603687345711011, -0.603687345711011, -0.603687345711011), Duration: 1.0, Reward: 384.37795507622866, Done: False
2024-07-21 23:14:44,689 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:44,689 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:47,416 - AirSimEnvLogger - INFO - Predictive model loss: 1.1460713148117065
2024-07-21 23:14:53,479 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.197481498787077, Velocity: -0.06778776177489203, Movement: 0.6637927505528977, Collision: 0, Height: -1.0, Movement Penalty: -0.15329636928713505, Smoothness: -0.0, Curiosity: 429.9004211425781, Exploration: 0.19998924441007815, Total: 406.2584411942617
2024-07-21 23:14:53,605 - AirSimEnvLogger - INFO - Action: [-0.76648185 -0.76648185 -0.76648185 -0.76648185], Velocity: (-0.7664818464356752, -0.7664818464356752, -0.7664818464356752), Duration: 1.0, Reward: 406.2584411942617, Done: False
2024-07-21 23:14:53,683 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:14:53,683 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:14:56,350 - AirSimEnvLogger - INFO - Predictive model loss: 1.0387375354766846
2024-07-21 23:15:02,246 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.516195045093525, Velocity: -0.11806612698602743, Movement: 0.6294281492582725, Collision: 0, Height: -1.0, Movement Penalty: -0.14536020456391663, Smoothness: -0.0, Curiosity: 444.9590148925781, Exploration: 0.2684747507797219, Total: 421.00820226607954
2024-07-21 23:15:02,262 - AirSimEnvLogger - INFO - Action: [-0.72680102 -0.72680102 -0.72680102 -0.72680102], Velocity: (-0.7268010228195831, -0.7268010228195831, -0.7268010228195831), Duration: 1.0, Reward: 421.00820226607954, Done: False
2024-07-21 23:15:02,406 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:02,406 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:05,084 - AirSimEnvLogger - INFO - Predictive model loss: 0.599201500415802
2024-07-21 23:15:11,013 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.048622637355724, Velocity: -0.022228319984290316, Movement: 0.13318818004973723, Collision: 0, Height: -1.0, Movement Penalty: -0.030758492641836856, Smoothness: -0.0, Curiosity: 430.83984375, Exploration: 0.15422690797956481, Total: 407.3277375683672
2024-07-21 23:15:11,090 - AirSimEnvLogger - INFO - Action: [-0.15379246 -0.15379246 -0.15379246 -0.15379246], Velocity: (-0.15379246320918427, -0.15379246320918427, -0.15379246320918427), Duration: 1.0, Reward: 407.3277375683672, Done: False
2024-07-21 23:15:11,170 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:11,170 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:14,020 - AirSimEnvLogger - INFO - Predictive model loss: 0.06751574575901031
2024-07-21 23:15:19,767 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.297509209479106, Velocity: -0.03004103611780422, Movement: 0.3628005233476548, Collision: 0, Height: -1.0, Movement Penalty: -0.08378519192676225, Smoothness: -0.0, Curiosity: 408.4934387207031, Exploration: 0.4078742405876313, Total: 385.7956359602527
2024-07-21 23:15:19,876 - AirSimEnvLogger - INFO - Action: [0.41892596 0.41892596 0.41892596 0.41892596], Velocity: (0.4189259596338112, 0.4189259596338112, 0.4189259596338112), Duration: 1.0, Reward: 385.7956359602527, Done: False
2024-07-21 23:15:19,938 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:19,938 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:22,598 - AirSimEnvLogger - INFO - Predictive model loss: 0.688927412033081
2024-07-21 23:15:28,423 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.694588099215586, Velocity: -0.08057280838708743, Movement: 0.6125570749397715, Collision: 0, Height: -1.0, Movement Penalty: -0.14146399684419472, Smoothness: -0.0, Curiosity: 389.749267578125, Exploration: 0.421024249665153, Total: 367.6584837675632
2024-07-21 23:15:28,502 - AirSimEnvLogger - INFO - Action: [0.70731998 0.70731998 0.70731998 0.70731998], Velocity: (0.7073199842209736, 0.7073199842209736, 0.7073199842209736), Duration: 1.0, Reward: 367.6584837675632, Done: False
2024-07-21 23:15:28,581 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:28,581 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:31,355 - AirSimEnvLogger - INFO - Predictive model loss: 2.383434295654297
2024-07-21 23:15:37,099 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.393223432310513, Velocity: -0.008434212379427188, Movement: 0.06365142420669896, Collision: 0, Height: -1.0, Movement Penalty: -0.014699666760016284, Smoothness: -0.0, Curiosity: 406.8097229003906, Exploration: 0.08275974502664885, Total: 383.9362521542072
2024-07-21 23:15:37,270 - AirSimEnvLogger - INFO - Action: [-0.07349833 -0.07349833 -0.07349833 -0.07349833], Velocity: (-0.07349833380008142, -0.07349833380008142, -0.07349833380008142), Duration: 1.0, Reward: 383.9362521542072, Done: False
2024-07-21 23:15:37,332 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:37,332 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:40,218 - AirSimEnvLogger - INFO - Predictive model loss: 1.260074496269226
2024-07-21 23:15:46,027 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.108527500457953, Velocity: -0.05015673793328779, Movement: 0.46476083043332056, Collision: 0, Height: -1.0, Movement Penalty: -0.10733191622378867, Smoothness: -0.0, Curiosity: 430.955078125, Exploration: 0.4549710417592197, Total: 407.45757884850207
2024-07-21 23:15:46,120 - AirSimEnvLogger - INFO - Action: [-0.53665958 -0.53665958 -0.53665958 -0.53665958], Velocity: (-0.5366595811189433, -0.5366595811189433, -0.5366595811189433), Duration: 1.0, Reward: 407.45757884850207, Done: False
2024-07-21 23:15:46,183 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:46,183 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:49,022 - AirSimEnvLogger - INFO - Predictive model loss: 0.4299458861351013
2024-07-21 23:15:54,746 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.336731601963002, Velocity: -0.024202011506913448, Movement: 0.19698637544125783, Collision: 0, Height: -1.0, Movement Penalty: -0.04549205475507956, Smoothness: -0.0, Curiosity: 405.9554443359375, Exploration: 0.028863494028438203, Total: 383.1277632016346
2024-07-21 23:15:54,887 - AirSimEnvLogger - INFO - Action: [0.22746027 0.22746027 0.22746027 0.22746027], Velocity: (0.2274602737753978, 0.2274602737753978, 0.2274602737753978), Duration: 1.0, Reward: 383.1277632016346, Done: False
2024-07-21 23:15:54,934 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:15:54,934 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:15:57,671 - AirSimEnvLogger - INFO - Predictive model loss: 0.6610963344573975
2024-07-21 23:16:03,189 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.793642878763325, Velocity: -0.037819588134225285, Movement: 0.45793396018143956, Collision: 0, Height: -1.0, Movement Penalty: -0.1057553180727302, Smoothness: -0.0, Curiosity: 389.54583740234375, Exploration: 0.411654081834939, Total: 367.35432609897805
2024-07-21 23:16:03,329 - AirSimEnvLogger - INFO - Action: [0.52877659 0.52877659 0.52877659 0.52877659], Velocity: (0.528776590363651, 0.528776590363651, 0.528776590363651), Duration: 1.0, Reward: 367.35432609897805, Done: False
2024-07-21 23:16:03,408 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:03,408 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:06,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.8265080451965332
2024-07-21 23:16:11,900 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.524299026848897, Velocity: -0.026287318102504266, Movement: 0.1826379495824852, Collision: 0, Height: -1.0, Movement Penalty: -0.04217842774227565, Smoothness: -0.0, Curiosity: 409.9627685546875, Exploration: 0.07579574361496659, Total: 386.9577539161302
2024-07-21 23:16:12,011 - AirSimEnvLogger - INFO - Action: [-0.21089214 -0.21089214 -0.21089214 -0.21089214], Velocity: (-0.21089213871137824, -0.21089213871137824, -0.21089213871137824), Duration: 1.0, Reward: 386.9577539161302, Done: False
2024-07-21 23:16:12,121 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:12,121 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:14,980 - AirSimEnvLogger - INFO - Predictive model loss: 0.06845429539680481
2024-07-21 23:16:20,937 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.851920030371108, Velocity: -0.053111110105144486, Movement: 0.3335946329557275, Collision: 0, Height: -1.0, Movement Penalty: -0.07704038045488147, Smoothness: -0.0, Curiosity: 388.515869140625, Exploration: 0.04696835525390211, Total: 366.17762377933815
2024-07-21 23:16:21,126 - AirSimEnvLogger - INFO - Action: [0.3852019 0.3852019 0.3852019 0.3852019], Velocity: (0.3852019022744073, 0.3852019022744073, 0.3852019022744073), Duration: 1.0, Reward: 366.17762377933815, Done: False
2024-07-21 23:16:21,189 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:21,189 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:23,950 - AirSimEnvLogger - INFO - Predictive model loss: 0.11465978622436523
2024-07-21 23:16:30,028 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.844555512786965, Velocity: -0.03690418143818517, Movement: 0.2044244117976015, Collision: 0, Height: -1.0, Movement Penalty: -0.047209795672110456, Smoothness: -0.0, Curiosity: 386.6591796875, Exploration: 0.20648438446991765, Total: 364.3634396848129
2024-07-21 23:16:30,153 - AirSimEnvLogger - INFO - Action: [0.23604898 0.23604898 0.23604898 0.23604898], Velocity: (0.23604897836055228, 0.23604897836055228, 0.23604897836055228), Duration: 1.0, Reward: 364.3634396848129, Done: False
2024-07-21 23:16:30,277 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:30,277 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:33,014 - AirSimEnvLogger - INFO - Predictive model loss: 0.08935748785734177
2024-07-21 23:16:38,851 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.57669686747734, Velocity: -0.03224127565531972, Movement: 0.3256250581237366, Collision: 0, Height: -1.0, Movement Penalty: -0.07519988598505074, Smoothness: -0.0, Curiosity: 409.21368408203125, Exploration: 0.23679323422743048, Total: 386.19621309600547
2024-07-21 23:16:38,930 - AirSimEnvLogger - INFO - Action: [-0.37599943 -0.37599943 -0.37599943 -0.37599943], Velocity: (-0.3759994299252537, -0.3759994299252537, -0.3759994299252537), Duration: 1.0, Reward: 386.19621309600547, Done: False
2024-07-21 23:16:39,008 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:39,008 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:41,736 - AirSimEnvLogger - INFO - Predictive model loss: 0.41383183002471924
2024-07-21 23:16:47,795 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.80248838824015, Velocity: -0.06213040323017631, Movement: 0.33088940603998235, Collision: 0, Height: -1.0, Movement Penalty: -0.07641563505967168, Smoothness: -0.0, Curiosity: 417.8070983886719, Exploration: 0.29653307532046835, Total: 394.57471626674817
2024-07-21 23:16:47,843 - AirSimEnvLogger - INFO - Action: [-0.38207818 -0.38207818 -0.38207818 -0.38207818], Velocity: (-0.38207817529835836, -0.38207817529835836, -0.38207817529835836), Duration: 1.0, Reward: 394.57471626674817, Done: False
2024-07-21 23:16:47,950 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:47,950 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:50,727 - AirSimEnvLogger - INFO - Predictive model loss: 0.7419775128364563
2024-07-21 23:16:56,550 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.582756313838292, Velocity: -0.008743139740655301, Movement: 0.07741973151926393, Collision: 0, Height: -1.0, Movement Penalty: -0.017879321133294237, Smoothness: -0.0, Curiosity: 411.72698974609375, Exploration: 0.04788659670674203, Total: 388.65627213737
2024-07-21 23:16:56,660 - AirSimEnvLogger - INFO - Action: [-0.08939661 -0.08939661 -0.08939661 -0.08939661], Velocity: (-0.08939660566647117, -0.08939660566647117, -0.08939660566647117), Duration: 1.0, Reward: 388.65627213737, Done: False
2024-07-21 23:16:56,721 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:16:56,721 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:16:59,326 - AirSimEnvLogger - INFO - Predictive model loss: 0.7515255808830261
2024-07-21 23:17:05,392 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.915941686520767, Velocity: -0.0405513110087042, Movement: 0.36125043194506157, Collision: 0, Height: -1.0, Movement Penalty: -0.08342721365133995, Smoothness: -0.0, Curiosity: 391.9075927734375, Exploration: 0.28675869799343257, Total: 369.562411910001
2024-07-21 23:17:05,578 - AirSimEnvLogger - INFO - Action: [0.41713607 0.41713607 0.41713607 0.41713607], Velocity: (0.41713606825669974, 0.41713606825669974, 0.41713606825669974), Duration: 1.0, Reward: 369.562411910001, Done: False
2024-07-21 23:17:05,609 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:05,609 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:17:08,310 - AirSimEnvLogger - INFO - Predictive model loss: 0.38480186462402344
2024-07-21 23:17:14,257 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.37255011816224, Velocity: -0.007303295011707903, Movement: 0.07633076790656079, Collision: 0, Height: -1.0, Movement Penalty: -0.017627835759321486, Smoothness: -0.0, Curiosity: 404.6260070800781, Exploration: 0.06385273377175182, Total: 381.7692851508095
2024-07-21 23:17:14,382 - AirSimEnvLogger - INFO - Action: [-0.08813918 -0.08813918 -0.08813918 -0.08813918], Velocity: (-0.08813917879660743, -0.08813917879660743, -0.08813917879660743), Duration: 1.0, Reward: 381.7692851508095, Done: False
2024-07-21 23:17:14,429 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:14,429 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:17:17,144 - AirSimEnvLogger - INFO - Predictive model loss: 0.5036105513572693
2024-07-21 23:17:22,892 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.721429969976978, Velocity: -0.06590670570206482, Movement: 0.38367154258779324, Collision: 0, Height: -1.0, Movement Penalty: -0.08860514735738456, Smoothness: -0.0, Curiosity: 384.2544250488281, Exploration: 0.06074508352596582, Total: 362.0497872197755
2024-07-21 23:17:22,970 - AirSimEnvLogger - INFO - Action: [0.44302574 0.44302574 0.44302574 0.44302574], Velocity: (0.44302573678692275, 0.44302573678692275, 0.44302573678692275), Duration: 1.0, Reward: 362.0497872197755, Done: False
2024-07-21 23:17:23,017 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:23,017 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:17:25,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.122344471514225
2024-07-21 23:17:31,513 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.13658631493488, Velocity: -0.07560979819410905, Movement: 0.6233568349969694, Collision: 0, Height: -1.0, Movement Penalty: -0.14395809459467737, Smoothness: -0.0, Curiosity: 366.18096923828125, Exploration: 0.37556577232623073, Total: 344.6384929807642
2024-07-21 23:17:31,576 - AirSimEnvLogger - INFO - Action: [0.71979047 0.71979047 0.71979047 0.71979047], Velocity: (0.7197904729733868, 0.7197904729733868, 0.7197904729733868), Duration: 1.0, Reward: 344.6384929807642, Done: False
2024-07-21 23:17:31,655 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:31,655 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:17:34,455 - AirSimEnvLogger - INFO - Predictive model loss: 0.047813091427087784
2024-07-21 23:17:40,790 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.72383387251196, Velocity: -0.08996247748285342, Movement: 0.6735704795162109, Collision: 0, Height: -1.0, Movement Penalty: -0.15555443906674787, Smoothness: -0.0, Curiosity: 351.1439208984375, Exploration: 0.3003218280318701, Total: 329.99668744622346
2024-07-21 23:17:40,914 - AirSimEnvLogger - INFO - Action: [0.7777722 0.7777722 0.7777722 0.7777722], Velocity: (0.7777721953337393, 0.7777721953337393, 0.7777721953337393), Duration: 1.0, Reward: 329.99668744622346, Done: False
2024-07-21 23:17:40,992 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:40,992 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:17:43,698 - AirSimEnvLogger - INFO - Predictive model loss: 0.2683747708797455
2024-07-21 23:17:49,646 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.531318705693426, Velocity: -0.013920031308974302, Movement: 0.08210769256759666, Collision: 0, Height: -1.0, Movement Penalty: -0.018961959362576386, Smoothness: -0.0, Curiosity: 368.5774841308594, Exploration: 0.1938489842556025, Total: 346.591372785692
2024-07-21 23:17:49,803 - AirSimEnvLogger - INFO - Action: [-0.0948098 -0.0948098 -0.0948098 -0.0948098], Velocity: (-0.09480979681288193, -0.09480979681288193, -0.09480979681288193), Duration: 1.0, Reward: 346.591372785692, Done: False
2024-07-21 23:17:49,867 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:49,867 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:17:52,504 - AirSimEnvLogger - INFO - Predictive model loss: 0.07459054887294769
2024-07-21 23:17:58,463 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.00228939583289, Velocity: -0.05241258610195344, Movement: 0.316690772968819, Collision: 0, Height: -1.0, Movement Penalty: -0.07313660120936732, Smoothness: -0.0, Curiosity: 352.018798828125, Exploration: 0.13710457251424413, Total: 330.55057063297113
2024-07-21 23:17:58,572 - AirSimEnvLogger - INFO - Action: [0.36568301 0.36568301 0.36568301 0.36568301], Velocity: (0.3656830060468366, 0.3656830060468366, 0.3656830060468366), Duration: 1.0, Reward: 330.55057063297113, Done: False
2024-07-21 23:17:58,634 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:17:58,634 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:01,196 - AirSimEnvLogger - INFO - Predictive model loss: 0.2193814218044281
2024-07-21 23:18:07,213 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.483710147721325, Velocity: -0.011285173959536586, Movement: 0.10863444009437631, Collision: 0, Height: -1.0, Movement Penalty: -0.025088049292700976, Smoothness: -0.0, Curiosity: 364.7934265136719, Exploration: 0.03976695092865867, Total: 342.82039904423505
2024-07-21 23:18:07,291 - AirSimEnvLogger - INFO - Action: [-0.12544025 -0.12544025 -0.12544025 -0.12544025], Velocity: (-0.12544024646350488, -0.12544024646350488, -0.12544024646350488), Duration: 1.0, Reward: 342.82039904423505, Done: False
2024-07-21 23:18:07,353 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:18:07,353 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:09,996 - AirSimEnvLogger - INFO - Predictive model loss: 0.10872875899076462
2024-07-21 23:18:16,156 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.9177213411046, Velocity: -0.05162111005852209, Movement: 0.32399755337419656, Collision: 0, Height: -1.0, Movement Penalty: -0.074824029862949, Smoothness: -0.0, Curiosity: 378.8966064453125, Exploration: 0.28650374013560465, Total: 356.54756649597414
2024-07-21 23:18:16,330 - AirSimEnvLogger - INFO - Action: [-0.37412015 -0.37412015 -0.37412015 -0.37412015], Velocity: (-0.374120149314745, -0.374120149314745, -0.374120149314745), Duration: 1.0, Reward: 356.54756649597414, Done: False
2024-07-21 23:18:16,408 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:18:16,408 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:19,085 - AirSimEnvLogger - INFO - Predictive model loss: 0.06736423075199127
2024-07-21 23:18:25,019 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.559486769535987, Velocity: -0.06377174680184229, Movement: 0.594466457925959, Collision: 0, Height: -1.0, Movement Penalty: -0.1372861478031023, Smoothness: -0.0, Curiosity: 402.6934814453125, Exploration: 0.32517290462825993, Total: 379.7169894993689
2024-07-21 23:18:25,052 - AirSimEnvLogger - INFO - Action: [-0.68643074 -0.68643074 -0.68643074 -0.68643074], Velocity: (-0.6864307390155115, -0.6864307390155115, -0.6864307390155115), Duration: 1.0, Reward: 379.7169894993689, Done: False
2024-07-21 23:18:25,113 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:18:25,113 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:27,767 - AirSimEnvLogger - INFO - Predictive model loss: 0.18609443306922913
2024-07-21 23:18:33,772 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.13775404706914, Velocity: -0.09813790353790205, Movement: 0.7285955638211333, Collision: 0, Height: -1.0, Movement Penalty: -0.16826193796099942, Smoothness: -0.0, Curiosity: 426.47442626953125, Exploration: 0.36811478341845855, Total: 402.929397242249
2024-07-21 23:18:33,930 - AirSimEnvLogger - INFO - Action: [-0.84130969 -0.84130969 -0.84130969 -0.84130969], Velocity: (-0.841309689804997, -0.841309689804997, -0.841309689804997), Duration: 1.0, Reward: 402.929397242249, Done: False
2024-07-21 23:18:33,993 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:18:33,993 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:36,663 - AirSimEnvLogger - INFO - Predictive model loss: 0.43836602568626404
2024-07-21 23:18:42,644 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.25417918694636, Velocity: -0.009684231667207666, Movement: 0.06287669239855259, Collision: 0, Height: -1.0, Movement Penalty: -0.014520750112823055, Smoothness: -0.0, Curiosity: 398.086181640625, Exploration: 0.17027340627752266, Total: 375.3717392758352
2024-07-21 23:18:42,756 - AirSimEnvLogger - INFO - Action: [0.07260375 0.07260375 0.07260375 0.07260375], Velocity: (0.07260375056411528, 0.07260375056411528, 0.07260375056411528), Duration: 1.0, Reward: 375.3717392758352, Done: False
2024-07-21 23:18:42,817 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:18:42,817 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:45,470 - AirSimEnvLogger - INFO - Predictive model loss: 0.11692933738231659
2024-07-21 23:18:51,274 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.597793746427538, Velocity: -0.06202086529376808, Movement: 0.42670156057278036, Collision: 0, Height: -1.0, Movement Penalty: -0.09854250434413125, Smoothness: -0.0, Curiosity: 379.6982116699219, Exploration: 0.49779198565080596, Total: 357.71917495997275
2024-07-21 23:18:51,432 - AirSimEnvLogger - INFO - Action: [0.49271252 0.49271252 0.49271252 0.49271252], Velocity: (0.49271252172065627, 0.49271252172065627, 0.49271252172065627), Duration: 1.0, Reward: 357.71917495997275, Done: False
2024-07-21 23:18:51,479 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:18:51,479 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:18:54,212 - AirSimEnvLogger - INFO - Predictive model loss: 0.013541066087782383
2024-07-21 23:19:00,069 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.01633815267223, Velocity: -0.10536780325721937, Movement: 0.6462259171796797, Collision: 0, Height: -1.0, Movement Penalty: -0.14923948289640035, Smoothness: -0.0, Curiosity: 361.82440185546875, Exploration: 0.3797332945958673, Total: 340.40071746718485
2024-07-21 23:19:00,162 - AirSimEnvLogger - INFO - Action: [0.74619741 0.74619741 0.74619741 0.74619741], Velocity: (0.7461974144820017, 0.7461974144820017, 0.7461974144820017), Duration: 1.0, Reward: 340.40071746718485, Done: False
2024-07-21 23:19:00,225 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:00,225 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:03,006 - AirSimEnvLogger - INFO - Predictive model loss: 0.1625085026025772
2024-07-21 23:19:08,819 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.68293759081502, Velocity: -0.0048507657046173, Movement: 0.019335439354943244, Collision: 0, Height: -1.0, Movement Penalty: -0.004465328446590466, Smoothness: -0.0, Curiosity: 376.2808532714844, Exploration: 0.09649484889888174, Total: 354.120098716632
2024-07-21 23:19:08,945 - AirSimEnvLogger - INFO - Action: [-0.02232664 -0.02232664 -0.02232664 -0.02232664], Velocity: (-0.022326642232952332, -0.022326642232952332, -0.022326642232952332), Duration: 1.0, Reward: 354.120098716632, Done: False
2024-07-21 23:19:08,992 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:08,992 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:11,253 - AirSimEnvLogger - INFO - Predictive model loss: 0.02291647531092167
2024-07-21 23:19:17,015 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.877203005897755, Velocity: -0.015889371181474855, Movement: 0.10862397894142477, Collision: 0, Height: -1.0, Movement Penalty: -0.025085633392911935, Smoothness: -0.0, Curiosity: 381.2854309082031, Exploration: 0.292178540786645, Total: 358.97650456992346
2024-07-21 23:19:17,141 - AirSimEnvLogger - INFO - Action: [-0.12542817 -0.12542817 -0.12542817 -0.12542817], Velocity: (-0.12542816696455966, -0.12542816696455966, -0.12542816696455966), Duration: 1.0, Reward: 358.97650456992346, Done: False
2024-07-21 23:19:17,233 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:17,233 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:19,811 - AirSimEnvLogger - INFO - Predictive model loss: 0.01437608152627945
2024-07-21 23:19:25,638 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.526702640061057, Velocity: -0.07277837110640507, Movement: 0.46605319366443326, Collision: 0, Height: -1.0, Movement Penalty: -0.10763037472753814, Smoothness: -0.0, Curiosity: 403.778076171875, Exploration: 0.2612233387919521, Total: 380.81560932237466
2024-07-21 23:19:25,749 - AirSimEnvLogger - INFO - Action: [-0.53815187 -0.53815187 -0.53815187 -0.53815187], Velocity: (-0.5381518736376907, -0.5381518736376907, -0.5381518736376907), Duration: 1.0, Reward: 380.81560932237466, Done: False
2024-07-21 23:19:25,811 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:25,811 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:28,688 - AirSimEnvLogger - INFO - Predictive model loss: 0.06124962493777275
2024-07-21 23:19:34,490 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.12421868030018, Velocity: -0.004161429916941386, Movement: 0.041034961029825553, Collision: 0, Height: -1.0, Movement Penalty: -0.009476618318702236, Smoothness: -0.0, Curiosity: 391.4930419921875, Exploration: 0.07843989917733452, Total: 368.88745493109127
2024-07-21 23:19:34,630 - AirSimEnvLogger - INFO - Action: [-0.04738309 -0.04738309 -0.04738309 -0.04738309], Velocity: (-0.04738309159351117, -0.04738309159351117, -0.04738309159351117), Duration: 1.0, Reward: 368.88745493109127, Done: False
2024-07-21 23:19:34,676 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:34,676 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:37,533 - AirSimEnvLogger - INFO - Predictive model loss: 0.01747380755841732
2024-07-21 23:19:43,384 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.06421604487408, Velocity: -0.0011931802712123304, Movement: 0.00739323277677906, Collision: 0, Height: -1.0, Movement Penalty: -0.001707393973541982, Smoothness: -0.0, Curiosity: 390.7008972167969, Exploration: 0.1607047764429618, Total: 368.1737053080795
2024-07-21 23:19:43,415 - AirSimEnvLogger - INFO - Action: [-0.00853697 -0.00853697 -0.00853697 -0.00853697], Velocity: (-0.00853696986770991, -0.00853696986770991, -0.00853696986770991), Duration: 1.0, Reward: 368.1737053080795, Done: False
2024-07-21 23:19:43,524 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:43,524 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:46,034 - AirSimEnvLogger - INFO - Predictive model loss: 0.011890683323144913
2024-07-21 23:19:51,913 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.18321874533097, Velocity: -0.006836950735154234, Movement: 0.0848569460375006, Collision: 0, Height: -1.0, Movement Penalty: -0.01959687225494421, Smoothness: -0.0, Curiosity: 394.8937683105469, Exploration: 0.028370100957841614, Total: 372.21847252968047
2024-07-21 23:19:52,054 - AirSimEnvLogger - INFO - Action: [-0.09798436 -0.09798436 -0.09798436 -0.09798436], Velocity: (-0.09798436127472104, -0.09798436127472104, -0.09798436127472104), Duration: 1.0, Reward: 372.21847252968047, Done: False
2024-07-21 23:19:52,118 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:19:52,118 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:19:54,798 - AirSimEnvLogger - INFO - Predictive model loss: 0.011561265215277672
2024-07-21 23:20:00,578 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.77324057670234, Velocity: -0.04100318938428274, Movement: 0.4201120161500263, Collision: 0, Height: -1.0, Movement Penalty: -0.09702071424560565, Smoothness: -0.0, Curiosity: 415.73602294921875, Exploration: 0.21540739738394088, Total: 392.5185310801145
2024-07-21 23:20:00,672 - AirSimEnvLogger - INFO - Action: [-0.48510357 -0.48510357 -0.48510357 -0.48510357], Velocity: (-0.4851035712280282, -0.4851035712280282, -0.4851035712280282), Duration: 1.0, Reward: 392.5185310801145, Done: False
2024-07-21 23:20:00,703 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:00,703 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:03,373 - AirSimEnvLogger - INFO - Predictive model loss: 0.07966195046901703
2024-07-21 23:20:09,215 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.389595597233033, Velocity: -0.03132106463360965, Movement: 0.6377297684824996, Collision: 0, Height: -1.0, Movement Penalty: -0.14727738140144356, Smoothness: -0.0, Curiosity: 440.23870849609375, Exploration: 0.3639000396573737, Total: 416.44532127840534
2024-07-21 23:20:09,341 - AirSimEnvLogger - INFO - Action: [-0.73638691 -0.73638691 -0.73638691 -0.73638691], Velocity: (-0.7363869070072178, -0.7363869070072178, -0.7363869070072178), Duration: 1.0, Reward: 416.44532127840534, Done: False
2024-07-21 23:20:09,419 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:09,419 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:12,067 - AirSimEnvLogger - INFO - Predictive model loss: 0.27175238728523254
2024-07-21 23:20:17,954 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.95416343994155, Velocity: -0.12775634175678927, Movement: 0.7501283426433248, Collision: 0, Height: -1.0, Movement Penalty: -0.1732347202207566, Smoothness: -0.0, Curiosity: 464.65667724609375, Exploration: 0.35780084585992, Total: 440.290432974384
2024-07-21 23:20:18,097 - AirSimEnvLogger - INFO - Action: [-0.8661736 -0.8661736 -0.8661736 -0.8661736], Velocity: (-0.8661736011037828, -0.8661736011037828, -0.8661736011037828), Duration: 1.0, Reward: 440.290432974384, Done: False
2024-07-21 23:20:18,160 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:18,160 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:21,022 - AirSimEnvLogger - INFO - Predictive model loss: 0.4527837038040161
2024-07-21 23:20:26,836 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.73584852301085, Velocity: -0.03955591319657609, Movement: 0.3552709118070792, Collision: 0, Height: -1.0, Movement Penalty: -0.0820463026268244, Smoothness: -0.0, Curiosity: 458.956298828125, Exploration: 0.15690515425293458, Total: 434.76129767561224
2024-07-21 23:20:26,869 - AirSimEnvLogger - INFO - Action: [-0.41023151 -0.41023151 -0.41023151 -0.41023151], Velocity: (-0.410231513134122, -0.410231513134122, -0.410231513134122), Duration: 1.0, Reward: 434.76129767561224, Done: False
2024-07-21 23:20:26,976 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:26,976 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:29,835 - AirSimEnvLogger - INFO - Predictive model loss: 0.2894749343395233
2024-07-21 23:20:35,839 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.93384794863461, Velocity: -0.03055735556640573, Movement: 0.24708731589557165, Collision: 0, Height: -1.0, Movement Penalty: -0.05706237133826016, Smoothness: -0.0, Curiosity: 434.1263427734375, Exploration: 0.3633427123208927, Total: 410.7790689516363
2024-07-21 23:20:35,950 - AirSimEnvLogger - INFO - Action: [0.28531186 0.28531186 0.28531186 0.28531186], Velocity: (0.28531185669130077, 0.28531185669130077, 0.28531185669130077), Duration: 1.0, Reward: 410.7790689516363, Done: False
2024-07-21 23:20:36,013 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:36,013 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:38,832 - AirSimEnvLogger - INFO - Predictive model loss: 0.02342555858194828
2024-07-21 23:20:44,095 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.5853048943964, Velocity: -0.03315589295187146, Movement: 0.2780314412058839, Collision: 0, Height: -1.0, Movement Penalty: -0.06420861096935869, Smoothness: -0.0, Curiosity: 456.6602783203125, Exploration: 0.07711385028631643, Total: 432.5962141184709
2024-07-21 23:20:44,187 - AirSimEnvLogger - INFO - Action: [-0.32104305 -0.32104305 -0.32104305 -0.32104305], Velocity: (-0.3210430548467934, -0.3210430548467934, -0.3210430548467934), Duration: 1.0, Reward: 432.5962141184709, Done: False
2024-07-21 23:20:44,267 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:44,267 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:46,843 - AirSimEnvLogger - INFO - Predictive model loss: 0.03135089948773384
2024-07-21 23:20:52,673 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.23758170073112, Velocity: -0.09356086452332942, Movement: 0.5689976048920975, Collision: 0, Height: -1.0, Movement Penalty: -0.13140436814108192, Smoothness: -0.0, Curiosity: 482.37646484375, Exploration: 0.3998065420401758, Total: 457.7354400418501
2024-07-21 23:20:52,800 - AirSimEnvLogger - INFO - Action: [-0.65702184 -0.65702184 -0.65702184 -0.65702184], Velocity: (-0.6570218407054096, -0.6570218407054096, -0.6570218407054096), Duration: 1.0, Reward: 457.7354400418501, Done: False
2024-07-21 23:20:52,863 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:20:52,863 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:20:55,413 - AirSimEnvLogger - INFO - Predictive model loss: 0.07642916589975357
2024-07-21 23:21:01,042 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.81121852366411, Velocity: -0.11987235001307252, Movement: 0.707006055094478, Collision: 0, Height: -1.0, Movement Penalty: -0.1632760544909969, Smoothness: -0.0, Curiosity: 507.73468017578125, Exploration: 0.3695483879586242, Total: 482.5138133668141
2024-07-21 23:21:01,150 - AirSimEnvLogger - INFO - Action: [-0.81638027 -0.81638027 -0.81638027 -0.81638027], Velocity: (-0.8163802724549845, -0.8163802724549845, -0.8163802724549845), Duration: 1.0, Reward: 482.5138133668141, Done: False
2024-07-21 23:21:01,197 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:01,197 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:04,008 - AirSimEnvLogger - INFO - Predictive model loss: 0.17455768585205078
2024-07-21 23:21:10,099 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.493846946605757, Velocity: -0.02885193465982327, Movement: 0.26086707988749486, Collision: 0, Height: -1.0, Movement Penalty: -0.060244671518302706, Smoothness: -0.0, Curiosity: 497.4896240234375, Exploration: 0.13002549657307694, Total: 472.52919680300266
2024-07-21 23:21:10,208 - AirSimEnvLogger - INFO - Action: [-0.30122336 -0.30122336 -0.30122336 -0.30122336], Velocity: (-0.30122335759151353, -0.30122335759151353, -0.30122335759151353), Duration: 1.0, Reward: 472.52919680300266, Done: False
2024-07-21 23:21:10,287 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:10,287 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:12,983 - AirSimEnvLogger - INFO - Predictive model loss: 0.005848465953022242
2024-07-21 23:21:19,074 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.092615164356253, Velocity: -0.09822771115884803, Movement: 0.5576442250771628, Collision: 0, Height: -1.0, Movement Penalty: -0.1287824173841361, Smoothness: -0.0, Curiosity: 524.511474609375, Exploration: 0.13114564604369985, Total: 498.9528791553117
2024-07-21 23:21:19,137 - AirSimEnvLogger - INFO - Action: [-0.64391209 -0.64391209 -0.64391209 -0.64391209], Velocity: (-0.6439120869206805, -0.6439120869206805, -0.6439120869206805), Duration: 1.0, Reward: 498.9528791553117, Done: False
2024-07-21 23:21:19,199 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:19,199 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:21,853 - AirSimEnvLogger - INFO - Predictive model loss: 0.022569365799427032
2024-07-21 23:21:27,817 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.62993299338951, Velocity: -0.07196408543481468, Movement: 0.681253598977334, Collision: 0, Height: -1.0, Movement Penalty: -0.15732877950238605, Smoothness: -0.0, Curiosity: 549.5263671875, Exploration: 0.3439669660992795, Total: 523.4850613018755
2024-07-21 23:21:27,927 - AirSimEnvLogger - INFO - Action: [-0.7866439 -0.7866439 -0.7866439 -0.7866439], Velocity: (-0.7866438975119302, -0.7866438975119302, -0.7866438975119302), Duration: 1.0, Reward: 523.4850613018755, Done: False
2024-07-21 23:21:27,974 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:27,974 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:30,555 - AirSimEnvLogger - INFO - Predictive model loss: 0.06341627985239029
2024-07-21 23:21:36,419 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.93635619323047, Velocity: -0.11721147592755557, Movement: 0.6259717082819949, Collision: 0, Height: -1.0, Movement Penalty: -0.14456197371267984, Smoothness: -0.0, Curiosity: 565.3969116210938, Exploration: 0.2631344938573672, Total: 539.0247102661609
2024-07-21 23:21:36,450 - AirSimEnvLogger - INFO - Action: [-0.72280987 -0.72280987 -0.72280987 -0.72280987], Velocity: (-0.7228098685633992, -0.7228098685633992, -0.7228098685633992), Duration: 1.0, Reward: 539.0247102661609, Done: False
2024-07-21 23:21:36,482 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:36,482 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:39,259 - AirSimEnvLogger - INFO - Predictive model loss: 0.023322055116295815
2024-07-21 23:21:45,248 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.44467367105302, Velocity: -0.06360922969276646, Movement: 0.7187406064009889, Collision: 0, Height: -1.0, Movement Penalty: -0.16598603303325032, Smoothness: -0.0, Curiosity: 591.3128662109375, Exploration: 0.26094262394254786, Total: 564.4394790888015
2024-07-21 23:21:45,421 - AirSimEnvLogger - INFO - Action: [-0.82993017 -0.82993017 -0.82993017 -0.82993017], Velocity: (-0.8299301651662516, -0.8299301651662516, -0.8299301651662516), Duration: 1.0, Reward: 564.4394790888015, Done: False
2024-07-21 23:21:45,483 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:45,483 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:48,137 - AirSimEnvLogger - INFO - Predictive model loss: 0.06014996021986008
2024-07-21 23:21:53,810 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.43496233301885, Velocity: -0.07763527334427558, Movement: 0.4603069177337509, Collision: 0, Height: -1.0, Movement Penalty: -0.1063033291453712, Smoothness: -0.0, Curiosity: 593.8222045898438, Exploration: 0.18921139314351348, Total: 566.9342886538235
2024-07-21 23:21:53,872 - AirSimEnvLogger - INFO - Action: [-0.53151665 -0.53151665 -0.53151665 -0.53151665], Velocity: (-0.531516645726856, -0.531516645726856, -0.531516645726856), Duration: 1.0, Reward: 566.9342886538235, Done: False
2024-07-21 23:21:53,904 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:21:53,904 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:21:56,570 - AirSimEnvLogger - INFO - Predictive model loss: 0.0026658831629902124
2024-07-21 23:22:02,546 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.010211583399602, Velocity: -0.06008603048331612, Movement: 0.6630791050852879, Collision: 0, Height: -1.0, Movement Penalty: -0.15313155992600286, Smoothness: -0.0, Curiosity: 622.9755859375, Exploration: 0.197858333691491, Total: 595.5211384625501
2024-07-21 23:22:02,671 - AirSimEnvLogger - INFO - Action: [-0.7656578 -0.7656578 -0.7656578 -0.7656578], Velocity: (-0.7656577996300142, -0.7656577996300142, -0.7656577996300142), Duration: 1.0, Reward: 595.5211384625501, Done: False
2024-07-21 23:22:02,750 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:02,750 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:05,453 - AirSimEnvLogger - INFO - Predictive model loss: 0.013784464448690414
2024-07-21 23:22:11,257 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.385876395208907, Velocity: -0.10868940470592518, Movement: 0.6522387951087796, Collision: 0, Height: -1.0, Movement Penalty: -0.15062809757278842, Smoothness: -0.0, Curiosity: 642.8934326171875, Exploration: 0.2937913984899589, Total: 615.0802586856895
2024-07-21 23:22:11,365 - AirSimEnvLogger - INFO - Action: [-0.75314049 -0.75314049 -0.75314049 -0.75314049], Velocity: (-0.753140487863942, -0.753140487863942, -0.753140487863942), Duration: 1.0, Reward: 615.0802586856895, Done: False
2024-07-21 23:22:11,427 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:11,427 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:14,133 - AirSimEnvLogger - INFO - Predictive model loss: 0.013120895251631737
2024-07-21 23:22:20,134 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.81405713802517, Velocity: -0.06749940441563214, Movement: 0.6821995319390927, Collision: 0, Height: -1.0, Movement Penalty: -0.15754723336242876, Smoothness: -0.0, Curiosity: 666.187744140625, Exploration: 0.25609064425595923, Total: 637.9425722281273
2024-07-21 23:22:20,274 - AirSimEnvLogger - INFO - Action: [-0.78773617 -0.78773617 -0.78773617 -0.78773617], Velocity: (-0.7877361668121438, -0.7877361668121438, -0.7877361668121438), Duration: 1.0, Reward: 637.9425722281273, Done: False
2024-07-21 23:22:20,322 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:20,322 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:23,176 - AirSimEnvLogger - INFO - Predictive model loss: 0.008554890751838684
2024-07-21 23:22:29,169 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.199195695559265, Velocity: -0.0098896622717034, Movement: 0.0564791252812783, Collision: 0, Height: -1.0, Movement Penalty: -0.01304329527389625, Smoothness: -0.0, Curiosity: 641.93359375, Exploration: 0.1758236127353017, Total: 614.2752339490452
2024-07-21 23:22:29,248 - AirSimEnvLogger - INFO - Action: [-0.06521648 -0.06521648 -0.06521648 -0.06521648], Velocity: (-0.06521647636948125, -0.06521647636948125, -0.06521647636948125), Duration: 1.0, Reward: 614.2752339490452, Done: False
2024-07-21 23:22:29,310 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:29,310 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:32,077 - AirSimEnvLogger - INFO - Predictive model loss: 0.5281949639320374
2024-07-21 23:22:38,119 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.76471858677234, Velocity: -0.05185619824577739, Movement: 0.4173827863741356, Collision: 0, Height: -1.0, Movement Penalty: -0.09639042562728932, Smoothness: -0.0, Curiosity: 669.303466796875, Exploration: 0.1124508318227377, Total: 641.069664658876
2024-07-21 23:22:38,245 - AirSimEnvLogger - INFO - Action: [-0.48195213 -0.48195213 -0.48195213 -0.48195213], Velocity: (-0.48195212813644656, -0.48195212813644656, -0.48195212813644656), Duration: 1.0, Reward: 641.069664658876, Done: False
2024-07-21 23:22:38,309 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:38,309 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:40,924 - AirSimEnvLogger - INFO - Predictive model loss: 0.18926844000816345
2024-07-21 23:22:46,600 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.372632011560338, Velocity: -0.12090947346467594, Movement: 0.6360544322020373, Collision: 0, Height: -1.0, Movement Penalty: -0.1468904790604403, Smoothness: -0.0, Curiosity: 699.6826782226562, Exploration: 0.3555483044451538, Total: 670.8953337547499
2024-07-21 23:22:46,757 - AirSimEnvLogger - INFO - Action: [-0.7344524 -0.7344524 -0.7344524 -0.7344524], Velocity: (-0.7344523953022015, -0.7344523953022015, -0.7344523953022015), Duration: 1.0, Reward: 670.8953337547499, Done: False
2024-07-21 23:22:46,818 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:46,818 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:49,511 - AirSimEnvLogger - INFO - Predictive model loss: 0.01879104971885681
2024-07-21 23:22:55,475 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.82701305157821, Velocity: -0.0072129473713753904, Movement: 0.057171974265394075, Collision: 0, Height: -1.0, Movement Penalty: -0.013203301892891052, Smoothness: -0.0, Curiosity: 676.9898071289062, Exploration: 0.09476880095911128, Total: 648.6852720322622
2024-07-21 23:22:55,633 - AirSimEnvLogger - INFO - Action: [-0.06601651 -0.06601651 -0.06601651 -0.06601651], Velocity: (-0.06601650946445525, -0.06601650946445525, -0.06601650946445525), Duration: 1.0, Reward: 648.6852720322622, Done: False
2024-07-21 23:22:55,711 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:22:55,711 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:22:58,572 - AirSimEnvLogger - INFO - Predictive model loss: 0.41383418440818787
2024-07-21 23:23:04,350 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.621472169022123, Velocity: -0.011982232259303177, Movement: 0.07508227984530319, Collision: 0, Height: -1.0, Movement Penalty: -0.017339509792022645, Smoothness: -0.0, Curiosity: 669.985595703125, Exploration: 0.2730569904593974, Total: 641.9275701829649
2024-07-21 23:23:04,459 - AirSimEnvLogger - INFO - Action: [0.08669755 0.08669755 0.08669755 0.08669755], Velocity: (0.08669754896011322, 0.08669754896011322, 0.08669754896011322), Duration: 1.0, Reward: 641.9275701829649, Done: False
2024-07-21 23:23:04,537 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:23:04,537 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:23:07,275 - AirSimEnvLogger - INFO - Predictive model loss: 0.4772496521472931
2024-07-21 23:23:13,059 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.002848955696226, Velocity: -0.055531593857447395, Movement: 0.44715171263576176, Collision: 0, Height: -1.0, Movement Penalty: -0.10326526466354369, Smoothness: -0.0, Curiosity: 645.1162109375, Exploration: 0.2686284894986847, Total: 617.68056198244
2024-07-21 23:23:13,170 - AirSimEnvLogger - INFO - Action: [0.51632632 0.51632632 0.51632632 0.51632632], Velocity: (0.5163263233177184, 0.5163263233177184, 0.5163263233177184), Duration: 1.0, Reward: 617.68056198244, Done: False
2024-07-21 23:23:13,249 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:23:13,249 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:23:16,044 - AirSimEnvLogger - INFO - Predictive model loss: 0.7090147137641907
2024-07-21 23:23:21,953 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.693384197505516, Velocity: -0.026777367964108206, Movement: 0.18273886033550304, Collision: 0, Height: -1.0, Movement Penalty: -0.042201732082443245, Smoothness: -0.0, Curiosity: 671.0885620117188, Exploration: 0.04704376382010485, Total: 642.9078027175012
2024-07-21 23:23:21,954 - AirSimEnvLogger - INFO - Action: [-0.21100866 -0.21100866 -0.21100866 -0.21100866], Velocity: (-0.21100866041221622, -0.21100866041221622, -0.21100866041221622), Duration: 1.0, Reward: 642.9078027175012, Done: False
2024-07-21 23:23:22,017 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:23:22,017 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:23:24,617 - AirSimEnvLogger - INFO - Predictive model loss: 0.027479182928800583
2024-07-21 23:23:30,301 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.299162062644935, Velocity: -0.04694920724108794, Movement: 0.4809468482684143, Collision: 0, Height: -1.0, Movement Penalty: -0.11106991692546844, Smoothness: -0.0, Curiosity: 698.800048828125, Exploration: 0.41790171868457227, Total: 670.1041068403483
2024-07-21 23:23:30,429 - AirSimEnvLogger - INFO - Action: [-0.55534958 -0.55534958 -0.55534958 -0.55534958], Velocity: (-0.5553495846273422, -0.5553495846273422, -0.5553495846273422), Duration: 1.0, Reward: 670.1041068403483, Done: False
2024-07-21 23:23:30,460 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:23:30,460 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:23:33,242 - AirSimEnvLogger - INFO - Predictive model loss: 0.5628119111061096
2024-07-21 23:23:39,111 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.894838114116965, Velocity: -0.08371592343338677, Movement: 0.6614894515465823, Collision: 0, Height: -1.0, Movement Penalty: -0.15276444516660692, Smoothness: -0.0, Curiosity: 729.2574462890625, Exploration: 0.36611633594620735, Total: 699.9546696405031
2024-07-21 23:23:39,222 - AirSimEnvLogger - INFO - Action: [-0.76382223 -0.76382223 -0.76382223 -0.76382223], Velocity: (-0.7638222258330345, -0.7638222258330345, -0.7638222258330345), Duration: 1.0, Reward: 699.9546696405031, Done: False
2024-07-21 23:23:39,316 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:23:39,316 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:23:42,126 - AirSimEnvLogger - INFO - Predictive model loss: 1.441259503364563
2024-07-21 23:23:48,219 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -28.08784390235348, Velocity: -0.014673148552955578, Movement: 0.09003506666062162, Collision: 0, Height: -1.0, Movement Penalty: -0.020792707989206318, Smoothness: -0.0, Curiosity: 694.5118408203125, Exploration: 0.13827575066220446, Total: 665.9565415810939
2024-07-21 23:23:48,329 - AirSimEnvLogger - INFO - Action: [0.10396354 0.10396354 0.10396354 0.10396354], Velocity: (0.10396353994603158, 0.10396353994603158, 0.10396353994603158), Duration: 1.0, Reward: 665.9565415810939, Done: False
2024-07-21 23:23:48,423 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:23:48,423 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:23:51,214 - AirSimEnvLogger - INFO - Predictive model loss: 0.22505874931812286
2024-07-21 23:23:56,265 - AirSimEnvLogger - ERROR - Error during training: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
2024-07-21 23:23:56,265 - AirSimEnvLogger - ERROR - An error occurred in main: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 335, in train
    self.update()
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 219, in update
    actions, states, visuals, log_probs, rewards, dones, goals = self.memory.get_tensors(self.device)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 67, in get_tensors
    torch.tensor(np.vstack(self.goals), device=device).float() if self.goals else None
TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.
2024-07-21 23:24:01,742 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 23:24:01,804 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 23:27:36,924 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 23:27:36,941 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 23:27:36,942 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 10000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 256,
    "batch_size": 32,
    "n_epochs": 3,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 1000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 200,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.05,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 23:27:38,139 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 23:27:38,139 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 23:27:44,185 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 23:27:48,645 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 23:27:49,361 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:27:49,361 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:27:55,321 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.126056712789726, Velocity: -0.05758848843684066, Movement: 0.43297806545158823, Collision: 0, Height: -1.0, Movement Penalty: -0.09999200105667115, Smoothness: -0.0, Curiosity: 1.7936456203460693, Exploration: 0, Total: -7.827549012652453
2024-07-21 23:27:55,494 - AirSimEnvLogger - INFO - Action: [0.49996001 0.49996001 0.49996001 0.49996001], Velocity: (0.4999600052833557, 0.4999600052833557, 0.4999600052833557), Duration: 1.0, Reward: -7.827549012652453, Done: False
2024-07-21 23:27:55,586 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:27:55,586 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:27:58,853 - AirSimEnvLogger - INFO - Predictive model loss: 0.05392604321241379
2024-07-21 23:28:04,911 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.356640127067758, Velocity: -0.0715999443077975, Movement: 0.5425481225269689, Collision: 0, Height: -1.0, Movement Penalty: -0.12529612183570862, Smoothness: -0.0, Curiosity: 2.5145931243896484, Exploration: 0.6389701475171855, Total: -7.188935186944015
2024-07-21 23:28:04,912 - AirSimEnvLogger - INFO - Action: [0.62648061 0.62648061 0.62648061 0.62648061], Velocity: (0.6264806091785431, 0.6264806091785431, 0.6264806091785431), Duration: 1.0, Reward: -7.188935186944015, Done: False
2024-07-21 23:28:04,927 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:28:04,927 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:28:07,608 - AirSimEnvLogger - INFO - Predictive model loss: 0.06861467659473419
2024-07-21 23:28:13,299 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.049565641147039, Velocity: -0.019792687672112605, Movement: 0.15265596964592149, Collision: 0, Height: -1.0, Movement Penalty: -0.035254386067390446, Smoothness: -0.0, Curiosity: 0.25234052538871765, Exploration: 0.28396699383288687, Total: -10.230147333227235
2024-07-21 23:28:13,424 - AirSimEnvLogger - INFO - Action: [-0.17627193 -0.17627193 -0.17627193 -0.17627193], Velocity: (-0.1762719303369522, -0.1762719303369522, -0.1762719303369522), Duration: 1.0, Reward: -10.230147333227235, Done: False
2024-07-21 23:28:13,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:28:13,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:28:20,277 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 23:28:20,356 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 23:31:03,904 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 23:31:03,921 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 23:31:03,921 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 5000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 128,
    "batch_size": 32,
    "n_epochs": 2,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 500
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 100,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 23:31:05,281 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 23:31:05,283 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 23:31:10,333 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 23:31:15,565 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 23:31:16,504 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:31:16,504 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:31:22,564 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.608216007967508, Velocity: -0.1832868923727636, Movement: 0.4296079294543536, Collision: 0, Height: -1.0, Movement Penalty: -0.09921370148658754, Smoothness: -0.0, Curiosity: 1.4735965728759766, Exploration: 0, Total: -8.64240986494849
2024-07-21 23:31:22,677 - AirSimEnvLogger - INFO - Action: [-0.49606851 -0.49606851 -0.49606851 -0.49606851], Velocity: (-0.4960685074329376, -0.4960685074329376, -0.4960685074329376), Duration: 1.0, Reward: -8.64240986494849, Done: False
2024-07-21 23:31:22,738 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:31:22,738 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:31:25,931 - AirSimEnvLogger - INFO - Predictive model loss: 0.026633556932210922
2024-07-21 23:31:31,917 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.708427271449105, Velocity: -0.017678286805703693, Movement: 0.21806105481534713, Collision: 0, Height: -1.0, Movement Penalty: -0.05035904347896576, Smoothness: -0.0, Curiosity: 0.34329384565353394, Exploration: 0.7028893725423733, Total: -9.69988767285673
2024-07-21 23:31:31,996 - AirSimEnvLogger - INFO - Action: [0.25179522 0.25179522 0.25179522 0.25179522], Velocity: (0.2517952173948288, 0.2517952173948288, 0.2517952173948288), Duration: 1.0, Reward: -9.69988767285673, Done: False
2024-07-21 23:31:32,042 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:31:32,042 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:31:34,873 - AirSimEnvLogger - INFO - Predictive model loss: 0.011995338834822178
2024-07-21 23:31:40,818 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.674157368215628, Velocity: -0.04956681911714914, Movement: 0.29993019909051066, Collision: 0, Height: -1.0, Movement Penalty: -0.06926591247320175, Smoothness: -0.0, Curiosity: 0.6909832954406738, Exploration: 0.22737810091915495, Total: -9.428476519839192
2024-07-21 23:31:40,913 - AirSimEnvLogger - INFO - Action: [0.34632956 0.34632956 0.34632956 0.34632956], Velocity: (0.34632956236600876, 0.34632956236600876, 0.34632956236600876), Duration: 1.0, Reward: -9.428476519839192, Done: False
2024-07-21 23:31:40,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:31:40,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:31:43,834 - AirSimEnvLogger - INFO - Predictive model loss: 0.019681120291352272
2024-07-21 23:35:48,217 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 23:35:48,234 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 23:35:48,235 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 5000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 128,
    "batch_size": 32,
    "n_epochs": 2,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 500
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 100,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 23:35:49,534 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 23:35:49,534 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 23:35:54,682 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 23:35:58,840 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 23:35:59,702 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:35:59,702 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:05,595 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.605500476679648, Velocity: -0.19177359947613432, Movement: 0.4329502169274146, Collision: 0, Height: -1.0, Movement Penalty: -0.09998556971549988, Smoothness: -0.0, Curiosity: 1.373044490814209, Exploration: 0, Total: -8.741013100300965
2024-07-21 23:36:05,706 - AirSimEnvLogger - INFO - Action: [-0.49992785 -0.49992785 -0.49992785 -0.49992785], Velocity: (-0.4999278485774994, -0.4999278485774994, -0.4999278485774994), Duration: 1.0, Reward: -8.741013100300965, Done: False
2024-07-21 23:36:05,798 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:05,798 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:09,093 - AirSimEnvLogger - INFO - Predictive model loss: 0.02784854732453823
2024-07-21 23:36:15,181 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.603908785150475, Velocity: -0.06310542456200549, Movement: 0.6346308420906416, Collision: 0, Height: -1.0, Movement Penalty: -0.14656171500682832, Smoothness: -0.0, Curiosity: 3.5094218254089355, Exploration: 0.9997767966134321, Total: -7.3552813785873745
2024-07-21 23:36:15,212 - AirSimEnvLogger - INFO - Action: [-0.73280858 -0.73280858 -0.73280858 -0.73280858], Velocity: (-0.7328085750341415, -0.7328085750341415, -0.7328085750341415), Duration: 1.0, Reward: -7.3552813785873745, Done: False
2024-07-21 23:36:15,275 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:15,276 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:18,015 - AirSimEnvLogger - INFO - Predictive model loss: 0.08275473117828369
2024-07-21 23:36:23,839 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.649893509328097, Velocity: -0.07676569544262102, Movement: 0.5087523059871119, Collision: 0, Height: -1.0, Movement Penalty: -0.11749131232500076, Smoothness: -0.0, Curiosity: 3.2865028381347656, Exploration: 0.3854374457197659, Total: -7.769936961546352
2024-07-21 23:36:23,871 - AirSimEnvLogger - INFO - Action: [-0.58745656 -0.58745656 -0.58745656 -0.58745656], Velocity: (-0.5874565616250038, -0.5874565616250038, -0.5874565616250038), Duration: 1.0, Reward: -7.769936961546352, Done: False
2024-07-21 23:36:23,934 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:23,934 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:26,601 - AirSimEnvLogger - INFO - Predictive model loss: 0.11289511620998383
2024-07-21 23:36:32,426 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.618283179986507, Velocity: -0.07023646675411663, Movement: 0.4382560092478884, Collision: 0, Height: -1.0, Movement Penalty: -0.1012108899652958, Smoothness: -0.0, Curiosity: 3.423614740371704, Exploration: 0.13701863020451874, Total: -7.65942740503112
2024-07-21 23:36:32,473 - AirSimEnvLogger - INFO - Action: [-0.50605445 -0.50605445 -0.50605445 -0.50605445], Velocity: (-0.506054449826479, -0.506054449826479, -0.506054449826479), Duration: 1.0, Reward: -7.65942740503112, Done: False
2024-07-21 23:36:32,536 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:32,536 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:35,250 - AirSimEnvLogger - INFO - Predictive model loss: 0.13835400342941284
2024-07-21 23:36:40,897 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.895374947152906, Velocity: -0.10823462205738303, Movement: 0.6521026374029512, Collision: 0, Height: -1.0, Movement Penalty: -0.15059665329754354, Smoothness: -0.0, Curiosity: 6.861743927001953, Exploration: 0.23763406721025943, Total: -4.47380260431069
2024-07-21 23:36:40,928 - AirSimEnvLogger - INFO - Action: [-0.75298327 -0.75298327 -0.75298327 -0.75298327], Velocity: (-0.7529832664877176, -0.7529832664877176, -0.7529832664877176), Duration: 1.0, Reward: -4.47380260431069, Done: False
2024-07-21 23:36:40,991 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:40,991 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:43,863 - AirSimEnvLogger - INFO - Predictive model loss: 0.2631339728832245
2024-07-21 23:36:49,998 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.295725698211315, Velocity: -0.009820380522175086, Movement: 0.05037937152708204, Collision: 0, Height: -1.0, Movement Penalty: -0.011634617485105992, Smoothness: -0.0, Curiosity: 2.6157143115997314, Exploration: 0.11829537546610015, Total: -8.152549685035362
2024-07-21 23:36:50,107 - AirSimEnvLogger - INFO - Action: [-0.05817309 -0.05817309 -0.05817309 -0.05817309], Velocity: (-0.05817308742552996, -0.05817308742552996, -0.05817308742552996), Duration: 1.0, Reward: -8.152549685035362, Done: False
2024-07-21 23:36:50,185 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:50,185 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:36:52,920 - AirSimEnvLogger - INFO - Predictive model loss: 0.13998857140541077
2024-07-21 23:36:59,289 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.65027874578315, Velocity: -0.012937533783766084, Movement: 0.43258972057264694, Collision: 0, Height: -1.0, Movement Penalty: -0.09990231664851308, Smoothness: -0.0, Curiosity: 5.77069091796875, Exploration: 0.09635219842447486, Total: -5.348109172999234
2024-07-21 23:36:59,416 - AirSimEnvLogger - INFO - Action: [-0.49951158 -0.49951158 -0.49951158 -0.49951158], Velocity: (-0.4995115832425654, -0.4995115832425654, -0.4995115832425654), Duration: 1.0, Reward: -5.348109172999234, Done: False
2024-07-21 23:36:59,494 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:36:59,494 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:02,368 - AirSimEnvLogger - INFO - Predictive model loss: 0.2558138966560364
2024-07-21 23:37:08,525 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.695112198613032, Velocity: -0.05887644102954217, Movement: 0.38243782631325324, Collision: 0, Height: -1.0, Movement Penalty: -0.08832023278810085, Smoothness: -0.0, Curiosity: 6.470794677734375, Exploration: 0.23751020142285234, Total: -4.666196639364549
2024-07-21 23:37:08,556 - AirSimEnvLogger - INFO - Action: [-0.44160116 -0.44160116 -0.44160116 -0.44160116], Velocity: (-0.4416011639405042, -0.4416011639405042, -0.4416011639405042), Duration: 1.0, Reward: -4.666196639364549, Done: False
2024-07-21 23:37:08,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:37:08,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:11,496 - AirSimEnvLogger - INFO - Predictive model loss: 0.2949596345424652
2024-07-21 23:37:17,221 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.01082833215744, Velocity: -0.040895809726480215, Movement: 0.22828942251513232, Collision: 0, Height: -1.0, Movement Penalty: -0.05272118381690234, Smoothness: -0.0, Curiosity: 3.418163299560547, Exploration: 0.20989565626192297, Total: -7.042878685385632
2024-07-21 23:37:17,344 - AirSimEnvLogger - INFO - Action: [0.26360592 0.26360592 0.26360592 0.26360592], Velocity: (0.2636059190845117, 0.2636059190845117, 0.2636059190845117), Duration: 1.0, Reward: -7.042878685385632, Done: False
2024-07-21 23:37:17,488 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:37:17,488 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:20,220 - AirSimEnvLogger - INFO - Predictive model loss: 0.1459461897611618
2024-07-21 23:37:26,246 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.51799981836245, Velocity: -0.030367710090649314, Movement: 0.3039863484220929, Collision: 0, Height: -1.0, Movement Penalty: -0.07020264003658667, Smoothness: -0.0, Curiosity: 5.958414077758789, Exploration: 0.054457597087891586, Total: -5.042640495521711
2024-07-21 23:37:26,355 - AirSimEnvLogger - INFO - Action: [-0.3510132 -0.3510132 -0.3510132 -0.3510132], Velocity: (-0.35101320018293336, -0.35101320018293336, -0.35101320018293336), Duration: 1.0, Reward: -5.042640495521711, Done: False
2024-07-21 23:37:26,481 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:37:26,481 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:29,339 - AirSimEnvLogger - INFO - Predictive model loss: 0.25001516938209534
2024-07-21 23:37:35,073 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.975971127713619, Velocity: -0.04934793541077922, Movement: 0.5811512686998563, Collision: 0, Height: -1.0, Movement Penalty: -0.13421113656950184, Smoothness: -0.0, Curiosity: 10.490022659301758, Exploration: 0.4046729488718576, Total: -0.8835528743791827
2024-07-21 23:37:35,215 - AirSimEnvLogger - INFO - Action: [-0.67105568 -0.67105568 -0.67105568 -0.67105568], Velocity: (-0.6710556828475092, -0.6710556828475092, -0.6710556828475092), Duration: 1.0, Reward: -0.8835528743791827, Done: False
2024-07-21 23:37:35,278 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:37:35,278 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:38,036 - AirSimEnvLogger - INFO - Predictive model loss: 0.3963194191455841
2024-07-21 23:37:44,026 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.297808382174278, Velocity: -0.06968302257129454, Movement: 0.7054884473416775, Collision: 0, Height: -1.0, Movement Penalty: -0.16292557799315546, Smoothness: -0.0, Curiosity: 14.985082626342773, Exploration: 0.36421052731889575, Total: 3.281379956825968
2024-07-21 23:37:44,152 - AirSimEnvLogger - INFO - Action: [-0.81462789 -0.81462789 -0.81462789 -0.81462789], Velocity: (-0.8146278899657773, -0.8146278899657773, -0.8146278899657773), Duration: 1.0, Reward: 3.281379956825968, Done: False
2024-07-21 23:37:44,215 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:37:44,215 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:46,785 - AirSimEnvLogger - INFO - Predictive model loss: 0.5545849800109863
2024-07-21 23:37:52,686 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.432480215382899, Velocity: -0.010331746172498879, Movement: 0.07559124244819805, Collision: 0, Height: -1.0, Movement Penalty: -0.017457049670338166, Smoothness: -0.0, Curiosity: 7.859059810638428, Exploration: 0.16949917175637927, Total: -3.0336145207501755
2024-07-21 23:37:52,795 - AirSimEnvLogger - INFO - Action: [0.08728525 0.08728525 0.08728525 0.08728525], Velocity: (0.08728524835169083, 0.08728524835169083, 0.08728524835169083), Duration: 1.0, Reward: -3.0336145207501755, Done: False
2024-07-21 23:37:52,857 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:37:52,857 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:37:55,520 - AirSimEnvLogger - INFO - Predictive model loss: 0.29175952076911926
2024-07-21 23:38:01,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.540370568546498, Velocity: -0.013918974997236926, Movement: 0.11089384363513717, Collision: 0, Height: -1.0, Movement Penalty: -0.025609836189687485, Smoothness: -0.0, Curiosity: 9.051297187805176, Exploration: 0.2390603255880963, Total: -1.932761183341708
2024-07-21 23:38:01,454 - AirSimEnvLogger - INFO - Action: [-0.12804918 -0.12804918 -0.12804918 -0.12804918], Velocity: (-0.12804918094843742, -0.12804918094843742, -0.12804918094843742), Duration: 1.0, Reward: -1.932761183341708, Done: False
2024-07-21 23:38:01,501 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:01,501 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:04,299 - AirSimEnvLogger - INFO - Predictive model loss: 0.3085588812828064
2024-07-21 23:38:10,298 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.956970362097618, Velocity: -0.06377483310962934, Movement: 0.3764754806722508, Collision: 0, Height: -1.0, Movement Penalty: -0.08694328804376711, Smoothness: -0.0, Curiosity: 6.77902889251709, Exploration: 0.1627140133617096, Total: -3.6376598065460666
2024-07-21 23:38:10,392 - AirSimEnvLogger - INFO - Action: [0.43471644 0.43471644 0.43471644 0.43471644], Velocity: (0.4347164402188355, 0.4347164402188355, 0.4347164402188355), Duration: 1.0, Reward: -3.6376598065460666, Done: False
2024-07-21 23:38:10,455 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:10,455 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:13,246 - AirSimEnvLogger - INFO - Predictive model loss: 0.1694788783788681
2024-07-21 23:38:19,201 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.598787406220334, Velocity: -0.02019137978690778, Movement: 0.2419856698929899, Collision: 0, Height: -1.0, Movement Penalty: -0.05588419666109985, Smoothness: -0.0, Curiosity: 9.33456802368164, Exploration: 0.01959024129469235, Total: -1.7557968695651354
2024-07-21 23:38:19,342 - AirSimEnvLogger - INFO - Action: [-0.27942098 -0.27942098 -0.27942098 -0.27942098], Velocity: (-0.27942098330549925, -0.27942098330549925, -0.27942098330549925), Duration: 1.0, Reward: -1.7557968695651354, Done: False
2024-07-21 23:38:19,421 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:19,421 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:21,874 - AirSimEnvLogger - INFO - Predictive model loss: 0.22794625163078308
2024-07-21 23:38:27,842 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.204270936881569, Velocity: -0.029510141574357537, Movement: 0.16800456652104542, Collision: 0, Height: -1.0, Movement Penalty: -0.03879899268240479, Smoothness: -0.0, Curiosity: 6.978896141052246, Exploration: 0.08383351080229322, Total: -3.704922959530487
2024-07-21 23:38:27,952 - AirSimEnvLogger - INFO - Action: [0.19399496 0.19399496 0.19399496 0.19399496], Velocity: (0.19399496341202394, 0.19399496341202394, 0.19399496341202394), Duration: 1.0, Reward: -3.704922959530487, Done: False
2024-07-21 23:38:28,014 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:28,014 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:30,697 - AirSimEnvLogger - INFO - Predictive model loss: 0.13443847000598907
2024-07-21 23:38:36,752 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.845044535121234, Velocity: -0.03323871167904938, Movement: 0.4020727180591163, Collision: 0, Height: -1.0, Movement Penalty: -0.09285471680209412, Smoothness: -0.0, Curiosity: 5.938157081604004, Exploration: 0.29279431659602095, Total: -4.333005809740823
2024-07-21 23:38:36,894 - AirSimEnvLogger - INFO - Action: [0.46427358 0.46427358 0.46427358 0.46427358], Velocity: (0.4642735840104706, 0.4642735840104706, 0.4642735840104706), Duration: 1.0, Reward: -4.333005809740823, Done: False
2024-07-21 23:38:36,956 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:36,956 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:39,853 - AirSimEnvLogger - INFO - Predictive model loss: 0.11084935814142227
2024-07-21 23:38:45,737 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.74512363130808, Velocity: -0.03604658667359596, Movement: 0.404079303540067, Collision: 0, Height: -1.0, Movement Penalty: -0.09331811786912568, Smoothness: -0.0, Curiosity: 4.996203422546387, Exploration: 0.2125590964037075, Total: -5.1937242316951515
2024-07-21 23:38:45,847 - AirSimEnvLogger - INFO - Action: [0.46659059 0.46659059 0.46659059 0.46659059], Velocity: (0.46659058934562836, 0.46659058934562836, 0.46659058934562836), Duration: 1.0, Reward: -5.1937242316951515, Done: False
2024-07-21 23:38:45,908 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:45,908 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:48,625 - AirSimEnvLogger - INFO - Predictive model loss: 0.11878158152103424
2024-07-21 23:38:54,226 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.456769584908546, Velocity: -0.07042564867574944, Movement: 0.6150115077984583, Collision: 0, Height: -1.0, Movement Penalty: -0.142030823832863, Smoothness: -0.0, Curiosity: 4.69229793548584, Exploration: 0.23003069863215755, Total: -5.203520954429531
2024-07-21 23:38:54,337 - AirSimEnvLogger - INFO - Action: [0.71015412 0.71015412 0.71015412 0.71015412], Velocity: (0.710154119164315, 0.710154119164315, 0.710154119164315), Duration: 1.0, Reward: -5.203520954429531, Done: False
2024-07-21 23:38:54,416 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:38:54,416 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:38:57,052 - AirSimEnvLogger - INFO - Predictive model loss: 0.1846679449081421
2024-07-21 23:39:03,078 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.174411320786138, Velocity: -0.014318177803080867, Movement: 0.08583500897140674, Collision: 0, Height: -1.0, Movement Penalty: -0.01982274621421425, Smoothness: -0.0, Curiosity: 4.135766506195068, Exploration: 0.1332683674589051, Total: -6.5073193797069395
2024-07-21 23:39:03,174 - AirSimEnvLogger - INFO - Action: [-0.09911373 -0.09911373 -0.09911373 -0.09911373], Velocity: (-0.09911373107107124, -0.09911373107107124, -0.09911373107107124), Duration: 1.0, Reward: -6.5073193797069395, Done: False
2024-07-21 23:39:03,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:03,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:05,903 - AirSimEnvLogger - INFO - Predictive model loss: 0.07625845074653625
2024-07-21 23:39:11,765 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.868517118442522, Velocity: -0.04696928872804322, Movement: 0.27152691825573244, Collision: 0, Height: -1.0, Movement Penalty: -0.06270645573887067, Smoothness: -0.0, Curiosity: 3.0723369121551514, Exploration: 0.12604287018099977, Total: -7.265226734332176
2024-07-21 23:39:11,796 - AirSimEnvLogger - INFO - Action: [0.31353228 0.31353228 0.31353228 0.31353228], Velocity: (0.3135322786943533, 0.3135322786943533, 0.3135322786943533), Duration: 1.0, Reward: -7.265226734332176, Done: False
2024-07-21 23:39:11,858 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:11,858 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:14,796 - AirSimEnvLogger - INFO - Predictive model loss: 0.09665445983409882
2024-07-21 23:39:20,576 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.840414768523312, Velocity: -0.03059578135071587, Movement: 0.2375655459142397, Collision: 0, Height: -1.0, Movement Penalty: -0.05486341275350668, Smoothness: -0.0, Curiosity: 2.6187477111816406, Exploration: 0.17510371285144746, Total: -7.678625311469668
2024-07-21 23:39:20,734 - AirSimEnvLogger - INFO - Action: [0.27431706 0.27431706 0.27431706 0.27431706], Velocity: (0.2743170637675334, 0.2743170637675334, 0.2743170637675334), Duration: 1.0, Reward: -7.678625311469668, Done: False
2024-07-21 23:39:20,827 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:20,827 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:23,540 - AirSimEnvLogger - INFO - Predictive model loss: 0.08414261043071747
2024-07-21 23:39:29,640 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.457380984234005, Velocity: -0.05056078941401588, Movement: 0.3142299031255312, Collision: 0, Height: -1.0, Movement Penalty: -0.07256828766278219, Smoothness: -0.0, Curiosity: 4.88570499420166, Exploration: 0.21030565982553845, Total: -6.0206537246076905
2024-07-21 23:39:29,782 - AirSimEnvLogger - INFO - Action: [-0.36284144 -0.36284144 -0.36284144 -0.36284144], Velocity: (-0.3628414383139109, -0.3628414383139109, -0.3628414383139109), Duration: 1.0, Reward: -6.0206537246076905, Done: False
2024-07-21 23:39:29,846 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:29,846 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:32,592 - AirSimEnvLogger - INFO - Predictive model loss: 0.03086930140852928
2024-07-21 23:39:38,409 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.916179988760614, Velocity: -0.07549032267423135, Movement: 0.5855996486055945, Collision: 0, Height: -1.0, Movement Penalty: -0.13523844590391612, Smoothness: -0.0, Curiosity: 9.134686470031738, Exploration: 0.4312519604404606, Total: -2.175489872242168
2024-07-21 23:39:38,472 - AirSimEnvLogger - INFO - Action: [-0.67619223 -0.67619223 -0.67619223 -0.67619223], Velocity: (-0.6761922295195806, -0.6761922295195806, -0.6761922295195806), Duration: 1.0, Reward: -2.175489872242168, Done: False
2024-07-21 23:39:38,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:38,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:41,327 - AirSimEnvLogger - INFO - Predictive model loss: 0.07360313832759857
2024-07-21 23:39:47,146 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.313006423607998, Velocity: -0.004580107524799404, Movement: 0.020963922694402812, Collision: 0, Height: -1.0, Movement Penalty: -0.004841410564353588, Smoothness: -0.0, Curiosity: 4.743587017059326, Exploration: 0.06520422103647887, Total: -6.054366202567722
2024-07-21 23:39:47,209 - AirSimEnvLogger - INFO - Action: [0.02420705 0.02420705 0.02420705 0.02420705], Velocity: (0.024207052821767938, 0.024207052821767938, 0.024207052821767938), Duration: 1.0, Reward: -6.054366202567722, Done: False
2024-07-21 23:39:47,286 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:47,286 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:50,034 - AirSimEnvLogger - INFO - Predictive model loss: 0.023674990981817245
2024-07-21 23:39:56,165 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.497591894968782, Velocity: -0.024738197750815174, Movement: 0.2082063046916904, Collision: 0, Height: -1.0, Movement Penalty: -0.04808318642428988, Smoothness: -0.0, Curiosity: 6.357176780700684, Exploration: 0.12995662778313857, Total: -4.607891620208367
2024-07-21 23:39:56,289 - AirSimEnvLogger - INFO - Action: [-0.24041593 -0.24041593 -0.24041593 -0.24041593], Velocity: (-0.2404159321214494, -0.2404159321214494, -0.2404159321214494), Duration: 1.0, Reward: -4.607891620208367, Done: False
2024-07-21 23:39:56,350 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:39:56,350 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:39:59,122 - AirSimEnvLogger - INFO - Predictive model loss: 0.0454053096473217
2024-07-21 23:40:04,913 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.901653756644018, Velocity: -0.05817292516715807, Movement: 0.32886350527656344, Collision: 0, Height: -1.0, Movement Penalty: -0.07594777331922714, Smoothness: -0.0, Curiosity: 4.227956771850586, Exploration: 0.13985716493260672, Total: -6.139280125296542
2024-07-21 23:40:05,007 - AirSimEnvLogger - INFO - Action: [0.37973887 0.37973887 0.37973887 0.37973887], Velocity: (0.37973886659613565, 0.37973886659613565, 0.37973886659613565), Duration: 1.0, Reward: -6.139280125296542, Done: False
2024-07-21 23:40:05,069 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:05,069 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:40:07,713 - AirSimEnvLogger - INFO - Predictive model loss: 0.02186768129467964
2024-07-21 23:40:13,711 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.49670970258815, Velocity: -0.040473141047316995, Movement: 0.2683234471912128, Collision: 0, Height: -1.0, Movement Penalty: -0.06196664578629403, Smoothness: -0.0, Curiosity: 6.510947227478027, Exploration: 0.005892018980535251, Total: -4.48187266513571
2024-07-21 23:40:13,806 - AirSimEnvLogger - INFO - Action: [-0.30983323 -0.30983323 -0.30983323 -0.30983323], Velocity: (-0.30983322893147014, -0.30983322893147014, -0.30983322893147014), Duration: 1.0, Reward: -4.48187266513571, Done: False
2024-07-21 23:40:13,869 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:13,869 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:40:16,641 - AirSimEnvLogger - INFO - Predictive model loss: 0.04680313169956207
2024-07-21 23:40:21,923 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.99245917054282, Velocity: -0.0765770365642493, Movement: 0.5669703750410079, Collision: 0, Height: -1.0, Movement Penalty: -0.13093619946098758, Smoothness: -0.0, Curiosity: 11.245798110961914, Exploration: 0.43010257326237283, Total: -0.14148741861173691
2024-07-21 23:40:21,924 - AirSimEnvLogger - INFO - Action: [-0.654681 -0.654681 -0.654681 -0.654681], Velocity: (-0.6546809973049379, -0.6546809973049379, -0.6546809973049379), Duration: 1.0, Reward: -0.14148741861173691, Done: False
2024-07-21 23:40:21,955 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:21,955 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:40:24,681 - AirSimEnvLogger - INFO - Predictive model loss: 0.13750311732292175
2024-07-21 23:40:30,845 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.34545999867225, Velocity: -0.0800278896882633, Movement: 0.7085415737991962, Collision: 0, Height: -1.0, Movement Penalty: -0.16363066734600284, Smoothness: -0.0, Curiosity: 16.134191513061523, Exploration: 0.375746078638318, Total: 4.384530810166147
2024-07-21 23:40:30,953 - AirSimEnvLogger - INFO - Action: [-0.81815334 -0.81815334 -0.81815334 -0.81815334], Velocity: (-0.8181533367300141, -0.8181533367300141, -0.8181533367300141), Duration: 1.0, Reward: 4.384530810166147, Done: False
2024-07-21 23:40:31,015 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:31,015 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:40:33,698 - AirSimEnvLogger - INFO - Predictive model loss: 0.2500457763671875
2024-07-21 23:40:39,398 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.471931001370672, Velocity: -0.011037770785478742, Movement: 0.07842577359076543, Collision: 0, Height: -1.0, Movement Penalty: -0.018111656597613223, Smoothness: -0.0, Curiosity: 8.759376525878906, Exploration: 0.16410546718504382, Total: -2.173990215113854
2024-07-21 23:40:39,524 - AirSimEnvLogger - INFO - Action: [0.09055828 0.09055828 0.09055828 0.09055828], Velocity: (0.09055828298806612, 0.09055828298806612, 0.09055828298806612), Duration: 1.0, Reward: -2.173990215113854, Done: False
2024-07-21 23:40:39,587 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:39,587 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:40:42,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.07577049732208252
2024-07-21 23:40:48,462 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.85423450357108, Velocity: -0.07746439363589054, Movement: 0.47015406111835134, Collision: 0, Height: -1.0, Movement Penalty: -0.10857742949891036, Smoothness: -0.0, Curiosity: 7.162297248840332, Exploration: 0.5118581703431122, Total: -3.0704234611083074
2024-07-21 23:40:48,573 - AirSimEnvLogger - INFO - Action: [0.54288715 0.54288715 0.54288715 0.54288715], Velocity: (0.5428871474945518, 0.5428871474945518, 0.5428871474945518), Duration: 1.0, Reward: -3.0704234611083074, Done: False
2024-07-21 23:40:48,635 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:48,635 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:40:51,368 - AirSimEnvLogger - INFO - Predictive model loss: 0.03385377675294876
2024-07-21 23:40:57,394 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.538217732017547, Velocity: -0.03175220057481794, Movement: 0.19793092237248022, Collision: 0, Height: -1.0, Movement Penalty: -0.04571018852508096, Smoothness: -0.0, Curiosity: 9.095693588256836, Exploration: 0.02825096637339513, Total: -1.9343464066827045
2024-07-21 23:40:57,396 - AirSimEnvLogger - INFO - Action: [-0.22855094 -0.22855094 -0.22855094 -0.22855094], Velocity: (-0.22855094262540476, -0.22855094262540476, -0.22855094262540476), Duration: 1.0, Reward: -1.9343464066827045, Done: False
2024-07-21 23:40:57,504 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:40:57,504 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:41:00,126 - AirSimEnvLogger - INFO - Predictive model loss: 0.044277820736169815
2024-07-21 23:41:06,342 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.626445917578888, Velocity: -0.03169661405692886, Movement: 0.17371824934274946, Collision: 0, Height: -1.0, Movement Penalty: -0.040118511208474775, Smoothness: -0.0, Curiosity: 9.604961395263672, Exploration: 0.27556879113658606, Total: -1.4570120624557463
2024-07-21 23:41:06,468 - AirSimEnvLogger - INFO - Action: [-0.20059256 -0.20059256 -0.20059256 -0.20059256], Velocity: (-0.20059255604237386, -0.20059255604237386, -0.20059255604237386), Duration: 1.0, Reward: -1.4570120624557463, Done: False
2024-07-21 23:41:06,499 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:41:06,499 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:41:09,306 - AirSimEnvLogger - INFO - Predictive model loss: 0.035127680748701096
2024-07-21 23:41:15,279 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.125661817485682, Velocity: -0.06667497308200479, Movement: 0.5197696206732072, Collision: 0, Height: -1.0, Movement Penalty: -0.12003565216490633, Smoothness: -0.0, Curiosity: 14.966865539550781, Exploration: 0.23303838472626115, Total: 3.400884974055735
2024-07-21 23:41:15,391 - AirSimEnvLogger - INFO - Action: [-0.60017826 -0.60017826 -0.60017826 -0.60017826], Velocity: (-0.6001782608245316, -0.6001782608245316, -0.6001782608245316), Duration: 1.0, Reward: 3.400884974055735, Done: False
2024-07-21 23:41:15,423 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 23:41:15,423 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 23:41:18,074 - AirSimEnvLogger - INFO - Predictive model loss: 0.08619578182697296
2024-07-22 00:29:24,088 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-22 00:29:24,110 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-22 00:29:24,111 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 5000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 128,
    "batch_size": 32,
    "n_epochs": 2,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 500
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 100,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-22 00:29:25,532 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-22 00:29:25,533 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-22 00:29:32,317 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-22 00:29:37,731 - AirSimEnvLogger - ERROR - Error during training: 'AirSimEnv' object has no attribute 'log'
2024-07-22 00:29:37,731 - AirSimEnvLogger - ERROR - Error during training: 'AirSimEnv' object has no attribute 'log'
2024-07-22 00:29:37,731 - AirSimEnvLogger - ERROR - An error occurred in main: 'AirSimEnv' object has no attribute 'log'
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 334, in train
    observation = env.reset()
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 77, in reset
    obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self._seeds[env_idx], **maybe_options)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\shimmy\openai_gym_compatibility.py", line 235, in reset
    obs = self.gym_env.reset()
  File "e:\Project\Drone\source\envs\airsim_env.py", line 298, in reset
    self.log("Environment reset and takeoff completed.")
AttributeError: 'AirSimEnv' object has no attribute 'log'
2024-07-22 00:29:43,268 - AirSimEnvLogger - INFO - Environment closed.
2024-07-22 00:29:43,330 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-22 00:34:21,809 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-22 00:34:21,826 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-22 00:34:21,826 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 5000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 128,
    "batch_size": 32,
    "n_epochs": 2,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 500
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 100,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-22 00:34:23,530 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-22 00:34:23,530 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-22 00:34:29,996 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-22 00:34:34,242 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-22 00:34:34,900 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:34:34,900 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:34:36,267 - AirSimEnvLogger - ERROR - Error in select_action: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-22 00:34:36,267 - AirSimEnvLogger - ERROR - Error during training: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-22 00:34:36,267 - AirSimEnvLogger - ERROR - Error during training: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-22 00:34:36,267 - AirSimEnvLogger - ERROR - An error occurred in main: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 134, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 336, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 212, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 105, in forward
    visual = visual.permute(0, 3, 1, 2)
RuntimeError: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-22 00:34:41,825 - AirSimEnvLogger - INFO - Environment closed.
2024-07-22 00:34:41,887 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-22 00:37:55,706 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-22 00:37:55,725 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-22 00:37:55,725 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 5000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 128,
    "batch_size": 32,
    "n_epochs": 2,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 500
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 100,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128,
      128
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      128,
      128
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 5,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 5,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 128
    },
    "inverse_model": {
      "hidden_dim": 128
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      128,
      128
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-22 00:37:57,155 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-22 00:37:57,156 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-22 00:38:04,381 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-22 00:38:09,568 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-22 00:38:10,214 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:38:10,214 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:38:17,223 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.59915525606885, Velocity: -0.14406632434723196, Movement: 0.24276758498618145, Collision: 0, Height: -1.0, Movement Penalty: -0.05595627992435059, Smoothness: -0.0, Curiosity: 0.6596298217773438, Exploration: 0, Large Movement: 0.9710703399447258, Direction Change: 0, Total: -8.476852404845612
2024-07-22 00:38:17,396 - AirSimEnvLogger - INFO - Action: [0.27780092 0.27928311 0.28385207 0.27814767], Velocity: (0.27780091762542725, 0.2792831063270569, 0.28385207056999207), Duration: 1.5, Reward: -8.476852404845612, Done: False
2024-07-22 00:38:17,458 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:38:17,458 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:38:20,766 - AirSimEnvLogger - INFO - Predictive model loss: 0.016325751319527626
2024-07-22 00:38:27,574 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.826949557494938, Velocity: -0.03767594150311649, Movement: 0.09256593833117963, Collision: 0, Height: -1.0, Movement Penalty: -0.019929647044778568, Smoothness: -0.0, Curiosity: 0.20414738357067108, Exploration: 1.2299842640026413, Large Movement: 0.3702637533247185, Direction Change: 0.08505115787322537, Total: -9.300312910438826
2024-07-22 00:38:27,716 - AirSimEnvLogger - INFO - Action: [0.07123401 0.06164273 0.15937284 0.07379208], Velocity: (0.07123400717973707, 0.061642727255821206, 0.15937284007668495), Duration: 1.5, Reward: -9.300312910438826, Done: False
2024-07-22 00:38:27,748 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:38:27,748 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:38:30,667 - AirSimEnvLogger - INFO - Predictive model loss: 0.006762190256267786
2024-07-22 00:38:37,586 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.852718168040793, Velocity: -0.06438458963598852, Movement: 0.1151066107074977, Collision: 0, Height: -1.0, Movement Penalty: -0.023887578125881107, Smoothness: -0.0, Curiosity: 0.33861270546913147, Exploration: 0.1393028708835773, Large Movement: 0.4604264428299908, Direction Change: 0.03529351308714479, Total: -9.453319655791647
2024-07-22 00:38:37,712 - AirSimEnvLogger - INFO - Action: [0.13610231 0.09036282 0.16220003 0.06374568], Velocity: (0.13610230848193167, 0.09036281526088713, 0.1622000325471163), Duration: 1.5, Reward: -9.453319655791647, Done: False
2024-07-22 00:38:37,759 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:38:37,759 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:38:40,462 - AirSimEnvLogger - INFO - Predictive model loss: 0.011866356246173382
2024-07-22 00:38:47,126 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.670939375716994, Velocity: -0.20762518168384178, Movement: 0.33847205387087886, Collision: 0, Height: -1.0, Movement Penalty: -0.07594471597179356, Smoothness: -0.0, Curiosity: 1.8079469203948975, Exploration: 0.35916163554931063, Large Movement: 1.3538882154835155, Direction Change: 0.014706819829985895, Total: -6.908432212614625
2024-07-22 00:38:47,254 - AirSimEnvLogger - INFO - Action: [0.39493242 0.36263383 0.41325346 0.34424797], Velocity: (0.3949324178248644, 0.3626338260769844, 0.41325345557183024), Duration: 1.5, Reward: -6.908432212614625, Done: False
2024-07-22 00:38:47,314 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:38:47,314 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:38:50,119 - AirSimEnvLogger - INFO - Predictive model loss: 0.06056063994765282
2024-07-22 00:38:56,844 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.011920464173231, Velocity: -0.018958684852406493, Movement: 0.024329717221679387, Collision: 0, Height: -1.0, Movement Penalty: -0.007462708124982795, Smoothness: -0.0, Curiosity: 0.6221877336502075, Exploration: 0.16086557328185147, Large Movement: 0.09731886888671755, Direction Change: 0.16825909143579165, Total: -9.421117659046594
2024-07-22 00:38:56,923 - AirSimEnvLogger - INFO - Action: [-0.01991278 -0.04344718 -0.00914134 -0.05658145], Velocity: (-0.019912777216732513, -0.04344718335270881, -0.009141341399401481), Duration: 1.5, Reward: -9.421117659046594, Done: False
2024-07-22 00:38:56,985 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:38:56,985 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:38:59,800 - AirSimEnvLogger - INFO - Predictive model loss: 0.029808277264237404
2024-07-22 00:39:06,174 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.999192421143716, Velocity: -0.022341163430403693, Movement: 0.03858076049451543, Collision: 0, Height: -1.0, Movement Penalty: -0.009886728828595953, Smoothness: -0.0, Curiosity: 0.7670976519584656, Exploration: 0.1903802075245449, Large Movement: 0.1543230419780617, Direction Change: 0.1618245834369325, Total: -9.212111286890785
2024-07-22 00:39:06,300 - AirSimEnvLogger - INFO - Action: [0.04714762 0.0413697  0.0449394  0.06181295], Velocity: (0.04714762492969631, 0.04136970217287542, 0.04493939838223156), Duration: 1.5, Reward: -9.212111286890785, Done: False
2024-07-22 00:39:06,393 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:39:06,393 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:39:09,214 - AirSimEnvLogger - INFO - Predictive model loss: 0.03597083315253258
2024-07-22 00:39:15,906 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.200495007316999, Velocity: -0.09904020488574614, Movement: 0.15911320452339814, Collision: 0, Height: -1.0, Movement Penalty: -0.03670380964489754, Smoothness: -0.0, Curiosity: 0.5206543803215027, Exploration: 0.15016151291827948, Large Movement: 0.6364528180935926, Direction Change: 0.00928273667068047, Total: -9.496265264267086
2024-07-22 00:39:16,001 - AirSimEnvLogger - INFO - Action: [-0.15848259 -0.18916491 -0.20091777 -0.18289045], Velocity: (-0.15848258905811607, -0.18916491267400978, -0.20091777470719072), Duration: 1.5, Reward: -9.496265264267086, Done: False
2024-07-22 00:39:16,078 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:39:16,078 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:39:18,877 - AirSimEnvLogger - INFO - Predictive model loss: 0.02203408069908619
2024-07-22 00:39:25,511 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.412404082011438, Velocity: -0.22742963500280328, Movement: 0.35613352875800147, Collision: 0, Height: -1.0, Movement Penalty: -0.08261089098758152, Smoothness: -0.0, Curiosity: 0.9757654666900635, Exploration: 0.49978424703847807, Large Movement: 1.4245341150320059, Direction Change: 0.0017694148079809846, Total: -8.407805008822505
2024-07-22 00:39:25,607 - AirSimEnvLogger - INFO - Action: [-0.38863666 -0.41952496 -0.42459948 -0.41848724], Velocity: (-0.3886366570119306, -0.4195249628834188, -0.42459947664447506), Duration: 1.5, Reward: -8.407805008822505, Done: False
2024-07-22 00:39:25,669 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:39:25,669 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:39:28,539 - AirSimEnvLogger - INFO - Predictive model loss: 0.032249148935079575
2024-07-22 00:39:35,079 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.0406143111131, Velocity: -0.0103798860657645, Movement: 0.01340765284452935, Collision: 0, Height: -1.0, Movement Penalty: -0.0027339608431801395, Smoothness: -0.0, Curiosity: 0.09825500100851059, Exploration: 0.18136490343550862, Large Movement: 0.0536306113781174, Direction Change: 0.3329413507824598, Total: -9.681659967001318
2024-07-22 00:39:35,186 - AirSimEnvLogger - INFO - Action: [0.0263185  0.00505423 0.00092295 0.00532856], Velocity: (0.02631850253671697, 0.005054226888360436, 0.0009229485269833249), Duration: 1.5, Reward: -9.681659967001318, Done: False
2024-07-22 00:39:35,248 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:39:35,248 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:39:38,040 - AirSimEnvLogger - INFO - Predictive model loss: 0.006236876826733351
2024-07-22 00:39:44,821 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.788530904315586, Velocity: -0.1516895869520403, Movement: 0.23329458951850163, Collision: 0, Height: -1.0, Movement Penalty: -0.053777833667045, Smoothness: -0.0, Curiosity: 0.8099015951156616, Exploration: 0.4670710428079091, Large Movement: 0.9331783580740065, Direction Change: 0.29250932342366676, Total: -7.862402597428901
2024-07-22 00:39:44,852 - AirSimEnvLogger - INFO - Action: [0.27574601 0.26709226 0.26520053 0.26739498], Velocity: (0.27574600823512874, 0.26709226366003286, 0.26520053474639993), Duration: 1.5, Reward: -7.862402597428901, Done: False
2024-07-22 00:39:44,944 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:39:44,944 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:39:47,418 - AirSimEnvLogger - INFO - Predictive model loss: 0.01863892190158367
2024-07-22 00:39:54,083 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.807056156641458, Velocity: -0.11256154496581453, Movement: 0.19021723301285545, Collision: 0, Height: -1.0, Movement Penalty: -0.04177500810663426, Smoothness: -0.0, Curiosity: 0.9811242818832397, Exploration: 0.33735354875786594, Large Movement: 0.7608689320514218, Direction Change: 0.007220535993478938, Total: -8.478543800433629
2024-07-22 00:39:54,271 - AirSimEnvLogger - INFO - Action: [0.23930268 0.17891677 0.23548546 0.17258258], Velocity: (0.23930268296056828, 0.17891676635577233, 0.23548545514518615), Duration: 1.5, Reward: -8.478543800433629, Done: False
2024-07-22 00:39:54,319 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:39:54,319 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:39:57,252 - AirSimEnvLogger - INFO - Predictive model loss: 0.03207872062921524
2024-07-22 00:40:04,096 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.122703785326754, Velocity: -0.08444946831633156, Movement: 0.12880143646916298, Collision: 0, Height: -1.0, Movement Penalty: -0.03137954608661721, Smoothness: -0.0, Curiosity: 0.41193878650665283, Exploration: 0.19548570583974817, Large Movement: 0.5152057458766519, Direction Change: 0.032679488699733916, Total: -9.591341081970098
2024-07-22 00:40:04,175 - AirSimEnvLogger - INFO - Action: [-0.13248539 -0.17475458 -0.13515805 -0.17918803], Velocity: (-0.1324853920348715, -0.17475457998391225, -0.13515804755208674), Duration: 1.5, Reward: -9.591341081970098, Done: False
2024-07-22 00:40:04,253 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:40:04,253 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:40:06,883 - AirSimEnvLogger - INFO - Predictive model loss: 0.013659371994435787
2024-07-22 00:40:13,816 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.381033323157835, Velocity: -0.23000929254545205, Movement: 0.34940050114838145, Collision: 0, Height: -1.0, Movement Penalty: -0.08181166114958978, Smoothness: -0.0, Curiosity: 0.9862841367721558, Exploration: 0.5675448525304249, Large Movement: 1.3976020045935258, Direction Change: 0.004679305078353124, Total: -8.372243945222015
2024-07-22 00:40:13,865 - AirSimEnvLogger - INFO - Action: [-0.39273961 -0.42232807 -0.3946105  -0.42543149], Velocity: (-0.3927396075314047, -0.4223280688980556, -0.3946104961957777), Duration: 1.5, Reward: -8.372243945222015, Done: False
2024-07-22 00:40:13,942 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:40:13,942 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:40:16,878 - AirSimEnvLogger - INFO - Predictive model loss: 0.031093595549464226
2024-07-22 00:40:23,686 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.123146098722533, Velocity: -0.07000895838516939, Movement: 0.1024258934420418, Collision: 0, Height: -1.0, Movement Penalty: -0.025462199688071625, Smoothness: -0.0, Curiosity: 0.19342409074306488, Exploration: 0.29063898591177495, Large Movement: 0.4097035737681672, Direction Change: 0.009259162540559585, Total: -9.940045512971139
2024-07-22 00:40:23,748 - AirSimEnvLogger - INFO - Action: [-0.12252012 -0.1361188  -0.09178642 -0.15122204], Velocity: (-0.1225201224754287, -0.13611879732226684, -0.09178642161969053), Duration: 1.5, Reward: -9.940045512971139, Done: False
2024-07-22 00:40:23,811 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:40:23,811 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:40:26,660 - AirSimEnvLogger - INFO - Predictive model loss: 0.01502823457121849
2024-07-22 00:40:33,658 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.85020988100007, Velocity: -0.08979078533246314, Movement: 0.1474133015770629, Collision: 0, Height: -1.0, Movement Penalty: -0.03353604241097086, Smoothness: -0.0, Curiosity: 0.2709422707557678, Exploration: 0.3166596684795822, Large Movement: 0.5896532063082516, Direction Change: 0.02563127913288188, Total: -9.370629359437926
2024-07-22 00:40:33,753 - AirSimEnvLogger - INFO - Action: [0.16865345 0.15582442 0.18492563 0.15982455], Velocity: (0.1686534529100832, 0.1558244153836295, 0.18492563456497), Duration: 1.5, Reward: -9.370629359437926, Done: False
2024-07-22 00:40:33,863 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:40:33,863 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:40:36,554 - AirSimEnvLogger - INFO - Predictive model loss: 0.005449523217976093
2024-07-22 00:40:43,388 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.89034488244811, Velocity: -0.07103356036159283, Movement: 0.118568540823917, Collision: 0, Height: -1.0, Movement Penalty: -0.02663750229657102, Smoothness: -0.0, Curiosity: 0.3169296085834503, Exploration: 0.2712929274218267, Large Movement: 0.474274163295668, Direction Change: 0.011344102603910056, Total: -9.517878043523247
2024-07-22 00:40:43,512 - AirSimEnvLogger - INFO - Action: [0.15693342 0.13066353 0.12055264 0.12133284], Velocity: (0.15693341782884634, 0.13066352940949277, 0.12055264386674744), Duration: 1.5, Reward: -9.517878043523247, Done: False
2024-07-22 00:40:43,590 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:40:43,590 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:40:46,324 - AirSimEnvLogger - INFO - Predictive model loss: 0.0071503957733511925
2024-07-22 00:40:53,372 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.137555249861167, Velocity: -0.07856772282305356, Movement: 0.12692031541823262, Collision: 0, Height: -1.0, Movement Penalty: -0.02964579327956784, Smoothness: -0.0, Curiosity: 0.22789154946804047, Exploration: 0.16736621381446234, Large Movement: 0.5076812616729305, Direction Change: 0.01656861276557897, Total: -9.835261808775249
2024-07-22 00:40:53,499 - AirSimEnvLogger - INFO - Action: [-0.13134343 -0.15354738 -0.15364625 -0.15314124], Velocity: (-0.131343433620157, -0.15354738060487577, -0.15364625025591566), Duration: 1.5, Reward: -9.835261808775249, Done: False
2024-07-22 00:40:53,562 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:40:53,562 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:40:56,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.009270079433918
2024-07-22 00:41:03,060 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.38392553676113, Velocity: -0.21599994008483436, Movement: 0.3286615595195798, Collision: 0, Height: -1.0, Movement Penalty: -0.07628206836518835, Smoothness: -0.0, Curiosity: 1.1491029262542725, Exploration: 0.49658772215815705, Large Movement: 1.3146462380783193, Direction Change: 0.001321727202541756, Total: -8.317046151785414
2024-07-22 00:41:03,219 - AirSimEnvLogger - INFO - Action: [-0.3685433 -0.3858866 -0.3838503 -0.3870681], Velocity: (-0.36854329760187415, -0.38588659760055233, -0.38385030208191195), Duration: 1.5, Reward: -8.317046151785414, Done: False
2024-07-22 00:41:03,250 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:41:03,250 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:41:06,013 - AirSimEnvLogger - INFO - Predictive model loss: 0.04866502806544304
2024-07-22 00:41:11,799 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.635445186889088, Velocity: -0.3060276521295283, Movement: 0.4888940192335692, Collision: 0, Height: -1.0, Movement Penalty: -0.1131402210183127, Smoothness: -0.0, Curiosity: 3.4065873622894287, Exploration: 0.6809503054209928, Large Movement: 1.9555760769342767, Direction Change: 5.7383837465674326e-05, Total: -5.635276122773033
2024-07-22 00:41:11,922 - AirSimEnvLogger - INFO - Action: [-0.55683287 -0.56923601 -0.56743014 -0.56921131], Velocity: (-0.5568328712256698, -0.5692360079058816, -0.567430144595232), Duration: 1.5, Reward: -5.635276122773033, Done: False
2024-07-22 00:41:11,985 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:41:11,985 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:41:14,580 - AirSimEnvLogger - INFO - Predictive model loss: 0.16084370017051697
2024-07-22 00:41:20,937 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.200868425513866, Velocity: -0.053988912069877895, Movement: 0.08676323606213848, Collision: 0, Height: -1.0, Movement Penalty: -0.020160592156036702, Smoothness: -0.0, Curiosity: 1.353305459022522, Exploration: 0.30774007875937737, Large Movement: 0.34705294424855393, Direction Change: 0.0004317153682473762, Total: -8.932198707627945
2024-07-22 00:41:21,047 - AirSimEnvLogger - INFO - Action: [-0.09477876 -0.10401704 -0.10153265 -0.10263289], Velocity: (-0.09477876123888929, -0.10401703844846949, -0.1015326482385282), Duration: 1.5, Reward: -8.932198707627945, Done: False
2024-07-22 00:41:21,093 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:41:21,093 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:41:23,841 - AirSimEnvLogger - INFO - Predictive model loss: 0.10106772184371948
2024-07-22 00:41:30,769 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.485751225459724, Velocity: -0.21169674367074537, Movement: 0.3202242793799406, Collision: 0, Height: -1.0, Movement Penalty: -0.07403419658956902, Smoothness: -0.0, Curiosity: 3.5373778343200684, Exploration: 0.215362994028632, Large Movement: 1.2808971175197623, Direction Change: 0.0004971994650179168, Total: -6.130303759611938
2024-07-22 00:41:30,816 - AirSimEnvLogger - INFO - Action: [-0.36597577 -0.37244683 -0.37083615 -0.3713918 ], Velocity: (-0.36597577480447835, -0.3724468305832859, -0.3708361458411732), Duration: 1.5, Reward: -6.130303759611938, Done: False
2024-07-22 00:41:30,895 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:41:30,895 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:41:33,597 - AirSimEnvLogger - INFO - Predictive model loss: 0.21495439112186432
2024-07-22 00:41:40,382 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.231911104730447, Velocity: -0.04157254692847084, Movement: 0.07464087056727839, Collision: 0, Height: -1.0, Movement Penalty: -0.015804433379045118, Smoothness: -0.0, Curiosity: 2.678715944290161, Exploration: 0.2532320627124252, Large Movement: 0.29856348226911356, Direction Change: 0.0011519155394301706, Total: -7.695694222171404
2024-07-22 00:41:40,509 - AirSimEnvLogger - INFO - Action: [-0.09063697 -0.08596973 -0.08172628 -0.05189386], Velocity: (-0.09063696785351416, -0.08596972511099787, -0.08172627887543471), Duration: 1.5, Reward: -7.695694222171404, Done: False
2024-07-22 00:41:40,557 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:41:40,557 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:41:43,483 - AirSimEnvLogger - INFO - Predictive model loss: 0.18846197426319122
2024-07-22 00:41:50,611 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.99467931838892, Velocity: -0.04880798761351058, Movement: 0.09278748784147847, Collision: 0, Height: -1.0, Movement Penalty: -0.023266351023284816, Smoothness: -0.0, Curiosity: 2.039673089981079, Exploration: 0.2653691798619412, Large Movement: 0.3711499513659139, Direction Change: 0.026385347011747995, Total: -7.973574397625371
2024-07-22 00:41:50,690 - AirSimEnvLogger - INFO - Action: [0.07661961 0.11702781 0.1219508  0.14033616], Velocity: (0.0766196137020335, 0.117027812996299, 0.12195079901822903), Duration: 1.5, Reward: -7.973574397625371, Done: False
2024-07-22 00:41:50,738 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:41:50,738 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:41:53,942 - AirSimEnvLogger - INFO - Predictive model loss: 0.14058804512023926
2024-07-22 00:42:00,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.332252962921288, Velocity: -0.12022638930626042, Movement: 0.18264671658861717, Collision: 0, Height: -1.0, Movement Penalty: -0.04104609240548167, Smoothness: -0.0, Curiosity: 3.502858877182007, Exploration: 0.1072437945325217, Large Movement: 0.7305868663544687, Direction Change: 0.032777316619368, Total: -6.515561658214557
2024-07-22 00:42:00,752 - AirSimEnvLogger - INFO - Action: [-0.23015817 -0.20099565 -0.20016808 -0.18718675], Velocity: (-0.23015817008966785, -0.20099564625903601, -0.2001680774956351), Duration: 1.5, Reward: -6.515561658214557, Done: False
2024-07-22 00:42:00,815 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:42:00,815 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:42:03,764 - AirSimEnvLogger - INFO - Predictive model loss: 0.22986584901809692
2024-07-22 00:42:10,563 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.35394233336644, Velocity: -0.10137405011711556, Movement: 0.1532768918917633, Collision: 0, Height: -1.0, Movement Penalty: -0.03471429886583121, Smoothness: -0.0, Curiosity: 4.104593276977539, Exploration: 0.32586797391366223, Large Movement: 0.6131075675670532, Direction Change: 0.0008581277766099848, Total: -6.065611388848268
2024-07-22 00:42:10,674 - AirSimEnvLogger - INFO - Action: [-0.20266043 -0.16125266 -0.16401692 -0.16288963], Velocity: (-0.2026604277569871, -0.16125266466398283, -0.16401692446853555), Duration: 1.5, Reward: -6.065611388848268, Done: False
2024-07-22 00:42:10,706 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:42:10,706 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:42:13,473 - AirSimEnvLogger - INFO - Predictive model loss: 0.26997050642967224
2024-07-22 00:42:20,420 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.707564256550954, Velocity: -0.2376888851460359, Movement: 0.36487127589971696, Collision: 0, Height: -1.0, Movement Penalty: -0.08379763765634847, Smoothness: -0.0, Curiosity: 7.624456882476807, Exploration: 0.4087522927268813, Large Movement: 1.4594851035988679, Direction Change: 0.002803083327723055, Total: -2.038588805209468
2024-07-22 00:42:20,545 - AirSimEnvLogger - INFO - Action: [-0.44019967 -0.41063452 -0.41246543 -0.41192258], Velocity: (-0.44019966958713275, -0.41063452064694433, -0.4124654328003534), Duration: 1.5, Reward: -2.038588805209468, Done: False
2024-07-22 00:42:20,609 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:42:20,609 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:42:23,345 - AirSimEnvLogger - INFO - Predictive model loss: 0.45851975679397583
2024-07-22 00:42:30,327 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.943960336964327, Velocity: -0.2924831270555268, Movement: 0.4309329490431223, Collision: 0, Height: -1.0, Movement Penalty: -0.09944881833149086, Smoothness: -0.0, Curiosity: 11.591188430786133, Exploration: 0.6321606174648721, Large Movement: 1.7237317961724892, Direction Change: 0.000743834718569425, Total: 1.9992023924430926
2024-07-22 00:42:30,453 - AirSimEnvLogger - INFO - Action: [-0.50829525 -0.51194791 -0.47154863 -0.49617932], Velocity: (-0.5082952504071218, -0.5119479051211578, -0.47154862648472246), Duration: 1.5, Reward: 1.9992023924430926, Done: False
2024-07-22 00:42:30,517 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:42:30,517 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:42:33,316 - AirSimEnvLogger - INFO - Predictive model loss: 0.6903353333473206
2024-07-22 00:42:40,149 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.120467066290857, Velocity: -0.2607714951971461, Movement: 0.40654341509638764, Collision: 0, Height: -1.0, Movement Penalty: -0.09426306981797873, Smoothness: -0.0, Curiosity: 14.851083755493164, Exploration: 0.5545452053884965, Large Movement: 1.6261736603855506, Direction Change: 0.003524082621123137, Total: 4.975091305754248
2024-07-22 00:42:40,305 - AirSimEnvLogger - INFO - Action: [-0.43565717 -0.47421591 -0.49641947 -0.47690926], Velocity: (-0.435657170146588, -0.47421591189358353, -0.49641947226084804), Duration: 1.5, Reward: 4.975091305754248, Done: False
2024-07-22 00:42:40,369 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:42:40,369 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:42:43,267 - AirSimEnvLogger - INFO - Predictive model loss: 0.8989856839179993
2024-07-22 00:42:50,291 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.659420429167708, Velocity: -0.013973314380925448, Movement: 0.029636510680488757, Collision: 0, Height: -1.0, Movement Penalty: -0.006937445123342892, Smoothness: -0.0, Curiosity: 12.130559921264648, Exploration: 0.23635767025930407, Large Movement: 0.11854604272195503, Direction Change: 0.11086604252312693, Total: 1.3650629415202944
2024-07-22 00:42:50,418 - AirSimEnvLogger - INFO - Action: [-0.00679073 -0.03325513 -0.04859294 -0.0360489 ], Velocity: (-0.006790734043636337, -0.033255127020653086, -0.048592936927579544), Duration: 1.5, Reward: 1.3650629415202944, Done: False
2024-07-22 00:42:50,480 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:42:50,480 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:42:53,254 - AirSimEnvLogger - INFO - Predictive model loss: 0.7742696404457092
2024-07-22 00:43:00,253 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.418499532886461, Velocity: -0.07656500538424188, Movement: 0.11102983309392024, Collision: 0, Height: -1.0, Movement Penalty: -0.02587055515759218, Smoothness: -0.0, Curiosity: 10.653481483459473, Exploration: 0.37065631713921565, Large Movement: 0.44411933237568096, Direction Change: 0.20545077142662438, Total: 0.6702061772598903
2024-07-22 00:43:00,346 - AirSimEnvLogger - INFO - Action: [0.14740904 0.1255872  0.10866888 0.13273307], Velocity: (0.1474090380493374, 0.1255872002605764, 0.10866888229633184), Duration: 1.5, Reward: 0.6702061772598903, Done: False
2024-07-22 00:43:00,438 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:00,438 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:43:03,204 - AirSimEnvLogger - INFO - Predictive model loss: 0.6725051999092102
2024-07-22 00:43:10,177 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.822625245495475, Velocity: -0.11447596747878515, Movement: 0.18201297827184545, Collision: 0, Height: -1.0, Movement Penalty: -0.04182161449854995, Smoothness: -0.0, Curiosity: 13.595717430114746, Exploration: 0.1038712164739528, Large Movement: 0.7280519130873818, Direction Change: 0.015640341176304084, Total: 3.0494385728091755
2024-07-22 00:43:10,255 - AirSimEnvLogger - INFO - Action: [-0.19577904 -0.21079194 -0.22305206 -0.20588795], Velocity: (-0.19577903806005917, -0.2107919448658906, -0.2230520595589343), Duration: 1.5, Reward: 3.0494385728091755, Done: False
2024-07-22 00:43:10,318 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:10,318 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:43:13,125 - AirSimEnvLogger - INFO - Predictive model loss: 0.8502692580223083
2024-07-22 00:43:19,965 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.260650622873813, Velocity: -0.2424267314740167, Movement: 0.381789727927991, Collision: 0, Height: -1.0, Movement Penalty: -0.08807509546642568, Smoothness: -0.0, Curiosity: 19.255640029907227, Exploration: 0.5741747282484612, Large Movement: 1.527158911711964, Direction Change: 0.000619163694653202, Total: 9.140617174661134
2024-07-22 00:43:20,091 - AirSimEnvLogger - INFO - Action: [-0.43014273 -0.44282424 -0.44937457 -0.43894038], Velocity: (-0.43014273326778907, -0.44282423812464455, -0.4493745748833164), Duration: 1.5, Reward: 9.140617174661134, Done: False
2024-07-22 00:43:20,233 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:20,233 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:43:23,015 - AirSimEnvLogger - INFO - Predictive model loss: 1.165116548538208
2024-07-22 00:43:29,907 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.720885598805886, Velocity: -0.33674379751229344, Movement: 0.5192551590073515, Collision: 0, Height: -1.0, Movement Penalty: -0.11985077014086198, Smoothness: -0.0, Curiosity: 27.235145568847656, Exploration: 0.7179159348584132, Large Movement: 2.077020636029406, Direction Change: 4.0111308242885e-05, Total: 17.23557745114447
2024-07-22 00:43:30,015 - AirSimEnvLogger - INFO - Action: [-0.5926164  -0.59774506 -0.60828474 -0.59826167], Velocity: (-0.5926163961174999, -0.5977450624189864, -0.6082847425584734), Duration: 1.5, Reward: 17.23557745114447, Done: False
2024-07-22 00:43:30,093 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:30,093 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:43:32,919 - AirSimEnvLogger - INFO - Predictive model loss: 1.615300178527832
2024-07-22 00:43:39,875 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.502341209166264, Velocity: -0.14546958756104014, Movement: 0.2318264234865731, Collision: 0, Height: -1.0, Movement Penalty: -0.05178241146970474, Smoothness: -0.0, Curiosity: 26.77109146118164, Exploration: 0.43776290418158215, Large Movement: 0.9273056939462924, Direction Change: 0.00028530556228067994, Total: 15.789329559845157
2024-07-22 00:43:39,954 - AirSimEnvLogger - INFO - Action: [-0.2626472  -0.25996429 -0.28001603 -0.23058155], Velocity: (-0.2626471995093465, -0.25996429429029055, -0.2800160295239407), Duration: 1.5, Reward: 15.789329559845157, Done: False
2024-07-22 00:43:40,000 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:40,000 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:43:42,848 - AirSimEnvLogger - INFO - Predictive model loss: 1.646378993988037
2024-07-22 00:43:49,862 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.892673125755257, Velocity: -0.24061680717247555, Movement: 0.3839450960530106, Collision: 0, Height: -1.0, Movement Penalty: -0.08803042025949163, Smoothness: -0.0, Curiosity: 33.855796813964844, Exploration: 0.3582731172392224, Large Movement: 1.5357803842120423, Direction Change: 0.00030783042520932113, Total: 23.067597972837497
2024-07-22 00:43:49,925 - AirSimEnvLogger - INFO - Action: [-0.44154579 -0.43991622 -0.44851575 -0.4304418 ], Velocity: (-0.4415457939251888, -0.43991622223359583, -0.4485157482355756), Duration: 1.5, Reward: 23.067597972837497, Done: False
2024-07-22 00:43:49,987 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:49,987 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:43:52,690 - AirSimEnvLogger - INFO - Predictive model loss: 2.0449419021606445
2024-07-22 00:43:59,549 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.289754365723448, Velocity: -0.2877499656037431, Movement: 0.4565711323555067, Collision: 0, Height: -1.0, Movement Penalty: -0.10443989579882468, Smoothness: -0.0, Curiosity: 41.96718215942383, Exploration: 0.6245724888609049, Large Movement: 1.826284529422027, Direction Change: 0.00019004861868510137, Total: 31.13066910686777
2024-07-22 00:43:59,704 - AirSimEnvLogger - INFO - Action: [-0.53434938 -0.50869194 -0.53808182 -0.50689288], Velocity: (-0.5343493766189853, -0.5086919449342446, -0.538081819491679), Duration: 1.5, Reward: 31.13066910686777, Done: False
2024-07-22 00:43:59,765 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:43:59,765 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:44:02,565 - AirSimEnvLogger - INFO - Predictive model loss: 2.51113224029541
2024-07-22 00:44:09,450 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.10891128289337, Velocity: -0.13446120882374124, Movement: 0.20371416171236964, Collision: 0, Height: -1.0, Movement Penalty: -0.050008124333059616, Smoothness: -0.0, Curiosity: 42.07948684692383, Exploration: 0.37525730249294226, Large Movement: 0.8148566468494786, Direction Change: 2.078722419285306e-05, Total: 30.361852764492525
2024-07-22 00:44:09,498 - AirSimEnvLogger - INFO - Action: [-0.2403609  -0.22522712 -0.23978579 -0.2899714 ], Velocity: (-0.24036089704641378, -0.22522711732002193, -0.23978578674193834), Duration: 1.5, Reward: 30.361852764492525, Done: False
2024-07-22 00:44:09,513 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:44:09,513 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:44:12,328 - AirSimEnvLogger - INFO - Predictive model loss: 2.5461294651031494
2024-07-22 00:44:19,313 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.463074308828379, Velocity: -0.21642682693463236, Movement: 0.32547187441751957, Collision: 0, Height: -1.0, Movement Penalty: -0.07666015502850582, Smoothness: -0.0, Curiosity: 49.14942932128906, Exploration: 0.3070519749937487, Large Movement: 1.3018874976700783, Direction Change: 0.0001289308988161597, Total: 37.544715628387706
2024-07-22 00:44:19,436 - AirSimEnvLogger - INFO - Action: [-0.38780668 -0.36466569 -0.37463673 -0.40490761], Velocity: (-0.38780667968934845, -0.36466568991152903, -0.3746367279606822), Duration: 1.5, Reward: 37.544715628387706, Done: False
2024-07-22 00:44:19,498 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:44:19,498 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:44:22,375 - AirSimEnvLogger - INFO - Predictive model loss: 2.9328927993774414
2024-07-22 00:44:29,314 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.039087772691033, Velocity: -0.32149231066284856, Movement: 0.4869180219799118, Collision: 0, Height: -1.0, Movement Penalty: -0.1134923813770832, Smoothness: -0.0, Curiosity: 60.96937561035156, Exploration: 0.636532091469541, Large Movement: 1.9476720879196472, Direction Change: 9.033820256432179e-05, Total: 49.503816084345665
2024-07-22 00:44:29,455 - AirSimEnvLogger - INFO - Action: [-0.57061818 -0.55443311 -0.56156519 -0.58283396], Velocity: (-0.5706181825383727, -0.5544331093552304, -0.5615651854616834), Duration: 1.5, Reward: 49.503816084345665, Done: False
2024-07-22 00:44:29,471 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:44:29,471 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:44:32,375 - AirSimEnvLogger - INFO - Predictive model loss: 3.5774006843566895
2024-07-22 00:44:39,286 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.624938921170022, Velocity: -0.38621053225520685, Movement: 0.5777547281975558, Collision: 0, Height: -1.0, Movement Penalty: -0.13429930775338636, Smoothness: -0.0, Curiosity: 74.9365463256836, Exploration: 0.7955183346487141, Large Movement: 2.311018912790223, Direction Change: 6.66616515841989e-05, Total: 63.280843812421615
2024-07-22 00:44:39,429 - AirSimEnvLogger - INFO - Action: [-0.66820801 -0.66779559 -0.66539403 -0.68441822], Velocity: (-0.6682080120671746, -0.6677955879713907, -0.6653940254568759), Duration: 1.5, Reward: 63.280843812421615, Done: False
2024-07-22 00:44:39,491 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:44:39,491 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:44:42,374 - AirSimEnvLogger - INFO - Predictive model loss: 4.346858501434326
2024-07-22 00:44:49,379 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.18278175096956, Velocity: -0.4056155885040538, Movement: 0.6069650218324882, Collision: 0, Height: -1.0, Movement Penalty: -0.14115347202517511, Smoothness: -0.0, Curiosity: 89.99365234375, Exploration: 0.8259345138099627, Large Movement: 2.427860087329953, Direction Change: 2.087816398588238e-05, Total: 77.90257418280821
2024-07-22 00:44:49,505 - AirSimEnvLogger - INFO - Action: [-0.69924929 -0.70792176 -0.69535844 -0.72028058], Velocity: (-0.6992492887954185, -0.7079217628401587, -0.6953584404670985), Duration: 1.5, Reward: 77.90257418280821, Done: False
2024-07-22 00:44:49,599 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:44:49,599 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:44:52,374 - AirSimEnvLogger - INFO - Predictive model loss: 5.178770065307617
2024-07-22 00:44:59,239 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.93040666149245, Velocity: -0.1606348201871172, Movement: 0.23859792950996184, Collision: 0, Height: -1.0, Movement Penalty: -0.056361728991562866, Smoothness: -0.0, Curiosity: 89.22537231445312, Exploration: 0.48570816392961386, Large Movement: 0.9543917180398473, Direction Change: 0.00023502544961562233, Total: 75.85069966322045
2024-07-22 00:44:59,349 - AirSimEnvLogger - INFO - Action: [-0.27762671 -0.28363834 -0.2649313  -0.29991426], Velocity: (-0.27762670512062193, -0.2836383389466453, -0.2649312989233587), Duration: 1.5, Reward: 75.85069966322045, Done: False
2024-07-22 00:44:59,445 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:44:59,445 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:45:02,349 - AirSimEnvLogger - INFO - Predictive model loss: 5.142708778381348
2024-07-22 00:45:09,196 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.561843759779025, Velocity: -0.015663654260985982, Movement: 0.03096400796007011, Collision: 0, Height: -1.0, Movement Penalty: -0.006209029148878758, Smoothness: -0.0, Curiosity: 85.18466186523438, Exploration: 0.36702039987284396, Large Movement: 0.12385603184028043, Direction Change: 0.055874201990822714, Total: 71.44250131436189
2024-07-22 00:45:09,308 - AirSimEnvLogger - INFO - Action: [0.0342412  0.02070375 0.04726493 0.00448611], Velocity: (0.034241200123793725, 0.02070375234649441, 0.04726493423948114), Duration: 1.5, Reward: 71.44250131436189, Done: False
2024-07-22 00:45:09,370 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:45:09,370 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:45:12,168 - AirSimEnvLogger - INFO - Predictive model loss: 4.821523666381836
2024-07-22 00:45:19,088 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.659167970037949, Velocity: -0.031299724958154765, Movement: 0.04184251708012093, Collision: 0, Height: -1.0, Movement Penalty: -0.011018788288612164, Smoothness: -0.0, Curiosity: 87.00997924804688, Exploration: 0.14802406113599165, Large Movement: 0.16737006832048373, Direction Change: 0.1351783786050892, Total: 73.31980222335196
2024-07-22 00:45:19,228 - AirSimEnvLogger - INFO - Action: [-0.07215328 -0.0309065  -0.02901512 -0.07168113], Velocity: (-0.0721532832143736, -0.03090649529758987, -0.029015120406562123), Duration: 1.5, Reward: 73.31980222335196, Done: False
2024-07-22 00:45:19,322 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:45:19,322 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:45:21,846 - AirSimEnvLogger - INFO - Predictive model loss: 4.812353134155273
2024-07-22 00:45:28,594 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.09984519861273, Velocity: -0.15111886781862827, Movement: 0.22969990886206026, Collision: 0, Height: -1.0, Movement Penalty: -0.05358477574784498, Smoothness: -0.0, Curiosity: 95.13411712646484, Exploration: 0.28624543765554106, Large Movement: 0.9187996354482411, Direction Change: 0.08143811222404451, Total: 81.67203795099411
2024-07-22 00:45:28,718 - AirSimEnvLogger - INFO - Action: [-0.27226776 -0.25764377 -0.26559018 -0.27583442], Velocity: (-0.27226776494489335, -0.25764377195760335, -0.2655901795404452), Duration: 1.5, Reward: 81.67203795099411, Done: False
2024-07-22 00:45:28,812 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:45:28,812 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:45:31,606 - AirSimEnvLogger - INFO - Predictive model loss: 5.126495361328125
2024-07-22 00:45:38,642 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.48312994793914, Velocity: -0.1532368365493354, Movement: 0.2978098114518013, Collision: 0, Height: -1.0, Movement Penalty: -0.06698976891648878, Smoothness: -0.0, Curiosity: 104.33772277832031, Exploration: 0.4587036843342925, Large Movement: 1.1912392458072052, Direction Change: 0.00016449786656380638, Total: 90.64453772480847
2024-07-22 00:45:38,721 - AirSimEnvLogger - INFO - Action: [-0.35218102 -0.32664993 -0.35218048 -0.30659449], Velocity: (-0.35218102166786147, -0.32664992804713355, -0.3521804759943593), Duration: 1.5, Reward: 90.64453772480847, Done: False
2024-07-22 00:45:38,783 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:45:38,783 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:45:41,495 - AirSimEnvLogger - INFO - Predictive model loss: 5.472739219665527
2024-07-22 00:45:48,590 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.035389105925566, Velocity: -0.026950309271156422, Movement: 0.03647661533916233, Collision: 0, Height: -1.0, Movement Penalty: -0.009777793488829933, Smoothness: -0.0, Curiosity: 97.89262390136719, Exploration: 0.14487026848437454, Large Movement: 0.1459064613566493, Direction Change: 0.06931078660591017, Total: 83.67260579023744
2024-07-22 00:45:48,685 - AirSimEnvLogger - INFO - Action: [0.03433928 0.05897212 0.02579295 0.06510262], Velocity: (0.03433928190853153, 0.058972121509203196, 0.02579295386563979), Duration: 1.5, Reward: 83.67260579023744, Done: False
2024-07-22 00:45:48,715 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:45:48,715 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:45:51,437 - AirSimEnvLogger - INFO - Predictive model loss: 4.93118953704834
2024-07-22 00:45:58,451 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.762870432530583, Velocity: -0.09696289879011408, Movement: 0.14428866393171128, Collision: 0, Height: -1.0, Movement Penalty: -0.0347810574119908, Smoothness: -0.0, Curiosity: 92.90904235839844, Exploration: 0.36323850934623375, Large Movement: 0.5771546557268451, Direction Change: 0.08466746495052946, Total: 79.46931996301082
2024-07-22 00:45:58,608 - AirSimEnvLogger - INFO - Action: [0.21139506 0.14637054 0.131014   0.19415283], Velocity: (0.21139505713612008, 0.14637054439644237, 0.13101399813874431), Duration: 1.5, Reward: 79.46931996301082, Done: False
2024-07-22 00:45:58,718 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:45:58,718 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:46:01,537 - AirSimEnvLogger - INFO - Predictive model loss: 4.434453964233398
2024-07-22 00:46:08,566 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.242391776373958, Velocity: -0.09896283607589612, Movement: 0.16231373631484533, Collision: 0, Height: -1.0, Movement Penalty: -0.03636914430879813, Smoothness: -0.0, Curiosity: 100.34782409667969, Exploration: 0.1122859563759587, Large Movement: 0.6492549452593813, Direction Change: 0.05764530995320738, Total: 86.39044672276009
2024-07-22 00:46:08,690 - AirSimEnvLogger - INFO - Action: [-0.15189455 -0.1974191  -0.20817479 -0.16397704], Velocity: (-0.15189454707899575, -0.19741909697272228, -0.2081747888292004), Duration: 1.5, Reward: 86.39044672276009, Done: False
2024-07-22 00:46:08,751 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:46:08,751 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:46:11,618 - AirSimEnvLogger - INFO - Predictive model loss: 4.559438228607178
2024-07-22 00:46:18,345 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -14.846073924672233, Velocity: -0.24196415490981252, Movement: 0.37272782000462334, Collision: 0, Height: -1.0, Movement Penalty: -0.0853071430909231, Smoothness: -0.0, Curiosity: 113.70413208007812, Exploration: 0.5643606829226048, Large Movement: 1.4909112800184934, Direction Change: 0.004123633923718839, Total: 99.97235156810822
2024-07-22 00:46:18,484 - AirSimEnvLogger - INFO - Action: [-0.40630074 -0.43815634 -0.44569366 -0.41476108], Velocity: (-0.40630074369290686, -0.43815633551510663, -0.4456936644649099), Duration: 1.5, Reward: 99.97235156810822, Done: False
2024-07-22 00:46:18,547 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:46:18,547 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:46:21,377 - AirSimEnvLogger - INFO - Predictive model loss: 4.90128231048584
2024-07-22 00:46:28,146 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -15.523938416154317, Velocity: -0.3212213910104115, Movement: 0.5206004136525638, Collision: 0, Height: -1.0, Movement Penalty: -0.11969011709034486, Smoothness: -0.0, Curiosity: 131.5650634765625, Exploration: 0.7209747850233217, Large Movement: 2.0824016546102553, Direction Change: 0.00019590190314167089, Total: 117.77065957964523
2024-07-22 00:46:28,270 - AirSimEnvLogger - INFO - Action: [-0.58439086 -0.60669869 -0.61196665 -0.59031623], Velocity: (-0.5843908629731879, -0.6066986881431216, -0.6119666525716496), Duration: 1.5, Reward: 117.77065957964523, Done: False
2024-07-22 00:46:28,365 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:46:28,365 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:46:31,163 - AirSimEnvLogger - INFO - Predictive model loss: 5.39794921875
2024-07-22 00:46:38,061 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.26024093131139, Velocity: -0.41045477336872715, Movement: 0.6222747529227475, Collision: 0, Height: -1.0, Movement Penalty: -0.14331182752861363, Smoothness: -0.0, Curiosity: 153.2831268310547, Exploration: 0.8394337110041146, Large Movement: 2.48909901169099, Direction Change: 3.933273481504518e-05, Total: 139.17955040474288
2024-07-22 00:46:38,170 - AirSimEnvLogger - INFO - Action: [-0.70752246 -0.72205904 -0.72591059 -0.71058041], Velocity: (-0.7075224647265257, -0.7220590386703999, -0.7259105861481466), Duration: 1.5, Reward: 139.17955040474288, Done: False
2024-07-22 00:46:38,232 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:46:38,232 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:46:40,807 - AirSimEnvLogger - INFO - Predictive model loss: 6.007007598876953
2024-07-22 00:46:47,680 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -16.888268637386158, Velocity: -0.4213699994575113, Movement: 0.6253085926790823, Collision: 0, Height: -1.0, Movement Penalty: -0.14211113473022025, Smoothness: -0.0, Curiosity: 174.14649963378906, Exploration: 0.8542791558983496, Large Movement: 2.5012343707163294, Direction Change: 9.999557229700695e-05, Total: 159.43050019978466
2024-07-22 00:46:47,836 - AirSimEnvLogger - INFO - Action: [-0.71247731 -0.73720353 -0.71620554 -0.67491786], Velocity: (-0.7124773145257539, -0.7372035276632283, -0.7162055427042153), Duration: 1.5, Reward: 159.43050019978466, Done: False
2024-07-22 00:46:47,899 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:46:47,899 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:46:50,732 - AirSimEnvLogger - INFO - Predictive model loss: 6.548351287841797
2024-07-22 00:46:57,392 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.36986304775337, Velocity: -0.35943418435960334, Movement: 0.5524843078519949, Collision: 0, Height: -1.0, Movement Penalty: -0.12739834607466913, Smoothness: -0.0, Curiosity: 191.85592651367188, Exploration: 0.7505580689575113, Large Movement: 2.2099372314079795, Direction Change: 0.0008768600852950836, Total: 176.34808798498332
2024-07-22 00:46:57,487 - AirSimEnvLogger - INFO - Action: [-0.65899046 -0.61608043 -0.63806905 -0.63409638], Velocity: (-0.6589904612677668, -0.6160804315625172, -0.6380690522934253), Duration: 1.5, Reward: 176.34808798498332, Done: False
2024-07-22 00:46:57,566 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:46:57,566 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:00,407 - AirSimEnvLogger - INFO - Predictive model loss: 6.906013011932373
2024-07-22 00:47:07,323 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.054919923370964, Velocity: -0.4027141179039614, Movement: 0.6027279571007583, Collision: 0, Height: -1.0, Movement Penalty: -0.13893337294580005, Smoothness: -0.0, Curiosity: 215.73890686035156, Exploration: 0.7525171459295412, Large Movement: 2.4109118284030333, Direction Change: 0.00021839911632803855, Total: 199.7430583229109
2024-07-22 00:47:07,430 - AirSimEnvLogger - INFO - Action: [-0.70555927 -0.69250155 -0.68974755 -0.69074181], Velocity: (-0.705559266653077, -0.6925015527956664, -0.6897475492518821), Duration: 1.5, Reward: 199.7430583229109, Done: False
2024-07-22 00:47:07,492 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:47:07,492 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:10,329 - AirSimEnvLogger - INFO - Predictive model loss: 7.423275947570801
2024-07-22 00:47:17,276 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.649577157828784, Velocity: -0.38839406273299015, Movement: 0.581151541811321, Collision: 0, Height: -1.0, Movement Penalty: -0.13378827823933798, Smoothness: -0.0, Curiosity: 237.75357055664062, Exploration: 0.782799389569574, Large Movement: 2.324606167245284, Direction Change: 0.00021962242094397677, Total: 221.08471034096064
2024-07-22 00:47:17,340 - AirSimEnvLogger - INFO - Action: [-0.6604789  -0.67611318 -0.6764518  -0.66255708], Velocity: (-0.6604788990144842, -0.6761131848163491, -0.6764518042498007), Duration: 1.5, Reward: 221.08471034096064, Done: False
2024-07-22 00:47:17,403 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:47:17,403 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:19,988 - AirSimEnvLogger - INFO - Predictive model loss: 7.785702228546143
2024-07-22 00:47:26,906 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.892436996392966, Velocity: -0.26509536897007907, Movement: 0.4133444524890087, Collision: 0, Height: -1.0, Movement Penalty: -0.09714646124134635, Smoothness: -0.0, Curiosity: 249.66192626953125, Exploration: 0.5843320509924242, Large Movement: 1.6533778099560348, Direction Change: 0.0007977574302777679, Total: 232.0416444255139
2024-07-22 00:47:27,016 - AirSimEnvLogger - INFO - Action: [-0.44235147 -0.49595281 -0.49170167 -0.51022441], Velocity: (-0.4423514664349396, -0.49595281016697085, -0.4917016736234448), Duration: 1.5, Reward: 232.0416444255139, Done: False
2024-07-22 00:47:27,079 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:47:27,079 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:29,774 - AirSimEnvLogger - INFO - Predictive model loss: 7.60897970199585
2024-07-22 00:47:36,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.479939486572512, Velocity: -0.30851557773392446, Movement: 0.48211677790849233, Collision: 0, Height: -1.0, Movement Penalty: -0.1120121520129967, Smoothness: -0.0, Curiosity: 271.3993835449219, Exploration: 0.5415246733572744, Large Movement: 1.9284671116339693, Direction Change: 0.001018577572986179, Total: 253.4551377151288
2024-07-22 00:47:36,539 - AirSimEnvLogger - INFO - Action: [-0.54694153 -0.54794128 -0.57477096 -0.57002269], Velocity: (-0.5469415348891623, -0.5479412791362277, -0.5747709649896279), Duration: 1.5, Reward: 253.4551377151288, Done: False
2024-07-22 00:47:36,631 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:47:36,631 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:39,431 - AirSimEnvLogger - INFO - Predictive model loss: 7.714006423950195
2024-07-22 00:47:46,279 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.173327236762294, Velocity: -0.06872707429496733, Movement: 0.1095537043991744, Collision: 0, Height: -1.0, Movement Penalty: -0.02594274258655296, Smoothness: -0.0, Curiosity: 266.3517150878906, Exploration: 0.325289744025572, Large Movement: 0.4382148175966976, Direction Change: 0.0013543971518936715, Total: 247.18962154404784
2024-07-22 00:47:46,294 - AirSimEnvLogger - INFO - Action: [-0.1213406  -0.11777916 -0.13932905 -0.13890476], Velocity: (-0.12134059602885888, -0.11777916173752245, -0.13932905442187765), Duration: 1.5, Reward: 247.18962154404784, Done: False
2024-07-22 00:47:46,372 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:47:46,372 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:48,898 - AirSimEnvLogger - INFO - Predictive model loss: 6.746401309967041
2024-07-22 00:47:55,892 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.673589073107074, Velocity: -0.11050628108255013, Movement: 0.1676417647464032, Collision: 0, Height: -1.0, Movement Penalty: -0.03782310551539868, Smoothness: -0.0, Curiosity: 254.1394500732422, Exploration: 0.4405175924322951, Large Movement: 0.6705670589856128, Direction Change: 0.005422770446235514, Total: 235.74210031247117
2024-07-22 00:47:55,987 - AirSimEnvLogger - INFO - Action: [0.19949382 0.19578519 0.18516323 0.17505338], Velocity: (0.19949381515971212, 0.19578519429334734, 0.18516322679650082), Duration: 1.5, Reward: 235.74210031247117, Done: False
2024-07-22 00:47:56,034 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:47:56,034 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:47:58,899 - AirSimEnvLogger - INFO - Predictive model loss: 5.688065528869629
2024-07-22 00:48:05,641 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.044565602091293, Velocity: -0.24177880112425024, Movement: 0.3764969908124022, Collision: 0, Height: -1.0, Movement Penalty: -0.08631485213722846, Smoothness: -0.0, Curiosity: 236.55523681640625, Exploration: 0.5646457669841083, Large Movement: 1.5059879632496087, Direction Change: 0.0002341329993118535, Total: 219.63237024250222
2024-07-22 00:48:05,673 - AirSimEnvLogger - INFO - Action: [0.43893064 0.43630271 0.42892864 0.4219306 ], Velocity: (0.4389306354116794, 0.4363027123202693, 0.4289286384496282), Duration: 1.5, Reward: 219.63237024250222, Done: False
2024-07-22 00:48:05,751 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:48:05,751 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:48:08,429 - AirSimEnvLogger - INFO - Predictive model loss: 5.053198337554932
2024-07-22 00:48:15,492 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.37592271022303, Velocity: -0.3206880868433034, Movement: 0.5074321971694749, Collision: 0, Height: -1.0, Movement Penalty: -0.11680078440643843, Smoothness: -0.0, Curiosity: 216.5680389404297, Exploration: 0.6715178753750971, Large Movement: 2.0297287886778994, Direction Change: 8.891142060707402e-06, Total: 200.85688321922112
2024-07-22 00:48:15,587 - AirSimEnvLogger - INFO - Action: [0.59085604 0.58546268 0.58143988 0.57818041], Velocity: (0.5908560368730602, 0.58546268297363, 0.581439876025839), Duration: 1.5, Reward: 200.85688321922112, Done: False
2024-07-22 00:48:15,603 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:48:15,603 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:48:18,488 - AirSimEnvLogger - INFO - Predictive model loss: 5.233565330505371
2024-07-22 00:48:25,387 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.614601859296847, Velocity: -0.09022702279850428, Movement: 0.1410845891037103, Collision: 0, Height: -1.0, Movement Penalty: -0.03268008193762597, Smoothness: -0.0, Curiosity: 216.2286834716797, Exploration: 0.33888394547712397, Large Movement: 0.5643383564148412, Direction Change: 0.00025432613894305067, Total: 198.7512610277612
2024-07-22 00:48:25,466 - AirSimEnvLogger - INFO - Action: [0.15910036 0.16481733 0.16474759 0.16486155], Velocity: (0.1591003566304628, 0.1648173317034053, 0.16474759124333294), Duration: 1.5, Reward: 198.7512610277612, Done: False
2024-07-22 00:48:25,529 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:48:25,529 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:48:28,338 - AirSimEnvLogger - INFO - Predictive model loss: 4.623281002044678
2024-07-22 00:48:35,272 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.122251677092255, Velocity: -0.10099701598094052, Movement: 0.15503116490925803, Collision: 0, Height: -1.0, Movement Penalty: -0.03558535537630526, Smoothness: -0.0, Curiosity: 227.51910400390625, Exploration: 0.4124032868310787, Large Movement: 0.6201246596370321, Direction Change: 0.0003876329565187797, Total: 209.60641699213627
2024-07-22 00:48:35,380 - AirSimEnvLogger - INFO - Action: [-0.18174933 -0.17886867 -0.17638545 -0.17462275], Velocity: (-0.18174932991228018, -0.17886867033409032, -0.17638545354375995), Duration: 1.5, Reward: 209.60641699213627, Done: False
2024-07-22 00:48:35,444 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:48:35,444 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:48:38,361 - AirSimEnvLogger - INFO - Predictive model loss: 3.79093074798584
2024-07-22 00:48:45,159 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.598814924986062, Velocity: -0.19391145480407337, Movement: 0.28926815282385027, Collision: 0, Height: -1.0, Movement Penalty: -0.06648096180659437, Smoothness: -0.0, Curiosity: 242.18890380859375, Exploration: 0.49323555202195496, Large Movement: 1.157072611295401, Direction Change: 0.0007747487213415338, Total: 224.35002108432246
2024-07-22 00:48:45,269 - AirSimEnvLogger - INFO - Action: [-0.35727587 -0.32309889 -0.32041429 -0.32751728], Velocity: (-0.35727586961123403, -0.32309888735291226, -0.32041429265458216), Duration: 1.5, Reward: 224.35002108432246, Done: False
2024-07-22 00:48:45,332 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:48:45,332 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:48:48,115 - AirSimEnvLogger - INFO - Predictive model loss: 3.195216178894043
2024-07-22 00:48:54,986 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.216784506240852, Velocity: -0.018711627945138847, Movement: 0.03642402940659892, Collision: 0, Height: -1.0, Movement Penalty: -0.008652778489837856, Smoothness: -0.0, Curiosity: 234.30650329589844, Exploration: 0.14269952133291142, Large Movement: 0.14569611762639567, Direction Change: 0.058950424895523934, Total: 215.88503845905907
2024-07-22 00:48:55,128 - AirSimEnvLogger - INFO - Action: [0.02368816 0.04394949 0.05304859 0.0466928 ], Velocity: (0.023688161060878232, 0.0439494922318677, 0.053048589340019436), Duration: 1.5, Reward: 215.88503845905907, Done: False
2024-07-22 00:48:55,207 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:48:55,207 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:48:58,079 - AirSimEnvLogger - INFO - Predictive model loss: 2.756208896636963
2024-07-22 00:49:05,022 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.651843676917142, Velocity: -0.17751959654535804, Movement: 0.2803147412825837, Collision: 0, Height: -1.0, Movement Penalty: -0.0649683116932987, Smoothness: -0.0, Curiosity: 220.07635498046875, Exploration: 0.476402594047857, Large Movement: 1.1212589651303349, Direction Change: 0.03702038167834376, Total: 203.21839157694802
2024-07-22 00:49:05,149 - AirSimEnvLogger - INFO - Action: [0.31322074 0.32675994 0.3307962  0.32830281], Velocity: (0.313220737953949, 0.3267599396100674, 0.3307962029753305), Duration: 1.5, Reward: 203.21839157694802, Done: False
2024-07-22 00:49:05,228 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:49:05,228 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:49:08,144 - AirSimEnvLogger - INFO - Predictive model loss: 2.3922557830810547
2024-07-22 00:49:14,957 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.07164491459905, Velocity: -0.26269563176229, Movement: 0.41460104677349596, Collision: 0, Height: -1.0, Movement Penalty: -0.09591611268794345, Smoothness: -0.0, Curiosity: 203.341796875, Exploration: 0.5856519923310628, Large Movement: 1.6584041870939839, Direction Change: 0.00033396663924711856, Total: 187.54774056603767
2024-07-22 00:49:15,145 - AirSimEnvLogger - INFO - Action: [0.4719575  0.49217373 0.47180213 0.48209331], Velocity: (0.4719574978013611, 0.49217372899589173, 0.4718021329944059), Duration: 1.5, Reward: 187.54774056603767, Done: False
2024-07-22 00:49:15,223 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:49:15,225 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:49:18,055 - AirSimEnvLogger - INFO - Predictive model loss: 2.117964744567871
2024-07-22 00:49:24,901 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.447629361613938, Velocity: -0.022246435000341954, Movement: 0.031572613333079536, Collision: 0, Height: -1.0, Movement Penalty: -0.007393038326346921, Smoothness: -0.0, Curiosity: 207.53285217285156, Exploration: 0.23578874457370932, Large Movement: 0.12629045333231814, Direction Change: 0.012644646643210966, Total: 189.78953297231175
2024-07-22 00:49:24,978 - AirSimEnvLogger - INFO - Action: [0.03134346 0.04506168 0.03121462 0.03844973], Velocity: (0.031343461179916376, 0.04506167606250505, 0.03121461904975842), Duration: 1.5, Reward: 189.78953297231175, Done: False
2024-07-22 00:49:25,041 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:49:25,041 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:49:28,019 - AirSimEnvLogger - INFO - Predictive model loss: 1.656713604927063
2024-07-22 00:49:34,529 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.01293897073958, Velocity: -0.15350170677375055, Movement: 0.23700349087364747, Collision: 0, Height: -1.0, Movement Penalty: -0.05461523606978659, Smoothness: -0.0, Curiosity: 221.56283569335938, Exploration: 0.4819797937567988, Large Movement: 0.9480139634945899, Direction Change: 0.019100631497035336, Total: 204.13749000168258
2024-07-22 00:49:34,656 - AirSimEnvLogger - INFO - Action: [-0.27666228 -0.26721346 -0.27701547 -0.2712928 ], Velocity: (-0.27666227740751803, -0.26721345598483837, -0.2770154723937), Duration: 1.5, Reward: 204.13749000168258, Done: False
2024-07-22 00:49:34,719 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:49:34,719 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:49:37,111 - AirSimEnvLogger - INFO - Predictive model loss: 1.5594985485076904
2024-07-22 00:49:43,719 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.169378285807017, Velocity: -0.10951681478163952, Movement: 0.18805646554694605, Collision: 0, Height: -1.0, Movement Penalty: -0.042631755188681925, Smoothness: -0.0, Curiosity: 227.79722595214844, Exploration: 0.3690210228982272, Large Movement: 0.7522258621877842, Direction Change: 0.0009603455589682897, Total: 209.96092940101778
2024-07-22 00:49:43,814 - AirSimEnvLogger - INFO - Action: [-0.2320853  -0.20191054 -0.21640122 -0.20071302], Velocity: (-0.23208529582515328, -0.20191053723781632, -0.21640121845804447), Duration: 1.5, Reward: 209.96092940101778, Done: False
2024-07-22 00:49:43,877 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:49:43,877 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:49:46,534 - AirSimEnvLogger - INFO - Predictive model loss: 1.5973261594772339
2024-07-22 00:49:53,262 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -17.947829227245563, Velocity: -0.009615567303615206, Movement: 0.012420267246431471, Collision: 0, Height: -1.0, Movement Penalty: -0.004628125253176264, Smoothness: -0.0, Curiosity: 223.69805908203125, Exploration: 0.12479189904899624, Large Movement: 0.049681068985725885, Direction Change: 0.7830798183218544, Total: 206.89323591525397
2024-07-22 00:49:53,279 - AirSimEnvLogger - INFO - Action: [ 0.01870256 -0.01583138  0.00407846  0.03905   ], Velocity: (0.018702560845747812, -0.01583137733581194, 0.004078463356872841), Duration: 1.5, Reward: 206.89323591525397, Done: False
2024-07-22 00:49:53,355 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:49:53,355 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:49:56,173 - AirSimEnvLogger - INFO - Predictive model loss: 1.449775218963623
2024-07-22 00:50:03,155 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.50443412955257, Velocity: -0.16770479920678646, Movement: 0.2582755092021115, Collision: 0, Height: -1.0, Movement Penalty: -0.05839935196252391, Smoothness: -0.0, Curiosity: 239.2308349609375, Exploration: 0.18691216568188965, Large Movement: 1.033102036808446, Direction Change: 0.8713408005512799, Total: 223.03536268278916
2024-07-22 00:50:03,281 - AirSimEnvLogger - INFO - Action: [-0.28663353 -0.31079286 -0.29677259 -0.27243986], Velocity: (-0.2866335313234581, -0.31079286372651427, -0.29677258834359416), Duration: 1.5, Reward: 223.03536268278916, Done: False
2024-07-22 00:50:03,361 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:50:03,361 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:50:05,999 - AirSimEnvLogger - INFO - Predictive model loss: 1.7408232688903809
2024-07-22 00:50:12,829 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.94268466031595, Velocity: -0.20607941367517166, Movement: 0.32371690617935867, Collision: 0, Height: -1.0, Movement Penalty: -0.07383065227801792, Smoothness: -0.0, Curiosity: 254.14170837402344, Exploration: 0.5116977664291449, Large Movement: 1.2948676247174347, Direction Change: 0.00034512624147087223, Total: 236.10006917654513
2024-07-22 00:50:12,970 - AirSimEnvLogger - INFO - Action: [-0.3723123  -0.37876886 -0.37025429 -0.35486051], Velocity: (-0.37231229946014255, -0.3787688645242933, -0.37025429143093186), Duration: 1.5, Reward: 236.10006917654513, Done: False
2024-07-22 00:50:13,047 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:50:13,047 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:50:15,434 - AirSimEnvLogger - INFO - Predictive model loss: 2.0530788898468018
2024-07-22 00:50:22,618 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -18.951417097898567, Velocity: -0.10673287863074879, Movement: 0.17281009278011433, Collision: 0, Height: -1.0, Movement Penalty: -0.04079151140634837, Smoothness: -0.0, Curiosity: 256.83642578125, Exploration: 0.282231004068472, Large Movement: 0.6912403711204573, Direction Change: 0.0026747485635317814, Total: 238.13963605195764
2024-07-22 00:50:22,728 - AirSimEnvLogger - INFO - Action: [-0.18644552 -0.19306386 -0.21775612 -0.2166597 ], Velocity: (-0.1864455167799977, -0.19306385892267014, -0.21775612119344384), Duration: 1.5, Reward: 238.13963605195764, Done: False
2024-07-22 00:50:22,854 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:50:22,854 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:50:25,607 - AirSimEnvLogger - INFO - Predictive model loss: 1.8800873756408691
2024-07-22 00:50:31,952 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.59845912578573, Velocity: -0.22113897893573656, Movement: 0.37980359238861894, Collision: 0, Height: -1.0, Movement Penalty: -0.08834025296529648, Smoothness: -0.0, Curiosity: 278.3961486816406, Exploration: 0.368611775117107, Large Movement: 1.5192143695544758, Direction Change: 0.0010540662228077924, Total: 259.8906812446901
2024-07-22 00:50:32,078 - AirSimEnvLogger - INFO - Action: [-0.42947672 -0.43429752 -0.45159549 -0.45099551], Velocity: (-0.4294767198011131, -0.434297522548283, -0.4515954873805767), Duration: 1.5, Reward: 259.8906812446901, Done: False
2024-07-22 00:50:32,139 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:50:32,139 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:50:34,687 - AirSimEnvLogger - INFO - Predictive model loss: 2.27130389213562
2024-07-22 00:50:41,679 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.24980683721879, Velocity: -0.3278719065517589, Movement: 0.4931867268536757, Collision: 0, Height: -1.0, Movement Penalty: -0.11328288386036507, Smoothness: -0.0, Curiosity: 302.31439208984375, Exploration: 0.6816182518664898, Large Movement: 1.9727469074147028, Direction Change: 0.00015846630920579585, Total: 283.67403879976155
2024-07-22 00:50:41,788 - AirSimEnvLogger - INFO - Action: [-0.56594021 -0.57024433 -0.57224617 -0.55710734], Velocity: (-0.5659402064414218, -0.5702443326020354, -0.5722461656025915), Duration: 1.5, Reward: 283.67403879976155, Done: False
2024-07-22 00:50:41,805 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:50:41,805 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:50:44,742 - AirSimEnvLogger - INFO - Predictive model loss: 2.4847259521484375
2024-07-22 00:50:51,803 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.988197437383153, Velocity: -0.07899935504112746, Movement: 0.12188267573895174, Collision: 0, Height: -1.0, Movement Penalty: -0.02716605537314449, Smoothness: -0.0, Curiosity: 298.4910888671875, Exploration: 0.33450848526152877, Large Movement: 0.48753070295580697, Direction Change: 0.00011590050793552908, Total: 278.5631714967989
2024-07-22 00:50:51,866 - AirSimEnvLogger - INFO - Action: [-0.13965206 -0.14362929 -0.13888656 -0.11990792], Velocity: (-0.1396520591131823, -0.14362929097343285, -0.13888655709269715), Duration: 1.5, Reward: 278.5631714967989, Done: False
2024-07-22 00:50:51,929 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:50:51,929 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:50:54,862 - AirSimEnvLogger - INFO - Predictive model loss: 1.873207449913025
2024-07-22 00:51:01,990 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.476102755790713, Velocity: -0.10818890582949303, Movement: 0.16941324984919776, Collision: 0, Height: -1.0, Movement Penalty: -0.039934195699506896, Smoothness: -0.0, Curiosity: 285.15740966796875, Exploration: 0.4338147566498499, Large Movement: 0.677652999396791, Direction Change: 0.00014699690503650054, Total: 265.95196316596054
2024-07-22 00:51:02,116 - AirSimEnvLogger - INFO - Action: [0.19438509 0.19542934 0.19704113 0.21135421], Velocity: (0.19438508555825715, 0.19542934018303793, 0.19704113379190216), Duration: 1.5, Reward: 265.95196316596054, Done: False
2024-07-22 00:51:02,195 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:51:02,195 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:51:05,053 - AirSimEnvLogger - INFO - Predictive model loss: 1.0393437147140503
2024-07-22 00:51:11,908 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.948248186044335, Velocity: -0.08863941528819391, Movement: 0.14069562127949853, Collision: 0, Height: -1.0, Movement Penalty: -0.031953085232681235, Smoothness: -0.0, Curiosity: 297.6806945800781, Exploration: 0.023016460894511866, Large Movement: 0.5627824851179941, Direction Change: 5.2098960909630065e-05, Total: 277.79548376591333
2024-07-22 00:51:12,033 - AirSimEnvLogger - INFO - Action: [-0.16338781 -0.1624512  -0.16153966 -0.15139001], Velocity: (-0.16338781134411381, -0.16245119708229216, -0.16153966404449052), Duration: 1.5, Reward: 277.79548376591333, Done: False
2024-07-22 00:51:12,128 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:51:12,128 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:51:15,056 - AirSimEnvLogger - INFO - Predictive model loss: 0.93594890832901
2024-07-22 00:51:22,006 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -19.699051979132943, Velocity: -0.03577105893008024, Movement: 0.04982357919099858, Collision: 0, Height: -1.0, Movement Penalty: -0.01553817257728745, Smoothness: -0.0, Curiosity: 290.9852600097656, Exploration: 0.11239639188104938, Large Movement: 0.1992943167639943, Direction Change: 0.014124794778530791, Total: 271.0352322159914
2024-07-22 00:51:22,052 - AirSimEnvLogger - INFO - Action: [0.06718315 0.05941885 0.04342098 0.11922216], Velocity: (0.06718314755670945, 0.059418846605078596, 0.04342098025266554), Duration: 1.5, Reward: 271.0352322159914, Done: False
2024-07-22 00:51:22,083 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:51:22,085 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:51:25,045 - AirSimEnvLogger - INFO - Predictive model loss: 0.6694435477256775
2024-07-22 00:51:32,495 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.224483299167606, Velocity: -0.1336884530036882, Movement: 0.22059617708429946, Collision: 0, Height: -1.0, Movement Penalty: -0.04888287011877205, Smoothness: -0.0, Curiosity: 306.8127746582031, Exploration: 0.13962895932527628, Large Movement: 0.8823847083371978, Direction Change: 0.019898531363703342, Total: 287.035660975177
2024-07-22 00:51:32,607 - AirSimEnvLogger - INFO - Action: [-0.24673228 -0.25360531 -0.26354929 -0.21048232], Velocity: (-0.2467322752068549, -0.2536053075328476, -0.2635492852186267), Duration: 1.5, Reward: 287.035660975177, Done: False
2024-07-22 00:51:32,669 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:51:32,669 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:51:35,557 - AirSimEnvLogger - INFO - Predictive model loss: 0.6419132947921753
2024-07-22 00:51:42,358 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.29383787213858, Velocity: -0.09605069183215043, Movement: 0.13762977236347965, Collision: 0, Height: -1.0, Movement Penalty: -0.028980164369161843, Smoothness: -0.0, Curiosity: 311.365234375, Exploration: 0.3058899647646161, Large Movement: 0.5505190894539186, Direction Change: 0.0076633620523045565, Total: 291.20277002507896
2024-07-22 00:51:42,453 - AirSimEnvLogger - INFO - Action: [-0.15416468 -0.1808304  -0.1389296  -0.09064864], Velocity: (-0.15416468331049646, -0.18083039656326222, -0.13892960469907253), Duration: 1.5, Reward: 291.20277002507896, Done: False
2024-07-22 00:51:42,531 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:51:42,531 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:51:45,056 - AirSimEnvLogger - INFO - Predictive model loss: 0.5909006595611572
2024-07-22 00:51:51,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -20.82967377805935, Velocity: -0.2056042868478309, Movement: 0.31140501855020475, Collision: 0, Height: -1.0, Movement Penalty: -0.07019735427096332, Smoothness: -0.0, Curiosity: 330.12396240234375, Exploration: 0.3253420130719128, Large Movement: 1.245620074200819, Direction Change: 0.0031237748276894406, Total: 310.10892250792386
2024-07-22 00:51:52,122 - AirSimEnvLogger - INFO - Action: [-0.34540985 -0.37919688 -0.35326209 -0.32384335], Velocity: (-0.3454098513144407, -0.37919687841863964, -0.353262090133528), Duration: 1.5, Reward: 310.10892250792386, Done: False
2024-07-22 00:51:52,169 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:51:52,170 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:51:54,933 - AirSimEnvLogger - INFO - Predictive model loss: 0.5195814371109009
2024-07-22 00:52:01,915 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -21.53340142675461, Velocity: -0.3040361394760321, Movement: 0.4655522622639466, Collision: 0, Height: -1.0, Movement Penalty: -0.10608151981013417, Smoothness: -0.0, Curiosity: 357.0840759277344, Exploration: 0.6349907482338629, Large Movement: 1.8622090490557863, Direction Change: 0.00018751988656651175, Total: 337.0410394047667
2024-07-22 00:52:02,038 - AirSimEnvLogger - INFO - Action: [-0.52823418 -0.55353055 -0.53059233 -0.50830429], Velocity: (-0.5282341825259435, -0.5535305473225521, -0.5305923267333011), Duration: 1.5, Reward: 337.0410394047667, Done: False
2024-07-22 00:52:02,101 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:52:02,101 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:52:04,805 - AirSimEnvLogger - INFO - Predictive model loss: 0.4627228379249573
2024-07-22 00:52:11,230 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.326754062972466, Velocity: -0.3859148278168931, Movement: 0.5802063299744837, Collision: 0, Height: -1.0, Movement Penalty: -0.13302843397360778, Smoothness: -0.0, Curiosity: 389.92181396484375, Exploration: 0.783252435003568, Large Movement: 2.320825319897935, Direction Change: 4.417443760107442e-05, Total: 369.5722448171232
2024-07-22 00:52:11,387 - AirSimEnvLogger - INFO - Action: [-0.66291596 -0.68105075 -0.66578513 -0.65046052], Velocity: (-0.6629159620508447, -0.681050752909188, -0.665785130946816), Duration: 1.5, Reward: 369.5722448171232, Done: False
2024-07-22 00:52:11,464 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:52:11,465 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:52:14,179 - AirSimEnvLogger - INFO - Predictive model loss: 0.586532473564148
2024-07-22 00:52:21,120 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.161713386056807, Velocity: -0.4233985439212265, Movement: 0.6438957927367877, Collision: 0, Height: -1.0, Movement Penalty: -0.14813479616161795, Smoothness: -0.0, Curiosity: 426.8975830078125, Exploration: 0.8727101523221182, Large Movement: 2.5755831709471506, Direction Change: 3.594129842743321e-06, Total: 405.9859214745068
2024-07-22 00:52:21,183 - AirSimEnvLogger - INFO - Action: [-0.73566446 -0.75822943 -0.73640553 -0.7321097 ], Velocity: (-0.7356644570602403, -0.7582294292428524, -0.7364055315278835), Duration: 1.5, Reward: 405.9859214745068, Done: False
2024-07-22 00:52:21,244 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:52:21,244 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:52:23,781 - AirSimEnvLogger - INFO - Predictive model loss: 0.9426823258399963
2024-07-22 00:52:30,837 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.24812737659892, Velocity: -0.2346726851500877, Movement: 0.35519085403373013, Collision: 0, Height: -1.0, Movement Penalty: -0.08154110618683424, Smoothness: -0.0, Curiosity: 435.5888977050781, Exploration: 0.5900459577283839, Large Movement: 1.4207634161349205, Direction Change: 0.0014565502290937449, Total: 413.3856463133062
2024-07-22 00:52:30,884 - AirSimEnvLogger - INFO - Action: [-0.43205825 -0.39031926 -0.4069628  -0.40031616], Velocity: (-0.4320582479080953, -0.3903192572817108, -0.40696279801289303), Duration: 1.5, Reward: 413.3856463133062, Done: False
2024-07-22 00:52:30,947 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:52:30,947 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:52:33,787 - AirSimEnvLogger - INFO - Predictive model loss: 0.6947792768478394
2024-07-22 00:52:40,621 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.850159527464427, Velocity: -0.00972898985533964, Movement: 0.014052624529248011, Collision: 0, Height: -1.0, Movement Penalty: -0.0033231886283563612, Smoothness: -0.0, Curiosity: 424.9699401855469, Exploration: 0.3612735403783451, Large Movement: 0.056210498116992046, Direction Change: 0.3649253192205659, Total: 402.48826758306194
2024-07-22 00:52:40,699 - AirSimEnvLogger - INFO - Action: [-0.00497506  0.02439366  0.01304236  0.01773283], Velocity: (-0.0049750617036202205, 0.024393657334900842, 0.013042364337523371), Duration: 1.5, Reward: 402.48826758306194, Done: False
2024-07-22 00:52:40,794 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:52:40,794 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:52:43,837 - AirSimEnvLogger - INFO - Predictive model loss: 0.3091783821582794
2024-07-22 00:52:50,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.292543164608215, Velocity: -0.16162648946048974, Movement: 0.2593852534973249, Collision: 0, Height: -1.0, Movement Penalty: -0.06012769328893072, Smoothness: -0.0, Curiosity: 406.0940246582031, Exploration: 0.5044613835400907, Large Movement: 1.0375410139892995, Direction Change: 0.3123913920113508, Total: 385.0699188497203
2024-07-22 00:52:51,132 - AirSimEnvLogger - INFO - Action: [0.28761507 0.30774336 0.30281089 0.30399196], Velocity: (0.2876150679963544, 0.3077433622298438, 0.30281088915758897), Duration: 1.5, Reward: 385.0699188497203, Done: False
2024-07-22 00:52:51,210 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:52:51,211 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:52:54,035 - AirSimEnvLogger - INFO - Predictive model loss: 0.3897421956062317
2024-07-22 00:53:01,113 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -22.71816127860218, Velocity: -0.04882095347532749, Movement: 0.0784431776061882, Collision: 0, Height: -1.0, Movement Penalty: -0.017947584334015675, Smoothness: -0.0, Curiosity: 418.2856140136719, Exploration: 0.08403068656489299, Large Movement: 0.3137727104247528, Direction Change: 0.0045005716226852455, Total: 395.4066798040001
2024-07-22 00:53:01,223 - AirSimEnvLogger - INFO - Action: [-0.09863039 -0.08453126 -0.08797636 -0.08716794], Velocity: (-0.09863039347883062, -0.08453125938848072, -0.08797636114452781), Duration: 1.5, Reward: 395.4066798040001, Done: False
2024-07-22 00:53:01,332 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:01,332 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:53:04,302 - AirSimEnvLogger - INFO - Predictive model loss: 0.3407832980155945
2024-07-22 00:53:10,783 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.296466525145338, Velocity: -0.19463209236397552, Movement: 0.2970997405583059, Collision: 0, Height: -1.0, Movement Penalty: -0.0685404236571988, Smoothness: -0.0, Curiosity: 440.01214599609375, Exploration: 0.5095159034175717, Large Movement: 1.1883989622332236, Direction Change: 0.0015085245829777882, Total: 417.5121446827468
2024-07-22 00:53:10,786 - AirSimEnvLogger - INFO - Action: [-0.34772583 -0.33595073 -0.3453938  -0.34162252], Velocity: (-0.34772583191940387, -0.3359507285510777, -0.34539380011981874), Duration: 1.5, Reward: 417.5121446827468, Done: False
2024-07-22 00:53:10,833 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:10,833 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:53:13,610 - AirSimEnvLogger - INFO - Predictive model loss: 0.2121989130973816
2024-07-22 00:53:20,120 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.815998099428658, Velocity: -0.24447591514469005, Movement: 0.3760864883318623, Collision: 0, Height: -1.0, Movement Penalty: -0.08698602979819695, Smoothness: -0.0, Curiosity: 462.8157958984375, Exploration: 0.5526019792891279, Large Movement: 1.5043459533274492, Direction Change: 3.119464371326153e-05, Total: 440.11601609881313
2024-07-22 00:53:20,244 - AirSimEnvLogger - INFO - Action: [-0.43839452 -0.43010375 -0.43426397 -0.43691275], Velocity: (-0.4383945151343389, -0.4301037524212097, -0.4342639728051255), Duration: 1.5, Reward: 440.11601609881313, Done: False
2024-07-22 00:53:20,354 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:20,354 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:53:23,143 - AirSimEnvLogger - INFO - Predictive model loss: 0.12480087578296661
2024-07-22 00:53:29,910 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.59703250200516, Velocity: -0.32648780486595597, Movement: 0.5203904962468424, Collision: 0, Height: -1.0, Movement Penalty: -0.12016953552063384, Smoothness: -0.0, Curiosity: 498.1016845703125, Exploration: 0.6711340655980474, Large Movement: 2.0815619849873697, Direction Change: 6.356263908768867e-06, Total: 475.2207087221377
2024-07-22 00:53:30,053 - AirSimEnvLogger - INFO - Action: [-0.60377462 -0.5974735  -0.60142057 -0.60070513], Velocity: (-0.6037746150217563, -0.5974735015479808, -0.6014205712639571), Duration: 1.5, Reward: 475.2207087221377, Done: False
2024-07-22 00:53:30,116 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:30,116 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:53:32,693 - AirSimEnvLogger - INFO - Predictive model loss: 0.18311777710914612
2024-07-22 00:53:39,141 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.921402111655446, Velocity: -0.24360377853593243, Movement: 0.39807727369767204, Collision: 0, Height: -1.0, Movement Penalty: -0.09173919263708019, Smoothness: -0.0, Curiosity: 515.7380981445312, Exploration: 0.5990513476612236, Large Movement: 1.5923090947906882, Direction Change: 0.0001695191230625115, Total: 492.0326268200202
2024-07-22 00:53:39,235 - AirSimEnvLogger - INFO - Action: [-0.45243536 -0.45518892 -0.47113411 -0.45579149], Velocity: (-0.45243536261175843, -0.4551889236463765, -0.4711341101918912), Duration: 1.5, Reward: 492.0326268200202, Done: False
2024-07-22 00:53:39,282 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:39,282 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:53:42,216 - AirSimEnvLogger - INFO - Predictive model loss: 0.19452889263629913
2024-07-22 00:53:49,521 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.31582666942589, Velocity: -0.24457102179267393, Movement: 0.3715848798759067, Collision: 0, Height: -1.0, Movement Penalty: -0.08454224865450172, Smoothness: -0.0, Curiosity: 536.560546875, Exploration: 0.4384809481959792, Large Movement: 1.4863395195036269, Direction Change: 0.00021726332202332, Total: 512.317638526483
2024-07-22 00:53:49,662 - AirSimEnvLogger - INFO - Action: [-0.43497557 -0.41898331 -0.43307105 -0.40303584], Velocity: (-0.4349755682470236, -0.4189833119497261, -0.4330710463023193), Duration: 1.5, Reward: 512.317638526483, Done: False
2024-07-22 00:53:49,724 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:49,724 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:53:52,204 - AirSimEnvLogger - INFO - Predictive model loss: 0.2368471920490265
2024-07-22 00:53:59,273 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.003624152881446, Velocity: -0.02094402576497174, Movement: 0.03360805423391631, Collision: 0, Height: -1.0, Movement Penalty: -0.006832893184076843, Smoothness: -0.0, Curiosity: 527.55126953125, Exploration: 0.24791095982251504, Large Movement: 0.13443221693566523, Direction Change: 0.0035681000960002818, Total: 502.24542759467965
2024-07-22 00:53:59,353 - AirSimEnvLogger - INFO - Action: [-0.04046085 -0.03320613 -0.04216963 -0.0122816 ], Velocity: (-0.04046084548486106, -0.03320613309640613, -0.04216963297200249), Duration: 1.5, Reward: 502.24542759467965, Done: False
2024-07-22 00:53:59,397 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:53:59,397 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:54:02,158 - AirSimEnvLogger - INFO - Predictive model loss: 0.06426932662725449
2024-07-22 00:54:09,397 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.561113806053, Velocity: -0.17466978350985124, Movement: 0.2751784446509644, Collision: 0, Height: -1.0, Movement Penalty: -0.06266604169358819, Smoothness: -0.0, Curiosity: 551.695068359375, Exploration: 0.18166200185753406, Large Movement: 1.1007137786038577, Direction Change: 0.004801613461318244, Total: 526.7757788023291
2024-07-22 00:54:09,476 - AirSimEnvLogger - INFO - Action: [-0.32068913 -0.31616661 -0.31636982 -0.29968412], Velocity: (-0.32068912631091456, -0.31616661114920425, -0.3163698213522676), Duration: 1.5, Reward: 526.7757788023291, Done: False
2024-07-22 00:54:09,537 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:54:09,537 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:54:12,485 - AirSimEnvLogger - INFO - Predictive model loss: 0.04719178006052971
2024-07-22 00:54:19,141 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.884558916688572, Velocity: -0.16527238698239832, Movement: 0.2707287014076222, Collision: 0, Height: -1.0, Movement Penalty: -0.0620956268780688, Smoothness: -0.0, Curiosity: 568.30859375, Exploration: 0.44594097374683767, Large Movement: 1.0829148056304887, Direction Change: 0.0029351186377751803, Total: 543.105713266239
2024-07-22 00:54:19,267 - AirSimEnvLogger - INFO - Action: [-0.2899347  -0.34262514 -0.30285641 -0.30399107], Velocity: (-0.28993469588033244, -0.3426251407901493, -0.3028564083633354), Duration: 1.5, Reward: 543.105713266239, Done: False
2024-07-22 00:54:19,314 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:54:19,314 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:54:22,133 - AirSimEnvLogger - INFO - Predictive model loss: 0.0345427431166172
2024-07-22 00:54:29,123 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.552054528415237, Velocity: -0.2850353328750637, Movement: 0.42838117341254706, Collision: 0, Height: -1.0, Movement Penalty: -0.09831929608334347, Smoothness: -0.0, Curiosity: 600.455322265625, Exploration: 0.5095760874975492, Large Movement: 1.7135246936501882, Direction Change: 0.0007544241925993012, Total: 575.2178139623182
2024-07-22 00:54:29,201 - AirSimEnvLogger - INFO - Action: [-0.47681608 -0.51633714 -0.48998378 -0.48231388], Velocity: (-0.4768160827800663, -0.5163371381668282, -0.4899837771694135), Duration: 1.5, Reward: 575.2178139623182, Done: False
2024-07-22 00:54:29,262 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:54:29,262 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:54:32,268 - AirSimEnvLogger - INFO - Predictive model loss: 0.04445723444223404
2024-07-22 00:54:39,418 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.364421788257463, Velocity: -0.3599409586195953, Movement: 0.5553328507147817, Collision: 0, Height: -1.0, Movement Penalty: -0.12781171955496326, Smoothness: -0.0, Curiosity: 642.0632934570312, Exploration: 0.752555967072151, Large Movement: 2.2213314028591267, Direction Change: 0.00011492220334685666, Total: 616.5713674319269
2024-07-22 00:54:39,542 - AirSimEnvLogger - INFO - Action: [-0.62866297 -0.65656959 -0.638183   -0.63245969], Velocity: (-0.62866297299579, -0.6565695874150219, -0.6381830016521801), Duration: 1.5, Reward: 616.5713674319269, Done: False
2024-07-22 00:54:39,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:54:39,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:54:42,337 - AirSimEnvLogger - INFO - Predictive model loss: 0.13064083456993103
2024-07-22 00:54:49,401 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -27.107799798505482, Velocity: -0.09074969506747295, Movement: 0.1358421523251463, Collision: 0, Height: -1.0, Movement Penalty: -0.03126763770592816, Smoothness: -0.0, Curiosity: 635.5149536132812, Exploration: 0.38830485094784056, Large Movement: 0.5433686093005852, Direction Change: 0.0004704020399397546, Total: 608.5350824568401
2024-07-22 00:54:49,576 - AirSimEnvLogger - INFO - Action: [-0.14855874 -0.16690975 -0.15454384 -0.1547713 ], Velocity: (-0.14855874433557836, -0.166909745393091, -0.15454383768873736), Duration: 1.5, Reward: 608.5350824568401, Done: False
2024-07-22 00:54:49,638 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:54:49,638 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:54:52,020 - AirSimEnvLogger - INFO - Predictive model loss: 0.049702275544404984
2024-07-22 00:54:59,297 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.831998358484935, Velocity: -0.038297540303193836, Movement: 0.06071634118920241, Collision: 0, Height: -1.0, Movement Penalty: -0.012170879567969027, Smoothness: -0.0, Curiosity: 625.3316040039062, Exploration: 0.3717682459982067, Large Movement: 0.24286536475680964, Direction Change: 0.0003730858068068388, Total: 598.3273089723235
2024-07-22 00:54:59,437 - AirSimEnvLogger - INFO - Action: [0.06827804 0.07202137 0.06997804 0.00819357], Velocity: (0.06827804448099481, 0.07202137436417194, 0.06997804388636225), Duration: 1.5, Reward: 598.3273089723235, Done: False
2024-07-22 00:54:59,454 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:54:59,454 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:55:02,367 - AirSimEnvLogger - INFO - Predictive model loss: 0.376323938369751
2024-07-22 00:55:09,292 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.40582262594173, Velocity: -0.14449080971755257, Movement: 0.2284203108747618, Collision: 0, Height: -1.0, Movement Penalty: -0.050382721433852855, Smoothness: -0.0, Curiosity: 606.918701171875, Exploration: 0.3646098258620165, Large Movement: 0.9136812434990472, Direction Change: 0.0016685137236228664, Total: 581.0060954786616
2024-07-22 00:55:09,370 - AirSimEnvLogger - INFO - Action: [0.27013048 0.24951093 0.27106674 0.21245825], Velocity: (0.27013047636870624, 0.24951092648668305, 0.2710667389796546), Duration: 1.5, Reward: 581.0060954786616, Done: False
2024-07-22 00:55:09,448 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:55:09,448 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:55:12,316 - AirSimEnvLogger - INFO - Predictive model loss: 0.7855271697044373
2024-07-22 00:55:19,385 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.726271729716814, Velocity: -0.2230683042638184, Movement: 0.4168130996086618, Collision: 0, Height: -1.0, Movement Penalty: -0.0945098560334078, Smoothness: -0.0, Curiosity: 576.505615234375, Exploration: 0.5531678279002118, Large Movement: 1.6672523984346472, Direction Change: 0.0002759989135463492, Total: 552.0631685768392
2024-07-22 00:55:19,496 - AirSimEnvLogger - INFO - Action: [0.48575644 0.47156698 0.48641331 0.44528491], Velocity: (0.48575643570151783, 0.47156698081606874, 0.48641330964530105), Duration: 1.5, Reward: 552.0631685768392, Done: False
2024-07-22 00:55:19,556 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:55:19,556 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:55:22,242 - AirSimEnvLogger - INFO - Predictive model loss: 1.2603014707565308
2024-07-22 00:55:29,312 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.433093271634945, Velocity: -0.226249573737844, Movement: 0.3317054235226374, Collision: 0, Height: -1.0, Movement Penalty: -0.07618126397281019, Smoothness: -0.0, Curiosity: 558.4823608398438, Exploration: 0.5108655178185633, Large Movement: 1.3268216940905495, Direction Change: 0.007680666637204681, Total: 533.994672834605
2024-07-22 00:55:29,453 - AirSimEnvLogger - INFO - Action: [0.43771018 0.37984048 0.32286988 0.37449238], Velocity: (0.43771017799425915, 0.3798404830356436, 0.32286988014494594), Duration: 1.5, Reward: 533.994672834605, Done: False
2024-07-22 00:55:29,515 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:55:29,515 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:55:32,368 - AirSimEnvLogger - INFO - Predictive model loss: 0.5086854696273804
2024-07-22 00:55:39,397 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.787256738030916, Velocity: -0.012440778480082309, Movement: 0.03783148860763856, Collision: 0, Height: -1.0, Movement Penalty: -0.00820090490215267, Smoothness: -0.0, Curiosity: 568.373046875, Exploration: 0.26080620589933884, Large Movement: 0.15132595443055424, Direction Change: 0.4382385378638656, Total: 543.1735304725903
2024-07-22 00:55:39,554 - AirSimEnvLogger - INFO - Action: [ 0.01170544 -0.02925184 -0.06879098 -0.03163223], Velocity: (0.011705442734103955, -0.029251838886539372, -0.06879097799300501), Duration: 1.5, Reward: 543.1735304725903, Done: False
2024-07-22 00:55:39,614 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:55:39,614 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:55:42,477 - AirSimEnvLogger - INFO - Predictive model loss: 0.030406629666686058
2024-07-22 00:55:49,683 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.89263270806588, Velocity: -0.029767211778423404, Movement: 0.07669785862732884, Collision: 0, Height: -1.0, Movement Penalty: -0.0179342092111266, Smoothness: -0.0, Curiosity: 572.0633544921875, Exploration: 0.2754882951061526, Large Movement: 0.30679143450931534, Direction Change: 0.0644210937304267, Total: 546.1685115883906
2024-07-22 00:55:49,825 - AirSimEnvLogger - INFO - Action: [-0.03009662 -0.06925266 -0.13352344 -0.09291577], Velocity: (-0.030096623671771003, -0.06925266421304163, -0.1335234354524567), Duration: 1.5, Reward: 546.1685115883906, Done: False
2024-07-22 00:55:49,889 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:55:49,890 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:55:52,721 - AirSimEnvLogger - INFO - Predictive model loss: 0.1484779417514801
2024-07-22 00:55:59,823 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.48786083828089, Velocity: -0.11424972012417804, Movement: 0.1560800618954554, Collision: 0, Height: -1.0, Movement Penalty: -0.03562820794591245, Smoothness: -0.0, Curiosity: 555.9994506835938, Exploration: 0.1546647255037284, Large Movement: 0.6243202475818216, Direction Change: 0.2553922317546061, Total: 531.1748795759498
2024-07-22 00:55:59,918 - AirSimEnvLogger - INFO - Action: [0.23268293 0.16206625 0.13052636 0.1717352 ], Velocity: (0.232682926097607, 0.1620662460175533, 0.1305263601330789), Duration: 1.5, Reward: 531.1748795759498, Done: False
2024-07-22 00:56:00,010 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:56:00,011 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:56:02,706 - AirSimEnvLogger - INFO - Predictive model loss: 0.1964799165725708
2024-07-22 00:56:09,699 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.937912066558074, Velocity: -0.09245275509516869, Movement: 0.15544424461125905, Collision: 0, Height: -1.0, Movement Penalty: -0.03588913654666341, Smoothness: -0.0, Curiosity: 573.3314208984375, Exploration: 0.04612952702091577, Large Movement: 0.6217769784450362, Direction Change: 0.08169226322729095, Total: 547.6838524784566
2024-07-22 00:56:09,716 - AirSimEnvLogger - INFO - Action: [-0.13657596 -0.18606223 -0.20827748 -0.179308  ], Velocity: (-0.1365759553041391, -0.18606222921479096, -0.20827747843548552), Duration: 1.5, Reward: 547.6838524784566, Done: False
2024-07-22 00:56:09,759 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:56:09,759 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:56:12,581 - AirSimEnvLogger - INFO - Predictive model loss: 0.6207764744758606
2024-07-22 00:56:19,378 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.761630292947117, Velocity: -0.008973286904071526, Movement: 0.014342524555361032, Collision: 0, Height: -1.0, Movement Penalty: -0.004063402869778808, Smoothness: -0.0, Curiosity: 567.7185668945312, Exploration: 0.1368862641523178, Large Movement: 0.05737009822144413, Direction Change: 0.0031203143843409054, Total: 541.5511100926417
2024-07-22 00:56:19,537 - AirSimEnvLogger - INFO - Action: [0.01278389 0.01872037 0.01757703 0.02878007], Velocity: (0.012783891750531129, 0.0187203677723026, 0.01757703002969993), Duration: 1.5, Reward: 541.5511100926417, Done: False
2024-07-22 00:56:19,583 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:56:19,583 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:56:22,390 - AirSimEnvLogger - INFO - Predictive model loss: 0.5371782779693604
2024-07-22 00:56:29,581 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.031279737541535, Velocity: -0.0788061068169884, Movement: 0.12187138976566571, Collision: 0, Height: -1.0, Movement Penalty: -0.027540511421622463, Smoothness: -0.0, Curiosity: 579.3346557617188, Exploration: 0.0725168953715102, Large Movement: 0.48748555906266283, Direction Change: 0.03050090723583032, Total: 553.3639534118216
2024-07-22 00:56:29,782 - AirSimEnvLogger - INFO - Action: [-0.15733646 -0.12585829 -0.13716949 -0.12820856], Velocity: (-0.1573364627806249, -0.12585829451079655, -0.13716949280424515), Duration: 1.5, Reward: 553.3639534118216, Done: False
2024-07-22 00:56:29,847 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:56:29,847 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:56:32,831 - AirSimEnvLogger - INFO - Predictive model loss: 0.6304868459701538
2024-07-22 00:56:40,030 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.534102236873586, Velocity: -0.1422969626450298, Movement: 0.27675218668508755, Collision: 0, Height: -1.0, Movement Penalty: -0.0624963004081355, Smoothness: -0.0, Curiosity: 602.2122192382812, Exploration: 0.3875441339213732, Large Movement: 1.1070087467403502, Direction Change: 0.0022944708868519736, Total: 576.372117300391
2024-07-22 00:56:40,188 - AirSimEnvLogger - INFO - Action: [-0.32923208 -0.29222842 -0.3355233  -0.29019246], Velocity: (-0.32923208025232276, -0.2922284158873244, -0.33552329519814766), Duration: 1.5, Reward: 576.372117300391, Done: False
2024-07-22 00:56:40,250 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:56:40,250 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:56:43,186 - AirSimEnvLogger - INFO - Predictive model loss: 0.8472017645835876
2024-07-22 00:56:50,240 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.24168423601668, Velocity: -0.007357440085425688, Movement: 0.014308517684091793, Collision: 0, Height: -1.0, Movement Penalty: -0.004006472380098184, Smoothness: -0.0, Curiosity: 591.9876098632812, Exploration: 0.13748130429824298, Large Movement: 0.057234070736367174, Direction Change: 0.775289754794021, Total: 566.8846241198547
2024-07-22 00:56:50,349 - AirSimEnvLogger - INFO - Action: [-0.00545398  0.01836874 -0.02125508  0.0280401 ], Velocity: (-0.0054539833763710965, 0.018368737204335267, -0.02125507638453006), Duration: 1.5, Reward: 566.8846241198547, Done: False
2024-07-22 00:56:50,459 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:56:50,459 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:56:53,347 - AirSimEnvLogger - INFO - Predictive model loss: 0.26824790239334106
2024-07-22 00:57:00,482 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.729404615194806, Velocity: -0.14765503876475283, Movement: 0.23356434229251033, Collision: 0, Height: -1.0, Movement Penalty: -0.05522331293853409, Smoothness: -0.0, Curiosity: 571.2118530273438, Exploration: 0.4234261135376353, Large Movement: 0.9342573691700413, Direction Change: 0.856971624667148, Total: 547.7183588843421
2024-07-22 00:57:00,625 - AirSimEnvLogger - INFO - Action: [0.26794816 0.27876669 0.26211091 0.2945373 ], Velocity: (0.2679481589784592, 0.2787666851405536, 0.262110906522284), Duration: 1.5, Reward: 547.7183588843421, Done: False
2024-07-22 00:57:00,671 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:00,671 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:57:03,085 - AirSimEnvLogger - INFO - Predictive model loss: 0.02736104279756546
2024-07-22 00:57:09,712 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.169881323331243, Velocity: -0.2249039165334211, Movement: 0.3718192715955922, Collision: 0, Height: -1.0, Movement Penalty: -0.08667728857116491, Smoothness: -0.0, Curiosity: 546.7181396484375, Exploration: 0.5366478218674356, Large Movement: 1.4872770863823688, Direction Change: 0.0007056435151665541, Total: 522.646601359633
2024-07-22 00:57:09,821 - AirSimEnvLogger - INFO - Action: [0.43958288 0.42111415 0.42711597 0.44530546], Velocity: (0.4395828782313326, 0.42111414510875583, 0.42711597122407663), Duration: 1.5, Reward: 522.646601359633, Done: False
2024-07-22 00:57:09,851 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:09,851 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:57:12,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.5189726948738098
2024-07-22 00:57:19,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.771070967519403, Velocity: -0.22616272966345094, Movement: 0.3614247084505191, Collision: 0, Height: -1.0, Movement Penalty: -0.08373998910866684, Smoothness: -0.0, Curiosity: 526.7096557617188, Exploration: 0.49094681793381456, Large Movement: 1.4456988338020764, Direction Change: 0.0002727496581642619, Total: 502.98386008710486
2024-07-22 00:57:19,424 - AirSimEnvLogger - INFO - Action: [0.41849333 0.42295737 0.4104652  0.42276151], Velocity: (0.4184933277125439, 0.42295736558446034, 0.41046520080526366), Duration: 1.5, Reward: 502.98386008710486, Done: False
2024-07-22 00:57:19,487 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:19,487 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:57:22,289 - AirSimEnvLogger - INFO - Predictive model loss: 0.6215496063232422
2024-07-22 00:57:28,959 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.063278519889582, Velocity: -0.3154108634044789, Movement: 0.5100376238600409, Collision: 0, Height: -1.0, Movement Penalty: -0.1179235781598737, Smoothness: -0.0, Curiosity: 495.69683837890625, Exploration: 0.5729972270725711, Large Movement: 2.0401504954401637, Direction Change: 1.577995606494298e-05, Total: 473.2864336685157
2024-07-22 00:57:29,069 - AirSimEnvLogger - INFO - Action: [0.58992845 0.59323147 0.58362176 0.59164476], Velocity: (0.5899284522244033, 0.5932314668203008, 0.583621760253626), Duration: 1.5, Reward: 473.2864336685157, Done: False
2024-07-22 00:57:29,116 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:29,116 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:57:31,581 - AirSimEnvLogger - INFO - Predictive model loss: 1.2696101665496826
2024-07-22 00:57:38,583 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.18039384005056, Velocity: -0.10646799058142771, Movement: 0.17591107183576857, Collision: 0, Height: -1.0, Movement Penalty: -0.0395634829466044, Smoothness: -0.0, Curiosity: 493.2237854003906, Exploration: 0.3711863863553568, Large Movement: 0.7036442873430743, Direction Change: 0.0017660270406587575, Total: 469.33013978015174
2024-07-22 00:57:38,693 - AirSimEnvLogger - INFO - Action: [0.21861276 0.19031289 0.19941988 0.18096435], Velocity: (0.21861275716186435, 0.19031289039782961, 0.19941987597064426), Duration: 1.5, Reward: 469.33013978015174, Done: False
2024-07-22 00:57:38,772 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:38,772 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:57:41,744 - AirSimEnvLogger - INFO - Predictive model loss: 0.669891893863678
2024-07-22 00:57:48,963 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.570956213784356, Velocity: -0.2332123384049569, Movement: 0.38150139924707427, Collision: 0, Height: -1.0, Movement Penalty: -0.08733808203789478, Smoothness: -0.0, Curiosity: 468.5343933105469, Exploration: 0.30938560876250937, Large Movement: 1.526005596988297, Direction Change: 0.0007831063586588582, Total: 446.0485875175186
2024-07-22 00:57:49,058 - AirSimEnvLogger - INFO - Action: [0.4514288  0.43188696 0.43801708 0.42499504], Velocity: (0.45142879564059385, 0.43188696079564687, 0.43801708422284696), Duration: 1.5, Reward: 446.0485875175186, Done: False
2024-07-22 00:57:49,152 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:49,152 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:57:52,174 - AirSimEnvLogger - INFO - Predictive model loss: 0.8532457947731018
2024-07-22 00:57:59,277 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.863731968430567, Velocity: -0.023693733804073233, Movement: 0.03614431570231956, Collision: 0, Height: -1.0, Movement Penalty: -0.007670172324499708, Smoothness: -0.0, Curiosity: 474.6296081542969, Exploration: 0.22389537086158937, Large Movement: 0.14457726280927824, Direction Change: 0.007618540297323562, Total: 450.4760422092241
2024-07-22 00:57:59,340 - AirSimEnvLogger - INFO - Action: [0.04930338 0.03519854 0.03944472 0.02564192], Velocity: (0.04930337602838697, 0.03519854427555019, 0.039444718574752013), Duration: 1.5, Reward: 450.4760422092241, Done: False
2024-07-22 00:57:59,371 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:57:59,372 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:58:01,681 - AirSimEnvLogger - INFO - Predictive model loss: 0.1458379179239273
2024-07-22 00:58:09,011 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.144631437545648, Velocity: -0.07414664543426712, Movement: 0.10547103269990672, Collision: 0, Height: -1.0, Movement Penalty: -0.022451375167535707, Smoothness: -0.0, Curiosity: 484.8885192871094, Exploration: 0.32153219258624516, Large Movement: 0.4218841307996269, Direction Change: 0.019350796530438585, Total: 460.7745515701699
2024-07-22 00:58:09,153 - AirSimEnvLogger - INFO - Action: [-0.12519265 -0.1367933  -0.10055321 -0.07687568], Velocity: (-0.12519264981685702, -0.13679330014362762, -0.10055321189692744), Duration: 1.5, Reward: 460.7745515701699, Done: False
2024-07-22 00:58:09,184 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:58:09,184 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:58:12,097 - AirSimEnvLogger - INFO - Predictive model loss: 0.02947601117193699
2024-07-22 00:58:19,126 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -23.731885736155593, Velocity: -0.0931293935289835, Movement: 0.15081381032638505, Collision: 0, Height: -1.0, Movement Penalty: -0.03613563966347682, Smoothness: -0.0, Curiosity: 470.1181335449219, Exploration: 0.06870768183220646, Large Movement: 0.6032552413055402, Direction Change: 0.012993173293788352, Total: 446.52502533774185
2024-07-22 00:58:19,175 - AirSimEnvLogger - INFO - Action: [0.16890571 0.16955656 0.18357738 0.19899554], Velocity: (0.16890571181101566, 0.16955655689813315, 0.1835773844217083), Duration: 1.5, Reward: 446.52502533774185, Done: False
2024-07-22 00:58:19,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:58:19,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:58:22,135 - AirSimEnvLogger - INFO - Predictive model loss: 0.04062332585453987
2024-07-22 00:58:28,803 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.101625340460178, Velocity: -0.06823011826589394, Movement: 0.10276163142737937, Collision: 0, Height: -1.0, Movement Penalty: -0.023163263272042586, Smoothness: -0.0, Curiosity: 483.0655517578125, Exploration: 0.02191703245387645, Large Movement: 0.4110465257095175, Direction Change: 0.012770235438195776, Total: 458.90153631633984
2024-07-22 00:58:28,944 - AirSimEnvLogger - INFO - Action: [-0.1269827  -0.12885901 -0.09752211 -0.10683569], Velocity: (-0.12698269918674154, -0.12885900524214572, -0.0975221126192436), Duration: 1.5, Reward: 458.90153631633984, Done: False
2024-07-22 00:58:29,022 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:58:29,022 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:58:31,720 - AirSimEnvLogger - INFO - Predictive model loss: 0.3300936222076416
2024-07-22 00:58:38,513 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.176520037853766, Velocity: -0.05183141122617925, Movement: 0.07386221949241842, Collision: 0, Height: -1.0, Movement Penalty: -0.016110794124449962, Smoothness: -0.0, Curiosity: 487.15289306640625, Exploration: 0.22308725307809388, Large Movement: 0.2954488779696737, Direction Change: 0.025156663997402462, Total: 462.87054747526895
2024-07-22 00:58:38,591 - AirSimEnvLogger - INFO - Action: [-0.11138823 -0.08619388 -0.04456215 -0.06429043], Velocity: (-0.11138823491414135, -0.08619388409822151, -0.04456215143099801), Duration: 1.5, Reward: 462.87054747526895, Done: False
2024-07-22 00:58:38,685 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:58:38,685 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:58:41,526 - AirSimEnvLogger - INFO - Predictive model loss: 0.5430769324302673
2024-07-22 00:58:48,262 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -24.770101249294253, Velocity: -0.19857907344433287, Movement: 0.30119623878276736, Collision: 0, Height: -1.0, Movement Penalty: -0.06904425152143763, Smoothness: -0.0, Curiosity: 511.6193542480469, Exploration: 0.3360468664433073, Large Movement: 1.2047849551310694, Direction Change: 0.037860612559177054, Total: 487.6948373986084
2024-07-22 00:58:48,263 - AirSimEnvLogger - INFO - Action: [-0.36770044 -0.35110452 -0.32310788 -0.3373932 ], Velocity: (-0.3677004357522311, -0.35110452103474016, -0.32310787963794435), Duration: 1.5, Reward: 487.6948373986084, Done: False
2024-07-22 00:58:48,339 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:58:48,339 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:58:51,141 - AirSimEnvLogger - INFO - Predictive model loss: 1.0263069868087769
2024-07-22 00:58:58,114 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -25.51670768037387, Velocity: -0.31371089930248885, Movement: 0.4702675781486842, Collision: 0, Height: -1.0, Movement Penalty: -0.10825346094161364, Smoothness: -0.0, Curiosity: 545.856689453125, Exploration: 0.6720466821350952, Large Movement: 1.8810703125947368, Direction Change: 0.0004270005053377268, Total: 521.8568164637874
2024-07-22 00:58:58,161 - AirSimEnvLogger - INFO - Action: [-0.5571204  -0.54554817 -0.52592817 -0.53598022], Velocity: (-0.5571203973136254, -0.54554816911122, -0.5259281683916721), Duration: 1.5, Reward: 521.8568164637874, Done: False
2024-07-22 00:58:58,192 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:58:58,192 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:59:01,008 - AirSimEnvLogger - INFO - Predictive model loss: 1.5986013412475586
2024-07-22 00:59:07,599 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -26.080777005039796, Velocity: -0.27436854885507633, Movement: 0.4637625376989951, Collision: 0, Height: -1.0, Movement Penalty: -0.10694235297693455, Smoothness: -0.0, Curiosity: 574.7037353515625, Exploration: 0.6773604950942244, Large Movement: 1.8550501507959805, Direction Change: 0.00043051753973932794, Total: 550.1186811701737
2024-07-22 00:59:07,758 - AirSimEnvLogger - INFO - Action: [-0.56226307 -0.51568901 -0.52747308 -0.53231938], Velocity: (-0.5622630697855686, -0.515689008242394, -0.5274730823704015), Duration: 1.5, Reward: 550.1186811701737, Done: False
2024-07-22 00:59:07,820 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-22 00:59:07,820 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-22 00:59:10,622 - AirSimEnvLogger - INFO - Predictive model loss: 2.13364839553833
2024-07-22 00:59:15,960 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4 and 16x128)
2024-07-22 00:59:15,960 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4 and 16x128)
2024-07-22 00:59:15,960 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4 and 16x128)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 134, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 361, in train
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 243, in update
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\critic_network.py", line 66, in forward
    state_features = F.relu(self.state_encoder(state))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4 and 16x128)
2024-07-22 00:59:21,528 - AirSimEnvLogger - INFO - Environment closed.
2024-07-22 00:59:21,591 - AirSimEnvLogger - INFO - Training script execution finished.
