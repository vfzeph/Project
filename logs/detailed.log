2024-06-03 17:10:54,471 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 17:10:56,061 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 17:10:56,261 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:10:56,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:10:56,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:10:56,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:10:57,399 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:11:03,945 - __main__ - INFO - Action: 6, Reward: tensor([-19.8890], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:04,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:04,709 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-19.8890], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:11:04,755 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:04,755 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:05,050 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:11:16,274 - __main__ - INFO - Action: 8, Reward: tensor([-22.7541], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:16,397 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:16,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:16,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:16,645 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:11:28,860 - __main__ - INFO - Action: 12, Reward: tensor([-74.5007], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:29,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:29,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:29,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:29,576 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:11:41,570 - __main__ - INFO - Action: 0, Reward: tensor([-24.6277], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:41,710 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:41,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:41,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:41,868 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:11:52,679 - __main__ - INFO - Action: 16, Reward: tensor([-24.7108], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:11:52,782 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:11:52,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:11:52,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:11:53,753 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:12:06,035 - __main__ - INFO - Action: 15, Reward: tensor([-24.8546], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:06,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:06,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:06,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:07,171 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:12:12,569 - __main__ - INFO - Action: 6, Reward: tensor([-24.9427], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:12,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:12,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:12,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:13,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:12:25,769 - __main__ - INFO - Action: 0, Reward: tensor([-25.0380], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:26,098 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:26,192 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:26,192 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:26,960 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:12:38,650 - __main__ - INFO - Action: 14, Reward: tensor([-25.1980], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:38,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:38,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:38,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:39,426 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 17:12:46,140 - __main__ - INFO - Action: 7, Reward: tensor([-75.2582], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:46,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:46,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:46,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:47,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:12:52,948 - __main__ - INFO - Action: 11, Reward: tensor([-24.3718], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:12:52,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:12:52,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:12:52,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:12:52,971 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-366.1457], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:12:53,196 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:13:05,026 - __main__ - INFO - Episode timed out.
2024-06-03 17:13:05,481 - __main__ - INFO - Action: 1, Reward: tensor([-22.8578], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:13:06,570 - Drone.source.models.ppo.ppo_agent - INFO - Model saved at e:\Project\models/checkpoints\ppo_agent_epoch_0.pt
2024-06-03 17:13:06,991 - __main__ - INFO - Checkpoint saved at epoch 0
2024-06-03 17:13:07,396 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:13:07,921 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:08,030 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:08,030 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:08,760 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:13:21,151 - __main__ - INFO - Action: 4, Reward: tensor([-22.5970], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:21,885 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-22.5970], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:13:21,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:21,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:21,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:22,858 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:13:29,289 - __main__ - INFO - Action: 10, Reward: tensor([-23.7592], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:29,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:29,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:29,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:30,084 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:13:42,356 - __main__ - INFO - Action: 9, Reward: tensor([-22.7700], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:42,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:42,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:42,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:43,737 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:13:56,205 - __main__ - INFO - Action: 4, Reward: tensor([-22.8452], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:13:56,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:13:56,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:13:56,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:13:57,508 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:14:04,117 - __main__ - INFO - Action: 6, Reward: tensor([-23.0838], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:04,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:04,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:04,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:05,315 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:14:18,153 - __main__ - INFO - Action: 15, Reward: tensor([-24.3568], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:18,464 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:18,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:18,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:19,435 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:14:32,036 - __main__ - INFO - Action: 15, Reward: tensor([-26.5830], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:32,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:32,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:32,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:33,106 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:14:45,868 - __main__ - INFO - Action: 1, Reward: tensor([-28.6251], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:46,148 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:14:46,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:14:46,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:14:47,066 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:14:59,661 - __main__ - INFO - Action: 12, Reward: tensor([-30.7287], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:14:59,974 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:00,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:00,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:00,892 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:15:07,475 - __main__ - INFO - Action: 6, Reward: tensor([-31.6827], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:07,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:07,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:07,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:08,482 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:15:20,112 - __main__ - INFO - Episode timed out.
2024-06-03 17:15:20,333 - __main__ - INFO - Action: 4, Reward: tensor([-34.4819], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:15:20,736 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-291.5134], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:15:21,000 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:15:21,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:21,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:21,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:21,729 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:15:28,307 - __main__ - INFO - Action: 10, Reward: tensor([-16.5137], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:29,093 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-16.5137], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:15:29,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:29,204 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:29,204 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:30,061 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:15:41,633 - __main__ - INFO - Action: 8, Reward: tensor([-20.5039], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:41,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:41,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:41,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:41,892 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:15:47,591 - __main__ - INFO - Action: 3, Reward: tensor([-21.5602], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:15:47,840 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:15:47,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:15:47,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:15:48,280 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:16:00,725 - __main__ - INFO - Action: 15, Reward: tensor([-23.4707], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:01,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:01,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:01,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:01,427 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:16:14,158 - __main__ - INFO - Action: 14, Reward: tensor([-74.1754], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:14,406 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:14,517 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:14,517 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:15,031 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:16:21,555 - __main__ - INFO - Action: 11, Reward: tensor([-23.5138], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:21,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:21,957 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:21,957 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:22,912 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:16:34,989 - __main__ - INFO - Action: 0, Reward: tensor([-22.3746], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:35,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:35,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:35,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:35,659 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:16:41,325 - __main__ - INFO - Action: 10, Reward: tensor([-23.2680], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:41,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:41,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:41,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:41,885 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:16:53,451 - __main__ - INFO - Action: 14, Reward: tensor([-24.6421], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:16:53,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:16:53,651 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:16:53,651 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:16:54,573 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:17:00,783 - __main__ - INFO - Action: 2, Reward: tensor([-24.9542], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:01,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:01,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:01,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:01,909 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:17:08,579 - __main__ - INFO - Action: 6, Reward: tensor([-75.0038], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:09,334 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-349.9803], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:17:09,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:09,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:09,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:09,631 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:17:21,439 - __main__ - INFO - Episode timed out.
2024-06-03 17:17:22,047 - __main__ - INFO - Action: 4, Reward: tensor([-25.1570], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:17:22,622 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:17:23,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:23,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:23,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:23,464 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:17:36,245 - __main__ - INFO - Action: 1, Reward: tensor([-17.0775], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:36,969 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-17.0775], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:17:36,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:37,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:37,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:37,823 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:17:44,451 - __main__ - INFO - Action: 10, Reward: tensor([-18.2853], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:44,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:44,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:44,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:45,020 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:17:57,046 - __main__ - INFO - Action: 0, Reward: tensor([-19.6392], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:17:57,372 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:17:57,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:17:57,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:17:58,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:18:10,682 - __main__ - INFO - Action: 16, Reward: tensor([-20.5892], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:10,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:10,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:10,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:11,914 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:18:24,595 - __main__ - INFO - Action: 8, Reward: tensor([-23.6994], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:24,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:25,919 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:18:32,604 - __main__ - INFO - Action: 6, Reward: tensor([-74.4044], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:32,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:33,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:33,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:34,097 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:18:46,469 - __main__ - INFO - Action: 15, Reward: tensor([-24.5843], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:46,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:46,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:46,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:46,905 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:18:52,773 - __main__ - INFO - Action: 3, Reward: tensor([-24.6401], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:18:53,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:18:53,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:18:53,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:18:53,659 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 17:19:05,926 - __main__ - INFO - Action: 4, Reward: tensor([-24.7601], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:06,174 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:06,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:06,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:07,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-03 17:19:13,474 - __main__ - INFO - Action: 3, Reward: tensor([-24.8369], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:13,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:13,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:13,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:14,623 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:19:20,867 - __main__ - INFO - Action: 3, Reward: tensor([-24.9209], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:21,613 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-297.4372], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:19:21,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:21,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:21,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:22,719 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:19:34,539 - __main__ - INFO - Episode timed out.
2024-06-03 17:19:34,617 - __main__ - INFO - Action: 12, Reward: tensor([-25.0386], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:19:35,084 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:19:35,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:35,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:35,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:36,151 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:19:48,684 - __main__ - INFO - Action: 0, Reward: tensor([-19.3565], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:19:49,540 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-19.3565], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:19:49,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:19:49,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:19:49,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:19:50,398 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:20:02,693 - __main__ - INFO - Action: 0, Reward: tensor([-19.7765], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:03,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:03,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:03,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:03,611 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:20:15,861 - __main__ - INFO - Action: 0, Reward: tensor([-20.8344], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:16,097 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:16,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:16,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:17,157 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:20:23,530 - __main__ - INFO - Action: 3, Reward: tensor([-20.8634], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:23,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:23,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:23,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:24,679 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:20:37,059 - __main__ - INFO - Action: 13, Reward: tensor([-20.6910], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:37,320 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:37,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:37,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:37,792 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:20:49,576 - __main__ - INFO - Action: 9, Reward: tensor([-18.4716], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:49,889 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:49,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:49,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:50,669 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:20:56,346 - __main__ - INFO - Action: 11, Reward: tensor([-17.3052], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:20:56,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:20:56,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:20:56,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:20:57,249 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:21:09,513 - __main__ - INFO - Action: 8, Reward: tensor([-18.1812], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:09,792 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:09,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:09,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:10,303 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:21:16,470 - __main__ - INFO - Action: 2, Reward: tensor([-18.7459], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:16,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:16,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:16,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:17,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:21:29,796 - __main__ - INFO - Action: 0, Reward: tensor([-20.3025], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:30,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:30,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:30,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:31,104 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:21:42,658 - __main__ - INFO - Episode timed out.
2024-06-03 17:21:42,736 - __main__ - INFO - Action: 14, Reward: tensor([-23.1933], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:21:43,330 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-217.7215], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:21:43,688 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:21:44,139 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:44,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:44,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:44,328 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:21:56,663 - __main__ - INFO - Action: 14, Reward: tensor([-16.4346], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:21:57,458 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-16.4346], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:21:57,473 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:21:57,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:21:57,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:21:58,615 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:22:11,232 - __main__ - INFO - Action: 9, Reward: tensor([-14.8396], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:11,499 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:11,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:11,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:12,326 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:22:18,713 - __main__ - INFO - Action: 11, Reward: tensor([-13.7488], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:18,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:19,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:19,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:19,259 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:22:31,364 - __main__ - INFO - Action: 4, Reward: tensor([-11.4678], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:31,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:31,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:31,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:32,345 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:22:37,930 - __main__ - INFO - Action: 11, Reward: tensor([-10.2464], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:38,083 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:38,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:38,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:38,894 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:22:51,275 - __main__ - INFO - Action: 12, Reward: tensor([-9.8550], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:22:51,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:22:51,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:22:51,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:22:52,162 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 17:23:04,415 - __main__ - INFO - Action: 1, Reward: tensor([-9.3453], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:04,667 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:04,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:04,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:05,478 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:23:17,883 - __main__ - INFO - Action: 4, Reward: tensor([-10.6159], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:18,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:18,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:18,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:19,288 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:23:30,513 - __main__ - INFO - Action: 15, Reward: tensor([-12.8370], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:30,710 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:30,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:30,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:31,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:23:36,863 - __main__ - INFO - Action: 2, Reward: tensor([-13.3480], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:23:37,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:37,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:37,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:38,242 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:23:50,169 - __main__ - INFO - Episode timed out.
2024-06-03 17:23:50,202 - __main__ - INFO - Action: 4, Reward: tensor([-16.1083], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:23:50,787 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-138.8469], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:23:51,020 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:23:51,610 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:23:51,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:23:51,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:23:51,938 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:24:04,482 - __main__ - INFO - Action: 1, Reward: tensor([-20.6464], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:05,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:05,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:05,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:05,137 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-20.6464], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:24:05,654 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:24:17,734 - __main__ - INFO - Action: 0, Reward: tensor([-21.0850], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:17,984 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:18,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:18,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:18,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:24:25,382 - __main__ - INFO - Action: 6, Reward: tensor([-21.3626], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:25,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:25,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:25,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:26,753 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:24:38,849 - __main__ - INFO - Action: 4, Reward: tensor([-22.2967], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:39,099 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:39,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:39,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:39,679 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:24:52,403 - __main__ - INFO - Action: 12, Reward: tensor([-23.6358], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:24:52,731 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:24:52,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:24:52,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:24:53,232 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:25:05,679 - __main__ - INFO - Action: 0, Reward: tensor([-25.0558], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:06,025 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:06,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:06,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:06,973 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:25:13,325 - __main__ - INFO - Action: 10, Reward: tensor([-26.3295], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:13,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:13,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:13,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:14,439 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:25:27,075 - __main__ - INFO - Action: 4, Reward: tensor([-77.3545], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:27,431 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:27,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:27,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:28,461 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:25:41,037 - __main__ - INFO - Action: 13, Reward: tensor([-77.4649], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:25:41,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:41,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:41,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:41,756 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:25:53,935 - __main__ - INFO - Episode timed out.
2024-06-03 17:25:54,510 - __main__ - INFO - Action: 0, Reward: tensor([-27.2230], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:25:55,199 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:25:55,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:25:55,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:25:55,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:25:56,854 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:26:09,642 - __main__ - INFO - Action: 12, Reward: tensor([-73.8171], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:10,451 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-73.8171], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:26:10,467 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:10,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:10,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:11,324 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:26:22,727 - __main__ - INFO - Action: 13, Reward: tensor([-73.9549], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:23,008 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:23,101 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:23,101 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:23,864 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:26:30,111 - __main__ - INFO - Action: 2, Reward: tensor([-74.0372], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:30,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:30,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:30,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:31,381 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:26:38,183 - __main__ - INFO - Action: 10, Reward: tensor([-24.1289], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:38,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:38,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:38,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:39,507 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:26:52,051 - __main__ - INFO - Action: 8, Reward: tensor([-24.2899], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:26:52,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:26:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:26:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:26:53,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:27:06,012 - __main__ - INFO - Action: 14, Reward: tensor([-24.4363], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:06,337 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:06,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:06,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:06,712 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-03 17:27:19,673 - __main__ - INFO - Action: 5, Reward: tensor([-24.5686], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:19,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:19,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:19,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:20,716 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:27:33,339 - __main__ - INFO - Action: 8, Reward: tensor([-24.6968], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:34,306 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:27:47,100 - __main__ - INFO - Action: 0, Reward: tensor([-24.8280], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:27:47,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:27:47,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:27:47,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:27:48,381 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:28:00,683 - __main__ - INFO - Episode timed out.
2024-06-03 17:28:01,090 - __main__ - INFO - Action: 14, Reward: tensor([-25.0019], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:28:01,822 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:28:02,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:02,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:02,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:02,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:28:09,314 - __main__ - INFO - Action: 10, Reward: tensor([-20.4511], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:10,194 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-20.4511], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:28:10,224 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:10,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:10,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:11,003 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:28:16,596 - __main__ - INFO - Action: 10, Reward: tensor([-22.1368], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:16,846 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:16,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:16,971 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:17,731 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:28:30,291 - __main__ - INFO - Action: 0, Reward: tensor([-74.0579], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:30,494 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:30,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:30,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:31,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 17:28:43,542 - __main__ - INFO - Action: 1, Reward: tensor([-24.1627], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:43,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:43,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:43,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:44,437 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:28:56,422 - __main__ - INFO - Action: 8, Reward: tensor([-24.3239], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:28:56,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:28:56,810 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:28:56,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:28:57,368 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:29:03,032 - __main__ - INFO - Action: 11, Reward: tensor([-23.9985], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:03,172 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:03,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:03,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:03,604 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:29:09,931 - __main__ - INFO - Action: 2, Reward: tensor([-24.1018], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:10,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:10,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:10,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:10,489 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 17:29:21,901 - __main__ - INFO - Action: 1, Reward: tensor([-23.9316], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:22,133 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:22,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:22,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:22,413 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:29:29,011 - __main__ - INFO - Action: 2, Reward: tensor([-24.0417], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:29,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:29,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:29,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:30,346 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:29:36,384 - __main__ - INFO - Action: 6, Reward: tensor([-24.1445], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:36,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:36,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:36,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:37,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:29:49,529 - __main__ - INFO - Action: 0, Reward: tensor([-24.2832], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:29:50,457 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-309.6338], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:29:50,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:29:50,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:29:50,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:29:51,250 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:30:03,073 - __main__ - INFO - Episode timed out.
2024-06-03 17:30:03,351 - __main__ - INFO - Action: 0, Reward: tensor([-24.9155], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:30:03,914 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:30:04,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:04,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:04,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:05,187 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:30:11,230 - __main__ - INFO - Action: 3, Reward: tensor([-20.6662], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:11,683 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-20.6662], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:30:11,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:11,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:11,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:12,210 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:30:18,008 - __main__ - INFO - Action: 3, Reward: tensor([-21.0325], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:18,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:18,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:18,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:19,058 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:30:30,822 - __main__ - INFO - Action: 8, Reward: tensor([-74.0322], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:31,102 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:31,195 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:31,195 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:31,551 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:30:37,936 - __main__ - INFO - Action: 2, Reward: tensor([-74.0917], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:38,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:38,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:38,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:39,340 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:30:46,003 - __main__ - INFO - Action: 2, Reward: tensor([-24.1971], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:46,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:46,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:46,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:30:47,064 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:30:59,284 - __main__ - INFO - Action: 14, Reward: tensor([-24.3629], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:30:59,390 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:30:59,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:30:59,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:00,324 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:31:07,088 - __main__ - INFO - Action: 6, Reward: tensor([-24.3841], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:07,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:07,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:07,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:08,258 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:31:15,092 - __main__ - INFO - Action: 11, Reward: tensor([-23.5534], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:15,436 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:15,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:15,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:16,467 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:31:29,171 - __main__ - INFO - Action: 9, Reward: tensor([-20.1364], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:29,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:29,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:29,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:30,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:31:42,964 - __main__ - INFO - Action: 1, Reward: tensor([-19.1860], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:43,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:43,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:43,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:44,168 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:31:50,956 - __main__ - INFO - Action: 3, Reward: tensor([-19.2350], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:51,645 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-344.8773], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:31:51,661 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:51,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:51,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:31:52,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:31:59,123 - __main__ - INFO - Action: 11, Reward: tensor([-18.3667], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:31:59,360 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:31:59,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:31:59,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:32:00,311 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:32:12,245 - __main__ - INFO - Episode timed out.
2024-06-03 17:32:12,713 - __main__ - INFO - Action: 9, Reward: tensor([-15.2723], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:32:14,027 - __main__ - INFO - Dropped rows with NA values.
2024-06-03 17:32:14,078 - __main__ - DEBUG - Transformed column values.
2024-06-03 17:34:02,146 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-03 17:34:28,748 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-03 17:34:47,149 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-03 17:35:04,709 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-03 17:40:43,811 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 17:40:46,705 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 17:40:46,782 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 17:40:49,451 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 17:40:49,466 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 17:40:49,512 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:40:49,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:40:50,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:40:50,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:40:50,807 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:40:57,646 - __main__ - INFO - Action: 2, Reward: tensor([-21.9299], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:40:58,548 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-21.9299], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:40:58,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:40:58,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:40:58,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:40:59,588 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:41:12,464 - __main__ - INFO - Action: 12, Reward: tensor([-22.6393], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:12,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:12,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:12,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:13,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:41:26,454 - __main__ - INFO - Action: 5, Reward: tensor([-23.0668], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:26,766 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:26,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:26,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:27,824 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:41:40,314 - __main__ - INFO - Action: 9, Reward: tensor([-21.1836], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:40,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:40,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:40,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:41,309 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:41:53,557 - __main__ - INFO - Action: 4, Reward: tensor([-20.4096], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:41:53,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:41:53,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:41:53,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:41:54,550 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 17:42:00,583 - __main__ - INFO - Action: 7, Reward: tensor([-20.2180], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:00,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:00,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:00,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:01,270 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:42:13,426 - __main__ - INFO - Action: 12, Reward: tensor([-21.0988], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:13,689 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:13,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:13,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:14,485 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:42:27,008 - __main__ - INFO - Action: 14, Reward: tensor([-22.0579], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:27,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:27,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:27,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:28,334 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:42:40,950 - __main__ - INFO - Action: 15, Reward: tensor([-22.2747], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:42:41,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:41,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:41,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:42,356 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:42:54,492 - __main__ - INFO - Episode timed out.
2024-06-03 17:42:54,869 - __main__ - INFO - Action: 16, Reward: tensor([-22.4385], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:42:55,573 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:42:56,008 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:42:56,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:42:56,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:42:56,439 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:43:08,834 - __main__ - INFO - Action: 8, Reward: tensor([-20.8966], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:09,595 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-20.8966], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:43:09,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:09,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:09,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:10,232 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:43:23,066 - __main__ - INFO - Action: 5, Reward: tensor([-22.6262], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:23,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:23,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:23,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:24,237 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:43:36,692 - __main__ - INFO - Action: 12, Reward: tensor([-23.4417], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:36,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:37,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:37,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:38,029 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:43:44,584 - __main__ - INFO - Action: 2, Reward: tensor([-23.6734], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:44,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:44,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:44,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:45,974 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:43:52,471 - __main__ - INFO - Action: 2, Reward: tensor([-23.9301], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:43:52,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:43:52,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:43:52,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:43:53,879 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:44:06,433 - __main__ - INFO - Action: 8, Reward: tensor([-74.7434], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:06,746 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:06,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:06,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:07,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:44:19,476 - __main__ - INFO - Action: 4, Reward: tensor([-24.8769], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:19,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:19,868 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:19,868 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:20,930 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:44:33,683 - __main__ - INFO - Action: 4, Reward: tensor([-25.0319], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:33,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:34,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:34,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:34,958 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:44:47,814 - __main__ - INFO - Action: 8, Reward: tensor([-25.1625], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:44:48,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:44:48,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:44:48,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:44:49,233 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:45:01,585 - __main__ - INFO - Episode timed out.
2024-06-03 17:45:02,151 - __main__ - INFO - Action: 0, Reward: tensor([-25.2963], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:45:02,918 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:45:03,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:03,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:03,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:04,463 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:45:16,979 - __main__ - INFO - Action: 1, Reward: tensor([-73.2076], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:17,728 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-73.2076], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:45:17,744 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:17,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:17,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:18,665 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:45:31,300 - __main__ - INFO - Action: 5, Reward: tensor([-23.7042], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:31,534 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:31,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:31,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:32,478 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:45:44,290 - __main__ - INFO - Action: 14, Reward: tensor([-24.6020], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:44,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:44,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:44,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:45,506 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:45:57,849 - __main__ - INFO - Action: 5, Reward: tensor([-25.8698], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:45:58,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:45:58,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:45:58,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:45:59,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 17:46:05,648 - __main__ - INFO - Action: 7, Reward: tensor([-26.5121], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:05,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:06,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:06,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:06,866 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:46:19,445 - __main__ - INFO - Action: 5, Reward: tensor([-27.9847], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:19,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:19,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:19,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:20,639 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:46:27,388 - __main__ - INFO - Action: 3, Reward: tensor([-28.3049], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:27,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:27,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:27,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:28,719 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:46:41,441 - __main__ - INFO - Action: 15, Reward: tensor([-28.1955], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:41,770 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:41,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:41,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:42,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:46:49,475 - __main__ - INFO - Action: 2, Reward: tensor([-28.3694], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:46:49,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:46:49,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:46:49,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:46:50,407 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:47:02,595 - __main__ - INFO - Action: 0, Reward: tensor([-28.1251], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:02,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:02,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:02,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:03,876 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:47:09,710 - __main__ - INFO - Episode timed out.
2024-06-03 17:47:10,117 - __main__ - INFO - Action: 11, Reward: tensor([-27.3610], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:47:10,879 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-342.2364], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:47:11,238 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:47:11,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:11,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:11,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:12,920 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:47:25,853 - __main__ - INFO - Action: 0, Reward: tensor([-73.4598], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:26,401 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:26,386 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-73.4598], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:47:26,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:26,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:27,398 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:47:40,159 - __main__ - INFO - Action: 14, Reward: tensor([-24.2943], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:40,418 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:40,527 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:40,527 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:41,386 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:47:53,891 - __main__ - INFO - Action: 13, Reward: tensor([-24.5271], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:47:54,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:47:54,332 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:47:54,332 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:47:55,203 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 17:48:07,958 - __main__ - INFO - Action: 13, Reward: tensor([-24.9323], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:08,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:08,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:08,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:09,350 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:48:22,232 - __main__ - INFO - Action: 14, Reward: tensor([-26.6796], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:22,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:22,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:22,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:23,735 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:48:36,314 - __main__ - INFO - Action: 5, Reward: tensor([-28.8332], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:36,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:36,627 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:36,627 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:37,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 17:48:50,319 - __main__ - INFO - Action: 12, Reward: tensor([-29.2838], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:50,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:50,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:50,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:51,741 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:48:58,417 - __main__ - INFO - Action: 2, Reward: tensor([-29.6062], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:48:58,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:48:58,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:48:58,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:48:59,774 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:49:06,545 - __main__ - INFO - Action: 6, Reward: tensor([-29.5435], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:06,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:07,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:07,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:07,881 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:49:20,117 - __main__ - INFO - Episode timed out.
2024-06-03 17:49:20,676 - __main__ - INFO - Action: 0, Reward: tensor([-30.3778], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:49:21,406 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:49:21,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:22,103 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:22,103 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:23,039 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 17:49:35,151 - __main__ - INFO - Action: 5, Reward: tensor([-73.8184], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:35,791 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-73.8184], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:49:35,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:35,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:35,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:36,270 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:49:42,992 - __main__ - INFO - Action: 11, Reward: tensor([-22.9771], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:43,307 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:43,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:43,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:44,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 17:49:56,862 - __main__ - INFO - Action: 0, Reward: tensor([-22.0366], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:49:57,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:49:57,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:49:57,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:49:58,297 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 17:50:05,120 - __main__ - INFO - Action: 2, Reward: tensor([-22.1583], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:05,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:05,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:05,495 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:06,381 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 17:50:18,998 - __main__ - INFO - Action: 0, Reward: tensor([-22.8756], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:19,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:19,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:19,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:20,342 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:50:33,065 - __main__ - INFO - Action: 1, Reward: tensor([-22.8173], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:33,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:33,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:33,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:34,293 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 17:50:39,848 - __main__ - INFO - Action: 2, Reward: tensor([-23.1356], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:40,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:40,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:40,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:41,062 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 17:50:53,695 - __main__ - INFO - Action: 1, Reward: tensor([-22.8391], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:50:53,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:50:54,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:50:54,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:50:54,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:51:01,466 - __main__ - INFO - Action: 3, Reward: tensor([-22.6859], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:01,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:01,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:01,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:02,764 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 17:51:15,027 - __main__ - INFO - Action: 15, Reward: tensor([-21.8917], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:15,272 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:15,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:15,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:16,401 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 17:51:22,625 - __main__ - INFO - Episode timed out.
2024-06-03 17:51:23,205 - __main__ - INFO - Action: 11, Reward: tensor([-20.9171], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:51:23,970 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-298.1526], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:51:24,500 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 17:51:25,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:25,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:25,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:26,189 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:51:38,843 - __main__ - INFO - Action: 14, Reward: tensor([-73.8977], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:39,642 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-73.8977], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:51:39,674 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:39,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:39,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:39,929 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 17:51:52,497 - __main__ - INFO - Action: 8, Reward: tensor([-24.0279], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:51:52,750 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:51:52,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:51:52,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:51:53,810 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:52:06,536 - __main__ - INFO - Action: 4, Reward: tensor([-24.1535], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:06,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:06,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:06,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:07,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 17:52:20,211 - __main__ - INFO - Action: 9, Reward: tensor([-23.1420], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:20,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:20,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:20,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:21,587 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 17:52:35,422 - __main__ - INFO - Action: 4, Reward: tensor([-22.3097], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:35,754 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:35,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:35,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:36,745 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 17:52:49,092 - __main__ - INFO - Action: 14, Reward: tensor([-21.7945], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:49,388 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:49,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:49,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:50,394 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:52:56,823 - __main__ - INFO - Action: 3, Reward: tensor([-21.4972], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:52:57,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:52:57,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:52:57,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:52:57,902 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 17:53:04,257 - __main__ - INFO - Action: 10, Reward: tensor([-22.4390], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:53:04,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:53:04,502 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:53:04,502 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:53:04,955 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 17:53:17,387 - __main__ - INFO - Action: 16, Reward: tensor([-23.1504], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:53:17,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:53:17,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:53:17,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:53:18,553 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 17:53:23,748 - __main__ - INFO - Action: 6, Reward: tensor([-23.7152], grad_fn=<AddBackward0>), Done: False
2024-06-03 17:53:23,913 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 17:53:23,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 17:53:23,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 17:53:24,678 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 17:53:30,789 - __main__ - INFO - Episode timed out.
2024-06-03 17:53:31,289 - __main__ - INFO - Action: 3, Reward: tensor([-23.9484], grad_fn=<AddBackward0>), Done: True
2024-06-03 17:53:32,178 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-304.0754], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 17:53:32,256 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-03 17:53:33,846 - __main__ - INFO - Dropped rows with NA values.
2024-06-03 17:53:33,874 - __main__ - DEBUG - Transformed column values.
2024-06-03 17:54:36,165 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-03 17:57:04,515 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-03 17:57:55,363 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-03 18:00:14,758 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-03 18:10:45,081 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 18:10:47,956 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 18:10:48,110 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 18:10:52,170 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 18:10:52,215 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 18:10:52,513 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:10:53,138 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:10:53,231 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:10:53,231 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:10:54,251 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:14:07,523 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 18:14:10,055 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 18:14:10,177 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 18:14:13,377 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 18:14:13,485 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 18:14:13,888 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:14:14,437 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:14,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:14,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:15,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:14:28,299 - __main__ - INFO - Action: 8, Reward: tensor([-73.8770], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:14:29,125 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-73.8770], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:14:29,141 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:29,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:29,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:30,127 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 18:14:36,812 - __main__ - INFO - Action: 3, Reward: tensor([-23.9703], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:14:37,059 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:37,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:37,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:37,859 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 18:14:50,583 - __main__ - INFO - Action: 9, Reward: tensor([-22.2869], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:14:50,897 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:14:50,928 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:14:50,928 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:14:51,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:15:03,721 - __main__ - INFO - Action: 5, Reward: tensor([-21.4110], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:04,034 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:04,160 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:04,160 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:04,664 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:15:17,567 - __main__ - INFO - Action: 15, Reward: tensor([-21.3748], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:17,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:18,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:18,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:19,033 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:15:31,764 - __main__ - INFO - Action: 8, Reward: tensor([-24.0044], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:32,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:32,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:32,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:33,216 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:15:45,706 - __main__ - INFO - Action: 14, Reward: tensor([-74.9845], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:45,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:46,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:46,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:15:46,680 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:15:59,427 - __main__ - INFO - Action: 12, Reward: tensor([-25.1309], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:15:59,737 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:15:59,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:15:59,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:00,299 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:16:12,552 - __main__ - INFO - Action: 0, Reward: tensor([-25.2769], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:16:12,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:16:12,834 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:12,834 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:13,739 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:16:25,440 - __main__ - INFO - Episode timed out.
2024-06-03 18:16:25,500 - __main__ - INFO - Action: 15, Reward: tensor([-25.3827], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:16:26,187 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:16:26,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:16:26,763 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:26,763 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:27,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:16:40,051 - __main__ - INFO - Action: 16, Reward: tensor([-23.3492], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:16:40,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
n=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:16:40,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:40,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:41,050 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:16:53,281 - __main__ - INFO - Action: 16, Reward: tensor([-23.4702], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:16:53,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:16:53,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:16:53,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:16:53,579 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:17:00,025 - __main__ - INFO - Action: 2, Reward: tensor([-23.6086], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:00,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:00,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:00,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:00,869 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:17:13,570 - __main__ - INFO - Action: 14, Reward: tensor([-23.7759], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:13,773 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:13,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:13,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:14,805 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:17:26,621 - __main__ - INFO - Action: 12, Reward: tensor([-24.7972], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:26,727 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:26,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:26,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:27,628 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:17:40,165 - __main__ - INFO - Action: 16, Reward: tensor([-25.3602], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:40,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:40,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:40,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:41,540 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:17:53,971 - __main__ - INFO - Action: 12, Reward: tensor([-26.8760], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:17:54,220 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:17:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:17:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:17:55,278 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 18:18:07,837 - __main__ - INFO - Action: 4, Reward: tensor([-28.2464], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:08,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:08,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:08,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:08,998 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:18:21,175 - __main__ - INFO - Action: 12, Reward: tensor([-30.6605], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:21,502 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:21,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:21,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:22,174 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:18:34,151 - __main__ - INFO - Episode timed out.
2024-06-03 18:18:34,638 - __main__ - INFO - Action: 0, Reward: tensor([-33.1924], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:18:35,356 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:18:35,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:36,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:36,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:36,765 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:18:43,326 - __main__ - INFO - Action: 11, Reward: tensor([-72.9635], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:43,779 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-72.9635], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:18:43,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:43,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:43,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:44,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:18:57,431 - __main__ - INFO - Action: 5, Reward: tensor([-21.7229], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:18:57,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:18:57,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:18:57,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:18:58,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:19:11,108 - __main__ - INFO - Action: 13, Reward: tensor([-22.0833], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:11,484 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:11,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:11,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:12,313 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:19:17,613 - __main__ - INFO - Action: 7, Reward: tensor([-22.4088], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:17,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:17,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:17,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:18,873 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 18:19:25,581 - __main__ - INFO - Action: 10, Reward: tensor([-23.5560], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:25,814 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:25,908 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:25,908 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:26,543 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:19:38,278 - __main__ - INFO - Action: 15, Reward: tensor([-24.6303], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:38,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:38,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:38,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:38,812 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:19:51,206 - __main__ - INFO - Action: 0, Reward: tensor([-24.9146], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:51,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:51,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:51,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:19:52,530 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 18:19:59,231 - __main__ - INFO - Action: 3, Reward: tensor([-25.3120], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:19:59,528 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:19:59,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:19:59,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:00,544 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 18:20:12,629 - __main__ - INFO - Action: 1, Reward: tensor([-75.7144], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:20:12,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:12,913 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:12,913 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:13,667 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 18:20:26,302 - __main__ - INFO - Action: 4, Reward: tensor([-25.8395], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:20:26,597 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:26,705 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:26,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:27,192 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:20:38,920 - __main__ - INFO - Episode timed out.
2024-06-03 18:20:38,937 - __main__ - INFO - Action: 16, Reward: tensor([-25.9471], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:20:39,513 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-365.0925], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:20:39,905 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:20:40,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:40,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:40,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:41,250 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:20:53,021 - __main__ - INFO - Action: 0, Reward: tensor([-73.5036], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:20:53,723 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-73.5036], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:20:53,739 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:20:53,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:20:53,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:20:54,358 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:21:06,853 - __main__ - INFO - Action: 14, Reward: tensor([-24.3959], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:07,118 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:07,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:07,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:07,756 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:21:13,781 - __main__ - INFO - Action: 7, Reward: tensor([-24.6870], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:13,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:14,051 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:14,051 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:14,880 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 18:21:21,376 - __main__ - INFO - Action: 6, Reward: tensor([-74.9528], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:21,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:21,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:21,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:22,339 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:21:35,069 - __main__ - INFO - Action: 8, Reward: tensor([-75.1211], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:35,307 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:35,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:35,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:36,392 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:21:43,065 - __main__ - INFO - Action: 11, Reward: tensor([-24.8045], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:43,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:43,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:43,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:44,551 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:21:57,361 - __main__ - INFO - Action: 14, Reward: tensor([-25.2265], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:21:57,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:21:57,731 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:21:57,731 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:21:58,639 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:22:05,530 - __main__ - INFO - Action: 7, Reward: tensor([-25.5168], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:05,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:05,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:05,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:06,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 18:22:12,541 - __main__ - INFO - Action: 10, Reward: tensor([-26.5632], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:12,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:12,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:12,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:13,515 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:22:25,903 - __main__ - INFO - Action: 14, Reward: tensor([-76.7314], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:26,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:26,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:26,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:26,591 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:22:38,734 - __main__ - INFO - Action: 13, Reward: tensor([-26.8631], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:22:38,966 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-478.3659], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:22:38,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:38,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:38,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:39,502 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:22:51,200 - __main__ - INFO - Episode timed out.
2024-06-03 18:22:51,635 - __main__ - INFO - Action: 5, Reward: tensor([-26.9949], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:22:52,338 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:22:52,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:22:53,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:22:53,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:22:53,692 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 18:23:06,600 - __main__ - INFO - Action: 1, Reward: tensor([-73.4694], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:07,329 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-73.4694], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:23:07,344 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:07,420 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:07,420 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:08,169 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 18:23:20,548 - __main__ - INFO - Action: 15, Reward: tensor([-74.1694], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:20,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:20,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:20,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:21,533 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:23:34,322 - __main__ - INFO - Action: 14, Reward: tensor([-23.9856], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:34,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:34,668 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:34,668 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:35,463 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:23:47,196 - __main__ - INFO - Action: 14, Reward: tensor([-23.8372], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:23:47,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:23:47,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:23:47,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:23:47,724 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:24:00,179 - __main__ - INFO - Action: 8, Reward: tensor([-74.5382], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:00,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:00,543 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:00,543 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:01,497 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 18:24:08,334 - __main__ - INFO - Action: 2, Reward: tensor([-24.6262], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:08,598 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:08,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:08,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:09,705 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 18:24:16,138 - __main__ - INFO - Action: 10, Reward: tensor([-24.7105], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:16,387 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:16,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:16,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:16,743 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 18:24:22,598 - __main__ - INFO - Action: 7, Reward: tensor([-24.7693], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:22,883 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:22,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:22,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:23,151 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 18:24:35,399 - __main__ - INFO - Action: 8, Reward: tensor([-24.9008], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:35,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:35,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:35,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:36,581 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:24:42,359 - __main__ - INFO - Action: 11, Reward: tensor([-24.6694], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:24:42,473 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:42,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:42,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:42,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 18:24:54,639 - __main__ - INFO - Episode timed out.
2024-06-03 18:24:54,688 - __main__ - INFO - Action: 5, Reward: tensor([-24.4835], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:24:55,188 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-418.1597], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:24:55,658 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:24:56,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:24:56,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:24:56,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:24:57,425 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:25:09,733 - __main__ - INFO - Action: 0, Reward: tensor([-73.8234], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:10,540 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-73.8234], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:25:10,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:10,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:10,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:10,607 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 18:25:22,503 - __main__ - INFO - Action: 12, Reward: tensor([-24.4939], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:22,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:22,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:22,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:23,357 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 18:25:35,722 - __main__ - INFO - Action: 0, Reward: tensor([-75.6006], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:36,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:36,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:36,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:36,891 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 18:25:48,268 - __main__ - INFO - Action: 4, Reward: tensor([-75.7290], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:48,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:48,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:48,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:48,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 18:25:54,644 - __main__ - INFO - Action: 3, Reward: tensor([-25.6895], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:25:54,957 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:25:55,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:25:55,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:25:55,954 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 18:26:08,148 - __main__ - INFO - Action: 0, Reward: tensor([-26.1804], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:08,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:08,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:08,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:09,051 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:26:14,672 - __main__ - INFO - Action: 2, Reward: tensor([-26.3731], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:14,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:15,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:15,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:15,906 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 18:26:27,527 - __main__ - INFO - Action: 1, Reward: tensor([-25.4603], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:27,667 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:27,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:27,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:28,521 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 18:26:40,209 - __main__ - INFO - Action: 14, Reward: tensor([-25.7706], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:40,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:40,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:40,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:41,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 18:26:53,527 - __main__ - INFO - Action: 1, Reward: tensor([-25.5965], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:26:53,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:26:53,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:26:53,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:26:54,726 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:27:00,668 - __main__ - INFO - Episode timed out.
2024-06-03 18:27:01,211 - __main__ - INFO - Action: 2, Reward: tensor([-26.1000], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:27:01,838 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-430.8173], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:27:02,229 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:27:02,776 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:02,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:02,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:03,771 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:27:15,300 - __main__ - INFO - Action: 16, Reward: tensor([-73.4724], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:15,889 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-73.4724], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:27:15,904 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:15,952 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:15,952 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:16,894 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 18:27:28,490 - __main__ - INFO - Action: 1, Reward: tensor([-74.0937], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:28,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:28,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:28,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:28,956 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:27:40,555 - __main__ - INFO - Action: 13, Reward: tensor([-24.2530], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:40,663 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:40,696 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:40,696 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:41,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 18:27:53,584 - __main__ - INFO - Action: 0, Reward: tensor([-24.1083], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:27:53,865 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:27:53,960 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:27:53,960 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:27:54,318 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 18:28:07,017 - __main__ - INFO - Action: 16, Reward: tensor([-23.9383], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:07,330 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:07,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:07,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:08,416 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 18:28:20,709 - __main__ - INFO - Action: 4, Reward: tensor([-23.8820], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:20,984 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:21,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:21,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:21,809 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 18:28:28,123 - __main__ - INFO - Action: 11, Reward: tensor([-23.0133], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:28,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:28,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:28,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:29,171 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 18:28:40,898 - __main__ - INFO - Action: 13, Reward: tensor([-22.3513], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:41,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:41,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:41,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:42,097 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 18:28:54,322 - __main__ - INFO - Action: 9, Reward: tensor([-19.7789], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:28:54,615 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:28:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:28:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:28:55,365 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 18:29:01,389 - __main__ - INFO - Action: 2, Reward: tensor([-19.1672], grad_fn=<AddBackward0>), Done: False
2024-06-03 18:29:01,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:29:01,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:29:01,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 18:29:02,689 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 18:29:14,861 - __main__ - INFO - Episode timed out.
2024-06-03 18:29:15,330 - __main__ - INFO - Action: 0, Reward: tensor([-18.6951], grad_fn=<AddBackward0>), Done: True
2024-06-03 18:29:16,204 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-346.7535], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 18:29:16,329 - __main__ - INFO - Early stopping triggered after 8 epochs.
2024-06-03 18:29:20,950 - __main__ - INFO - Dropped rows with NA values.
2024-06-03 18:29:20,982 - __main__ - DEBUG - Transformed column values.
2024-06-03 18:29:55,186 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-03 18:30:21,461 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-03 18:30:40,512 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-03 18:31:15,314 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-03 18:36:44,230 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 18:36:47,253 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 18:36:47,394 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 18:36:50,944 - __main__ - ERROR - Error loading checkpoint: Error(s) in loading state_dict for AdvancedPolicyNetwork:
	Missing key(s) in state_dict: "attention_layers.0.context_vector", "attention_layers.0.fc.weight", "attention_layers.0.fc.bias", "attention_layers.1.context_vector", "attention_layers.1.fc.weight", "attention_layers.1.fc.bias". 
2024-06-03 18:36:51,628 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 18:36:52,189 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 18:36:52,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 18:36:52,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:23:50,419 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 22:23:53,492 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 22:23:53,647 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 22:23:56,869 - __main__ - ERROR - Error loading checkpoint: Error(s) in loading state_dict for AdvancedPolicyNetwork:
	Missing key(s) in state_dict: "attention_layers.0.context_vector", "attention_layers.0.fc.weight", "attention_layers.0.fc.bias", "attention_layers.1.context_vector", "attention_layers.1.fc.weight", "attention_layers.1.fc.bias". 
2024-06-03 22:23:57,371 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:23:57,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:23:58,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:23:58,056 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:24,330 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 22:31:28,399 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 22:31:28,554 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 22:31:31,847 - __main__ - ERROR - Error loading checkpoint: Error(s) in loading state_dict for AdvancedPolicyNetwork:
	Missing key(s) in state_dict: "attention_layers.0.context_vector", "attention_layers.0.fc.weight", "attention_layers.0.fc.bias", "attention_layers.1.context_vector", "attention_layers.1.fc.weight", "attention_layers.1.fc.bias". 
2024-06-03 22:31:32,411 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:31:32,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:33,040 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:33,040 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:34,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 22:31:38,749 - __main__ - INFO - Action: 2, Reward: tensor([-124.8774], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:31:39,640 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-124.8774], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:31:39,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:39,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:39,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:40,780 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:31:49,303 - __main__ - INFO - Action: 9, Reward: tensor([-123.6867], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:31:49,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:49,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:49,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:31:50,975 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:31:59,207 - __main__ - INFO - Action: 9, Reward: tensor([-21.7931], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:31:59,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:31:59,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:31:59,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:00,425 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:32:08,775 - __main__ - INFO - Action: 12, Reward: tensor([-21.0015], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:09,133 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:09,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:09,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:10,447 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:32:18,464 - __main__ - INFO - Action: 0, Reward: tensor([-21.1119], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:18,587 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:18,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:18,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:19,306 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:32:23,535 - __main__ - INFO - Action: 6, Reward: tensor([-21.3886], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:23,876 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:23,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:23,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:24,407 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 22:32:32,868 - __main__ - INFO - Action: 5, Reward: tensor([-21.4900], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:33,136 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:33,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:33,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:33,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:32:43,000 - __main__ - INFO - Action: 16, Reward: tensor([-21.5154], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:43,330 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:43,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:43,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:44,750 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:32:53,791 - __main__ - INFO - Action: 14, Reward: tensor([-21.9326], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:32:54,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:32:54,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:32:54,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:32:55,355 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:33:04,434 - __main__ - INFO - Action: 16, Reward: tensor([-22.3983], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:04,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:05,296 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:33:13,751 - __main__ - INFO - Action: 14, Reward: tensor([-23.0391], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:14,412 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-444.2346], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:33:14,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:14,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:14,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:15,834 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:33:24,610 - __main__ - INFO - Action: 0, Reward: tensor([-24.0124], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:24,921 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:24,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:24,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:25,646 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:33:33,399 - __main__ - INFO - Episode timed out.
2024-06-03 22:33:34,009 - __main__ - INFO - Action: 14, Reward: tensor([-25.1596], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:33:34,943 - Drone.source.models.ppo.ppo_agent - INFO - Model saved at e:\Project\models/checkpoints\ppo_agent_epoch_0.pt
2024-06-03 22:33:35,742 - __main__ - INFO - Checkpoint saved at epoch 0
2024-06-03 22:33:36,151 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:33:36,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:36,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:36,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:37,936 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:33:47,110 - __main__ - INFO - Action: 9, Reward: tensor([-123.5851], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:47,986 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-123.5851], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:33:48,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:48,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:48,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:48,827 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 22:33:57,834 - __main__ - INFO - Action: 5, Reward: tensor([-23.2442], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:33:58,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:33:58,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:33:58,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:33:58,869 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:34:07,464 - __main__ - INFO - Action: 1, Reward: tensor([-23.3597], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:07,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:07,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:07,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:08,633 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:34:17,060 - __main__ - INFO - Action: 9, Reward: tensor([-22.1373], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:17,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:17,418 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:17,418 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:18,747 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:34:23,615 - __main__ - INFO - Action: 10, Reward: tensor([-22.5376], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:23,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:23,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:23,992 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:25,186 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:34:34,359 - __main__ - INFO - Action: 16, Reward: tensor([-23.3814], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:34,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:35,890 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:34:44,753 - __main__ - INFO - Action: 16, Reward: tensor([-24.0185], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:45,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:45,173 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:45,173 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:46,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:34:51,419 - __main__ - INFO - Action: 7, Reward: tensor([-24.3664], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:34:51,684 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:34:51,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:34:51,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:34:52,891 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:35:02,026 - __main__ - INFO - Action: 14, Reward: tensor([-24.8708], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:02,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:02,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:02,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:03,383 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 22:35:08,245 - __main__ - INFO - Action: 3, Reward: tensor([-25.1214], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:08,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:08,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:08,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:09,815 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:35:18,449 - __main__ - INFO - Action: 9, Reward: tensor([-24.1071], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:19,136 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-360.7294], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:35:19,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:19,247 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:19,247 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:20,467 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:35:28,549 - __main__ - INFO - Action: 4, Reward: tensor([-23.3953], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:28,690 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:28,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:28,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:30,142 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:35:34,283 - __main__ - INFO - Action: 10, Reward: tensor([-23.8912], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:34,312 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:34,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:34,422 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:35,614 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 22:35:44,269 - __main__ - INFO - Episode timed out.
2024-06-03 22:35:44,845 - __main__ - INFO - Action: 8, Reward: tensor([-26.1680], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:35:45,645 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:35:46,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:46,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:46,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:46,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 22:35:51,569 - __main__ - INFO - Action: 3, Reward: tensor([-23.1403], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:35:52,431 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-23.1403], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:35:52,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:35:52,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:35:52,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:35:53,104 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:36:01,793 - __main__ - INFO - Action: 12, Reward: tensor([-23.4525], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:02,123 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:02,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:02,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:03,421 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:36:12,299 - __main__ - INFO - Action: 1, Reward: tensor([-23.7282], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:12,437 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:12,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:12,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:13,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 22:36:18,105 - __main__ - INFO - Action: 3, Reward: tensor([-23.9074], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:18,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:18,465 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:18,465 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:19,684 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:36:24,617 - __main__ - INFO - Action: 7, Reward: tensor([-24.0427], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:24,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:25,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:25,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:26,260 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:36:35,317 - __main__ - INFO - Action: 14, Reward: tensor([-24.0669], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:35,643 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:35,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:35,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:36,915 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 22:36:46,081 - __main__ - INFO - Action: 13, Reward: tensor([-23.8853], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:46,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:46,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:46,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:47,833 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-03 22:36:56,768 - __main__ - INFO - Action: 5, Reward: tensor([-23.8923], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:36:57,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:36:57,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:36:57,145 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:36:58,130 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:37:07,092 - __main__ - INFO - Action: 0, Reward: tensor([-24.1345], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:07,469 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:07,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:07,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:08,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:37:17,049 - __main__ - INFO - Action: 14, Reward: tensor([-24.9217], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:17,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:18,423 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:37:27,055 - __main__ - INFO - Action: 14, Reward: tensor([-25.9729], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:27,711 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-265.1446], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:37:27,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:27,850 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:27,850 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:29,088 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:37:37,702 - __main__ - INFO - Action: 15, Reward: tensor([-26.0410], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:38,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:38,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:38,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:39,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:37:43,969 - __main__ - INFO - Action: 6, Reward: tensor([-26.1279], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:37:44,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:44,375 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:44,375 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:45,622 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 22:37:53,997 - __main__ - INFO - Episode timed out.
2024-06-03 22:37:54,545 - __main__ - INFO - Action: 13, Reward: tensor([-26.4828], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:37:55,297 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:37:55,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:37:56,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:37:56,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:37:57,244 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:38:05,995 - __main__ - INFO - Action: 0, Reward: tensor([-124.9624], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:06,665 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.9624], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:38:06,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:06,822 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:06,822 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:08,042 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:38:16,396 - __main__ - INFO - Action: 0, Reward: tensor([-25.2289], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:16,691 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:16,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:16,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:17,910 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:38:22,884 - __main__ - INFO - Action: 6, Reward: tensor([-25.3284], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:23,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:23,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:23,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:24,567 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 22:38:33,480 - __main__ - INFO - Action: 5, Reward: tensor([-25.4902], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:33,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:33,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:33,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:34,743 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:38:43,703 - __main__ - INFO - Action: 14, Reward: tensor([-125.7994], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:43,968 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:44,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:44,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:45,378 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:38:54,191 - __main__ - INFO - Action: 4, Reward: tensor([-125.8758], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:38:54,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:38:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:38:54,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:38:55,830 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:39:04,603 - __main__ - INFO - Action: 15, Reward: tensor([-26.0607], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:04,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:04,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:04,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:06,195 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:39:14,566 - __main__ - INFO - Action: 12, Reward: tensor([-26.3205], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:14,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:14,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:14,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:16,128 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:39:25,023 - __main__ - INFO - Action: 0, Reward: tensor([-26.8074], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:25,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:25,524 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:25,524 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:26,748 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:39:35,596 - __main__ - INFO - Action: 0, Reward: tensor([-27.3212], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:35,928 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:35,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:35,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:37,166 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:39:45,924 - __main__ - INFO - Action: 0, Reward: tensor([-128.1371], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:46,597 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-687.3320], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:39:46,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:46,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:46,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:47,916 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 22:39:52,491 - __main__ - INFO - Action: 2, Reward: tensor([-28.2860], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:39:52,675 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:52,770 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:52,770 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:39:53,885 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-03 22:39:57,961 - __main__ - INFO - Episode timed out.
2024-06-03 22:39:57,965 - __main__ - INFO - Action: 3, Reward: tensor([-28.2246], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:39:58,625 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:39:59,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:39:59,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:39:59,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:00,558 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 22:40:09,501 - __main__ - INFO - Action: 9, Reward: tensor([-123.5880], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:10,204 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-123.5880], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:40:10,218 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:10,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:10,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:11,507 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 22:40:16,001 - __main__ - INFO - Action: 6, Reward: tensor([-23.5145], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:16,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:16,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:16,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:16,826 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:40:26,018 - __main__ - INFO - Action: 1, Reward: tensor([-23.5565], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:26,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:26,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:26,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:27,619 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:40:32,295 - __main__ - INFO - Action: 10, Reward: tensor([-24.2382], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:32,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:32,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:32,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:34,011 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:40:39,043 - __main__ - INFO - Action: 10, Reward: tensor([-25.1888], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:39,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:39,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:39,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:40,680 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:40:49,024 - __main__ - INFO - Action: 12, Reward: tensor([-125.8320], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:49,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:49,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:49,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:49,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:40:58,200 - __main__ - INFO - Action: 0, Reward: tensor([-26.0123], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:40:58,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:40:58,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:40:58,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:40:59,856 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:41:08,773 - __main__ - INFO - Action: 14, Reward: tensor([-26.2500], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:09,087 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:09,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:09,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:10,122 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 22:41:18,761 - __main__ - INFO - Action: 8, Reward: tensor([-26.4088], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:18,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:19,025 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:19,025 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:20,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 22:41:25,114 - __main__ - INFO - Action: 11, Reward: tensor([-26.3039], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:25,442 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:25,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:25,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:26,662 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 22:41:31,015 - __main__ - INFO - Action: 11, Reward: tensor([-25.9270], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:31,796 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-476.8200], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:41:31,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:31,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:31,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:33,141 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:41:42,041 - __main__ - INFO - Action: 4, Reward: tensor([-25.1634], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:42,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:42,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:42,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:43,720 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 22:41:52,673 - __main__ - INFO - Action: 14, Reward: tensor([-24.7226], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:41:52,969 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:41:53,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:41:53,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:41:54,297 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:42:02,305 - __main__ - INFO - Episode timed out.
2024-06-03 22:42:02,916 - __main__ - INFO - Action: 16, Reward: tensor([-24.5570], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:42:03,666 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:42:04,232 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:04,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:04,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:05,669 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-03 22:42:14,128 - __main__ - INFO - Action: 4, Reward: tensor([-124.9585], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:14,843 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:14,826 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.9585], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:42:14,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:14,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:15,829 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:42:23,882 - __main__ - INFO - Action: 0, Reward: tensor([-25.1048], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:24,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:24,121 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:24,121 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:25,310 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:42:34,281 - __main__ - INFO - Action: 15, Reward: tensor([-25.3481], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:34,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:34,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:35,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-03 22:42:44,950 - __main__ - INFO - Action: 5, Reward: tensor([-125.5897], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:45,264 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:45,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:45,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:46,666 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 22:42:55,187 - __main__ - INFO - Action: 15, Reward: tensor([-25.8942], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:42:55,466 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:42:55,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:42:55,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:42:56,859 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:43:05,632 - __main__ - INFO - Action: 16, Reward: tensor([-26.0375], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:05,897 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:05,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:05,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:07,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 22:43:15,805 - __main__ - INFO - Action: 1, Reward: tensor([-26.2979], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:15,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:15,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:15,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:17,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:43:21,981 - __main__ - INFO - Action: 7, Reward: tensor([-26.3817], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:22,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:23,502 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 22:43:31,154 - __main__ - INFO - Action: 12, Reward: tensor([-26.5829], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:31,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:31,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:31,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:32,360 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 22:43:40,661 - __main__ - INFO - Action: 16, Reward: tensor([-26.7095], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:40,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:41,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:41,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:42,009 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:43:50,575 - __main__ - INFO - Action: 1, Reward: tensor([-26.9457], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:51,244 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-485.8506], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:43:51,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:51,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:51,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:52,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:43:57,166 - __main__ - INFO - Action: 7, Reward: tensor([-27.0968], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:43:57,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:43:57,617 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:43:57,617 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:43:58,765 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-03 22:44:03,437 - __main__ - INFO - Action: 2, Reward: tensor([-27.1910], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:03,672 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:03,782 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:03,782 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:05,096 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 22:44:09,407 - __main__ - INFO - Episode timed out.
2024-06-03 22:44:09,957 - __main__ - INFO - Action: 2, Reward: tensor([-27.3744], grad_fn=<AddBackward0>), Done: True
2024-06-03 22:44:10,615 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 22:44:11,193 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:12,491 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 22:44:16,689 - __main__ - INFO - Action: 7, Reward: tensor([-124.8812], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:17,419 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8812], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 22:44:17,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:18,841 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 22:44:27,484 - __main__ - INFO - Action: 4, Reward: tensor([-25.0410], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:27,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:27,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:27,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:29,131 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:44:37,505 - __main__ - INFO - Action: 0, Reward: tensor([-25.2990], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:37,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:37,672 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:37,672 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:38,833 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 22:44:47,530 - __main__ - INFO - Action: 0, Reward: tensor([-25.5364], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:47,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:47,967 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:47,967 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:49,199 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 22:44:57,337 - __main__ - INFO - Action: 1, Reward: tensor([-25.6276], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:44:57,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:44:57,682 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:44:57,682 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:44:58,983 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-03 22:45:07,835 - __main__ - INFO - Action: 1, Reward: tensor([-25.9061], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:08,193 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:08,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:08,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:09,490 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:45:18,455 - __main__ - INFO - Action: 0, Reward: tensor([-26.0484], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:18,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:18,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:18,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:19,530 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:45:24,494 - __main__ - INFO - Action: 10, Reward: tensor([-26.2327], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:24,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:24,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:24,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:26,062 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 22:45:30,984 - __main__ - INFO - Action: 10, Reward: tensor([-26.4104], grad_fn=<AddBackward0>), Done: False
2024-06-03 22:45:31,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 22:45:31,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 22:45:31,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 22:45:32,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 22:49:24,868 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 22:49:29,090 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 22:49:29,246 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 22:49:33,186 - __main__ - ERROR - Error loading checkpoint: 'icm_state_dict'
2024-06-03 22:49:35,122 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:01:09,309 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:29:14,094 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:42:51,186 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:45:34,435 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:45:39,541 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 23:45:39,696 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 23:45:43,792 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 23:45:45,038 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 23:45:45,448 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:45:45,964 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:45:46,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:45:46,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:45:47,275 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:45:51,972 - __main__ - INFO - Action: 11, Reward: tensor([-124.3182], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:47:58,009 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-03 23:48:03,573 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-03 23:48:03,714 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-03 23:48:07,189 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-03 23:48:08,686 - __main__ - INFO - Resuming training from epoch 1
2024-06-03 23:48:09,092 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:48:09,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:09,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:09,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:11,121 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:48:19,801 - __main__ - INFO - Action: 9, Reward: tensor([-123.5375], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:20,488 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-123.5375], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:48:20,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:20,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:20,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:22,052 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:48:30,757 - __main__ - INFO - Action: 9, Reward: tensor([-21.6875], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:31,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:31,509 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:31,509 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:32,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 23:48:41,566 - __main__ - INFO - Action: 13, Reward: tensor([-20.9474], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:42,237 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:42,346 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:42,346 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:43,569 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:48:52,029 - __main__ - INFO - Action: 0, Reward: tensor([-20.6387], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:48:52,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:48:52,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:48:52,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:48:53,845 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:49:02,609 - __main__ - INFO - Action: 15, Reward: tensor([-20.9335], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:03,217 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:03,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:03,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:04,493 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:49:13,215 - __main__ - INFO - Action: 15, Reward: tensor([-21.0516], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:13,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:13,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:13,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:15,061 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:49:19,541 - __main__ - INFO - Action: 7, Reward: tensor([-21.0263], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:20,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:20,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:20,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:21,378 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 23:49:30,005 - __main__ - INFO - Action: 14, Reward: tensor([-20.8035], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:30,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:30,648 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:30,648 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:31,929 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:49:40,720 - __main__ - INFO - Action: 4, Reward: tensor([-20.7368], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:41,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:41,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:41,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:42,661 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:49:50,677 - __main__ - INFO - Action: 9, Reward: tensor([-19.4768], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:49:51,049 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:49:51,174 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:49:51,174 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:49:52,444 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:50:01,304 - __main__ - INFO - Action: 5, Reward: tensor([-19.0873], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:02,161 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-329.9269], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:50:02,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:02,616 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:02,616 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:03,764 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-03 23:50:12,290 - __main__ - INFO - Episode timed out.
2024-06-03 23:50:12,805 - __main__ - INFO - Action: 12, Reward: tensor([-19.0988], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:50:13,522 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:50:14,257 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:14,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:14,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:15,435 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 23:50:24,147 - __main__ - INFO - Action: 14, Reward: tensor([-124.8637], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:24,851 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8637], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:50:25,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:25,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:25,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:26,447 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 23:50:34,258 - __main__ - INFO - Action: 8, Reward: tensor([-125.1672], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:34,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:34,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:34,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:36,161 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:50:40,958 - __main__ - INFO - Action: 3, Reward: tensor([-25.2516], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:41,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:41,572 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:41,572 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:42,829 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:50:51,540 - __main__ - INFO - Action: 16, Reward: tensor([-25.4276], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:52,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:52,229 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:52,229 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:50:53,487 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:50:58,273 - __main__ - INFO - Action: 7, Reward: tensor([-25.6082], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:50:58,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:50:59,020 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:50:59,020 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:00,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:51:04,919 - __main__ - INFO - Action: 6, Reward: tensor([-25.6930], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:05,484 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:05,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:05,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:06,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:51:11,615 - __main__ - INFO - Action: 11, Reward: tensor([-25.4803], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:12,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:12,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:12,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:13,511 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:51:22,311 - __main__ - INFO - Action: 15, Reward: tensor([-25.0998], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:22,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:23,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:23,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:24,219 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:51:33,001 - __main__ - INFO - Action: 4, Reward: tensor([-25.2463], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:33,615 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:33,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:33,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:34,901 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:51:43,779 - __main__ - INFO - Action: 5, Reward: tensor([-25.6765], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:44,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:44,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:44,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:45,782 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:51:54,637 - __main__ - INFO - Action: 4, Reward: tensor([-26.0885], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:51:55,219 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-479.6028], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:51:55,500 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:51:55,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:51:55,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:51:56,787 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 23:52:01,661 - __main__ - INFO - Action: 10, Reward: tensor([-26.8418], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:02,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:02,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:02,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:03,493 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:52:08,427 - __main__ - INFO - Action: 3, Reward: tensor([-127.0811], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:08,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:08,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:08,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:09,708 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:52:17,836 - __main__ - INFO - Episode timed out.
2024-06-03 23:52:17,837 - __main__ - INFO - Action: 16, Reward: tensor([-27.2313], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:52:18,433 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:52:19,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:19,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:19,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:20,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-03 23:52:29,488 - __main__ - INFO - Action: 14, Reward: tensor([-124.8970], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:30,254 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.8970], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:52:30,458 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:30,519 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:30,519 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:31,689 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:52:40,548 - __main__ - INFO - Action: 4, Reward: tensor([-24.9968], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:41,172 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:41,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:41,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:42,219 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:52:50,711 - __main__ - INFO - Action: 16, Reward: tensor([-24.8432], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:51,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:51,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:51,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:52,256 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:52:56,735 - __main__ - INFO - Action: 7, Reward: tensor([-24.9844], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:52:57,158 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:52:57,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:52:57,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:52:58,416 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:53:07,074 - __main__ - INFO - Action: 0, Reward: tensor([-24.9892], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:07,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:07,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:07,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:08,346 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:53:16,857 - __main__ - INFO - Action: 0, Reward: tensor([-24.9755], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:17,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:17,310 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:17,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:18,537 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:53:22,937 - __main__ - INFO - Action: 11, Reward: tensor([-24.6555], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:23,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:23,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:23,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:24,795 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:53:33,704 - __main__ - INFO - Action: 5, Reward: tensor([-24.1067], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:34,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:34,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:34,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:35,629 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-03 23:53:44,369 - __main__ - INFO - Action: 1, Reward: tensor([-24.0210], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:44,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:45,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:45,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:46,139 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:53:55,121 - __main__ - INFO - Action: 4, Reward: tensor([-24.0400], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:53:55,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:53:55,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:53:55,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:53:56,640 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:54:04,960 - __main__ - INFO - Action: 4, Reward: tensor([-24.2773], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:05,769 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-370.7867], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:54:06,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:06,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:06,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:07,335 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:54:15,000 - __main__ - INFO - Action: 5, Reward: tensor([-24.3969], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:15,589 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:15,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:15,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:16,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:54:23,970 - __main__ - INFO - Episode timed out.
2024-06-03 23:54:24,024 - __main__ - INFO - Action: 15, Reward: tensor([-24.4285], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:54:24,516 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:54:25,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:25,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:25,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:26,724 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-03 23:54:35,785 - __main__ - INFO - Action: 13, Reward: tensor([-124.9566], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:36,538 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.9566], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:54:36,865 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:36,974 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:36,974 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:38,178 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:54:47,133 - __main__ - INFO - Action: 16, Reward: tensor([-25.1164], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:47,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:47,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:47,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:48,963 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:54:57,091 - __main__ - INFO - Action: 0, Reward: tensor([-25.3210], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:54:57,594 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:54:57,594 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:54:57,595 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:54:58,728 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:55:03,625 - __main__ - INFO - Action: 2, Reward: tensor([-25.5109], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:04,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:04,286 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:04,286 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:05,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:55:10,593 - __main__ - INFO - Action: 3, Reward: tensor([-125.5480], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:11,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:11,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:11,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:12,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:55:17,280 - __main__ - INFO - Action: 2, Reward: tensor([-25.6955], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:17,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:18,017 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:18,017 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:19,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:55:28,013 - __main__ - INFO - Action: 5, Reward: tensor([-25.7750], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:28,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:28,653 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:28,653 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:29,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:55:34,611 - __main__ - INFO - Action: 11, Reward: tensor([-25.4699], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:35,217 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:35,326 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:35,326 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:36,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:55:41,069 - __main__ - INFO - Action: 3, Reward: tensor([-25.3986], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:41,507 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:41,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:41,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:42,884 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:55:51,480 - __main__ - INFO - Action: 0, Reward: tensor([-25.2994], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:52,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:52,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:52,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:55:53,442 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-03 23:55:58,315 - __main__ - INFO - Action: 3, Reward: tensor([-25.4328], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:55:59,115 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-479.5242], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:55:59,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:55:59,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:55:59,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:00,743 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:56:05,584 - __main__ - INFO - Action: 6, Reward: tensor([-25.5151], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:06,051 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:06,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:06,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:07,374 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 23:56:12,165 - __main__ - INFO - Action: 10, Reward: tensor([-26.1468], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:12,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:12,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:12,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:13,979 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:56:18,792 - __main__ - INFO - Action: 2, Reward: tensor([-26.5669], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:19,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:19,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:19,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:20,701 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:56:24,956 - __main__ - INFO - Episode timed out.
2024-06-03 23:56:25,395 - __main__ - INFO - Action: 6, Reward: tensor([-26.9326], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:56:25,926 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:56:26,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:26,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:26,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:28,109 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:56:36,860 - __main__ - INFO - Action: 0, Reward: tensor([-124.9084], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:37,578 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.9084], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:56:37,908 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:38,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:38,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:39,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:56:47,911 - __main__ - INFO - Action: 4, Reward: tensor([-125.1381], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:48,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:48,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:48,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:49,889 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:56:54,454 - __main__ - INFO - Action: 2, Reward: tensor([-25.3028], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:56:55,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:56:55,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:56:55,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:56:56,387 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-03 23:57:05,266 - __main__ - INFO - Action: 8, Reward: tensor([-25.4947], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:05,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:06,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:06,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:07,222 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:57:15,996 - __main__ - INFO - Action: 9, Reward: tensor([-25.0073], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:16,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:16,623 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:16,623 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:17,690 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:57:26,430 - __main__ - INFO - Action: 15, Reward: tensor([-24.9869], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:26,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:27,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:27,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:28,299 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-03 23:57:33,273 - __main__ - INFO - Action: 11, Reward: tensor([-24.6470], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:33,697 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:33,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:33,821 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:35,015 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-03 23:57:43,817 - __main__ - INFO - Action: 4, Reward: tensor([-24.2960], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:44,252 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:44,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:44,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:45,604 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:57:54,377 - __main__ - INFO - Action: 0, Reward: tensor([-24.3938], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:57:54,972 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:57:55,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:57:55,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:57:56,285 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-03 23:58:01,113 - __main__ - INFO - Action: 2, Reward: tensor([-24.6113], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:01,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:01,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:01,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:02,863 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-03 23:58:07,566 - __main__ - INFO - Action: 10, Reward: tensor([-25.2377], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:08,405 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-474.0241], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:58:08,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:08,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:08,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:09,976 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:58:18,758 - __main__ - INFO - Action: 0, Reward: tensor([-26.5905], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:19,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:19,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:19,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:20,562 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-03 23:58:28,219 - __main__ - INFO - Episode timed out.
2024-06-03 23:58:28,812 - __main__ - INFO - Action: 0, Reward: tensor([-127.7448], grad_fn=<AddBackward0>), Done: True
2024-06-03 23:58:29,487 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-03 23:58:30,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:30,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:30,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:31,814 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-03 23:58:36,460 - __main__ - INFO - Action: 3, Reward: tensor([-124.8358], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:37,228 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8358], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-03 23:58:37,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:37,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:37,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:38,832 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-03 23:58:47,428 - __main__ - INFO - Action: 15, Reward: tensor([-25.1353], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:47,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:48,052 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:48,052 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:49,373 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-03 23:58:58,092 - __main__ - INFO - Action: 0, Reward: tensor([-25.2151], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:58:58,595 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:58:58,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:58:58,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:58:59,895 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-03 23:59:08,537 - __main__ - INFO - Action: 9, Reward: tensor([-24.2709], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:09,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:09,164 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:09,164 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:10,400 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-03 23:59:19,266 - __main__ - INFO - Action: 5, Reward: tensor([-24.0182], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:19,863 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:19,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:19,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:21,086 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-03 23:59:29,722 - __main__ - INFO - Action: 16, Reward: tensor([-23.6766], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:30,272 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:30,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:30,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:31,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:59:36,398 - __main__ - INFO - Action: 7, Reward: tensor([-23.8591], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:36,823 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:36,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:36,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:38,021 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-03 23:59:42,863 - __main__ - INFO - Action: 6, Reward: tensor([-23.8959], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:43,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:43,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:43,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:44,850 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-03 23:59:49,647 - __main__ - INFO - Action: 7, Reward: tensor([-24.1227], grad_fn=<AddBackward0>), Done: False
2024-06-03 23:59:50,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-03 23:59:50,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-03 23:59:50,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-03 23:59:51,559 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 00:00:00,189 - __main__ - INFO - Action: 0, Reward: tensor([-24.6508], grad_fn=<AddBackward0>), Done: False
2024-06-04 00:00:00,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 00:00:00,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 00:00:00,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 00:00:02,164 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 00:00:10,788 - __main__ - INFO - Action: 0, Reward: tensor([-25.4066], grad_fn=<AddBackward0>), Done: False
2024-06-04 00:00:11,522 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-369.0869], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 00:00:11,773 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 00:00:11,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 00:00:11,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 00:00:12,997 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 00:00:21,530 - __main__ - INFO - Action: 8, Reward: tensor([-127.3891], grad_fn=<AddBackward0>), Done: False
2024-06-04 00:00:22,170 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 00:00:22,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 00:00:22,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 00:00:23,504 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 00:00:31,787 - __main__ - INFO - Episode timed out.
2024-06-04 00:00:32,157 - __main__ - INFO - Action: 12, Reward: tensor([-27.6410], grad_fn=<AddBackward0>), Done: True
2024-06-04 00:00:32,545 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-04 00:00:51,775 - __main__ - INFO - Dropped rows with NA values. Shape changed from (424, 2) to (424, 2).
2024-06-04 00:00:51,801 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 03:54:49,587 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 03:55:10,594 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 03:55:27,000 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 03:55:54,017 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 03:55:59,276 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 03:55:59,417 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 03:56:03,064 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 03:56:03,376 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 03:56:03,749 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 03:56:04,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:04,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:06,114 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 03:56:15,040 - __main__ - INFO - Action: 4, Reward: tensor([-124.9441], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:15,848 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.9441], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 03:56:16,082 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:16,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:16,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:17,440 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 03:56:26,169 - __main__ - INFO - Action: 9, Reward: tensor([-23.7795], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:26,792 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:26,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:26,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:28,152 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 03:56:37,014 - __main__ - INFO - Action: 4, Reward: tensor([-23.5944], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:37,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:37,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:37,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:38,931 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 03:56:47,734 - __main__ - INFO - Action: 13, Reward: tensor([-23.6331], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:48,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:48,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:48,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:49,596 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 03:56:54,116 - __main__ - INFO - Action: 2, Reward: tensor([-23.7464], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:56:54,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:56:54,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:56:54,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:56:56,133 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 03:57:00,968 - __main__ - INFO - Action: 7, Reward: tensor([-23.9713], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:01,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:01,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:01,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:02,740 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 03:57:11,464 - __main__ - INFO - Action: 13, Reward: tensor([-24.4470], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:11,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:12,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:12,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:13,193 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 03:57:17,625 - __main__ - INFO - Action: 7, Reward: tensor([-24.6821], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:18,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:18,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:18,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:19,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 03:57:28,174 - __main__ - INFO - Action: 1, Reward: tensor([-25.2934], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:28,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:28,842 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:28,842 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:30,095 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 03:57:34,889 - __main__ - INFO - Action: 11, Reward: tensor([-25.0735], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:35,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:35,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:35,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:36,772 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 03:57:45,375 - __main__ - INFO - Action: 1, Reward: tensor([-25.0531], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:46,220 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-368.2179], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 03:57:46,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:46,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:46,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:47,266 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 03:57:55,979 - __main__ - INFO - Action: 14, Reward: tensor([-25.1631], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:57:56,617 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:57:56,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:57:56,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:57:57,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 03:58:02,605 - __main__ - INFO - Action: 6, Reward: tensor([-25.1670], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:03,265 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:03,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:03,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:04,562 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 03:58:12,580 - __main__ - INFO - Episode timed out.
2024-06-04 03:58:13,131 - __main__ - INFO - Action: 9, Reward: tensor([-23.9102], grad_fn=<AddBackward0>), Done: True
2024-06-04 03:58:13,807 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 03:58:14,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:14,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:14,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:15,800 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 03:58:24,366 - __main__ - INFO - Action: 15, Reward: tensor([-124.9716], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:25,101 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.9716], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 03:58:25,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:25,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:25,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:26,824 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 03:58:35,684 - __main__ - INFO - Action: 1, Reward: tensor([-125.2090], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:36,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:36,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:36,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:37,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 03:58:46,467 - __main__ - INFO - Action: 4, Reward: tensor([-25.4024], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:47,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:47,124 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:47,124 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:48,424 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 03:58:53,300 - __main__ - INFO - Action: 7, Reward: tensor([-25.4537], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:58:53,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:58:54,004 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:58:54,004 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:58:55,020 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 03:59:03,722 - __main__ - INFO - Action: 13, Reward: tensor([-25.7808], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:04,193 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:04,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:04,319 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:05,556 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 03:59:14,508 - __main__ - INFO - Action: 5, Reward: tensor([-26.0037], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:14,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:15,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:15,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:16,268 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 03:59:21,271 - __main__ - INFO - Action: 2, Reward: tensor([-26.0654], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:21,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:21,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:21,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:23,118 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 03:59:28,078 - __main__ - INFO - Action: 11, Reward: tensor([-25.8075], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:28,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:28,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:28,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:29,944 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 03:59:39,025 - __main__ - INFO - Action: 15, Reward: tensor([-25.3535], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:39,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:39,691 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:39,691 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:40,968 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 03:59:49,491 - __main__ - INFO - Action: 0, Reward: tensor([-25.2500], grad_fn=<AddBackward0>), Done: False
2024-06-04 03:59:50,102 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 03:59:50,212 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 03:59:50,212 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 03:59:51,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:00:00,026 - __main__ - INFO - Action: 14, Reward: tensor([-25.4145], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:00,884 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-480.7121], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:00:01,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:01,373 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:01,373 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:02,595 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 04:00:11,305 - __main__ - INFO - Action: 4, Reward: tensor([-25.6069], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:11,870 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:11,996 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:11,996 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:13,215 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 04:00:17,589 - __main__ - INFO - Episode timed out.
2024-06-04 04:00:18,120 - __main__ - INFO - Action: 2, Reward: tensor([-25.8925], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:00:18,823 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:00:19,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:19,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:19,890 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:21,072 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 04:00:25,584 - __main__ - INFO - Action: 2, Reward: tensor([-124.8542], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:26,380 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.8542], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:00:26,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:26,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:26,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:27,964 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:00:36,785 - __main__ - INFO - Action: 5, Reward: tensor([-25.0853], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:37,363 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:37,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:37,489 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:38,448 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:00:47,073 - __main__ - INFO - Action: 12, Reward: tensor([-25.3158], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:47,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:47,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:47,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:49,059 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 04:00:53,978 - __main__ - INFO - Action: 7, Reward: tensor([-125.4170], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:00:54,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:00:54,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:00:54,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:00:55,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 04:01:04,625 - __main__ - INFO - Action: 1, Reward: tensor([-25.6124], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:05,189 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:05,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:05,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:06,461 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 04:01:15,008 - __main__ - INFO - Action: 13, Reward: tensor([-25.9223], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:15,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:15,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:15,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:16,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:01:25,711 - __main__ - INFO - Action: 15, Reward: tensor([-26.1256], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:26,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:26,461 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:26,461 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:27,681 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:01:32,500 - __main__ - INFO - Action: 10, Reward: tensor([-26.2323], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:33,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:33,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:33,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:34,315 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 04:01:42,916 - __main__ - INFO - Action: 8, Reward: tensor([-26.4599], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:43,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:43,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:43,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:44,797 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:01:49,641 - __main__ - INFO - Action: 10, Reward: tensor([-26.6114], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:01:50,171 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:01:50,297 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:01:50,297 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:01:51,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:01:59,900 - __main__ - INFO - Action: 5, Reward: tensor([-26.7613], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:00,435 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-484.3975], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:02:00,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:00,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:00,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:02,077 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 04:02:10,986 - __main__ - INFO - Action: 1, Reward: tensor([-26.9787], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:11,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:11,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:11,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:12,926 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:02:17,067 - __main__ - INFO - Action: 3, Reward: tensor([-27.1528], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:17,521 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:17,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:17,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:18,888 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:02:23,120 - __main__ - INFO - Episode timed out.
2024-06-04 04:02:23,621 - __main__ - INFO - Action: 11, Reward: tensor([-26.8503], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:02:24,351 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:02:25,150 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:25,276 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:25,276 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:26,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:02:34,950 - __main__ - INFO - Action: 15, Reward: tensor([-124.9780], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:35,602 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.9780], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:02:35,931 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:36,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:36,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:37,311 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 04:02:42,116 - __main__ - INFO - Action: 2, Reward: tensor([-25.0376], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:42,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:42,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:42,760 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:44,008 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:02:52,980 - __main__ - INFO - Action: 5, Reward: tensor([-24.9643], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:02:53,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:02:53,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:02:53,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:02:54,715 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:03:03,463 - __main__ - INFO - Action: 14, Reward: tensor([-25.2133], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:04,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:04,104 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:04,104 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:05,251 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:03:13,920 - __main__ - INFO - Action: 12, Reward: tensor([-25.5709], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:14,512 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:14,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:14,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:15,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 04:03:24,637 - __main__ - INFO - Action: 5, Reward: tensor([-26.1653], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:25,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:25,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:25,283 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:26,517 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:03:31,267 - __main__ - INFO - Action: 11, Reward: tensor([-25.9879], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:31,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:31,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:31,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:33,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 04:03:37,912 - __main__ - INFO - Action: 7, Reward: tensor([-26.0846], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:38,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:38,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:38,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:39,771 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:03:48,492 - __main__ - INFO - Action: 14, Reward: tensor([-26.8218], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:49,058 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:49,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:49,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:03:50,302 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:03:59,123 - __main__ - INFO - Action: 14, Reward: tensor([-28.1610], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:03:59,653 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:03:59,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:03:59,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:01,043 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 04:04:09,868 - __main__ - INFO - Action: 0, Reward: tensor([-29.3359], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:10,663 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-388.3207], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:04:10,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:11,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:11,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:12,265 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 04:04:21,067 - __main__ - INFO - Action: 8, Reward: tensor([-130.0104], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:21,646 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:21,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:21,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:22,962 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 04:04:31,401 - __main__ - INFO - Episode timed out.
2024-06-04 04:04:31,890 - __main__ - INFO - Action: 13, Reward: tensor([-30.2247], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:04:32,651 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:04:33,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:33,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:33,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:34,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 04:04:39,494 - __main__ - INFO - Action: 2, Reward: tensor([-124.8556], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:40,290 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.8556], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:04:40,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:40,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:40,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:42,052 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:04:46,960 - __main__ - INFO - Action: 3, Reward: tensor([-24.9511], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:47,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:47,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:47,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:48,862 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 04:04:53,493 - __main__ - INFO - Action: 6, Reward: tensor([-25.1365], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:04:53,994 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:04:54,117 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:04:54,117 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:04:55,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:05:00,106 - __main__ - INFO - Action: 3, Reward: tensor([-25.2912], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:00,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:00,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:00,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:02,075 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:05:07,032 - __main__ - INFO - Action: 10, Reward: tensor([-25.4351], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:07,594 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:07,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:07,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:08,998 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:05:13,690 - __main__ - INFO - Action: 11, Reward: tensor([-125.5134], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:14,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:14,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:14,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:15,547 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 04:05:20,537 - __main__ - INFO - Action: 3, Reward: tensor([-25.6224], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:21,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:21,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:21,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:22,415 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 04:05:31,384 - __main__ - INFO - Action: 5, Reward: tensor([-25.7305], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:31,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:32,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:32,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:33,243 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 04:05:41,630 - __main__ - INFO - Action: 4, Reward: tensor([-126.0842], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:41,955 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:41,955 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:41,955 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:43,205 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 04:05:52,188 - __main__ - INFO - Action: 4, Reward: tensor([-126.3581], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:52,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:52,846 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:52,846 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:05:53,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:05:58,593 - __main__ - INFO - Action: 3, Reward: tensor([-26.4798], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:05:59,215 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-681.4578], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:05:59,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:05:59,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:05:59,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:00,815 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 04:06:05,572 - __main__ - INFO - Action: 10, Reward: tensor([-26.6190], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:06,195 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:06,320 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:06,320 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:07,448 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:06:16,168 - __main__ - INFO - Action: 12, Reward: tensor([-26.8674], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:16,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:16,732 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:16,732 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:17,988 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:06:26,572 - __main__ - INFO - Action: 15, Reward: tensor([-27.0949], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:27,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:27,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:27,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:27,912 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 04:06:35,996 - __main__ - INFO - Episode timed out.
2024-06-04 04:06:36,559 - __main__ - INFO - Action: 5, Reward: tensor([-27.1210], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:06:37,040 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 04:06:37,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:37,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:37,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:39,212 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 04:06:47,988 - __main__ - INFO - Action: 4, Reward: tensor([-124.9249], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:48,613 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.9249], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:06:48,845 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:48,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:48,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:50,160 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 04:06:54,892 - __main__ - INFO - Action: 3, Reward: tensor([-125.0920], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:06:55,314 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:06:55,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:06:55,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:06:56,663 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 04:07:05,371 - __main__ - INFO - Action: 12, Reward: tensor([-125.3327], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:06,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:06,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:06,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:07,321 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 04:07:12,199 - __main__ - INFO - Action: 11, Reward: tensor([-25.0473], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:12,776 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:12,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:12,807 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:14,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 04:07:18,720 - __main__ - INFO - Action: 3, Reward: tensor([-24.9509], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:19,205 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:19,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:19,299 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:20,565 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 7 selected.
2024-06-04 04:07:25,235 - __main__ - INFO - Action: 7, Reward: tensor([-25.0307], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:25,798 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:25,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:25,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:27,125 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 04:07:35,953 - __main__ - INFO - Action: 13, Reward: tensor([-25.5300], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:36,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:36,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:36,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:36,706 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 04:07:45,235 - __main__ - INFO - Action: 0, Reward: tensor([-25.8734], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:45,827 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:45,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:45,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:47,082 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 04:07:55,516 - __main__ - INFO - Action: 14, Reward: tensor([-26.3254], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:07:56,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:07:56,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:07:56,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:07:57,515 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 04:08:06,630 - __main__ - INFO - Action: 9, Reward: tensor([-25.2726], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:08:07,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:08:07,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:08:07,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:08:08,521 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 04:08:17,213 - __main__ - INFO - Action: 8, Reward: tensor([-26.3476], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:08:18,072 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-579.7277], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 04:08:18,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:08:18,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:08:18,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:08:19,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 04:08:28,237 - __main__ - INFO - Action: 0, Reward: tensor([-27.1758], grad_fn=<AddBackward0>), Done: False
2024-06-04 04:08:28,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 04:08:28,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 04:08:28,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 04:08:29,711 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 04:08:38,026 - __main__ - INFO - Episode timed out.
2024-06-04 04:08:38,432 - __main__ - INFO - Action: 15, Reward: tensor([-27.2904], grad_fn=<AddBackward0>), Done: True
2024-06-04 04:08:38,730 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-04 04:08:59,654 - __main__ - INFO - Dropped rows with NA values. Shape changed from (506, 2) to (506, 2).
2024-06-04 04:08:59,657 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 07:11:20,524 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 07:11:36,311 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 07:11:55,430 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 07:12:13,576 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-04 07:13:29,905 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 07:13:35,495 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 07:13:35,665 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 07:13:39,533 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 07:13:40,632 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 07:13:40,979 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:13:41,848 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:13:41,958 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:13:41,958 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:13:43,150 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:13:51,700 - __main__ - INFO - Action: 15, Reward: tensor([-124.8712], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:13:52,572 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.8712], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:13:52,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:13:53,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:13:53,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:13:54,305 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 07:14:03,186 - __main__ - INFO - Action: 8, Reward: tensor([-25.0924], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:03,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:03,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:03,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:04,934 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:14:13,656 - __main__ - INFO - Action: 15, Reward: tensor([-25.3583], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:14,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:14,390 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:14,390 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:15,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:14:20,566 - __main__ - INFO - Action: 3, Reward: tensor([-25.4832], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:21,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:21,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:21,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:22,469 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:14:31,398 - __main__ - INFO - Action: 15, Reward: tensor([-25.7421], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:31,959 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:32,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:32,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:33,322 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:14:38,184 - __main__ - INFO - Action: 2, Reward: tensor([-25.7129], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:38,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:38,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:38,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:40,200 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:14:49,039 - __main__ - INFO - Action: 0, Reward: tensor([-25.8989], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:49,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:49,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:49,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:14:50,996 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 07:14:59,217 - __main__ - INFO - Action: 12, Reward: tensor([-26.2957], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:14:59,700 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:14:59,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:14:59,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:00,982 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:15:05,597 - __main__ - INFO - Action: 11, Reward: tensor([-26.0084], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:06,033 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:06,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:06,144 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:07,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 07:15:16,177 - __main__ - INFO - Action: 4, Reward: tensor([-25.8115], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:16,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:16,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:16,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:17,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:15:22,976 - __main__ - INFO - Action: 2, Reward: tensor([-26.0748], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:23,768 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-382.3494], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:15:24,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:24,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:24,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:25,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 07:15:34,121 - __main__ - INFO - Action: 9, Reward: tensor([-24.9760], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:34,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:34,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:34,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:35,933 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:15:44,255 - __main__ - INFO - Episode timed out.
2024-06-04 07:15:44,646 - __main__ - INFO - Action: 1, Reward: tensor([-24.2898], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:15:45,346 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:15:46,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:46,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:46,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:47,184 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:15:55,936 - __main__ - INFO - Action: 16, Reward: tensor([-124.5046], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:15:56,642 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.5046], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:15:56,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:15:57,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:15:57,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:15:58,226 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:16:06,768 - __main__ - INFO - Action: 0, Reward: tensor([-24.6631], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:07,330 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:07,439 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:07,439 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:08,628 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 07:16:17,550 - __main__ - INFO - Action: 8, Reward: tensor([-125.2922], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:18,113 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:18,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:18,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:19,518 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:16:28,456 - __main__ - INFO - Action: 15, Reward: tensor([-25.5285], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:28,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:29,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:29,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:30,346 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:16:39,176 - __main__ - INFO - Action: 1, Reward: tensor([-25.7651], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:39,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:39,914 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:39,914 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:40,757 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:16:45,340 - __main__ - INFO - Action: 11, Reward: tensor([-25.4647], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:45,765 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:45,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:45,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:47,019 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:16:56,028 - __main__ - INFO - Action: 14, Reward: tensor([-25.1145], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:16:56,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:16:56,764 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:16:56,764 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:16:57,801 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:17:06,660 - __main__ - INFO - Action: 1, Reward: tensor([-25.1229], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:07,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:07,410 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:07,410 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:08,676 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 07:17:17,452 - __main__ - INFO - Action: 12, Reward: tensor([-25.5076], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:17,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:17,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:17,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:19,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:17:23,844 - __main__ - INFO - Action: 11, Reward: tensor([-25.2505], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:24,405 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:24,512 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:24,512 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:25,638 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:17:30,160 - __main__ - INFO - Action: 11, Reward: tensor([-24.7565], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:30,911 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-476.9703], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:17:31,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:31,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:31,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:32,505 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:17:37,162 - __main__ - INFO - Action: 2, Reward: tensor([-24.6035], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:17:37,615 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:37,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:37,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:38,988 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:17:47,503 - __main__ - INFO - Episode timed out.
2024-06-04 07:17:48,048 - __main__ - INFO - Action: 0, Reward: tensor([-24.4970], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:17:48,689 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:17:49,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:17:49,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:17:49,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:17:50,957 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:17:59,904 - __main__ - INFO - Action: 13, Reward: tensor([-124.8738], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:00,670 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.8738], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:18:00,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:01,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:01,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:02,359 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:18:11,241 - __main__ - INFO - Action: 0, Reward: tensor([-25.0205], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:11,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:11,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:11,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:13,078 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:18:17,871 - __main__ - INFO - Action: 2, Reward: tensor([-25.2197], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:18,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:18,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:18,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:19,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 07:18:24,118 - __main__ - INFO - Action: 2, Reward: tensor([-125.3805], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:24,661 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:24,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:24,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:25,946 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:18:34,516 - __main__ - INFO - Action: 16, Reward: tensor([-25.5239], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:35,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:35,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:35,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:36,287 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 07:18:41,094 - __main__ - INFO - Action: 7, Reward: tensor([-25.6723], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:41,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:41,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:41,687 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:42,927 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:18:51,647 - __main__ - INFO - Action: 14, Reward: tensor([-25.8620], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:18:52,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:18:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:18:52,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:18:53,556 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:19:02,509 - __main__ - INFO - Action: 14, Reward: tensor([-26.2588], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:03,102 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:03,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:03,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:04,368 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:19:09,307 - __main__ - INFO - Action: 2, Reward: tensor([-26.4480], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:09,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:10,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:10,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:11,347 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:19:20,247 - __main__ - INFO - Action: 0, Reward: tensor([-26.9535], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:20,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:20,889 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:20,889 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:22,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 07:19:31,156 - __main__ - INFO - Action: 15, Reward: tensor([-27.1655], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:31,839 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-484.3785], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:19:32,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:32,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:32,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:33,403 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 07:19:42,125 - __main__ - INFO - Action: 5, Reward: tensor([-27.1543], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:42,700 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:42,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:42,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:44,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:19:48,634 - __main__ - INFO - Action: 3, Reward: tensor([-27.3107], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:19:49,150 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:49,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:49,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:50,539 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 07:19:54,935 - __main__ - INFO - Episode timed out.
2024-06-04 07:19:55,124 - __main__ - INFO - Action: 2, Reward: tensor([-27.5594], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:19:55,857 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:19:56,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:19:56,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:19:56,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:19:58,026 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:20:02,750 - __main__ - INFO - Action: 11, Reward: tensor([-124.3435], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:03,596 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.3435], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:20:03,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:03,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:03,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:05,162 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:20:10,146 - __main__ - INFO - Action: 3, Reward: tensor([-24.2980], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:10,645 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:10,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:10,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:12,021 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:20:20,976 - __main__ - INFO - Action: 0, Reward: tensor([-24.2763], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:21,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:21,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:21,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:22,947 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:20:31,646 - __main__ - INFO - Action: 5, Reward: tensor([-24.3023], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:32,017 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:32,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:32,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:33,185 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 07:20:41,851 - __main__ - INFO - Action: 12, Reward: tensor([-24.4531], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:42,447 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:42,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:42,526 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:43,471 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:20:48,198 - __main__ - INFO - Action: 10, Reward: tensor([-25.1221], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:48,697 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:48,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:48,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:49,966 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:20:54,527 - __main__ - INFO - Action: 3, Reward: tensor([-25.4018], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:20:55,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:20:55,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:20:55,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:20:56,517 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:21:01,334 - __main__ - INFO - Action: 3, Reward: tensor([-25.5778], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:01,867 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:01,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:01,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:03,163 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 07:21:07,867 - __main__ - INFO - Action: 6, Reward: tensor([-25.7038], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:08,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:08,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:08,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:09,761 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 07:21:18,807 - __main__ - INFO - Action: 4, Reward: tensor([-25.9649], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:19,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:19,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:19,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:20,852 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:21:29,631 - __main__ - INFO - Action: 0, Reward: tensor([-26.5366], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:30,362 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-375.9802], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:21:30,674 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:30,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:30,752 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:32,016 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:21:41,107 - __main__ - INFO - Action: 5, Reward: tensor([-126.9125], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:41,781 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:41,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:41,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:43,078 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:21:51,999 - __main__ - INFO - Action: 1, Reward: tensor([-27.1244], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:21:52,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:21:52,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:21:52,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:21:53,971 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:22:02,325 - __main__ - INFO - Episode timed out.
2024-06-04 07:22:02,808 - __main__ - INFO - Action: 1, Reward: tensor([-27.4002], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:22:03,436 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:22:04,315 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:04,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:04,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:05,725 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 07:22:10,780 - __main__ - INFO - Action: 2, Reward: tensor([-124.7547], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:11,622 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.7547], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:22:11,983 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:12,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:12,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:13,394 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:22:22,010 - __main__ - INFO - Action: 0, Reward: tensor([-125.0353], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:22,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:22,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:22,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:23,813 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:22:32,913 - __main__ - INFO - Action: 1, Reward: tensor([-25.1242], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:33,258 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:33,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:33,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:34,528 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:22:42,678 - __main__ - INFO - Action: 1, Reward: tensor([-25.4245], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:43,289 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:43,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:43,383 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:44,682 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:22:53,140 - __main__ - INFO - Action: 0, Reward: tensor([-25.5531], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:53,735 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:53,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:53,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:22:54,272 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:22:58,970 - __main__ - INFO - Action: 11, Reward: tensor([-25.3365], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:22:59,483 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:22:59,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:22:59,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:00,809 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 07:23:05,777 - __main__ - INFO - Action: 6, Reward: tensor([-25.3402], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:06,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:06,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:06,450 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:07,612 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 07:23:12,481 - __main__ - INFO - Action: 6, Reward: tensor([-25.5209], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:13,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:13,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:13,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:14,406 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 07:23:23,091 - __main__ - INFO - Action: 9, Reward: tensor([-24.2906], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:23,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:23,777 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:23,777 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:25,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:23:34,091 - __main__ - INFO - Action: 0, Reward: tensor([-23.5452], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:34,572 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:34,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:34,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:35,963 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:23:44,953 - __main__ - INFO - Action: 0, Reward: tensor([-23.4205], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:45,780 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-473.3457], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:23:46,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:46,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:46,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:47,208 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:23:51,765 - __main__ - INFO - Action: 10, Reward: tensor([-24.0467], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:23:52,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:23:52,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:23:52,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:23:53,734 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:24:02,451 - __main__ - INFO - Action: 0, Reward: tensor([-25.2198], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:03,058 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:03,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:03,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:04,406 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:24:08,741 - __main__ - INFO - Episode timed out.
2024-06-04 07:24:09,238 - __main__ - INFO - Action: 3, Reward: tensor([-25.4527], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:24:09,941 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:24:10,877 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:10,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:10,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:12,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:24:21,143 - __main__ - INFO - Action: 5, Reward: tensor([-124.8389], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:22,082 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8389], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:24:22,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:22,521 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:22,521 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:23,763 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:24:32,566 - __main__ - INFO - Action: 14, Reward: tensor([-25.1454], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:33,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:33,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:33,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:34,509 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:24:39,364 - __main__ - INFO - Action: 3, Reward: tensor([-25.1896], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:39,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:40,005 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:40,005 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:41,125 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 07:24:49,990 - __main__ - INFO - Action: 5, Reward: tensor([-25.4529], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:24:50,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:24:50,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:24:50,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:24:52,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:25:00,838 - __main__ - INFO - Action: 0, Reward: tensor([-25.6417], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:01,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:01,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:01,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:02,899 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 07:25:11,353 - __main__ - INFO - Action: 4, Reward: tensor([-25.8226], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:11,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:12,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:12,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:13,369 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 07:25:22,227 - __main__ - INFO - Action: 9, Reward: tensor([-24.7780], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:22,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:22,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:22,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:24,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:25:32,779 - __main__ - INFO - Action: 13, Reward: tensor([-24.1221], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:33,437 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:33,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:33,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:34,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:25:43,591 - __main__ - INFO - Action: 0, Reward: tensor([-24.2300], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:44,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:44,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:44,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:45,354 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:25:54,246 - __main__ - INFO - Action: 0, Reward: tensor([-24.6337], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:25:54,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:25:54,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:25:54,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:25:56,198 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:26:04,976 - __main__ - INFO - Action: 0, Reward: tensor([-25.1081], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:05,566 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-374.9631], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:26:05,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:06,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:06,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:07,257 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:26:15,707 - __main__ - INFO - Episode timed out.
2024-06-04 07:26:16,303 - __main__ - INFO - Action: 0, Reward: tensor([-25.8144], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:26:16,994 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:26:17,885 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:18,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:18,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:19,261 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:26:27,774 - __main__ - INFO - Action: 13, Reward: tensor([-124.8785], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:28,634 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-124.8785], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:26:28,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:29,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:29,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:30,215 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:26:39,029 - __main__ - INFO - Action: 0, Reward: tensor([-125.0350], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:39,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:39,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:39,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:40,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 07:26:49,481 - __main__ - INFO - Action: 8, Reward: tensor([-25.2803], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:26:50,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:26:50,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:26:50,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:26:51,383 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:27:00,156 - __main__ - INFO - Action: 0, Reward: tensor([-25.5095], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:00,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:00,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:00,841 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:02,114 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:27:11,027 - __main__ - INFO - Action: 0, Reward: tensor([-25.7587], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:11,689 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:11,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:11,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:13,118 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:27:21,982 - __main__ - INFO - Action: 0, Reward: tensor([-25.9958], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:22,544 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:22,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:22,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:23,965 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:27:32,915 - __main__ - INFO - Action: 16, Reward: tensor([-26.1081], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:33,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:33,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:33,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:34,848 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 07:27:39,603 - __main__ - INFO - Action: 2, Reward: tensor([-26.3073], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:40,230 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:40,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:40,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:41,602 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:27:46,478 - __main__ - INFO - Action: 10, Reward: tensor([-26.4289], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:47,058 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:47,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:47,182 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:48,482 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 07:27:56,978 - __main__ - INFO - Action: 0, Reward: tensor([-26.6657], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:27:57,602 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:27:57,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:27:57,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:27:58,883 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:28:03,818 - __main__ - INFO - Action: 11, Reward: tensor([-26.4889], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:04,492 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-484.4567], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:28:04,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:04,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:04,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:06,180 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 07:28:14,872 - __main__ - INFO - Action: 1, Reward: tensor([-26.2861], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:15,469 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:15,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:15,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:16,844 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:28:20,975 - __main__ - INFO - Episode timed out.
2024-06-04 07:28:21,553 - __main__ - INFO - Action: 10, Reward: tensor([-26.9247], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:28:22,100 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:28:22,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:22,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:22,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:24,149 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:28:29,207 - __main__ - INFO - Action: 3, Reward: tensor([-124.7613], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:29,969 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-124.7613], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:28:30,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:30,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:30,441 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:31,709 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:28:40,753 - __main__ - INFO - Action: 1, Reward: tensor([-25.0396], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:41,270 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:41,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:41,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:42,558 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:28:51,373 - __main__ - INFO - Action: 13, Reward: tensor([-25.2997], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:28:51,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:28:52,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:28:52,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:28:53,277 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:29:02,138 - __main__ - INFO - Action: 0, Reward: tensor([-25.3709], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:02,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:02,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:02,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:04,108 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:29:09,103 - __main__ - INFO - Action: 3, Reward: tensor([-25.5562], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:09,664 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:09,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:09,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:10,898 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 07:29:15,651 - __main__ - INFO - Action: 11, Reward: tensor([-25.6251], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:16,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:16,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:16,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:17,529 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:29:26,342 - __main__ - INFO - Action: 0, Reward: tensor([-25.1316], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:26,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:27,044 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:27,044 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:28,334 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 5 selected.
2024-06-04 07:29:36,900 - __main__ - INFO - Action: 5, Reward: tensor([-24.8325], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:37,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:37,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:37,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:38,657 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 07:29:47,320 - __main__ - INFO - Action: 13, Reward: tensor([-24.7477], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:47,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:48,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:48,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:49,181 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:29:57,969 - __main__ - INFO - Action: 16, Reward: tensor([-24.7858], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:29:58,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:29:58,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:29:58,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:29:59,787 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 07:30:04,615 - __main__ - INFO - Action: 7, Reward: tensor([-25.1095], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:05,352 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-376.2601], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:30:05,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:05,744 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:05,744 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:07,046 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 07:30:12,197 - __main__ - INFO - Action: 10, Reward: tensor([-25.7538], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:12,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:12,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:12,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:13,989 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:30:18,884 - __main__ - INFO - Action: 3, Reward: tensor([-26.2105], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:19,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:19,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:19,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:20,875 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:30:29,082 - __main__ - INFO - Episode timed out.
2024-06-04 07:30:29,630 - __main__ - INFO - Action: 0, Reward: tensor([-26.8885], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:30:30,407 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 07:30:31,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:31,171 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:31,171 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:32,467 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:30:41,225 - __main__ - INFO - Action: 1, Reward: tensor([-124.6648], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:42,036 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-124.6648], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:30:42,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:42,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:42,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:43,710 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:30:52,245 - __main__ - INFO - Action: 0, Reward: tensor([-24.7938], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:30:52,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:30:52,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:30:52,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:30:53,983 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 07:31:02,782 - __main__ - INFO - Action: 14, Reward: tensor([-25.1477], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:03,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:03,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:03,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:04,799 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 07:31:09,677 - __main__ - INFO - Action: 3, Reward: tensor([-25.2546], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:10,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:10,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:10,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:11,527 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 07:31:19,777 - __main__ - INFO - Action: 1, Reward: tensor([-25.4562], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:20,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:20,356 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:20,356 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:21,558 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:31:30,323 - __main__ - INFO - Action: 0, Reward: tensor([-25.4746], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:30,808 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:30,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:30,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:32,059 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:31:36,509 - __main__ - INFO - Action: 3, Reward: tensor([-25.7007], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:36,993 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:37,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:37,086 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:38,280 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 07:31:43,032 - __main__ - INFO - Action: 3, Reward: tensor([-25.9149], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:43,643 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:43,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:43,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:44,886 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:31:53,734 - __main__ - INFO - Action: 0, Reward: tensor([-26.0710], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:31:54,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:31:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:31:54,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:31:55,704 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 07:32:04,768 - __main__ - INFO - Action: 16, Reward: tensor([-26.4160], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:32:05,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:32:05,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:32:05,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:32:06,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:32:15,318 - __main__ - INFO - Action: 0, Reward: tensor([-26.6571], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:32:16,082 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-381.5514], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 07:32:16,380 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:32:16,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:32:16,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:32:17,713 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 07:32:26,556 - __main__ - INFO - Action: 4, Reward: tensor([-26.8809], grad_fn=<AddBackward0>), Done: False
2024-06-04 07:32:27,136 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 07:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 07:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 07:32:28,387 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 07:32:36,730 - __main__ - INFO - Episode timed out.
2024-06-04 07:32:37,274 - __main__ - INFO - Action: 0, Reward: tensor([-127.2464], grad_fn=<AddBackward0>), Done: True
2024-06-04 07:33:00,236 - __main__ - INFO - Dropped rows with NA values. Shape changed from (626, 2) to (626, 2).
2024-06-04 07:33:00,240 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 08:01:43,073 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 08:02:14,060 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 08:02:30,931 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 08:02:51,732 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 08:02:56,583 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 08:02:56,737 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 08:03:00,833 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 08:03:02,084 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 08:03:02,427 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:03:03,359 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:03,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:03,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:04,673 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:03:13,627 - __main__ - INFO - Action: 1, Reward: tensor([-124.7726], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:14,442 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.7726], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:03:14,768 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:14,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:14,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:15,975 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:03:24,574 - __main__ - INFO - Action: 1, Reward: tensor([-24.7839], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:25,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:25,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:25,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:26,426 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:03:35,160 - __main__ - INFO - Action: 0, Reward: tensor([-25.0020], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:35,804 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:35,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:35,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:37,218 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:03:41,617 - __main__ - INFO - Action: 3, Reward: tensor([-25.4140], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:42,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:42,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:42,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:43,671 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:03:48,554 - __main__ - INFO - Action: 2, Reward: tensor([-25.5312], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:49,148 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:49,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:49,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:03:50,464 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:03:59,315 - __main__ - INFO - Action: 16, Reward: tensor([-125.8895], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:03:59,860 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:03:59,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:03:59,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:01,252 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:04:10,197 - __main__ - INFO - Action: 12, Reward: tensor([-26.2409], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:10,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:10,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:10,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:12,015 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:04:20,830 - __main__ - INFO - Action: 0, Reward: tensor([-26.4620], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:21,345 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:21,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:21,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:22,616 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:04:31,528 - __main__ - INFO - Action: 15, Reward: tensor([-26.6351], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:32,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:32,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:32,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:33,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:04:38,369 - __main__ - INFO - Action: 11, Reward: tensor([-26.3956], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:38,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:39,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:39,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:40,108 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:04:48,731 - __main__ - INFO - Action: 8, Reward: tensor([-126.8963], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:04:49,611 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-584.0230], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:04:49,892 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:04:50,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:04:50,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:04:51,270 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:04:59,913 - __main__ - INFO - Action: 4, Reward: tensor([-27.1702], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:00,428 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:00,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:00,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:01,793 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:05:06,042 - __main__ - INFO - Episode timed out.
2024-06-04 08:05:06,274 - __main__ - INFO - Action: 7, Reward: tensor([-27.2210], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:05:06,883 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:05:07,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:07,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:07,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:09,056 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:05:17,887 - __main__ - INFO - Action: 5, Reward: tensor([-124.8964], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:18,700 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8964], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:05:19,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:19,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:19,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:20,253 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:05:29,326 - __main__ - INFO - Action: 0, Reward: tensor([-25.1372], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:29,877 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:30,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:30,019 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:31,225 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:05:40,115 - __main__ - INFO - Action: 8, Reward: tensor([-125.4085], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:40,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:40,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:40,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:41,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:05:51,053 - __main__ - INFO - Action: 8, Reward: tensor([-25.6646], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:51,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:51,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:51,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:52,949 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:05:57,705 - __main__ - INFO - Action: 3, Reward: tensor([-25.7478], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:05:58,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:05:58,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:05:58,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:05:59,653 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:06:04,325 - __main__ - INFO - Action: 6, Reward: tensor([-25.8855], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:04,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:05,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:05,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:06,238 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:06:15,039 - __main__ - INFO - Action: 15, Reward: tensor([-26.1784], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:15,683 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:15,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:15,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:16,946 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:06:21,674 - __main__ - INFO - Action: 3, Reward: tensor([-26.2977], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:22,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:22,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:22,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:23,455 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:06:32,223 - __main__ - INFO - Action: 9, Reward: tensor([-25.1168], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:32,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:32,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:32,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:34,030 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:06:42,873 - __main__ - INFO - Action: 0, Reward: tensor([-24.7312], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:43,387 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:43,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:43,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:44,755 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:06:53,702 - __main__ - INFO - Action: 0, Reward: tensor([-24.4748], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:06:54,516 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-479.5388], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:06:54,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:06:54,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:06:54,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:06:56,030 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:07:05,027 - __main__ - INFO - Action: 14, Reward: tensor([-24.6660], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:05,603 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:05,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:05,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:06,901 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:07:15,380 - __main__ - INFO - Episode timed out.
2024-06-04 08:07:15,974 - __main__ - INFO - Action: 16, Reward: tensor([-25.0251], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:07:16,645 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:07:17,504 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:17,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:17,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:18,860 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:07:27,625 - __main__ - INFO - Action: 8, Reward: tensor([-124.9285], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:28,507 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.9285], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:07:28,787 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:28,881 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:28,881 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:30,187 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:07:35,120 - __main__ - INFO - Action: 6, Reward: tensor([-25.0895], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:35,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:35,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:35,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:36,968 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:07:41,625 - __main__ - INFO - Action: 2, Reward: tensor([-25.2279], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:42,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:42,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:42,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:43,415 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:07:52,311 - __main__ - INFO - Action: 1, Reward: tensor([-25.3855], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:52,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:53,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:53,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:07:54,189 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:07:59,067 - __main__ - INFO - Action: 2, Reward: tensor([-25.5349], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:07:59,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:07:59,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:07:59,740 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:00,931 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:08:05,786 - __main__ - INFO - Action: 6, Reward: tensor([-25.7087], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:06,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:06,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:06,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:07,845 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:08:16,906 - __main__ - INFO - Action: 4, Reward: tensor([-25.9675], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:17,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:17,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:17,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:18,959 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:08:23,878 - __main__ - INFO - Action: 3, Reward: tensor([-26.0569], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:24,460 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:24,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:24,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:25,736 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:08:34,664 - __main__ - INFO - Action: 1, Reward: tensor([-26.3180], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:35,231 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:35,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:35,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:36,613 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:08:45,423 - __main__ - INFO - Action: 4, Reward: tensor([-26.4911], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:45,964 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:46,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:46,073 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:47,294 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:08:55,930 - __main__ - INFO - Action: 15, Reward: tensor([-126.9078], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:08:56,662 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-483.6164], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:08:57,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:08:57,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:08:57,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:08:58,330 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:09:07,209 - __main__ - INFO - Action: 8, Reward: tensor([-127.0488], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:07,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:07,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:07,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:09,150 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:09:17,604 - __main__ - INFO - Episode timed out.
2024-06-04 08:09:18,070 - __main__ - INFO - Action: 1, Reward: tensor([-27.2697], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:09:18,661 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:09:19,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:19,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:19,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:20,820 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:09:29,497 - __main__ - INFO - Action: 0, Reward: tensor([-124.9252], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:30,266 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.9252], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:09:30,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:30,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:30,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:31,876 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:09:40,630 - __main__ - INFO - Action: 5, Reward: tensor([-25.1549], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:41,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:41,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:41,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:42,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:09:51,375 - __main__ - INFO - Action: 14, Reward: tensor([-125.4542], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:09:51,940 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:09:52,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:09:52,067 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:09:53,331 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:10:02,346 - __main__ - INFO - Action: 9, Reward: tensor([-24.2403], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:02,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:03,034 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:03,034 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:04,238 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:10:13,062 - __main__ - INFO - Action: 14, Reward: tensor([-23.8970], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:13,531 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:13,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:13,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:14,749 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:10:23,559 - __main__ - INFO - Action: 9, Reward: tensor([-22.4644], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:24,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:24,263 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:24,263 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:25,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:10:34,318 - __main__ - INFO - Action: 15, Reward: tensor([-21.4787], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:34,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:34,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:34,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:36,044 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:10:40,922 - __main__ - INFO - Action: 10, Reward: tensor([-21.9236], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:41,529 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:41,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:41,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:42,923 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:10:51,676 - __main__ - INFO - Action: 0, Reward: tensor([-22.5388], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:10:52,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:10:52,300 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:10:52,300 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:10:53,629 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:11:02,312 - __main__ - INFO - Action: 9, Reward: tensor([-21.6449], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:02,827 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:02,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:02,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:04,271 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:11:12,963 - __main__ - INFO - Action: 16, Reward: tensor([-21.1656], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:13,529 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-454.8874], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:11:13,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:13,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:13,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:15,098 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:11:23,233 - __main__ - INFO - Episode timed out.
2024-06-04 08:11:23,859 - __main__ - INFO - Action: 5, Reward: tensor([-21.3429], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:11:24,626 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:11:25,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:25,377 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:25,377 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:26,600 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 08:11:35,357 - __main__ - INFO - Action: 8, Reward: tensor([-124.9235], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:36,090 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.9235], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:11:36,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:36,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:36,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:37,684 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:11:46,399 - __main__ - INFO - Action: 16, Reward: tensor([-25.1027], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:46,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:46,963 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:46,963 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:48,091 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:11:53,094 - __main__ - INFO - Action: 11, Reward: tensor([-25.2527], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:11:53,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:11:53,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:11:53,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:11:55,142 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:11:59,896 - __main__ - INFO - Action: 11, Reward: tensor([-24.8341], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:00,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:00,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:00,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:01,740 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:12:10,429 - __main__ - INFO - Action: 0, Reward: tensor([-24.1993], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:10,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:10,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:10,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:12,243 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:12:17,212 - __main__ - INFO - Action: 7, Reward: tensor([-24.3273], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:17,696 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:17,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:17,837 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:19,038 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:12:27,948 - __main__ - INFO - Action: 0, Reward: tensor([-24.8111], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:28,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:28,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:28,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:29,665 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:12:38,756 - __main__ - INFO - Action: 9, Reward: tensor([-23.7807], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:39,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:39,400 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:39,400 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:40,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:12:45,175 - __main__ - INFO - Action: 10, Reward: tensor([-24.2294], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:45,625 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:45,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:45,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:46,818 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:12:55,564 - __main__ - INFO - Action: 5, Reward: tensor([-25.1165], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:12:56,095 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:12:56,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:12:56,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:12:57,361 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:13:02,206 - __main__ - INFO - Action: 2, Reward: tensor([-25.3699], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:03,037 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-371.9472], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:13:03,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:03,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:03,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:04,696 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:13:13,569 - __main__ - INFO - Action: 4, Reward: tensor([-25.4303], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:14,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:14,348 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:14,348 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:15,412 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:13:23,804 - __main__ - INFO - Action: 1, Reward: tensor([-25.5444], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:24,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:24,539 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:24,539 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:25,770 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:13:34,103 - __main__ - INFO - Episode timed out.
2024-06-04 08:13:34,559 - __main__ - INFO - Action: 5, Reward: tensor([-25.8346], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:13:35,183 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:13:36,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:36,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:36,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:37,503 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:13:46,311 - __main__ - INFO - Action: 0, Reward: tensor([-124.9316], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:47,134 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.9316], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:13:47,431 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:47,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:47,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:48,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:13:57,720 - __main__ - INFO - Action: 4, Reward: tensor([-125.1676], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:13:58,255 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:13:58,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:13:58,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:13:59,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:14:04,420 - __main__ - INFO - Action: 2, Reward: tensor([-25.3254], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:04,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:05,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:05,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:06,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:14:11,049 - __main__ - INFO - Action: 7, Reward: tensor([-25.4273], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:11,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:11,851 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:11,851 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:13,109 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 08:14:21,752 - __main__ - INFO - Action: 4, Reward: tensor([-25.5592], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:22,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:22,429 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:22,429 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:23,643 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:14:28,302 - __main__ - INFO - Action: 2, Reward: tensor([-125.8025], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:28,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:28,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:28,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:30,175 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:14:39,080 - __main__ - INFO - Action: 4, Reward: tensor([-126.0704], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:39,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:39,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:39,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:41,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:14:49,848 - __main__ - INFO - Action: 0, Reward: tensor([-126.3391], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:50,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:50,520 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:50,520 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:51,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 08:14:56,515 - __main__ - INFO - Action: 3, Reward: tensor([-126.3763], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:14:57,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:14:57,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:14:57,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:14:58,465 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:15:03,423 - __main__ - INFO - Action: 11, Reward: tensor([-26.1955], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:04,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:04,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:04,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:05,373 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:15:14,194 - __main__ - INFO - Action: 9, Reward: tensor([-24.4027], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:14,538 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-881.5978], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:15:14,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:14,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:14,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:16,127 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:15:20,931 - __main__ - INFO - Action: 2, Reward: tensor([-24.3208], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:21,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:21,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:21,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:22,840 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:15:31,559 - __main__ - INFO - Action: 0, Reward: tensor([-24.2964], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:32,190 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:32,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:32,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:33,488 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:15:41,845 - __main__ - INFO - Episode timed out.
2024-06-04 08:15:42,390 - __main__ - INFO - Action: 0, Reward: tensor([-24.5583], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:15:43,063 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:15:43,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:44,077 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:44,077 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:45,236 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:15:54,243 - __main__ - INFO - Action: 13, Reward: tensor([-124.9701], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:15:54,963 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-124.9701], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:15:55,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:15:55,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:15:55,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:15:56,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:16:01,239 - __main__ - INFO - Action: 3, Reward: tensor([-25.1321], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:01,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:01,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:01,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:03,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:16:11,677 - __main__ - INFO - Action: 5, Reward: tensor([-125.3219], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:12,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:12,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:12,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:13,622 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:16:22,498 - __main__ - INFO - Action: 0, Reward: tensor([-25.4794], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:23,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:23,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:23,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:23,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:16:28,715 - __main__ - INFO - Action: 2, Reward: tensor([-25.6590], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:29,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:29,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:29,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:30,589 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:16:35,385 - __main__ - INFO - Action: 10, Reward: tensor([-25.7803], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:35,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:36,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:36,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:37,277 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:16:46,118 - __main__ - INFO - Action: 14, Reward: tensor([-26.0461], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:46,619 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:46,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:46,714 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:47,967 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:16:52,889 - __main__ - INFO - Action: 2, Reward: tensor([-26.1713], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:16:53,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:16:53,590 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:16:53,590 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:16:54,791 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:17:03,570 - __main__ - INFO - Action: 5, Reward: tensor([-26.3815], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:04,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:04,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:04,260 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:05,470 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:17:14,423 - __main__ - INFO - Action: 0, Reward: tensor([-26.6057], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:14,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:15,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:15,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:16,332 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:17:25,240 - __main__ - INFO - Action: 16, Reward: tensor([-26.7335], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:25,946 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-484.2811], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:17:26,181 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:26,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:26,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:27,395 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:17:36,232 - __main__ - INFO - Action: 0, Reward: tensor([-126.9926], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:36,806 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:36,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:36,932 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:38,157 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:17:46,641 - __main__ - INFO - Episode timed out.
2024-06-04 08:17:47,280 - __main__ - INFO - Action: 16, Reward: tensor([-27.1253], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:17:47,966 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:17:48,795 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:48,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:48,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:50,054 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:17:54,672 - __main__ - INFO - Action: 11, Reward: tensor([-124.4319], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:17:55,358 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-124.4319], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:17:55,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:17:55,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:17:55,826 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:17:57,062 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:18:01,740 - __main__ - INFO - Action: 2, Reward: tensor([-24.4264], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:02,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:02,394 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:02,394 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:03,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:18:08,420 - __main__ - INFO - Action: 2, Reward: tensor([-24.5944], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:08,965 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:09,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:09,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:10,313 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:18:19,161 - __main__ - INFO - Action: 9, Reward: tensor([-23.3342], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:19,643 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:19,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:19,751 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:20,819 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:18:29,340 - __main__ - INFO - Action: 0, Reward: tensor([-22.9494], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:29,827 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:29,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:29,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:31,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:18:40,176 - __main__ - INFO - Action: 15, Reward: tensor([-22.7957], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:40,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:40,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:40,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:42,076 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:18:50,800 - __main__ - INFO - Action: 1, Reward: tensor([-22.7977], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:18:51,412 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:18:51,538 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:18:51,538 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:18:52,702 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:19:01,422 - __main__ - INFO - Action: 4, Reward: tensor([-23.0879], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:02,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:02,110 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:02,110 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:03,406 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:19:07,909 - __main__ - INFO - Action: 7, Reward: tensor([-23.1693], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:08,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:08,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:08,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:09,556 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:19:18,318 - __main__ - INFO - Action: 1, Reward: tensor([-23.1953], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:18,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:18,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:18,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:20,050 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:19:28,980 - __main__ - INFO - Action: 14, Reward: tensor([-23.0656], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:29,796 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-357.8477], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:19:30,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:30,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:30,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:31,375 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:19:40,182 - __main__ - INFO - Action: 12, Reward: tensor([-23.1386], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:19:40,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:40,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:40,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:42,216 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:19:50,473 - __main__ - INFO - Episode timed out.
2024-06-04 08:19:50,925 - __main__ - INFO - Action: 5, Reward: tensor([-23.3401], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:19:51,562 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:19:52,343 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:19:52,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:19:52,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:19:53,684 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:20:02,372 - __main__ - INFO - Action: 16, Reward: tensor([-124.8412], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:03,198 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-124.8412], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:20:03,464 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:03,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:03,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:04,857 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:20:13,441 - __main__ - INFO - Action: 1, Reward: tensor([-25.1536], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:14,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:14,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:14,165 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:15,231 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 08:20:20,024 - __main__ - INFO - Action: 6, Reward: tensor([-125.2921], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:20,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:20,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:20,713 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:21,859 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:20:30,586 - __main__ - INFO - Action: 1, Reward: tensor([-25.3909], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:31,178 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:31,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:31,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:32,383 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:20:40,974 - __main__ - INFO - Action: 9, Reward: tensor([-24.1427], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:41,536 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:41,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:41,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:42,787 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:20:51,623 - __main__ - INFO - Action: 4, Reward: tensor([-23.8153], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:20:52,233 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:20:52,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:20:52,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:20:53,634 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:21:02,174 - __main__ - INFO - Action: 0, Reward: tensor([-23.3849], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:02,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:02,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:02,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:04,072 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:21:12,863 - __main__ - INFO - Action: 9, Reward: tensor([-21.9169], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:13,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:13,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:13,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:14,922 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:21:23,939 - __main__ - INFO - Action: 4, Reward: tensor([-21.7916], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:24,552 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:24,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:24,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:25,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:21:30,540 - __main__ - INFO - Action: 10, Reward: tensor([-22.4334], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:31,087 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:31,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:31,211 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:32,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:21:41,332 - __main__ - INFO - Action: 14, Reward: tensor([-23.0471], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:42,100 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-461.2096], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:21:42,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:43,791 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:21:48,549 - __main__ - INFO - Action: 10, Reward: tensor([-23.8211], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:21:49,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:21:49,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:21:49,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:21:50,514 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:21:58,936 - __main__ - INFO - Episode timed out.
2024-06-04 08:21:59,326 - __main__ - INFO - Action: 12, Reward: tensor([-25.2289], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:22:23,869 - __main__ - INFO - Dropped rows with NA values. Shape changed from (744, 2) to (744, 2).
2024-06-04 08:22:23,874 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 08:30:16,532 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 08:30:33,034 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 08:30:49,219 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 08:31:55,244 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 08:32:00,584 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 08:32:00,770 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 08:32:04,674 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 08:32:05,783 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 08:32:06,130 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:32:06,945 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:07,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:07,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:08,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:32:12,819 - __main__ - INFO - Action: 10, Reward: tensor([-124.7415], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:13,572 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.7415], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:32:13,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:13,948 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:13,948 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:15,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:32:19,947 - __main__ - INFO - Action: 7, Reward: tensor([-24.8849], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:20,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:20,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:20,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:21,576 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:32:26,590 - __main__ - INFO - Action: 10, Reward: tensor([-25.0397], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:27,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:27,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:28,392 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:32:37,139 - __main__ - INFO - Action: 0, Reward: tensor([-25.2380], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:37,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:37,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:37,785 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:39,067 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:32:47,909 - __main__ - INFO - Action: 12, Reward: tensor([-25.5074], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:48,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:48,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:48,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:49,813 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:32:54,430 - __main__ - INFO - Action: 7, Reward: tensor([-25.5467], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:32:54,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:32:55,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:32:55,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:32:56,371 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:33:00,886 - __main__ - INFO - Action: 11, Reward: tensor([-25.6067], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:01,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:01,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:01,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:02,589 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:33:11,119 - __main__ - INFO - Action: 9, Reward: tensor([-23.7648], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:11,620 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:11,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:11,698 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:12,908 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:33:17,588 - __main__ - INFO - Action: 2, Reward: tensor([-23.5046], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:18,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:18,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:18,296 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:18,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:33:27,935 - __main__ - INFO - Action: 1, Reward: tensor([-22.8873], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:28,573 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:28,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:28,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:29,873 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:33:38,457 - __main__ - INFO - Action: 15, Reward: tensor([-22.9180], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:39,211 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-369.6396], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:33:39,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:39,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:39,569 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:40,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:33:49,406 - __main__ - INFO - Action: 9, Reward: tensor([-21.6332], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:49,923 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:49,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:49,985 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:51,156 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:33:56,043 - __main__ - INFO - Action: 7, Reward: tensor([-21.5716], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:33:56,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:33:56,781 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:33:56,781 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:33:58,065 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:34:06,437 - __main__ - INFO - Episode timed out.
2024-06-04 08:34:06,888 - __main__ - INFO - Action: 12, Reward: tensor([-21.6569], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:34:07,450 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:34:08,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:08,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:08,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:09,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:34:18,667 - __main__ - INFO - Action: 13, Reward: tensor([-124.8710], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:19,558 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8710], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:34:19,936 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:20,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:20,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:21,298 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:34:29,922 - __main__ - INFO - Action: 15, Reward: tensor([-125.1174], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:30,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:30,547 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:30,547 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:31,609 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:34:36,428 - __main__ - INFO - Action: 10, Reward: tensor([-125.2883], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:37,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:37,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:37,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:38,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:34:47,512 - __main__ - INFO - Action: 1, Reward: tensor([-25.5265], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:48,013 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:48,107 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:48,107 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:49,295 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 08:34:58,225 - __main__ - INFO - Action: 4, Reward: tensor([-25.7076], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:34:58,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:34:58,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:34:58,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:34:59,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:35:08,860 - __main__ - INFO - Action: 0, Reward: tensor([-25.8894], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:09,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:09,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:09,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:10,896 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:35:19,751 - __main__ - INFO - Action: 15, Reward: tensor([-26.1573], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:20,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:20,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:20,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:21,724 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:35:26,514 - __main__ - INFO - Action: 11, Reward: tensor([-25.9076], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:27,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:27,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:27,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:28,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:35:37,278 - __main__ - INFO - Action: 5, Reward: tensor([-25.2528], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:37,959 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:38,054 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:38,054 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:39,295 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:35:48,151 - __main__ - INFO - Action: 1, Reward: tensor([-25.1895], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:48,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:35:48,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:35:48,747 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:35:49,957 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:35:58,978 - __main__ - INFO - Action: 0, Reward: tensor([-25.1544], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:35:59,832 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-580.0618], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:36:00,146 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:00,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:00,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:01,438 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:36:05,807 - __main__ - INFO - Action: 7, Reward: tensor([-25.3248], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:06,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:06,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:06,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:07,651 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:36:15,741 - __main__ - INFO - Episode timed out.
2024-06-04 08:36:16,238 - __main__ - INFO - Action: 0, Reward: tensor([-25.0791], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:36:16,958 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:36:17,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:17,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:17,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:18,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:36:23,503 - __main__ - INFO - Action: 7, Reward: tensor([-124.7340], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:24,301 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.7340], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:36:24,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:24,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:24,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:25,910 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:36:34,879 - __main__ - INFO - Action: 1, Reward: tensor([-24.9703], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:35,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:35,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:35,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:36,813 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:36:45,709 - __main__ - INFO - Action: 0, Reward: tensor([-25.1345], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:46,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:46,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:46,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:47,634 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:36:52,182 - __main__ - INFO - Action: 3, Reward: tensor([-25.2822], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:52,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:52,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:52,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:36:54,024 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:36:58,765 - __main__ - INFO - Action: 10, Reward: tensor([-25.4360], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:36:59,405 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:36:59,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:36:59,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:00,668 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:37:09,452 - __main__ - INFO - Action: 1, Reward: tensor([-25.6803], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:10,030 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:10,141 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:10,141 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:11,294 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:37:19,919 - __main__ - INFO - Action: 13, Reward: tensor([-25.9415], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:20,369 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:20,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:20,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:21,670 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:37:30,281 - __main__ - INFO - Action: 15, Reward: tensor([-26.1578], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:30,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:30,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:30,954 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:32,124 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:37:36,903 - __main__ - INFO - Action: 3, Reward: tensor([-26.2894], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:37,462 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:37,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:37,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:38,753 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:37:47,520 - __main__ - INFO - Action: 1, Reward: tensor([-26.5117], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:48,209 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:48,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:48,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:49,632 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 08:37:54,437 - __main__ - INFO - Action: 7, Reward: tensor([-26.5772], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:37:55,157 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-382.7147], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:37:55,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:37:55,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:37:55,558 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:37:56,762 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:38:01,638 - __main__ - INFO - Action: 11, Reward: tensor([-126.2683], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:02,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:02,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:02,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:03,595 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:38:12,215 - __main__ - INFO - Action: 1, Reward: tensor([-25.7693], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:12,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:12,873 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:12,873 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:14,121 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:38:22,632 - __main__ - INFO - Episode timed out.
2024-06-04 08:38:23,192 - __main__ - INFO - Action: 14, Reward: tensor([-25.6465], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:38:23,752 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:38:24,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:24,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:24,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:26,004 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:38:34,737 - __main__ - INFO - Action: 14, Reward: tensor([-124.6853], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:35,470 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.6853], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:38:35,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:35,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:35,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:37,258 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:38:41,713 - __main__ - INFO - Action: 10, Reward: tensor([-124.9942], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:42,262 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:42,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:42,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:43,675 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 08:38:52,604 - __main__ - INFO - Action: 1, Reward: tensor([-25.1799], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:53,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:53,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:53,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:38:54,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:38:59,118 - __main__ - INFO - Action: 2, Reward: tensor([-25.2848], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:38:59,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:38:59,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:38:59,778 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:00,951 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 08:39:05,709 - __main__ - INFO - Action: 3, Reward: tensor([-25.4197], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:06,316 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:06,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:06,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:07,657 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:39:16,696 - __main__ - INFO - Action: 16, Reward: tensor([-25.5933], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:17,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:17,397 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:17,397 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:18,714 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 08:39:27,653 - __main__ - INFO - Action: 0, Reward: tensor([-25.8778], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:28,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:28,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:28,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:29,410 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:39:34,330 - __main__ - INFO - Action: 2, Reward: tensor([-26.0661], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:34,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:35,038 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:35,038 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:36,308 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 08:39:41,219 - __main__ - INFO - Action: 3, Reward: tensor([-26.1037], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:41,764 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:41,858 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:41,858 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:43,053 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 08:39:51,679 - __main__ - INFO - Action: 14, Reward: tensor([-26.3748], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:39:52,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:39:52,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:39:52,366 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:39:53,384 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:40:01,826 - __main__ - INFO - Action: 12, Reward: tensor([-26.6292], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:02,558 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-482.2089], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:40:02,918 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:03,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:03,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:04,315 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 08:40:13,259 - __main__ - INFO - Action: 13, Reward: tensor([-26.7769], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:13,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:13,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:13,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:15,164 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 08:40:20,088 - __main__ - INFO - Action: 10, Reward: tensor([-127.1543], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:20,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:20,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:20,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:22,042 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 08:40:30,404 - __main__ - INFO - Episode timed out.
2024-06-04 08:40:30,886 - __main__ - INFO - Action: 1, Reward: tensor([-27.3958], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:40:31,496 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:40:32,329 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:32,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:32,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:33,683 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:40:42,403 - __main__ - INFO - Action: 0, Reward: tensor([-124.8052], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:43,199 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-124.8052], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:40:43,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:43,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:43,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:44,887 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:40:53,681 - __main__ - INFO - Action: 9, Reward: tensor([-23.6599], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:40:54,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:40:54,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:40:54,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:40:55,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 08:41:00,252 - __main__ - INFO - Action: 3, Reward: tensor([-23.6893], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:00,832 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:00,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:00,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:02,067 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:41:07,027 - __main__ - INFO - Action: 2, Reward: tensor([-23.7998], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:07,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:07,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:07,650 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:08,883 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 08:41:13,867 - __main__ - INFO - Action: 11, Reward: tensor([-23.5111], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:14,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:14,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:14,523 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:15,836 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:41:24,508 - __main__ - INFO - Action: 15, Reward: tensor([-23.0928], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:25,008 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:25,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:25,134 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:26,405 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 08:41:31,213 - __main__ - INFO - Action: 2, Reward: tensor([-23.0979], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:31,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:31,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:31,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:33,102 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:41:41,882 - __main__ - INFO - Action: 4, Reward: tensor([-23.4894], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:42,429 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:42,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:43,553 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 08:41:48,081 - __main__ - INFO - Action: 2, Reward: tensor([-23.7372], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:48,599 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:48,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:48,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:41:49,988 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:41:58,723 - __main__ - INFO - Action: 0, Reward: tensor([-24.1373], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:41:59,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:41:59,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:41:59,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:00,682 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:42:09,644 - __main__ - INFO - Action: 0, Reward: tensor([-24.9593], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:10,505 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-361.9790], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:42:10,834 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:10,944 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:10,944 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:11,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:42:20,629 - __main__ - INFO - Action: 12, Reward: tensor([-25.9861], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:21,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:21,208 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:21,208 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:22,367 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:42:31,344 - __main__ - INFO - Action: 4, Reward: tensor([-26.8723], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:31,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:32,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:32,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:33,252 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:42:41,488 - __main__ - INFO - Episode timed out.
2024-06-04 08:42:42,049 - __main__ - INFO - Action: 9, Reward: tensor([-26.1789], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:42:42,679 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 08:42:43,636 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:43,746 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:43,746 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:44,870 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:42:53,753 - __main__ - INFO - Action: 0, Reward: tensor([-124.8253], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:42:54,491 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-124.8253], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:42:54,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:42:54,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:42:54,742 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:42:55,887 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 08:43:04,910 - __main__ - INFO - Action: 12, Reward: tensor([-125.1189], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:05,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:05,567 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:05,567 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:06,725 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 08:43:15,609 - __main__ - INFO - Action: 4, Reward: tensor([-25.3284], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:16,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:16,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:16,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:17,422 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:43:26,141 - __main__ - INFO - Action: 5, Reward: tensor([-25.4185], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:26,759 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:26,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:26,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:28,053 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:43:36,834 - __main__ - INFO - Action: 0, Reward: tensor([-25.5781], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:37,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:37,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:37,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:38,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 08:43:47,586 - __main__ - INFO - Action: 16, Reward: tensor([-25.6779], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:48,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:48,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:48,290 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:43:49,482 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:43:58,278 - __main__ - INFO - Action: 15, Reward: tensor([-26.1144], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:43:58,792 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:43:58,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:43:58,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:00,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:44:08,891 - __main__ - INFO - Action: 5, Reward: tensor([-126.2877], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:09,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:09,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:09,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:10,781 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 08:44:19,612 - __main__ - INFO - Action: 9, Reward: tensor([-25.1436], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:20,173 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:20,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:20,313 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:21,615 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 08:44:30,424 - __main__ - INFO - Action: 0, Reward: tensor([-24.7825], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:30,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:31,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:31,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:32,376 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 08:44:41,306 - __main__ - INFO - Action: 15, Reward: tensor([-24.8744], grad_fn=<AddBackward0>), Done: False
2024-06-04 08:44:42,091 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-579.1499], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 08:44:42,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 08:44:42,587 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 08:44:42,587 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 08:44:43,670 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 08:44:51,988 - __main__ - INFO - Episode timed out.
2024-06-04 08:44:52,253 - __main__ - INFO - Action: 5, Reward: tensor([-24.8878], grad_fn=<AddBackward0>), Done: True
2024-06-04 08:44:52,505 - __main__ - INFO - Early stopping triggered after 7 epochs.
2024-06-04 08:45:18,068 - __main__ - INFO - Dropped rows with NA values. Shape changed from (825, 2) to (825, 2).
2024-06-04 08:45:18,073 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 09:46:24,567 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 09:46:47,813 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 09:47:00,031 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 09:47:14,429 - __main__ - INFO - Scatter plot created successfully for steps vs values.
2024-06-04 09:51:00,806 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 09:51:06,099 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 09:51:06,254 - __main__ - INFO - Loading checkpoint from e:\Project\models/checkpoints\ppo_agent_checkpoint.pt
2024-06-04 09:51:09,971 - Drone.source.models.ppo.ppo_agent - INFO - Model loaded
2024-06-04 09:51:11,110 - __main__ - INFO - Resuming training from epoch 1
2024-06-04 09:51:11,486 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:51:12,474 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:12,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:12,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:13,675 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 09:51:18,655 - __main__ - INFO - Action: 2, Reward: tensor([-124.8188], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:19,572 - __main__ - INFO - Epoch 1, Iteration 0: Reward: tensor([-124.8188], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:51:19,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:20,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:20,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:21,279 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 09:51:30,214 - __main__ - INFO - Action: 14, Reward: tensor([-25.0060], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:30,651 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:30,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:30,790 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:32,015 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:51:40,951 - __main__ - INFO - Action: 0, Reward: tensor([-25.4946], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:41,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:41,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:41,560 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:42,732 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:51:51,625 - __main__ - INFO - Action: 1, Reward: tensor([-25.6425], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:51:52,160 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:51:52,268 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:51:52,268 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:51:53,520 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:52:02,378 - __main__ - INFO - Action: 8, Reward: tensor([-126.0157], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:03,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:03,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:03,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:04,351 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 09:52:08,959 - __main__ - INFO - Action: 3, Reward: tensor([-26.1706], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:09,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:09,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:09,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:10,430 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 09:52:19,348 - __main__ - INFO - Action: 4, Reward: tensor([-26.3751], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:19,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:20,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:20,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:21,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:52:30,159 - __main__ - INFO - Action: 1, Reward: tensor([-26.6168], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:30,783 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:30,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:30,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:32,128 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:52:41,108 - __main__ - INFO - Action: 0, Reward: tensor([-26.7511], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:41,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:41,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:41,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:42,936 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:52:51,882 - __main__ - INFO - Action: 0, Reward: tensor([-27.0249], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:52:52,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:52:52,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:52:52,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:52:53,709 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:53:02,518 - __main__ - INFO - Action: 16, Reward: tensor([-27.1731], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:03,282 - __main__ - INFO - Epoch 1, Iteration 10: Reward: tensor([-487.0893], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:53:03,671 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:03,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:03,749 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:04,906 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:53:13,007 - __main__ - INFO - Episode timed out.
2024-06-04 09:53:13,539 - __main__ - INFO - Action: 0, Reward: tensor([-27.4654], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:53:14,378 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:53:15,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:15,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:15,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:16,580 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:53:25,239 - __main__ - INFO - Action: 5, Reward: tensor([-124.8914], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:26,046 - __main__ - INFO - Epoch 2, Iteration 0: Reward: tensor([-124.8914], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:53:26,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:26,494 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:26,494 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:27,750 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 09:53:32,514 - __main__ - INFO - Action: 2, Reward: tensor([-25.0479], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:32,967 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:33,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:33,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:34,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:53:42,953 - __main__ - INFO - Action: 0, Reward: tensor([-25.2254], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:43,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:43,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:43,626 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:44,796 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 09:53:53,769 - __main__ - INFO - Action: 4, Reward: tensor([-25.4586], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:53:54,262 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:53:54,372 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:53:54,372 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:53:55,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 09:54:00,468 - __main__ - INFO - Action: 6, Reward: tensor([-25.7124], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:01,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:01,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:01,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:01,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:54:10,972 - __main__ - INFO - Action: 8, Reward: tensor([-125.9801], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:11,505 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:11,598 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:11,598 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:12,752 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 09:54:17,830 - __main__ - INFO - Action: 3, Reward: tensor([-26.0892], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:18,391 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:18,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:18,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:19,645 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:54:28,243 - __main__ - INFO - Action: 16, Reward: tensor([-26.2571], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:28,666 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:28,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:28,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:29,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 09:54:39,023 - __main__ - INFO - Action: 4, Reward: tensor([-26.5204], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:39,527 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:39,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:39,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:40,953 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:54:49,671 - __main__ - INFO - Action: 5, Reward: tensor([-26.6591], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:50,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:50,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:50,421 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:51,592 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 09:54:56,541 - __main__ - INFO - Action: 10, Reward: tensor([-26.8637], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:54:57,338 - __main__ - INFO - Epoch 2, Iteration 10: Reward: tensor([-484.7054], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:54:57,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:54:57,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:54:57,854 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:54:59,091 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:55:03,875 - __main__ - INFO - Action: 11, Reward: tensor([-26.8042], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:04,469 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:04,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:04,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:05,811 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 09:55:14,668 - __main__ - INFO - Action: 15, Reward: tensor([-27.0465], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:15,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:15,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:15,433 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:16,621 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:55:24,825 - __main__ - INFO - Episode timed out.
2024-06-04 09:55:25,374 - __main__ - INFO - Action: 5, Reward: tensor([-27.2232], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:55:26,125 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:55:27,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:27,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:27,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:28,444 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:55:33,156 - __main__ - INFO - Action: 11, Reward: tensor([-124.4017], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:33,514 - __main__ - INFO - Epoch 3, Iteration 0: Reward: tensor([-124.4017], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:55:33,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:33,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:33,948 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:35,236 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 09:55:44,118 - __main__ - INFO - Action: 15, Reward: tensor([-24.3176], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:44,660 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:44,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:44,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:45,916 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 09:55:50,523 - __main__ - INFO - Action: 7, Reward: tensor([-24.3623], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:55:51,129 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:55:51,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:55:51,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:55:52,224 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:56:00,806 - __main__ - INFO - Action: 5, Reward: tensor([-24.9164], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:01,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:01,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:01,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:02,619 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:56:11,119 - __main__ - INFO - Action: 16, Reward: tensor([-25.2901], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:11,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:11,774 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:11,774 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:12,880 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:56:21,607 - __main__ - INFO - Action: 1, Reward: tensor([-25.8781], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:22,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:22,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:23,593 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:56:28,626 - __main__ - INFO - Action: 11, Reward: tensor([-25.6740], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:29,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:29,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:29,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:30,630 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 09:56:35,379 - __main__ - INFO - Action: 6, Reward: tensor([-25.5907], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:36,022 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:36,132 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:36,132 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:37,255 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:56:45,837 - __main__ - INFO - Action: 16, Reward: tensor([-25.5433], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:46,430 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:46,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:46,540 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:47,778 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:56:56,609 - __main__ - INFO - Action: 5, Reward: tensor([-25.7135], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:56:57,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:56:57,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:56:57,072 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:56:57,480 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 09:57:05,902 - __main__ - INFO - Action: 13, Reward: tensor([-25.7741], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:06,543 - __main__ - INFO - Epoch 3, Iteration 10: Reward: tensor([-377.4617], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:57:06,745 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:06,855 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:06,855 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:08,040 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:57:17,043 - __main__ - INFO - Action: 8, Reward: tensor([-127.3534], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:17,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:17,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:17,640 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:18,909 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 09:57:27,156 - __main__ - INFO - Episode timed out.
2024-06-04 09:57:27,546 - __main__ - INFO - Action: 1, Reward: tensor([-127.5957], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:57:28,382 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:57:29,398 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:29,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:29,493 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:30,757 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 09:57:35,730 - __main__ - INFO - Action: 11, Reward: tensor([-124.4065], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:36,530 - __main__ - INFO - Epoch 4, Iteration 0: Reward: tensor([-124.4065], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:57:36,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:36,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:36,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:37,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:57:46,645 - __main__ - INFO - Action: 8, Reward: tensor([-124.9980], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:47,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:47,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:47,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:48,508 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 09:57:57,182 - __main__ - INFO - Action: 4, Reward: tensor([-25.2576], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:57:57,772 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:57:57,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:57:57,896 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:57:59,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:58:07,994 - __main__ - INFO - Action: 0, Reward: tensor([-25.4825], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:08,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:08,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:08,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:09,738 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 09:58:18,704 - __main__ - INFO - Action: 8, Reward: tensor([-25.6952], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:19,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:19,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:19,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:20,574 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 09:58:29,577 - __main__ - INFO - Action: 16, Reward: tensor([-25.8510], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:30,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:30,295 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:30,295 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:31,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:58:40,017 - __main__ - INFO - Action: 5, Reward: tensor([-26.1134], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:40,578 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:40,703 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:40,703 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:41,202 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 09:58:50,288 - __main__ - INFO - Action: 5, Reward: tensor([-26.3639], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:58:50,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:58:50,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:58:50,927 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:58:52,224 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 09:59:00,364 - __main__ - INFO - Action: 0, Reward: tensor([-26.5610], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:00,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:00,727 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:00,727 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:00,987 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 09:59:09,326 - __main__ - INFO - Action: 1, Reward: tensor([-26.6582], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:09,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:09,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:09,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:11,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 09:59:19,816 - __main__ - INFO - Action: 9, Reward: tensor([-25.5355], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:20,506 - __main__ - INFO - Epoch 4, Iteration 10: Reward: tensor([-482.9229], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:59:20,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:20,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:20,910 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:22,145 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 09:59:30,352 - __main__ - INFO - Episode timed out.
2024-06-04 09:59:30,819 - __main__ - INFO - Action: 0, Reward: tensor([-25.1440], grad_fn=<AddBackward0>), Done: True
2024-06-04 09:59:31,553 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 09:59:32,438 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:32,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:32,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:33,659 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 09:59:42,058 - __main__ - INFO - Action: 9, Reward: tensor([-123.5264], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:42,731 - __main__ - INFO - Epoch 5, Iteration 0: Reward: tensor([-123.5264], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 09:59:42,901 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:42,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:42,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:44,292 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 09:59:52,982 - __main__ - INFO - Action: 1, Reward: tensor([-23.1318], grad_fn=<AddBackward0>), Done: False
2024-06-04 09:59:53,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 09:59:53,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 09:59:53,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 09:59:54,728 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 09:59:59,629 - __main__ - INFO - Action: 6, Reward: tensor([-23.1878], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:00,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:00,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:00,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:01,595 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:00:06,457 - __main__ - INFO - Action: 3, Reward: tensor([-23.2988], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:06,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:07,069 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:07,069 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:08,332 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:00:12,951 - __main__ - INFO - Action: 10, Reward: tensor([-23.9015], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:13,532 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:13,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:13,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:14,812 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:00:23,500 - __main__ - INFO - Action: 15, Reward: tensor([-24.9879], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:24,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:24,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:24,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:25,084 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 10:00:29,886 - __main__ - INFO - Action: 6, Reward: tensor([-25.3956], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:30,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:30,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:30,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:31,773 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:00:36,025 - __main__ - INFO - Action: 11, Reward: tensor([-25.1314], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:36,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:36,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:36,647 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:37,935 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:00:46,760 - __main__ - INFO - Action: 0, Reward: tensor([-24.6850], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:47,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:47,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:47,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:48,691 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:00:53,716 - __main__ - INFO - Action: 3, Reward: tensor([-24.7853], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:00:54,060 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:00:54,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:00:54,185 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:00:55,391 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:01:03,896 - __main__ - INFO - Action: 0, Reward: tensor([-24.9169], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:04,689 - __main__ - INFO - Epoch 5, Iteration 10: Reward: tensor([-366.9485], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:01:04,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:04,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:04,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:05,972 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:01:14,693 - __main__ - INFO - Action: 0, Reward: tensor([-25.0035], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:15,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:15,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:15,350 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:16,500 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:01:21,004 - __main__ - INFO - Action: 3, Reward: tensor([-25.0616], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:21,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:21,690 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:21,690 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:22,938 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 10:01:27,551 - __main__ - INFO - Action: 6, Reward: tensor([-25.3797], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:28,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:28,235 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:28,235 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:29,013 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:01:37,021 - __main__ - INFO - Episode timed out.
2024-06-04 10:01:37,580 - __main__ - INFO - Action: 15, Reward: tensor([-25.9383], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:01:38,408 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:01:39,106 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:39,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:39,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:40,454 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 10:01:49,337 - __main__ - INFO - Action: 9, Reward: tensor([-123.5322], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:01:49,963 - __main__ - INFO - Epoch 6, Iteration 0: Reward: tensor([-123.5322], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:01:50,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:01:50,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:01:50,318 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:01:51,520 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:02:00,484 - __main__ - INFO - Action: 16, Reward: tensor([-23.1886], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:00,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:00,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:00,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:01,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 10:02:10,365 - __main__ - INFO - Action: 13, Reward: tensor([-23.4014], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:10,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:11,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:11,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:12,290 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:02:21,150 - __main__ - INFO - Action: 0, Reward: tensor([-23.5623], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:21,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:21,804 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:21,804 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:23,060 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 10:02:28,017 - __main__ - INFO - Action: 3, Reward: tensor([-23.7401], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:28,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:28,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:28,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:29,686 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:02:37,607 - __main__ - INFO - Action: 0, Reward: tensor([-23.9704], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:38,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:38,281 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:38,282 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:38,879 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:02:47,470 - __main__ - INFO - Action: 16, Reward: tensor([-24.3627], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:48,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:48,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:48,155 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:02:49,480 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:02:58,243 - __main__ - INFO - Action: 0, Reward: tensor([-25.0507], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:02:58,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:02:58,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:02:58,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:00,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 10:03:08,646 - __main__ - INFO - Action: 4, Reward: tensor([-25.3271], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:09,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:09,398 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:09,398 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:10,337 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 10:03:14,697 - __main__ - INFO - Action: 2, Reward: tensor([-25.5329], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:14,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:14,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:14,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:16,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:03:20,938 - __main__ - INFO - Action: 10, Reward: tensor([-26.1656], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:21,641 - __main__ - INFO - Epoch 6, Iteration 10: Reward: tensor([-367.8339], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:03:21,668 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:21,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:21,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:22,961 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:03:31,979 - __main__ - INFO - Action: 5, Reward: tensor([-127.1256], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:32,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:32,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:32,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:33,003 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:03:40,859 - __main__ - INFO - Episode timed out.
2024-06-04 10:03:41,159 - __main__ - INFO - Action: 0, Reward: tensor([-27.3431], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:03:41,809 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:03:42,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:42,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:42,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:43,966 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:03:48,788 - __main__ - INFO - Action: 7, Reward: tensor([-124.8132], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:03:49,709 - __main__ - INFO - Epoch 7, Iteration 0: Reward: tensor([-124.8132], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:03:50,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:03:50,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:03:50,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:03:51,215 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:04:00,228 - __main__ - INFO - Action: 0, Reward: tensor([-25.0567], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:00,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:00,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:00,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:02,158 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 10:04:06,967 - __main__ - INFO - Action: 3, Reward: tensor([-25.1361], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:07,498 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:07,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:07,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:08,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 10:04:17,732 - __main__ - INFO - Action: 8, Reward: tensor([-25.4061], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:18,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:18,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:18,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:19,481 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:04:27,851 - __main__ - INFO - Action: 15, Reward: tensor([-25.6570], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:28,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:28,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:28,535 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:29,041 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:04:36,959 - __main__ - INFO - Action: 0, Reward: tensor([-25.7725], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:37,446 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:37,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:37,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:38,756 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:04:48,045 - __main__ - INFO - Action: 0, Reward: tensor([-26.0229], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:48,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:48,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:48,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:49,157 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:04:53,041 - __main__ - INFO - Action: 7, Reward: tensor([-26.1312], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:04:53,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:04:53,561 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:04:53,561 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:04:54,857 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 10:05:03,764 - __main__ - INFO - Action: 13, Reward: tensor([-26.3704], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:04,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:04,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:04,497 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:05,373 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 10:05:09,976 - __main__ - INFO - Action: 6, Reward: tensor([-26.3811], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:10,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:10,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:10,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:11,963 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:05:20,584 - __main__ - INFO - Action: 0, Reward: tensor([-26.6444], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:21,411 - __main__ - INFO - Epoch 7, Iteration 10: Reward: tensor([-383.3917], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:05:21,597 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:21,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:21,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:22,770 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:05:31,591 - __main__ - INFO - Action: 0, Reward: tensor([-26.8626], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:32,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:32,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:32,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:33,514 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:05:38,624 - __main__ - INFO - Action: 11, Reward: tensor([-26.5662], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:05:39,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:39,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:39,284 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:40,547 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:05:49,092 - __main__ - INFO - Episode timed out.
2024-06-04 10:05:49,652 - __main__ - INFO - Action: 15, Reward: tensor([-26.0751], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:05:50,372 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:05:51,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:05:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:05:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:05:52,557 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:06:01,517 - __main__ - INFO - Action: 5, Reward: tensor([-124.8956], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:01,809 - __main__ - INFO - Epoch 8, Iteration 0: Reward: tensor([-124.8956], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:06:01,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:01,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:01,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:02,765 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 10:06:07,626 - __main__ - INFO - Action: 2, Reward: tensor([-25.0332], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:08,223 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:08,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:08,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:09,571 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:06:18,772 - __main__ - INFO - Action: 5, Reward: tensor([-125.2036], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:19,405 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:19,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:19,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:20,741 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:06:29,751 - __main__ - INFO - Action: 0, Reward: tensor([-125.5371], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:30,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:30,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:30,270 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:31,545 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 10:06:40,631 - __main__ - INFO - Action: 4, Reward: tensor([-125.7091], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:41,208 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:41,317 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:41,317 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:42,465 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 10:06:46,906 - __main__ - INFO - Action: 2, Reward: tensor([-125.9245], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:47,435 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:47,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:47,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:48,697 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:06:53,227 - __main__ - INFO - Action: 11, Reward: tensor([-25.6616], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:06:53,849 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:06:53,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:06:53,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:06:55,144 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 10:07:04,032 - __main__ - INFO - Action: 9, Reward: tensor([-23.8762], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:04,515 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:05,718 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:07:10,676 - __main__ - INFO - Action: 7, Reward: tensor([-23.6739], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:11,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:11,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:11,376 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:12,438 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 10:07:20,846 - __main__ - INFO - Action: 9, Reward: tensor([-21.9662], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:21,407 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:21,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:21,501 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:22,780 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 10:07:31,762 - __main__ - INFO - Action: 4, Reward: tensor([-21.1734], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:32,701 - __main__ - INFO - Epoch 8, Iteration 10: Reward: tensor([-768.6545], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:07:33,010 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:33,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:33,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:34,207 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 10:07:42,368 - __main__ - INFO - Action: 13, Reward: tensor([-20.7968], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:07:42,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:42,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:42,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:43,969 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 10:07:52,503 - __main__ - INFO - Episode timed out.
2024-06-04 10:07:52,974 - __main__ - INFO - Action: 15, Reward: tensor([-20.8198], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:07:53,642 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 10:07:54,593 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:07:54,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:07:54,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:07:55,440 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:08:00,130 - __main__ - INFO - Action: 10, Reward: tensor([-124.7957], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:00,973 - __main__ - INFO - Epoch 9, Iteration 0: Reward: tensor([-124.7957], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:08:01,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:01,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:01,444 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:02,742 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:08:11,871 - __main__ - INFO - Action: 16, Reward: tensor([-25.0093], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:12,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:12,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:12,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:13,117 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 10:08:17,807 - __main__ - INFO - Action: 7, Reward: tensor([-25.1510], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:18,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:18,585 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:18,585 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:19,671 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 10:08:24,029 - __main__ - INFO - Action: 11, Reward: tensor([-24.9538], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:24,387 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:24,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:24,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:25,002 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:08:32,995 - __main__ - INFO - Action: 0, Reward: tensor([-24.7641], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:33,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:33,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:33,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:34,677 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 10:08:39,208 - __main__ - INFO - Action: 10, Reward: tensor([-25.3829], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:39,754 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:39,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:39,878 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:41,097 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 10:08:49,854 - __main__ - INFO - Action: 4, Reward: tensor([-125.8392], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:08:50,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:08:50,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:08:50,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:08:51,625 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 10:09:00,614 - __main__ - INFO - Action: 5, Reward: tensor([-26.0104], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:01,129 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:01,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:01,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:01,513 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:09:10,402 - __main__ - INFO - Action: 0, Reward: tensor([-26.2560], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:11,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:11,104 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:11,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:12,341 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:09:20,775 - __main__ - INFO - Action: 0, Reward: tensor([-26.5176], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:21,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:21,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:21,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:22,772 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 10:09:31,840 - __main__ - INFO - Action: 0, Reward: tensor([-26.7487], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:32,637 - __main__ - INFO - Epoch 9, Iteration 10: Reward: tensor([-481.4288], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 10:09:33,011 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:34,350 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 10:09:43,070 - __main__ - INFO - Action: 0, Reward: tensor([-26.9916], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:43,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:43,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:43,711 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:44,832 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 10:09:53,770 - __main__ - INFO - Action: 14, Reward: tensor([-27.2356], grad_fn=<AddBackward0>), Done: False
2024-06-04 10:09:54,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 10:09:54,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 10:09:54,550 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 10:09:55,768 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 10:10:04,133 - __main__ - INFO - Episode timed out.
2024-06-04 10:10:04,433 - __main__ - INFO - Action: 16, Reward: tensor([-27.3492], grad_fn=<AddBackward0>), Done: True
2024-06-04 10:10:32,396 - __main__ - INFO - Dropped rows with NA values. Shape changed from (945, 2) to (945, 2).
2024-06-04 10:10:32,472 - __main__ - DEBUG - Transformed column values using <lambda>.
2024-06-04 10:11:55,488 - __main__ - INFO - Time Series plot created successfully for steps vs values.
2024-06-04 10:12:32,569 - __main__ - INFO - Histogram plotted successfully for column: values.
2024-06-04 10:12:54,688 - __main__ - INFO - Correlation matrix heatmap generated successfully.
2024-06-04 10:32:58,403 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:40:09,843 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:40:17,302 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 13:40:17,443 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 13:40:17,880 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 13:40:17,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 13:40:17,976 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 13:40:18,990 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 13:40:27,750 - __main__ - INFO - Action: 9, Reward: tensor([-123.2824], grad_fn=<AddBackward0>), Done: False
2024-06-04 13:45:52,873 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:50:28,636 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:53:49,356 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 13:53:57,405 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 13:53:57,937 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 13:53:58,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 13:53:58,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 13:53:58,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 13:53:59,778 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 13:54:08,427 - __main__ - INFO - Action: 0, Reward: tensor([-124.7604], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:37:35,995 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-04 17:37:43,449 - __main__ - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-06-04 17:37:43,790 - __main__ - INFO - Environment reset and takeoff completed.
2024-06-04 17:37:44,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:37:44,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:37:44,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:37:45,654 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:37:54,530 - __main__ - INFO - Action: 12, Reward: tensor([-124.7073], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:37:55,307 - __main__ - INFO - Epoch 0, Iteration 0: Reward: tensor([-124.7073], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:37:55,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:37:55,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:37:55,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:37:56,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:38:01,362 - __main__ - INFO - Action: 6, Reward: tensor([-24.9723], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:01,596 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:01,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:01,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:02,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:38:11,226 - __main__ - INFO - Action: 13, Reward: tensor([-125.1077], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:11,584 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:11,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:11,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:12,849 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:38:21,556 - __main__ - INFO - Action: 13, Reward: tensor([-25.4075], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:21,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:22,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:22,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:23,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:38:30,955 - __main__ - INFO - Action: 1, Reward: tensor([-25.6059], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:31,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:31,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:31,157 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:32,168 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:38:36,750 - __main__ - INFO - Action: 2, Reward: tensor([-25.5505], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:36,980 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:37,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:37,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:37,867 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:38:45,450 - __main__ - INFO - Action: 13, Reward: tensor([-25.8587], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:45,568 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:45,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:45,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:46,174 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:38:54,713 - __main__ - INFO - Action: 8, Reward: tensor([-26.0122], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:38:54,978 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:38:55,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:38:55,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:38:55,836 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:39:04,158 - __main__ - INFO - Action: 8, Reward: tensor([-26.2379], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:04,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:04,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:04,564 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:05,727 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:39:13,771 - __main__ - INFO - Action: 4, Reward: tensor([-26.4053], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:13,941 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:14,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:14,035 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:14,331 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:39:22,640 - __main__ - INFO - Action: 1, Reward: tensor([-26.5824], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:23,325 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:23,310 - __main__ - INFO - Epoch 0, Iteration 10: Reward: tensor([-482.4476], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:39:23,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:23,386 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:24,120 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:39:32,217 - __main__ - INFO - Action: 14, Reward: tensor([-26.7355], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:32,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:32,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:32,514 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:33,549 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:39:41,935 - __main__ - INFO - Action: 4, Reward: tensor([-26.9058], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:42,215 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:42,308 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:42,308 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:42,712 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:39:47,059 - __main__ - INFO - Action: 7, Reward: tensor([-27.0098], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:47,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:47,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:47,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:48,435 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:39:52,863 - __main__ - INFO - Action: 2, Reward: tensor([-27.1685], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:39:53,112 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:39:53,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:39:53,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:39:53,862 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:40:01,685 - __main__ - INFO - Action: 1, Reward: tensor([-27.1195], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:01,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:02,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:02,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:02,710 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:40:07,185 - __main__ - INFO - Action: 7, Reward: tensor([-27.2852], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:07,467 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:07,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:07,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:08,653 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:40:16,536 - __main__ - INFO - Action: 0, Reward: tensor([-27.4662], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:16,706 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:16,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:16,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:17,626 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:40:21,814 - __main__ - INFO - Action: 6, Reward: tensor([-27.6572], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:22,032 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:22,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:22,156 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:22,573 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:40:30,780 - __main__ - INFO - Action: 16, Reward: tensor([-27.8626], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:31,091 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:31,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:31,216 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:31,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:40:39,760 - __main__ - INFO - Action: 9, Reward: tensor([-26.7809], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:40,364 - __main__ - INFO - Epoch 0, Iteration 20: Reward: tensor([-754.4388], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:40:40,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:40,534 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:40,534 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:40,676 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:40:48,159 - __main__ - INFO - Action: 14, Reward: tensor([-26.4016], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:48,236 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:48,236 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:48,236 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:48,873 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:40:56,207 - __main__ - INFO - Action: 15, Reward: tensor([-25.8747], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:40:56,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:40:56,515 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:40:56,515 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:40:56,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:41:04,890 - __main__ - INFO - Action: 4, Reward: tensor([-25.7439], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:05,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:05,188 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:05,188 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:05,588 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:41:09,167 - __main__ - INFO - Action: 6, Reward: tensor([-25.9603], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:09,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:09,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:09,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:09,744 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:41:13,093 - __main__ - INFO - Action: 10, Reward: tensor([-26.5257], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:13,409 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:13,532 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:13,532 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:13,967 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:41:21,305 - __main__ - INFO - Action: 13, Reward: tensor([-27.2891], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:21,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:21,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:21,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:21,939 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:41:29,455 - __main__ - INFO - Action: 0, Reward: tensor([-27.8095], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:29,658 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:29,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:29,796 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:30,091 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 17:41:33,502 - __main__ - INFO - Action: 3, Reward: tensor([-27.9721], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:33,611 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:33,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:33,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:34,121 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:41:42,017 - __main__ - INFO - Action: 8, Reward: tensor([-129.4059], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:42,269 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:42,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:42,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:42,812 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:41:50,588 - __main__ - INFO - Action: 8, Reward: tensor([-29.6310], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:50,821 - __main__ - INFO - Epoch 0, Iteration 30: Reward: tensor([-1127.0525], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:41:50,836 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:50,883 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:50,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:51,099 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:41:58,432 - __main__ - INFO - Action: 12, Reward: tensor([-29.7879], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:41:58,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:41:58,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:41:58,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:41:58,868 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:42:06,042 - __main__ - INFO - Action: 15, Reward: tensor([-29.9539], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:06,258 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:06,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:06,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:06,614 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:42:13,534 - __main__ - INFO - Action: 4, Reward: tensor([-30.1211], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:13,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:13,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:13,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:14,093 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:42:17,282 - __main__ - INFO - Action: 11, Reward: tensor([-29.7845], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:17,482 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:17,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:18,087 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:42:25,276 - __main__ - INFO - Action: 4, Reward: tensor([-29.5044], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:25,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:25,445 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:25,445 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:25,913 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:42:29,538 - __main__ - INFO - Action: 7, Reward: tensor([-29.5906], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:29,753 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:29,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:29,815 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:30,436 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:42:37,331 - __main__ - INFO - Action: 8, Reward: tensor([-130.6261], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:37,363 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:37,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:37,378 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:37,670 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 17:42:44,826 - __main__ - INFO - Action: 0, Reward: tensor([-30.7951], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:44,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:44,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:44,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:45,259 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:42:48,387 - __main__ - INFO - Action: 11, Reward: tensor([-30.6109], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:48,590 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:48,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:48,621 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:49,026 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:42:52,306 - __main__ - INFO - Action: 6, Reward: tensor([-30.6481], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:42:52,728 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:42:52,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:42:52,712 - __main__ - INFO - Epoch 0, Iteration 40: Reward: tensor([-1528.4753], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:42:52,729 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:42:53,009 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:43:00,433 - __main__ - INFO - Action: 16, Reward: tensor([-30.7394], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:00,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:00,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:00,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:00,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:43:04,264 - __main__ - INFO - Action: 2, Reward: tensor([-30.9092], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:04,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:04,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:04,563 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:04,830 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-04 17:43:07,971 - __main__ - INFO - Action: 3, Reward: tensor([-30.9363], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:08,204 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:08,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:08,251 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:08,606 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:43:12,271 - __main__ - INFO - Action: 11, Reward: tensor([-30.6545], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:12,597 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:12,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:12,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:12,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:43:20,072 - __main__ - INFO - Action: 15, Reward: tensor([-30.1745], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:20,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:20,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:20,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:20,679 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:43:27,823 - __main__ - INFO - Action: 15, Reward: tensor([-30.3310], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:27,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:28,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:28,023 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:28,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:43:35,920 - __main__ - INFO - Action: 16, Reward: tensor([-30.4803], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:36,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:36,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:36,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:36,820 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:43:40,804 - __main__ - INFO - Action: 7, Reward: tensor([-30.5946], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:41,100 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:41,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:41,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:41,814 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:43:45,371 - __main__ - INFO - Action: 10, Reward: tensor([-31.2410], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:45,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:45,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:45,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:46,314 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:43:53,935 - __main__ - INFO - Action: 14, Reward: tensor([-31.8477], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:43:54,587 - __main__ - INFO - Epoch 0, Iteration 50: Reward: tensor([-1836.3838], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:43:54,619 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:43:54,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:43:54,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:43:55,443 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:44:02,876 - __main__ - INFO - Action: 0, Reward: tensor([-31.9627], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:03,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:03,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:03,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:03,500 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:44:10,961 - __main__ - INFO - Action: 9, Reward: tensor([-30.7549], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:11,149 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:11,210 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:11,210 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:11,535 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 17:44:18,871 - __main__ - INFO - Action: 5, Reward: tensor([-29.9219], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:19,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:19,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:19,323 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:20,148 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:44:23,833 - __main__ - INFO - Action: 11, Reward: tensor([-29.4261], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:24,003 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:24,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:24,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:24,887 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:44:32,401 - __main__ - INFO - Action: 0, Reward: tensor([-28.7715], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:32,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:32,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:32,757 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:33,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:44:40,530 - __main__ - INFO - Action: 15, Reward: tensor([-28.5258], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:40,638 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:40,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:40,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:40,947 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:44:47,828 - __main__ - INFO - Action: 9, Reward: tensor([-27.1675], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:47,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:47,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:47,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:48,462 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:44:51,728 - __main__ - INFO - Action: 2, Reward: tensor([-27.0586], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:44:51,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:44:51,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:44:51,930 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:44:52,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:45:00,004 - __main__ - INFO - Action: 16, Reward: tensor([-26.6445], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:00,050 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:00,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:00,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:00,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:45:07,769 - __main__ - INFO - Action: 9, Reward: tensor([-25.2714], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:07,997 - __main__ - INFO - Epoch 0, Iteration 60: Reward: tensor([-2121.8887], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:45:08,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:08,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:08,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:08,320 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 4 selected.
2024-06-04 17:45:16,498 - __main__ - INFO - Action: 4, Reward: tensor([-24.8661], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:16,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:16,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:16,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:17,245 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:45:20,966 - __main__ - INFO - Action: 10, Reward: tensor([-25.4389], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:21,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:21,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:21,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:22,002 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:45:25,822 - __main__ - INFO - Action: 2, Reward: tensor([-25.7962], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:26,131 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:26,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:26,240 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:26,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:45:34,483 - __main__ - INFO - Action: 8, Reward: tensor([-27.7089], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:34,606 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:34,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:34,654 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:34,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:45:43,232 - __main__ - INFO - Action: 13, Reward: tensor([-28.3464], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:43,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:43,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:43,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:44,348 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:45:52,404 - __main__ - INFO - Action: 1, Reward: tensor([-28.6805], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:45:52,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:45:52,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:45:52,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:45:53,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 17:46:01,911 - __main__ - INFO - Action: 5, Reward: tensor([-28.6740], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:02,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:02,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:02,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:03,339 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 5 selected.
2024-06-04 17:46:11,452 - __main__ - INFO - Action: 5, Reward: tensor([-28.8288], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:11,762 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:11,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:11,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:12,910 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-04 17:46:20,844 - __main__ - INFO - Action: 0, Reward: tensor([-29.1719], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:21,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:21,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:21,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:21,850 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:46:30,607 - __main__ - INFO - Action: 1, Reward: tensor([-29.5338], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:31,198 - __main__ - INFO - Epoch 0, Iteration 70: Reward: tensor([-2398.9343], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:46:31,226 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:31,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:31,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:32,297 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:46:40,330 - __main__ - INFO - Action: 14, Reward: tensor([-29.9624], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:40,689 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:40,768 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:40,768 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:41,267 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:46:45,290 - __main__ - INFO - Action: 11, Reward: tensor([-29.6637], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:45,556 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:45,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:45,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:46,195 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:46:53,814 - __main__ - INFO - Action: 15, Reward: tensor([-29.1012], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:54,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:54,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:54,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:54,869 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:46:58,781 - __main__ - INFO - Action: 6, Reward: tensor([-29.2162], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:46:58,998 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:46:59,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:46:59,108 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:46:59,499 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:47:07,259 - __main__ - INFO - Action: 14, Reward: tensor([-29.5441], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:07,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:07,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:07,662 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:08,023 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:47:12,174 - __main__ - INFO - Action: 7, Reward: tensor([-29.8548], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:12,361 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:12,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:12,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:13,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:47:17,145 - __main__ - INFO - Action: 2, Reward: tensor([-30.1551], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:17,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:17,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:18,531 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:47:25,929 - __main__ - INFO - Action: 1, Reward: tensor([-30.2600], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:26,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:26,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:26,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:26,594 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:47:29,957 - __main__ - INFO - Action: 2, Reward: tensor([-30.4675], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:30,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:30,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:30,109 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:30,528 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:47:34,465 - __main__ - INFO - Action: 2, Reward: tensor([-30.6903], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:34,541 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:34,526 - __main__ - INFO - Epoch 0, Iteration 80: Reward: tensor([-2697.8499], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:47:34,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:34,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:34,828 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:47:42,702 - __main__ - INFO - Action: 1, Reward: tensor([-31.0019], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:42,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:43,118 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:43,118 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:43,693 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:47:52,038 - __main__ - INFO - Action: 15, Reward: tensor([-31.1251], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:52,334 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:52,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:52,427 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:53,242 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:47:57,393 - __main__ - INFO - Action: 2, Reward: tensor([-31.1954], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:47:57,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:47:57,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:47:57,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:47:58,533 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 8 selected.
2024-06-04 17:48:06,003 - __main__ - INFO - Action: 8, Reward: tensor([-32.9407], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:06,093 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:06,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:06,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:07,090 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 15 selected.
2024-06-04 17:48:15,140 - __main__ - INFO - Action: 15, Reward: tensor([-33.9088], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:15,468 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:15,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:15,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:16,337 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:48:23,722 - __main__ - INFO - Action: 14, Reward: tensor([-34.4533], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:23,893 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:24,000 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:24,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:24,190 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 7 selected.
2024-06-04 17:48:28,494 - __main__ - INFO - Action: 7, Reward: tensor([-34.7802], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:28,822 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:28,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:28,917 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:30,042 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:48:38,249 - __main__ - INFO - Action: 12, Reward: tensor([-35.0514], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:38,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:38,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:38,669 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:39,752 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:48:48,604 - __main__ - INFO - Action: 0, Reward: tensor([-35.6482], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:48,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:48,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:48,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:48:49,735 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:48:58,690 - __main__ - INFO - Action: 0, Reward: tensor([-36.3071], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:48:59,536 - __main__ - INFO - Epoch 0, Iteration 90: Reward: tensor([-3034.2622], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:48:59,552 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:48:59,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:48:59,659 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:00,704 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:49:09,044 - __main__ - INFO - Action: 0, Reward: tensor([-37.3258], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:09,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:09,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:09,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:10,500 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:49:18,937 - __main__ - INFO - Action: 4, Reward: tensor([-37.8156], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:19,293 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:19,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:19,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:20,365 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 7 selected.
2024-06-04 17:49:24,436 - __main__ - INFO - Action: 7, Reward: tensor([-38.0586], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:24,795 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:24,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:24,903 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:25,929 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:49:34,888 - __main__ - INFO - Action: 0, Reward: tensor([-39.0500], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:35,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:35,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:35,306 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:36,335 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 16 selected.
2024-06-04 17:49:44,578 - __main__ - INFO - Action: 16, Reward: tensor([-39.6343], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:44,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:44,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:44,950 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:45,878 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:49:50,907 - __main__ - INFO - Action: 2, Reward: tensor([-40.0855], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:51,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:51,294 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:52,522 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:49:57,655 - __main__ - INFO - Action: 2, Reward: tensor([-40.5093], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:49:57,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:49:58,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:49:58,075 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:49:59,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 10 selected.
2024-06-04 17:50:04,030 - __main__ - INFO - Action: 10, Reward: tensor([-41.1383], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:04,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:04,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:04,434 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:05,664 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:50:14,479 - __main__ - INFO - Action: 14, Reward: tensor([-142.3421], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:14,791 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:14,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:14,884 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:15,364 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:50:23,954 - __main__ - INFO - Action: 0, Reward: tensor([-42.5317], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:24,870 - __main__ - INFO - Epoch 0, Iteration 100: Reward: tensor([-3532.7532], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:50:24,886 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:24,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:24,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:26,114 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-04 17:50:34,837 - __main__ - INFO - Action: 1, Reward: tensor([-42.5943], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:35,147 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:35,255 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:35,255 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:36,545 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:50:41,338 - __main__ - INFO - Action: 11, Reward: tensor([-42.4510], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:41,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:41,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:41,707 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:42,981 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:50:48,027 - __main__ - INFO - Action: 3, Reward: tensor([-42.3248], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:48,337 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:48,462 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:48,462 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:49,734 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:50:58,507 - __main__ - INFO - Action: 1, Reward: tensor([-41.7030], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:50:58,772 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:50:58,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:50:58,895 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:50:59,826 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 11 selected.
2024-06-04 17:51:04,541 - __main__ - INFO - Action: 11, Reward: tensor([-41.2091], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:04,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:04,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:04,942 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:05,562 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:51:10,060 - __main__ - INFO - Action: 2, Reward: tensor([-41.2218], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:10,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:10,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:10,311 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:11,477 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:51:16,428 - __main__ - INFO - Action: 3, Reward: tensor([-41.0562], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:16,725 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:16,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:16,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:17,844 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:51:26,141 - __main__ - INFO - Action: 0, Reward: tensor([-41.6600], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:26,455 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:26,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:26,565 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:27,577 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:51:32,317 - __main__ - INFO - Action: 2, Reward: tensor([-42.1099], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:32,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:32,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:32,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:33,940 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-06-04 17:51:42,739 - __main__ - INFO - Action: 13, Reward: tensor([-42.1483], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:43,517 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:43,502 - __main__ - INFO - Epoch 0, Iteration 110: Reward: tensor([-3951.2312], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:51:43,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:43,657 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:44,818 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:51:53,261 - __main__ - INFO - Action: 0, Reward: tensor([-42.7145], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:51:53,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:51:53,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:51:53,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:51:54,614 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-04 17:52:03,321 - __main__ - INFO - Action: 14, Reward: tensor([-43.7600], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:03,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:04,725 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:52:09,843 - __main__ - INFO - Action: 3, Reward: tensor([-43.8498], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:10,110 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:10,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:10,219 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:11,498 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:52:16,386 - __main__ - INFO - Action: 6, Reward: tensor([-43.9645], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:16,652 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:16,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:16,730 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:17,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:52:22,656 - __main__ - INFO - Action: 2, Reward: tensor([-44.2905], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:23,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:23,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:23,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:24,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:52:29,209 - __main__ - INFO - Action: 2, Reward: tensor([-44.6609], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:29,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:29,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:29,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:30,811 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 6 selected.
2024-06-04 17:52:35,805 - __main__ - INFO - Action: 6, Reward: tensor([-44.7752], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:36,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:36,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:36,162 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:37,301 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-04 17:52:42,027 - __main__ - INFO - Action: 3, Reward: tensor([-44.6943], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:42,259 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:42,369 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:42,369 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:43,412 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:52:52,622 - __main__ - INFO - Action: 1, Reward: tensor([-44.2991], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:52:52,906 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:52:52,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:52:52,999 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:52:54,272 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-04 17:52:59,155 - __main__ - INFO - Action: 2, Reward: tensor([-44.4466], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:00,029 - __main__ - INFO - Epoch 0, Iteration 120: Reward: tensor([-4392.6865], grad_fn=<AddBackward0>), Policy Loss: None, Value Loss: None, Total Loss: None, Entropy: None
2024-06-04 17:53:00,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:00,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:00,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:01,277 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-04 17:53:06,200 - __main__ - INFO - Action: 2, Reward: tensor([-44.8662], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:06,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:06,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:06,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:07,581 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:53:16,690 - __main__ - INFO - Action: 0, Reward: tensor([-46.0379], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:17,016 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:17,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:17,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:18,332 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:53:27,337 - __main__ - INFO - Action: 1, Reward: tensor([-46.1244], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:27,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:27,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:27,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:28,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:53:37,625 - __main__ - INFO - Action: 4, Reward: tensor([-46.2451], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:37,936 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:38,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:38,014 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:39,198 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-04 17:53:48,198 - __main__ - INFO - Action: 0, Reward: tensor([-47.0115], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:48,524 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:48,633 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:48,633 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:49,876 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-04 17:53:58,602 - __main__ - INFO - Action: 1, Reward: tensor([-47.0206], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:53:58,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:53:58,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:53:58,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:53:59,917 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 12 selected.
2024-06-04 17:54:08,793 - __main__ - INFO - Action: 12, Reward: tensor([-47.6410], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:54:09,071 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:54:09,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:54:09,180 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:54:10,330 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 9 selected.
2024-06-04 17:54:19,016 - __main__ - INFO - Action: 9, Reward: tensor([-46.8375], grad_fn=<AddBackward0>), Done: False
2024-06-04 17:54:19,264 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-04 17:54:19,312 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-04 17:54:19,312 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-04 17:54:20,604 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 4 selected.
2024-06-04 17:54:28,897 - __main__ - INFO - Episode timed out.
2024-06-04 17:54:29,395 - __main__ - INFO - Action: 4, Reward: tensor([-46.5437], grad_fn=<AddBackward0>), Done: True
2024-06-05 17:11:43,829 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:16:10,189 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:20:08,414 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:23:22,104 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:23:31,424 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:23:31,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:23:31,548 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:23:32,513 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:27:33,952 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-05 17:27:43,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:44,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:44,018 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:45,098 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:27:48,966 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:49,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:49,045 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:50,093 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:27:53,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:53,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:53,214 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:53,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:27:56,716 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:27:56,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:27:56,793 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:27:57,167 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:00,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:00,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:00,298 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:01,252 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:28:04,453 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:04,562 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:05,696 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:08,843 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:08,922 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:08,922 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:10,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:13,076 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:13,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:13,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:14,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:17,453 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:17,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:17,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:18,657 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:28:21,843 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:21,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:21,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:23,013 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:26,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:26,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:26,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:27,201 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:28:30,803 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:30,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:30,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:32,038 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:28:34,256 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:34,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:34,379 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:34,941 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:37,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:37,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:37,888 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:38,866 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:41,813 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:41,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:41,891 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:42,902 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:45,977 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:46,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:46,041 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:46,944 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:28:50,197 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:50,305 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:50,305 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:51,207 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:28:54,210 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:54,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:54,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:55,209 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:28:57,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:28:58,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:28:58,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:28:58,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:01,789 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:01,867 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:01,867 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:02,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:05,364 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:05,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:05,506 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:06,597 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:29:10,230 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:10,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:10,309 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:11,018 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:29:13,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:13,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:13,718 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:14,605 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:29:17,463 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:17,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:17,559 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:18,668 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:21,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:21,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:21,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:22,971 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:29:25,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:26,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:26,085 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:27,207 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:30,188 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:30,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:30,280 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:31,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:33,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:33,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:33,741 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:34,900 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:29:38,099 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:38,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:38,207 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:38,998 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:29:42,115 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:42,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:42,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:43,300 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:46,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:46,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:46,203 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:47,185 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:29:50,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:50,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:50,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:51,751 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:29:54,702 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:54,765 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:54,765 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:55,698 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:29:58,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:29:58,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:29:58,935 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:29:59,884 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:02,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:02,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:02,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:03,877 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:30:06,799 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:06,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:06,924 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:07,984 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:11,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:11,302 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:12,223 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:30:15,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:15,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:15,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:16,548 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:19,504 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:19,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:19,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:20,713 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:30:23,699 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:23,823 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:23,823 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:24,930 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:27,830 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:27,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:27,909 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:28,993 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:30:32,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:32,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:32,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:33,580 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:30:36,644 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:36,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:36,722 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:37,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:40,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:40,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:40,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:42,089 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:44,831 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:44,956 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:44,956 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:46,048 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:30:49,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:49,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:49,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:50,382 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:30:53,432 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:53,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:53,557 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:54,602 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:30:57,496 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:30:57,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:30:57,574 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:30:58,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:31:01,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:01,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:01,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:02,807 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:31:05,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:05,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:05,975 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:07,061 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:09,918 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:10,042 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:10,042 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:10,742 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:31:14,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:14,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:14,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:15,466 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:31:18,518 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:18,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:18,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:19,669 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:31:22,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:22,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:22,642 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:23,742 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:31:26,865 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:26,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:26,990 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:28,092 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:30,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:30,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:30,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:31,794 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:34,766 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:34,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:34,874 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:35,990 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:39,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:39,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:39,184 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:40,104 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:31:42,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:43,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:43,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:43,938 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:31:46,912 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:46,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:46,989 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:48,047 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:51,119 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:51,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:51,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:52,274 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:55,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:55,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:55,812 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:31:56,822 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:31:59,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:31:59,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:31:59,723 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:00,857 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:03,882 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:04,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:04,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:05,176 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:07,779 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:07,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:07,905 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:08,884 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:12,033 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:12,158 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:12,158 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:13,254 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:16,265 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:16,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:16,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:17,526 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:20,665 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:20,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:20,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:21,965 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:25,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:25,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:25,159 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:26,172 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:29,140 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:29,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:29,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:30,421 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:32:33,368 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:33,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:33,492 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:34,584 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:37,697 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:37,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:37,712 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:38,538 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:32:41,684 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:41,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:41,809 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:42,839 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:32:45,861 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:45,939 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:45,939 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:47,122 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:32:50,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:50,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:50,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:51,302 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:32:54,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:54,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:54,370 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:55,415 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:32:58,661 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:32:58,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:32:58,771 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:32:59,911 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:03,001 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:03,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:03,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:04,153 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:33:07,336 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:07,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:07,382 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:08,411 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:33:11,533 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:11,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:11,628 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:12,717 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:33:15,801 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:15,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:15,864 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:16,937 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:33:20,630 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:20,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:20,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:21,734 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:33:24,844 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:24,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:25,950 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:33:29,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:29,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:29,161 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:30,202 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:33:33,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:33,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:33,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:34,411 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:33:37,288 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:37,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:37,396 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:38,441 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:33:41,571 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:41,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:41,649 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:42,731 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:33:45,766 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:45,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:45,828 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:46,996 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:49,984 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:50,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:50,047 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:51,075 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:54,244 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:54,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:54,353 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:55,382 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:33:58,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:33:58,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:33:58,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:33:59,745 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:34:03,479 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:03,588 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:04,709 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:34:07,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:07,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:07,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:08,536 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:34:11,519 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:11,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:11,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:12,703 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:34:15,601 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:15,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:15,679 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:16,797 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:34:19,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:19,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:19,612 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:20,745 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:34:23,788 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:23,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:23,852 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:24,972 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:34:28,078 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:28,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:28,187 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:29,281 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:34:32,355 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:32,466 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:32,466 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:33,631 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:34:36,925 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:37,050 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:37,050 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:38,173 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:34:41,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:41,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:41,198 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:42,138 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:34:45,787 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:45,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:45,898 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:46,996 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:34:49,922 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:50,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:50,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:51,168 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:34:54,292 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:54,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:54,402 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:55,524 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:34:58,739 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:34:58,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:34:58,835 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:34:59,950 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:35:03,020 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:03,036 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:03,036 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:03,908 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:35:07,042 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:07,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:07,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:08,166 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:35:11,423 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:11,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:11,470 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:12,484 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:15,720 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:15,816 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:15,816 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:16,986 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:20,148 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:20,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:20,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:21,503 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:35:24,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:24,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:24,605 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:25,602 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:35:29,124 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:29,202 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:29,202 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:30,382 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:35:33,440 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:33,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:33,551 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:34,583 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:35:37,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:37,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:37,457 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:38,423 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:35:41,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:41,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:41,570 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:42,631 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:35:45,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:45,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:45,943 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:47,017 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:35:50,132 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:50,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:50,242 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:51,320 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:54,411 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:54,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:54,537 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:55,615 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:35:58,586 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:35:58,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:35:58,694 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:35:59,773 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:36:02,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:02,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:02,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:03,846 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:36:06,814 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:06,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:06,937 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:07,965 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:36:11,416 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:11,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:11,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:12,603 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:36:15,838 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:15,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:15,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:17,023 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:36:20,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:20,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:20,176 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:21,156 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:36:24,473 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:24,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:24,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:25,737 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:28,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:29,039 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:29,039 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:30,034 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:36:32,946 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:33,088 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:34,149 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:37,254 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:37,316 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:37,316 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:38,341 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:41,326 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:41,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:41,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:42,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:36:45,600 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:45,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:45,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:46,815 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:36:49,916 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:50,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:50,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:51,146 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:36:55,002 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:55,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:55,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:36:56,214 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:36:59,130 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:36:59,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:36:59,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:00,312 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:03,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:03,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:03,384 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:04,525 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:37:07,648 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:07,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:07,726 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:08,711 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:11,915 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:12,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:12,009 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:12,510 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:15,170 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:15,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:15,279 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:16,359 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:37:19,458 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:19,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:19,566 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:20,658 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:37:23,592 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:23,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:23,685 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:24,775 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:37:27,947 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:28,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:28,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:29,040 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:37:32,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:32,360 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:32,360 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:33,137 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:37:36,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:37,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:37,084 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:38,110 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:41,136 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:41,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:41,199 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:42,353 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:37:45,225 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:45,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:45,352 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:46,396 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:37:49,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:49,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:49,451 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:50,526 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:37:53,404 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:53,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:53,452 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:54,572 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:37:57,624 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:37:57,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:37:57,717 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:37:58,855 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:38:01,969 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:02,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:02,079 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:02,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:38:05,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:05,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:05,911 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:06,989 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:38:10,000 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:10,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:10,094 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:11,129 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:38:14,101 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:14,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:14,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:15,286 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:38:19,154 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:19,232 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:19,232 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:20,404 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:38:23,432 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:23,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:23,542 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:24,613 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:38:27,579 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:27,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:27,704 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:28,619 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:38:31,724 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:31,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:31,833 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:32,784 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:38:35,177 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:35,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:35,303 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:36,376 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:38:39,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:39,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:39,142 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:40,262 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:38:43,333 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:43,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:43,365 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:44,504 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:38:47,456 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:47,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:47,549 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:48,560 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:38:51,670 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:51,797 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:51,797 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:52,795 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:38:56,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:38:56,091 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:38:56,091 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:38:57,123 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:00,918 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:01,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:01,028 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:02,136 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:39:05,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:05,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:05,228 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:06,367 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:39:09,471 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:09,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:09,581 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:10,592 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:39:13,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:13,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:13,756 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:14,532 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:17,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:17,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:17,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:18,567 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:39:21,800 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:21,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:21,862 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:23,019 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:39:25,933 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:26,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:26,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:27,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:30,114 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:30,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:30,239 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:31,250 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:34,528 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:34,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:34,575 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:35,721 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:39:38,855 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:38,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:38,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:40,054 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:39:43,866 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:44,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:44,006 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:45,001 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:39:47,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:48,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:48,029 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:49,179 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:39:52,273 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:52,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:52,399 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:53,510 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:39:56,582 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:39:56,660 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:39:56,660 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:39:57,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:00,775 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:00,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:00,869 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:01,976 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:40:04,688 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:04,767 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:04,767 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:05,888 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:40:09,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:09,205 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:09,205 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:10,327 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:40:13,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:13,573 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:13,573 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:14,567 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:40:17,491 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:17,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:17,634 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:18,677 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:21,618 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:21,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:21,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:22,861 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:40:26,576 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:26,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:26,686 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:27,772 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:30,641 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:30,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:30,734 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:31,719 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:40:34,680 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:34,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:34,743 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:35,913 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:39,074 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:39,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:39,151 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:40,240 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:40:43,317 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:43,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:43,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:44,599 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:40:47,750 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:47,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:47,829 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:48,937 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:40:52,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:52,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:52,168 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:53,139 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:40:56,196 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:40:56,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:40:56,274 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:40:57,443 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:41:00,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:00,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:00,511 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:01,557 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:41:04,546 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:04,625 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:04,625 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:05,620 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:41:09,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:09,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:09,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:10,282 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:13,265 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:13,345 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:13,345 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:14,278 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:41:17,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:17,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:17,639 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:18,687 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:41:21,784 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:21,923 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:21,923 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:23,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:41:26,066 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:26,190 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:26,190 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:27,299 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:41:30,480 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:30,606 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:30,606 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:31,702 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:34,794 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:34,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:34,856 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:35,981 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:39,048 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:39,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:39,126 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:40,234 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:41:43,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:43,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:43,322 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:44,336 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:47,076 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:47,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:47,186 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:48,290 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:41:51,797 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:51,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:51,907 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:53,014 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:41:55,986 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:41:56,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:41:56,127 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:41:57,247 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:00,243 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:00,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:00,351 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:01,447 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:04,335 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:04,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:04,413 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:05,477 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:42:08,522 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:08,599 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:08,599 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:09,630 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:42:12,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:12,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:12,900 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:13,977 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:42:17,061 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:17,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:17,152 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:18,197 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:21,223 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:21,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:21,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:22,410 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:42:25,337 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:25,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:25,477 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:26,566 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:42:29,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:29,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:29,736 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:30,830 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:34,485 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:34,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:34,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:35,546 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:42:37,695 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:37,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:37,805 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:38,882 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:42,026 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:42,166 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:42,166 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:43,212 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:42:46,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:46,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:46,448 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:47,568 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:42:50,715 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:50,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:50,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:51,985 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:42:55,105 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:55,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:55,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:42:56,271 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:42:59,342 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:42:59,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:42:59,389 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:00,296 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:43:03,408 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:03,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:03,503 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:04,561 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:43:07,227 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:07,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:07,321 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:08,349 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:11,417 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:11,496 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:11,496 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:12,429 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:16,135 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:16,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:16,241 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:17,272 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:43:20,249 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:20,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:20,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:21,335 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-05 17:43:24,431 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:24,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:24,555 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:25,650 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:43:28,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:28,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:28,656 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:29,606 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:32,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:32,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:32,738 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:33,839 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:36,887 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:36,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:36,997 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:38,057 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-05 17:43:41,153 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:41,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:41,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:42,293 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:45,349 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:45,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:45,443 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:46,472 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:49,607 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:49,745 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:49,745 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:50,858 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:43:54,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:54,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:54,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:55,292 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-05 17:43:58,859 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:43:58,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:43:58,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:43:59,970 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-05 17:44:02,871 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:02,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:02,981 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:04,048 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:44:06,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:07,093 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:07,093 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:08,247 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-05 17:44:11,403 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:11,449 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:11,449 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:12,417 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-05 17:44:15,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:15,472 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:15,472 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:16,608 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-05 17:44:19,676 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:19,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:19,786 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:20,848 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-05 17:44:23,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-05 17:44:24,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-05 17:44:24,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-05 17:44:25,117 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:52:29,739 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-09 00:52:38,577 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:38,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:38,701 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:39,687 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:52:42,608 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:42,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:42,609 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:43,452 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:52:45,614 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:45,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:45,769 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:46,240 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:52:48,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:48,593 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:48,593 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:49,041 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:52:51,362 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:51,486 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:51,486 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:51,937 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:52:54,331 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:54,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:54,392 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:55,483 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:52:58,007 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:52:58,146 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:52:58,146 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:52:59,084 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:01,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:01,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:01,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:02,419 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:05,291 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:05,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:05,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:06,133 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:08,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:08,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:08,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:09,712 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:12,113 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:12,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:12,222 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:13,030 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:53:15,505 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:15,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:15,613 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:16,440 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:19,220 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:19,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:19,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:20,021 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:22,483 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:22,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:22,513 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:23,232 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:53:25,213 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:25,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:25,324 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:26,257 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:28,677 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:28,800 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:28,800 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:29,376 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:31,970 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:32,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:32,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:32,921 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:35,385 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:35,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:35,478 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:36,261 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:39,043 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:39,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:39,169 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:40,181 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:42,858 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:42,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:42,953 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:43,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:46,172 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:46,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:46,250 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:47,124 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:53:50,062 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:50,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:50,092 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:50,649 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:53:52,325 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:52,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:52,371 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:52,808 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:53:54,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:54,708 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:54,708 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:55,191 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:57,261 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:57,277 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:57,278 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:57,650 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:53:59,297 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:53:59,344 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:53:59,344 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:53:59,748 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:54:02,175 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:02,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:02,221 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:03,198 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:04,919 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:04,949 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:04,949 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:05,261 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:06,966 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:06,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:06,982 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:07,513 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:54:09,414 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:09,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:09,459 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:10,201 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:54:12,510 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:12,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:12,635 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:13,053 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:15,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:15,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:15,206 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:15,856 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:17,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:17,508 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:17,509 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:17,897 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:20,267 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:20,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:20,328 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:20,779 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:54:23,049 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:23,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:23,111 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:23,871 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:25,853 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:25,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:25,929 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:26,676 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:28,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:28,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:28,857 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:29,790 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:32,748 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:32,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:32,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:33,671 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:54:36,305 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:36,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:36,415 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:37,007 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:54:39,920 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:40,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:40,015 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:40,953 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:43,899 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:43,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:43,962 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:44,757 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:47,754 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:47,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:47,879 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:48,717 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:54:51,525 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:51,602 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:51,602 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:52,536 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:54:55,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:55,095 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:55,095 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:56,088 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:54:58,761 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:54:58,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:54:58,839 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:54:59,244 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-09 00:55:01,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:01,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:01,926 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:02,487 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:05,012 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:05,059 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:05,059 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:05,760 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:55:08,341 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:08,435 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:08,435 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:09,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:55:11,802 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:11,818 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:11,819 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:12,336 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:55:14,268 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:14,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:14,347 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:15,316 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:55:17,863 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:17,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:17,988 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:18,872 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-09 00:55:21,961 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:22,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:22,070 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:22,476 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:55:25,233 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:25,342 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:25,342 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:26,231 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:55:28,944 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:28,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:28,991 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:29,816 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:32,811 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:32,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:32,872 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:33,436 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:55:35,939 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:36,064 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:36,064 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:36,983 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:55:39,490 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:39,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:39,631 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:40,691 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:55:42,987 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:43,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:43,065 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:44,140 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:46,595 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:46,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:46,673 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:47,080 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:55:49,632 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:49,758 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:49,758 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:50,515 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 00:55:52,795 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:52,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:52,875 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:53,766 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 2 selected.
2024-06-09 00:55:56,461 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:55:56,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:55:56,554 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:55:57,456 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:00,120 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:00,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:00,246 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:00,637 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:56:03,031 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:03,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:03,125 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:04,122 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 0 selected.
2024-06-09 00:56:06,583 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:06,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:06,693 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:07,474 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:09,338 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:09,447 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:09,447 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:10,292 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:12,234 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:12,282 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:12,282 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:13,060 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:16,033 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:16,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:16,080 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:17,115 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 3 selected.
2024-06-09 00:56:19,499 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:19,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:19,516 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:20,037 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:56:21,902 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:21,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:21,979 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:22,633 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:25,223 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:25,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:25,301 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:25,974 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:28,825 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:28,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:28,951 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:29,809 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:56:32,057 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:32,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:32,096 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:32,994 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:35,426 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:35,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:35,488 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:36,305 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:38,629 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:38,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:38,721 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:39,689 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 2 selected.
2024-06-09 00:56:41,732 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:41,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:41,824 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:42,790 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:45,340 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:45,357 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:45,358 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:46,262 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 1 selected.
2024-06-09 00:56:48,381 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:48,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:48,475 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:49,005 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 1 selected.
2024-06-09 00:56:50,995 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:51,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:51,027 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:51,386 - Drone.source.models.ppo.ppo_agent - INFO - Exploitation: Action 0 selected.
2024-06-09 00:56:53,530 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 00:56:53,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 00:56:53,591 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 00:56:54,385 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 3 selected.
2024-06-09 01:03:15,133 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-09 01:03:24,245 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 01:03:24,339 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 01:03:24,339 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 01:03:25,307 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 14 selected.
2024-06-09 01:10:49,985 - Drone.source.models.ppo.ppo_agent - INFO - Using device: cpu
2024-06-09 01:10:58,285 - Drone.source.models.ppo.ppo_agent - DEBUG - Original state shape: (15,)
2024-06-09 01:10:58,393 - Drone.source.models.ppo.ppo_agent - DEBUG - Corrected state tensor shape: torch.Size([1, 15])
2024-06-09 01:10:58,393 - Drone.source.models.ppo.ppo_agent - DEBUG - Visual tensor shape: torch.Size([1, 3, 144, 256])
2024-06-09 01:10:59,444 - Drone.source.models.ppo.ppo_agent - INFO - Exploration: Random action 13 selected.
2024-07-20 15:38:16,248 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-20 15:38:16,251 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 15,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_env_steps": 1000,
    "state_dim": 15,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.5,
    "smoothness_penalty": 0.5,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 5
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 3,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 15,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 18,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:31:15,687 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:31:15,706 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:31:15,706 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:31:16,742 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:31:16,742 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:31:22,242 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:31:22,304 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:31:23,540 - AirSimEnvLogger - ERROR - Error loading checkpoint: 'policy_state_dict'
2024-07-21 06:31:25,316 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:31:29,083 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:31:29,675 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:31:35,887 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:31:35,919 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 06:34:42,958 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:34:42,975 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:34:42,975 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:34:43,966 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:34:43,967 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:34:49,846 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:34:49,907 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:34:51,162 - AirSimEnvLogger - ERROR - Error loading checkpoint: 'policy_state_dict'
2024-07-21 06:34:52,487 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:34:56,834 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:34:57,481 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:35:03,906 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:35:04,000 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 06:50:56,343 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:50:56,363 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:50:56,363 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:50:57,559 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:50:57,560 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:51:03,922 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:51:03,951 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:51:05,319 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 06:51:07,430 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:51:11,742 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:51:12,275 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:51:19,043 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:51:19,105 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 06:54:37,293 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 06:54:37,320 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 06:54:37,320 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 06:54:38,757 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 06:54:38,758 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 06:55:04,232 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 06:55:04,322 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 06:55:05,745 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 06:55:07,496 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 06:55:12,282 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 06:55:12,860 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 06:55:19,110 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 06:55:19,172 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:03:23,586 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:03:23,607 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:03:23,608 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:03:24,774 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:03:24,774 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:03:31,289 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:03:31,352 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:03:33,041 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:03:35,281 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:03:40,609 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:03:41,217 - AirSimEnvLogger - ERROR - Error in training loop: float() argument must be a string or a number, not 'dict'
2024-07-21 07:03:41,217 - AirSimEnvLogger - ERROR - An error occurred: local variable 'observation' referenced before assignment
2024-07-21 07:03:47,683 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:03:47,778 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:07:50,211 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:07:50,232 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:07:50,233 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:07:51,402 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:07:51,403 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:07:58,207 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:07:58,302 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:07:59,831 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:08:01,840 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:08:06,771 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:08:07,352 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:08:07,352 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:08:07,352 - AirSimEnvLogger - ERROR - Error in training loop: float() argument must be a string or a number, not 'dict'
2024-07-21 07:08:07,352 - AirSimEnvLogger - ERROR - Current epoch: 1
2024-07-21 07:08:07,353 - AirSimEnvLogger - ERROR - An error occurred: local variable 'episode_steps' referenced before assignment
2024-07-21 07:08:14,106 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:08:14,169 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:10:27,692 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:10:27,710 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:10:27,711 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:10:28,741 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:10:28,742 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:10:35,298 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:10:35,392 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:10:36,948 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:10:39,363 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:10:44,614 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:10:45,255 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:10:45,255 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:10:45,255 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:10:51,454 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:10:51,517 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:21:12,654 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:21:12,689 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:21:12,689 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:21:13,790 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:21:13,790 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:21:19,878 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:21:19,957 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:21:21,435 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:21:23,888 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:21:28,274 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:21:28,918 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:21:28,918 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:21:28,918 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:21:35,195 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:21:35,259 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:28:09,103 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:28:09,121 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:28:09,122 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:28:10,422 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:28:10,423 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:28:17,156 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:28:17,259 - AirSimEnvLogger - INFO - Loading checkpoint from e:\Project\Drone\models/checkpoints\ppo_agent_checkpoint.pt
2024-07-21 07:28:18,947 - AirSimEnvLogger - INFO - Model loaded successfully
2024-07-21 07:28:21,436 - AirSimEnvLogger - INFO - Resuming training from epoch 1
2024-07-21 07:28:25,943 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:28:26,474 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:28:26,474 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:28:26,475 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:28:32,895 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:28:33,022 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:32:03,725 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:32:03,743 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:32:03,744 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:32:04,843 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:32:04,844 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:32:11,243 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:32:11,244 - AirSimEnvLogger - ERROR - An error occurred: train_agents() takes 4 positional arguments but 5 were given
2024-07-21 07:32:12,043 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:32:12,152 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:37:34,633 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:37:34,650 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:37:34,650 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:37:35,830 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:37:35,832 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:37:42,131 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:37:47,319 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:37:47,902 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:37:47,902 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:37:47,903 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:37:53,603 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:37:53,667 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:40:37,945 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:40:37,967 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:40:37,967 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:40:39,150 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:40:39,152 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:40:45,517 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:40:50,175 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:40:50,740 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:40:50,740 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:40:50,741 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:40:56,388 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:40:56,482 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:45:41,291 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:45:41,310 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:45:41,311 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:45:42,522 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:45:42,524 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:45:49,137 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:45:54,620 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 07:45:55,234 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 07:45:55,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 07:45:55,234 - AirSimEnvLogger - ERROR - An error occurred: float() argument must be a string or a number, not 'dict'
2024-07-21 07:46:00,896 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:46:00,958 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 07:49:26,101 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 07:49:26,122 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 07:49:26,123 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 07:49:27,149 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 07:49:27,151 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 07:49:34,092 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 07:49:34,092 - AirSimEnvLogger - ERROR - Error in reset function: 'AirSimEnv' object has no attribute '_reset_env'
2024-07-21 07:49:34,092 - AirSimEnvLogger - ERROR - An error occurred: 'AirSimEnv' object has no attribute '_reset_env'
2024-07-21 07:49:34,921 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 07:49:34,983 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 12:08:47,964 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 12:08:47,981 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 12:08:47,982 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 12:08:49,466 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 12:08:49,467 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 12:08:54,925 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 12:09:00,593 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 12:09:01,033 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 12:09:01,033 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 12:09:01,096 - AirSimEnvLogger - ERROR - Error during training: 'training'
2024-07-21 12:09:01,096 - AirSimEnvLogger - ERROR - An error occurred in main: 'training'
2024-07-21 12:09:06,772 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 12:09:06,868 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 19:18:38,383 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:18:38,399 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:18:38,400 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:18:39,887 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:18:39,888 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:18:46,083 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:18:50,956 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:18:51,522 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:18:51,522 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:18:53,156 - AirSimEnvLogger - ERROR - Error during training: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-21 19:18:53,156 - AirSimEnvLogger - ERROR - An error occurred in main: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 4
2024-07-21 19:18:58,848 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:18:58,879 - AirSimEnvLogger - INFO - Training completed and models saved.
2024-07-21 19:30:14,959 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:30:14,977 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:30:14,977 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:30:16,447 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:30:16,447 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:30:23,189 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:30:28,312 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:30:28,938 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:30:28,938 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:30:30,145 - AirSimEnvLogger - ERROR - Error during training: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 3, 144] to have 3 channels, but got 256 channels instead
2024-07-21 19:30:30,145 - AirSimEnvLogger - ERROR - An error occurred in main: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 3, 144] to have 3 channels, but got 256 channels instead
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 207, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 93, in forward
    visual_features = self.cnn(visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 35, in forward
    x = self.initial_relu(self.initial_conv(x))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 256, 3, 144] to have 3 channels, but got 256 channels instead
2024-07-21 19:30:36,000 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:30:36,095 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:33:49,411 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:33:49,427 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:33:49,428 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:33:50,950 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:33:50,952 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:33:57,254 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:34:02,107 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:34:02,704 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:34:02,704 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:34:02,847 - AirSimEnvLogger - INFO - Visual tensor shape after permute: torch.Size([1, 256, 3, 144])
2024-07-21 19:34:04,027 - AirSimEnvLogger - ERROR - Error during training: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:34:04,027 - AirSimEnvLogger - ERROR - An error occurred in main: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 328, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 211, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 93, in forward
    visual_features = self.cnn(visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 36, in forward
    x = self.initial_relu(self.initial_conv(x))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:34:09,549 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:34:09,675 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:36:28,801 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:36:28,817 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:36:28,818 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:36:30,189 - AirSimEnvLogger - ERROR - Error initializing policy network: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:36:30,190 - AirSimEnvLogger - ERROR - An error occurred in main: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 107, in main
    env = AirSimEnv(
  File "e:\Project\Drone\source\envs\airsim_env.py", line 133, in __init__
    self._initialize_policy_network(config)
  File "e:\Project\Drone\source\envs\airsim_env.py", line 200, in _initialize_policy_network
    self.policy_network = AdvancedPolicyNetwork(
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 60, in __init__
    cnn_out = self.cnn(dummy_input)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 31, in forward
    x = self.initial_relu(self.initial_conv(x))
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[1, 144, 256, 3] to have 3 channels, but got 144 channels instead
2024-07-21 19:36:30,200 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:41:49,192 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:41:49,207 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:41:49,207 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:41:50,797 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:41:50,799 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:41:56,361 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:42:00,928 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:42:01,553 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:42:01,553 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:42:01,649 - AirSimEnvLogger - INFO - Visual tensor shape after permute: torch.Size([1, 3, 144, 256])
2024-07-21 19:42:03,034 - AirSimEnvLogger - ERROR - Error during training: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:42:03,034 - AirSimEnvLogger - ERROR - An error occurred in main: Tensors must have same number of dimensions: got 3 and 2
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 328, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 211, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 96, in forward
    combined = torch.cat([state_features, visual_features], dim=1)
RuntimeError: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:42:08,614 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:42:08,645 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:44:53,807 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:44:53,822 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:44:53,823 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ]
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:44:55,443 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:44:55,445 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:45:01,317 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:45:06,042 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:45:06,576 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:45:06,576 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:45:06,686 - AirSimEnvLogger - INFO - Visual tensor shape after permute: torch.Size([1, 3, 144, 256])
2024-07-21 19:45:08,095 - AirSimEnvLogger - ERROR - Error during training: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:45:08,095 - AirSimEnvLogger - ERROR - An error occurred in main: Tensors must have same number of dimensions: got 3 and 2
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 328, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 211, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 186, in forward
    combined = torch.cat([state_features, visual_features], dim=1)
RuntimeError: Tensors must have same number of dimensions: got 3 and 2
2024-07-21 19:45:13,809 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:45:13,934 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:56:07,460 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:56:07,476 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:56:07,477 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:56:09,053 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:56:09,053 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:56:15,398 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:56:20,265 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:56:20,894 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:56:20,894 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:56:22,494 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:56:22,494 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 115, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:56:28,190 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:56:28,253 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 19:57:58,227 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 19:57:58,248 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 19:57:58,248 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 19:57:59,875 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 19:57:59,876 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 19:58:06,430 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 19:58:11,889 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 19:58:12,487 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 19:58:12,487 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 19:58:14,479 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:58:14,479 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 136, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 19:58:20,514 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 19:58:20,561 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:00:07,506 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:00:07,523 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:00:07,523 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:00:09,128 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:00:09,128 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:00:15,127 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:00:20,471 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:00:21,110 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:00:21,110 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:00:22,995 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:00:22,995 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 144, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:00:28,668 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 20:00:28,716 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:01:58,690 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:01:58,706 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:01:58,707 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:02:00,352 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:02:00,353 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:02:07,365 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:02:12,764 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:02:13,362 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:02:13,362 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:02:15,272 - AirSimEnvLogger - ERROR - Error during training: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:02:15,272 - AirSimEnvLogger - ERROR - An error occurred in main: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 132, in main
    episode_rewards, episode_lengths = train_agents(ppo_agent, env, config, logger)
  File "e:\Project\Drone\train.py", line 77, in train_agents
    episode_rewards, episode_lengths = ppo_agent.train(env, total_timesteps, ppo_save_path)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 324, in train
    action, log_prob = self.select_action(observation)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 204, in select_action
    mean, std = self.policy_network(state, visual)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "e:\Project\Drone\source\models\nn\policy_network.py", line 136, in forward
    x = layer(x)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\HP\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x4096 and 256x256)
2024-07-21 20:02:20,873 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 20:02:20,951 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:06:04,162 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:06:04,178 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:06:04,178 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -1000,
      "x_max": 1000,
      "y_min": -1000,
      "y_max": 1000,
      "z_min": -100,
      "z_max": 100
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:06:05,718 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:06:05,719 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:06:12,628 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:06:17,880 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:06:18,460 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:18,460 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:23,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -8.537314387274387, Velocity: -0.7330309600353913, Collision: 0, Height: -1.0, Movement: -0.052879256010055546, Smoothness: -0.0, Curiosity: 2.823000192642212, Exploration: 0, Total: -7.997617696571982
2024-07-21 20:06:23,792 - AirSimEnvLogger - INFO - Action: [-0.26439628 -0.26439628 -0.26439628 -0.26439628], Velocity: (-0.2643962800502777, -0.2643962800502777, -0.2643962800502777), Duration: 0.5, Reward: -7.997617696571982, Done: False
2024-07-21 20:06:23,896 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:23,896 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:27,318 - AirSimEnvLogger - INFO - Predictive model loss: 0.1573164314031601
2024-07-21 20:06:32,535 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.138746752489784, Velocity: -0.4632007639958223, Collision: 0, Height: -1.0, Movement: -0.019336587190628054, Smoothness: -0.0, Curiosity: 0.9239609837532043, Exploration: 0.18498252261020992, Total: -9.373303796808113
2024-07-21 20:06:32,661 - AirSimEnvLogger - INFO - Action: [0.09668294 0.09668294 0.09668294 0.09668294], Velocity: (0.09668293595314026, 0.09668293595314026, 0.09668293595314026), Duration: 0.5, Reward: -9.373303796808113, Done: False
2024-07-21 20:06:32,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:32,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:35,558 - AirSimEnvLogger - INFO - Predictive model loss: 0.047452572733163834
2024-07-21 20:06:40,362 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.263902943094497, Velocity: -0.13462194965032698, Collision: 0, Height: -1.0, Movement: -0.08582771122455597, Smoothness: -0.0, Curiosity: 1.2977185249328613, Exploration: 0.10438579047764592, Total: -9.170060268480155
2024-07-21 20:06:40,487 - AirSimEnvLogger - INFO - Action: [0.42913856 0.42913856 0.42913856 0.42913856], Velocity: (0.42913855612277985, 0.42913855612277985, 0.42913855612277985), Duration: 0.5, Reward: -9.170060268480155, Done: False
2024-07-21 20:06:40,550 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:40,550 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:43,329 - AirSimEnvLogger - INFO - Predictive model loss: 0.024116137996315956
2024-07-21 20:06:48,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.342956418604631, Velocity: -0.18901729958519944, Collision: 0, Height: -1.0, Movement: -0.11907327324151994, Smoothness: -0.0, Curiosity: 1.9479552507400513, Exploration: 0.058389168184958845, Total: -8.963716936714366
2024-07-21 20:06:48,501 - AirSimEnvLogger - INFO - Action: [0.59536637 0.59536637 0.59536637 0.59536637], Velocity: (0.5953663662075996, 0.5953663662075996, 0.5953663662075996), Duration: 0.5, Reward: -8.963716936714366, Done: False
2024-07-21 20:06:48,596 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:48,596 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:51,538 - AirSimEnvLogger - INFO - Predictive model loss: 0.020372124388813972
2024-07-21 20:06:55,921 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.593620887214255, Velocity: -0.08994530635256104, Collision: 0, Height: -1.0, Movement: -0.047808886319398884, Smoothness: -0.0, Curiosity: 0.43681299686431885, Exploration: 0.033507838106208, Total: -9.918266362969074
2024-07-21 20:06:56,046 - AirSimEnvLogger - INFO - Action: [0.23904443 0.23904443 0.23904443 0.23904443], Velocity: (0.2390444315969944, 0.2390444315969944, 0.2390444315969944), Duration: 0.5, Reward: -9.918266362969074, Done: False
2024-07-21 20:06:56,091 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:06:56,091 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:06:59,016 - AirSimEnvLogger - INFO - Predictive model loss: 0.006978273391723633
2024-07-21 20:07:04,028 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.85169826440182, Velocity: -0.018013674955812506, Collision: 0, Height: -1.0, Movement: -0.0063237067312002185, Smoothness: -0.0, Curiosity: 0.04456177353858948, Exploration: 0.05763924644480213, Total: -10.327528736494592
2024-07-21 20:07:04,153 - AirSimEnvLogger - INFO - Action: [-0.03161853 -0.03161853 -0.03161853 -0.03161853], Velocity: (-0.03161853365600109, -0.03161853365600109, -0.03161853365600109), Duration: 0.5, Reward: -10.327528736494592, Done: False
2024-07-21 20:07:04,217 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:04,217 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:07,198 - AirSimEnvLogger - INFO - Predictive model loss: 0.008090468123555183
2024-07-21 20:07:12,180 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.13676828917405, Velocity: -0.12224443177266792, Collision: 0, Height: -1.0, Movement: -0.06689289789646864, Smoothness: -0.0, Curiosity: 0.503887951374054, Exploration: 0.06249215160866182, Total: -10.44013738884127
2024-07-21 20:07:12,275 - AirSimEnvLogger - INFO - Action: [-0.33446449 -0.33446449 -0.33446449 -0.33446449], Velocity: (-0.3344644894823432, -0.3344644894823432, -0.3344644894823432), Duration: 0.5, Reward: -10.44013738884127, Done: False
2024-07-21 20:07:12,322 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:12,322 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:15,105 - AirSimEnvLogger - INFO - Predictive model loss: 0.022139564156532288
2024-07-21 20:07:19,726 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.161139918823583, Velocity: -0.06345651334579028, Collision: 0, Height: -1.0, Movement: -0.03629631316289306, Smoothness: -0.0, Curiosity: 0.19075673818588257, Exploration: 0.033050901826732565, Total: -10.594509257354481
2024-07-21 20:07:19,805 - AirSimEnvLogger - INFO - Action: [-0.18148157 -0.18148157 -0.18148157 -0.18148157], Velocity: (-0.18148156581446528, -0.18148156581446528, -0.18148156581446528), Duration: 0.5, Reward: -10.594509257354481, Done: False
2024-07-21 20:07:19,868 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:19,868 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:22,792 - AirSimEnvLogger - INFO - Predictive model loss: 0.020095307379961014
2024-07-21 20:07:27,763 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.076307145557221, Velocity: -0.007550631577839004, Collision: 0, Height: -1.0, Movement: -0.0028453450184315445, Smoothness: -0.0, Curiosity: 0.03463318198919296, Exploration: 0.015854131799084174, Total: -10.55987957849357
2024-07-21 20:07:27,921 - AirSimEnvLogger - INFO - Action: [-0.01422673 -0.01422673 -0.01422673 -0.01422673], Velocity: (-0.014226725092157722, -0.014226725092157722, -0.014226725092157722), Duration: 0.5, Reward: -10.55987957849357, Done: False
2024-07-21 20:07:28,015 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:28,015 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:30,893 - AirSimEnvLogger - INFO - Predictive model loss: 0.011675531975924969
2024-07-21 20:07:35,891 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.235270210211043, Velocity: -0.11141033527169202, Collision: 0, Height: -1.0, Movement: -0.06721836233045907, Smoothness: -0.0, Curiosity: 0.5587345361709595, Exploration: 0.01319519047892635, Total: -10.515690907898671
2024-07-21 20:07:36,016 - AirSimEnvLogger - INFO - Action: [-0.33609181 -0.33609181 -0.33609181 -0.33609181], Velocity: (-0.3360918116522953, -0.3360918116522953, -0.3360918116522953), Duration: 0.5, Reward: -10.515690907898671, Done: False
2024-07-21 20:07:36,079 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:36,079 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:39,011 - AirSimEnvLogger - INFO - Predictive model loss: 0.020203847438097
2024-07-21 20:07:43,855 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.427661354344618, Velocity: -0.20764629656550257, Collision: 0, Height: -1.0, Movement: -0.10976859879447148, Smoothness: -0.0, Curiosity: 1.521005630493164, Exploration: 0.04910039929700762, Total: -10.272138467400833
2024-07-21 20:07:43,963 - AirSimEnvLogger - INFO - Action: [-0.54884299 -0.54884299 -0.54884299 -0.54884299], Velocity: (-0.5488429939723574, -0.5488429939723574, -0.5488429939723574), Duration: 0.5, Reward: -10.272138467400833, Done: False
2024-07-21 20:07:44,025 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:44,025 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:46,930 - AirSimEnvLogger - INFO - Predictive model loss: 0.035912878811359406
2024-07-21 20:07:51,877 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.110416689522078, Velocity: -0.04088284619712446, Collision: 0, Height: -1.0, Movement: -0.02127511823200621, Smoothness: -0.0, Curiosity: 0.08999551832675934, Exploration: 0.021996356756950948, Total: -10.58358859392907
2024-07-21 20:07:52,018 - AirSimEnvLogger - INFO - Action: [0.10637559 0.10637559 0.10637559 0.10637559], Velocity: (0.10637559116003104, 0.10637559116003104, 0.10637559116003104), Duration: 0.5, Reward: -10.58358859392907, Done: False
2024-07-21 20:07:52,080 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:07:52,080 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:07:55,003 - AirSimEnvLogger - INFO - Predictive model loss: 0.006613536272197962
2024-07-21 20:08:00,046 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.192742839306845, Velocity: -0.07799967031893497, Collision: 0, Height: -1.0, Movement: -0.04679782825696748, Smoothness: -0.0, Curiosity: 0.309664785861969, Exploration: 0.028181478822430963, Total: -10.57595376859654
2024-07-21 20:08:00,205 - AirSimEnvLogger - INFO - Action: [-0.23398914 -0.23398914 -0.23398914 -0.23398914], Velocity: (-0.2339891412848374, -0.2339891412848374, -0.2339891412848374), Duration: 0.5, Reward: -10.57595376859654, Done: False
2024-07-21 20:08:00,299 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:00,299 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:03,205 - AirSimEnvLogger - INFO - Predictive model loss: 0.010729952715337276
2024-07-21 20:08:08,035 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.04895934672994, Velocity: -0.022373053732730415, Collision: 0, Height: -1.0, Movement: -0.010521877616702114, Smoothness: -0.0, Curiosity: 0.03870673477649689, Exploration: 0.007523994124849115, Total: -10.54033989514476
2024-07-21 20:08:08,113 - AirSimEnvLogger - INFO - Action: [0.05260939 0.05260939 0.05260939 0.05260939], Velocity: (0.052609388083510567, 0.052609388083510567, 0.052609388083510567), Duration: 0.5, Reward: -10.54033989514476, Done: False
2024-07-21 20:08:08,159 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:08,159 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:10,812 - AirSimEnvLogger - INFO - Predictive model loss: 0.002762722549960017
2024-07-21 20:08:15,824 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.121515681664402, Velocity: -0.057341753136278005, Collision: 0, Height: -1.0, Movement: -0.03251039047536324, Smoothness: -0.0, Curiosity: 0.16320854425430298, Exploration: 0.007525199103813907, Total: -10.570328285332161
2024-07-21 20:08:15,840 - AirSimEnvLogger - INFO - Action: [-0.16255195 -0.16255195 -0.16255195 -0.16255195], Velocity: (-0.1625519523768162, -0.1625519523768162, -0.1625519523768162), Duration: 0.5, Reward: -10.570328285332161, Done: False
2024-07-21 20:08:15,903 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:15,903 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:18,783 - AirSimEnvLogger - INFO - Predictive model loss: 0.005474381148815155
2024-07-21 20:08:23,932 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.137703405730269, Velocity: -0.04624782810507101, Collision: 0, Height: -1.0, Movement: -0.028796855904147378, Smoothness: -0.0, Curiosity: 0.14771482348442078, Exploration: 0.014900672771422303, Total: -10.586869459076723
2024-07-21 20:08:24,121 - AirSimEnvLogger - INFO - Action: [-0.14398428 -0.14398428 -0.14398428 -0.14398428], Velocity: (-0.1439842795207369, -0.1439842795207369, -0.1439842795207369), Duration: 0.5, Reward: -10.586869459076723, Done: False
2024-07-21 20:08:24,183 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:24,183 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:27,185 - AirSimEnvLogger - INFO - Predictive model loss: 0.005294164642691612
2024-07-21 20:08:32,094 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.322962974252313, Velocity: -0.14574575869552195, Collision: 0, Height: -1.0, Movement: -0.08745696758105624, Smoothness: -0.0, Curiosity: 1.003562569618225, Exploration: 0.02714718069176819, Total: -10.397370829410713
2024-07-21 20:08:32,219 - AirSimEnvLogger - INFO - Action: [-0.43728484 -0.43728484 -0.43728484 -0.43728484], Velocity: (-0.43728483790528117, -0.43728483790528117, -0.43728483790528117), Duration: 0.5, Reward: -10.397370829410713, Done: False
2024-07-21 20:08:32,345 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:32,345 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:35,289 - AirSimEnvLogger - INFO - Predictive model loss: 0.017199214547872543
2024-07-21 20:08:40,340 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.03519470477193, Velocity: -0.05305891114288633, Collision: 0, Height: -1.0, Movement: -0.03243093383871383, Smoothness: -0.0, Curiosity: 0.13918259739875793, Exploration: 0.01590590192586549, Total: -10.492194774642693
2024-07-21 20:08:40,479 - AirSimEnvLogger - INFO - Action: [0.16215467 0.16215467 0.16215467 0.16215467], Velocity: (0.16215466919356913, 0.16215466919356913, 0.16215466919356913), Duration: 0.5, Reward: -10.492194774642693, Done: False
2024-07-21 20:08:40,541 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:40,541 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:43,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.0011661252938210964
2024-07-21 20:08:48,653 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.17387330122136, Velocity: -0.10402691856872726, Collision: 0, Height: -1.0, Movement: -0.05047677212673989, Smoothness: -0.0, Curiosity: 0.3742421269416809, Exploration: 0.01641164932399651, Total: -10.540531044382758
2024-07-21 20:08:48,810 - AirSimEnvLogger - INFO - Action: [-0.25238386 -0.25238386 -0.25238386 -0.25238386], Velocity: (-0.25238386063369944, -0.25238386063369944, -0.25238386063369944), Duration: 0.5, Reward: -10.540531044382758, Done: False
2024-07-21 20:08:48,903 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:48,903 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:08:51,879 - AirSimEnvLogger - INFO - Predictive model loss: 0.007580197881907225
2024-07-21 20:08:56,829 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.936615692049871, Velocity: -0.07982571116182884, Collision: 0, Height: -1.0, Movement: -0.045333888859954645, Smoothness: -0.0, Curiosity: 0.23741808533668518, Exploration: 0.010344801492369032, Total: -10.360283933549965
2024-07-21 20:08:56,955 - AirSimEnvLogger - INFO - Action: [0.22666944 0.22666944 0.22666944 0.22666944], Velocity: (0.2266694442997732, 0.2266694442997732, 0.2266694442997732), Duration: 0.5, Reward: -10.360283933549965, Done: False
2024-07-21 20:08:57,003 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:08:57,003 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:00,027 - AirSimEnvLogger - INFO - Predictive model loss: 0.00047663264558650553
2024-07-21 20:09:05,059 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.75693772518964, Velocity: -0.1152243483962964, Collision: 0, Height: -1.0, Movement: -0.07558094398747245, Smoothness: -0.0, Curiosity: 0.6528724431991577, Exploration: 0.05430436177575783, Total: -9.984810899831807
2024-07-21 20:09:05,202 - AirSimEnvLogger - INFO - Action: [0.37790472 0.37790472 0.37790472 0.37790472], Velocity: (0.3779047199373622, 0.3779047199373622, 0.3779047199373622), Duration: 0.5, Reward: -9.984810899831807, Done: False
2024-07-21 20:09:05,264 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:05,264 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:08,080 - AirSimEnvLogger - INFO - Predictive model loss: 0.0035763317719101906
2024-07-21 20:09:12,371 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.870647094891332, Velocity: -0.018541081063435924, Collision: 0, Height: -1.0, Movement: -0.012108386241249037, Smoothness: -0.0, Curiosity: 0.0430259145796299, Exploration: 0.009408474699336875, Total: -10.357733821817494
2024-07-21 20:09:12,495 - AirSimEnvLogger - INFO - Action: [0.06054193 0.06054193 0.06054193 0.06054193], Velocity: (0.06054193120624518, 0.06054193120624518, 0.06054193120624518), Duration: 0.5, Reward: -10.357733821817494, Done: False
2024-07-21 20:09:12,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:12,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:15,471 - AirSimEnvLogger - INFO - Predictive model loss: 0.0008268192177638412
2024-07-21 20:09:20,326 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.706846201996552, Velocity: -0.12639366523603007, Collision: 0, Height: -1.0, Movement: -0.08221361074986647, Smoothness: -0.0, Curiosity: 0.7955585718154907, Exploration: 0.009018043110380595, Total: -9.878681501159733
2024-07-21 20:09:20,436 - AirSimEnvLogger - INFO - Action: [0.41106805 0.41106805 0.41106805 0.41106805], Velocity: (0.4110680537493323, 0.4110680537493323, 0.4110680537493323), Duration: 0.5, Reward: -9.878681501159733, Done: False
2024-07-21 20:09:20,500 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:20,500 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:23,478 - AirSimEnvLogger - INFO - Predictive model loss: 0.006337056402117014
2024-07-21 20:09:28,605 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.812811876067942, Velocity: -0.037371853016888515, Collision: 0, Height: -1.0, Movement: -0.024520853331399906, Smoothness: -0.0, Curiosity: 0.10540065914392471, Exploration: 0.010494075428823826, Total: -10.2791507432518
2024-07-21 20:09:28,714 - AirSimEnvLogger - INFO - Action: [0.12260427 0.12260427 0.12260427 0.12260427], Velocity: (0.12260426665699953, 0.12260426665699953, 0.12260426665699953), Duration: 0.5, Reward: -10.2791507432518, Done: False
2024-07-21 20:09:28,808 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:28,808 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:31,673 - AirSimEnvLogger - INFO - Predictive model loss: 0.002043942455202341
2024-07-21 20:09:36,313 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.820585561846329, Velocity: -0.05688574063079837, Collision: 0, Height: -1.0, Movement: -0.03559161135093021, Smoothness: -0.0, Curiosity: 0.17695245146751404, Exploration: 0.013938800446397821, Total: -10.261323607473784
2024-07-21 20:09:36,437 - AirSimEnvLogger - INFO - Action: [0.17795806 0.17795806 0.17795806 0.17795806], Velocity: (0.17795805675465104, 0.17795805675465104, 0.17795805675465104), Duration: 0.5, Reward: -10.261323607473784, Done: False
2024-07-21 20:09:36,530 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:36,530 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:39,549 - AirSimEnvLogger - INFO - Predictive model loss: 0.002339982194826007
2024-07-21 20:09:44,609 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.654983770096472, Velocity: -0.13697055018309698, Collision: 0, Height: -1.0, Movement: -0.09395522330470706, Smoothness: -0.0, Curiosity: 1.0762226581573486, Exploration: 0.026359410192032157, Total: -9.689481356401409
2024-07-21 20:09:44,718 - AirSimEnvLogger - INFO - Action: [0.46977612 0.46977612 0.46977612 0.46977612], Velocity: (0.46977611652353524, 0.46977611652353524, 0.46977611652353524), Duration: 0.5, Reward: -9.689481356401409, Done: False
2024-07-21 20:09:44,779 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:44,779 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:47,761 - AirSimEnvLogger - INFO - Predictive model loss: 0.010871022939682007
2024-07-21 20:09:52,363 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.87125544117941, Velocity: -0.012601288137176552, Collision: 0, Height: -1.0, Movement: -0.0024559385862557904, Smoothness: -0.0, Curiosity: 0.03256514295935631, Exploration: 0.008419612672315184, Total: -10.359835185092482
2024-07-21 20:09:52,472 - AirSimEnvLogger - INFO - Action: [-0.01227969 -0.01227969 -0.01227969 -0.01227969], Velocity: (-0.012279692931278952, -0.012279692931278952, -0.012279692931278952), Duration: 0.5, Reward: -10.359835185092482, Done: False
2024-07-21 20:09:52,567 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:09:52,567 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:09:55,487 - AirSimEnvLogger - INFO - Predictive model loss: 0.001450380776077509
2024-07-21 20:10:00,690 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.077916448749, Velocity: -0.07401189265829702, Collision: 0, Height: -1.0, Movement: -0.043867013220320585, Smoothness: -0.0, Curiosity: 0.2244262844324112, Exploration: 0.05340637030443451, Total: -10.496414680123088
2024-07-21 20:10:00,815 - AirSimEnvLogger - INFO - Action: [-0.21933507 -0.21933507 -0.21933507 -0.21933507], Velocity: (-0.21933506610160292, -0.21933506610160292, -0.21933506610160292), Duration: 0.5, Reward: -10.496414680123088, Done: False
2024-07-21 20:10:00,924 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:00,924 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:03,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.00197173235937953
2024-07-21 20:10:07,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.325555908262503, Velocity: -0.18497227716760323, Collision: 0, Height: -1.0, Movement: -0.09809292423940225, Smoothness: -0.0, Curiosity: 1.1639407873153687, Exploration: 0.05412164251623195, Total: -10.335056617109313
2024-07-21 20:10:08,100 - AirSimEnvLogger - INFO - Action: [-0.49046462 -0.49046462 -0.49046462 -0.49046462], Velocity: (-0.4904646211970112, -0.4904646211970112, -0.4904646211970112), Duration: 0.5, Reward: -10.335056617109313, Done: False
2024-07-21 20:10:08,162 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:08,163 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:10,447 - AirSimEnvLogger - INFO - Predictive model loss: 0.013175143860280514
2024-07-21 20:10:15,409 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.050836749993959, Velocity: -0.05801876976822896, Collision: 0, Height: -1.0, Movement: -0.027112955509540827, Smoothness: -0.0, Curiosity: 0.10841643810272217, Exploration: 0.01557023015487841, Total: -10.52523516534669
2024-07-21 20:10:15,502 - AirSimEnvLogger - INFO - Action: [0.13556478 0.13556478 0.13556478 0.13556478], Velocity: (0.13556477754770413, 0.13556477754770413, 0.13556477754770413), Duration: 0.5, Reward: -10.52523516534669, Done: False
2024-07-21 20:10:15,581 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:15,581 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:18,504 - AirSimEnvLogger - INFO - Predictive model loss: 0.0012343639973551035
2024-07-21 20:10:22,964 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.051195935506616, Velocity: -0.015405424003432377, Collision: 0, Height: -1.0, Movement: -0.008186325103582248, Smoothness: -0.0, Curiosity: 0.03148689121007919, Exploration: 0.03574698115764957, Total: -10.536824438182121
2024-07-21 20:10:23,041 - AirSimEnvLogger - INFO - Action: [-0.04093163 -0.04093163 -0.04093163 -0.04093163], Velocity: (-0.04093162551791124, -0.04093162551791124, -0.04093162551791124), Duration: 0.5, Reward: -10.536824438182121, Done: False
2024-07-21 20:10:23,119 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:23,119 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:26,023 - AirSimEnvLogger - INFO - Predictive model loss: 0.000905893393792212
2024-07-21 20:10:30,980 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.81209494332104, Velocity: -0.11308257310379843, Collision: 0, Height: -1.0, Movement: -0.07206625507745083, Smoothness: -0.0, Curiosity: 0.6050204634666443, Exploration: 0.026789909028163267, Total: -10.06797464184173
2024-07-21 20:10:31,135 - AirSimEnvLogger - INFO - Action: [0.36033128 0.36033128 0.36033128 0.36033128], Velocity: (0.3603312753872541, 0.3603312753872541, 0.3603312753872541), Duration: 0.5, Reward: -10.06797464184173, Done: False
2024-07-21 20:10:31,197 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:31,197 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:34,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.003806555410847068
2024-07-21 20:10:39,235 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.603098258845964, Velocity: -0.16834931568663722, Collision: 0, Height: -1.0, Movement: -0.11219254516796735, Smoothness: -0.0, Curiosity: 1.533277153968811, Exploration: 0.057143269449527856, Total: -9.420424940331769
2024-07-21 20:10:39,377 - AirSimEnvLogger - INFO - Action: [0.56096273 0.56096273 0.56096273 0.56096273], Velocity: (0.5609627258398368, 0.5609627258398368, 0.5609627258398368), Duration: 0.5, Reward: -9.420424940331769, Done: False
2024-07-21 20:10:39,440 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:39,440 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:42,448 - AirSimEnvLogger - INFO - Predictive model loss: 0.014840662479400635
2024-07-21 20:10:47,421 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.876252912961696, Velocity: -0.024749882508368713, Collision: 0, Height: -1.0, Movement: -0.011179066781021808, Smoothness: -0.0, Curiosity: 0.046928297728300095, Exploration: 0.016875258957369476, Total: -10.36290656023836
2024-07-21 20:10:47,545 - AirSimEnvLogger - INFO - Action: [-0.05589533 -0.05589533 -0.05589533 -0.05589533], Velocity: (-0.055895333905109035, -0.055895333905109035, -0.055895333905109035), Duration: 0.5, Reward: -10.36290656023836, Done: False
2024-07-21 20:10:47,607 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:47,607 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:50,604 - AirSimEnvLogger - INFO - Predictive model loss: 0.0012741959653794765
2024-07-21 20:10:55,615 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.742665508643187, Velocity: -0.11054951401511699, Collision: 0, Height: -1.0, Movement: -0.07056988423873105, Smoothness: -0.0, Curiosity: 0.6529353857040405, Exploration: 0.015518176209379031, Total: -9.975425925980721
2024-07-21 20:10:55,740 - AirSimEnvLogger - INFO - Action: [0.35284942 0.35284942 0.35284942 0.35284942], Velocity: (0.3528494211936552, 0.3528494211936552, 0.3528494211936552), Duration: 0.5, Reward: -9.975425925980721, Done: False
2024-07-21 20:10:55,834 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:10:55,834 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:10:58,748 - AirSimEnvLogger - INFO - Predictive model loss: 0.007610654458403587
2024-07-21 20:11:03,528 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.579947796083099, Velocity: -0.1646349071836375, Collision: 0, Height: -1.0, Movement: -0.11056218716407379, Smoothness: -0.0, Curiosity: 1.5818194150924683, Exploration: 0.04625992776621783, Total: -9.373159775291846
2024-07-21 20:11:03,654 - AirSimEnvLogger - INFO - Action: [0.55281094 0.55281094 0.55281094 0.55281094], Velocity: (0.5528109358203689, 0.5528109358203689, 0.5528109358203689), Duration: 0.5, Reward: -9.373159775291846, Done: False
2024-07-21 20:11:03,670 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:03,670 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:06,516 - AirSimEnvLogger - INFO - Predictive model loss: 0.019102804362773895
2024-07-21 20:11:11,430 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.893659588325779, Velocity: -0.040865662350957836, Collision: 0, Height: -1.0, Movement: -0.02087832404720505, Smoothness: -0.0, Curiosity: 0.09950833022594452, Exploration: 0.02337617293020826, Total: -10.361750852206963
2024-07-21 20:11:11,556 - AirSimEnvLogger - INFO - Action: [-0.10439162 -0.10439162 -0.10439162 -0.10439162], Velocity: (-0.10439162023602522, -0.10439162023602522, -0.10439162023602522), Duration: 0.5, Reward: -10.361750852206963, Done: False
2024-07-21 20:11:11,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:11,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:14,571 - AirSimEnvLogger - INFO - Predictive model loss: 0.0022179498337209225
2024-07-21 20:11:19,475 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.814724449187755, Velocity: -0.08470758805561489, Collision: 0, Height: -1.0, Movement: -0.04825679802125832, Smoothness: -0.0, Curiosity: 0.3820393681526184, Exploration: 0.026264885839294386, Total: -10.165631261773521
2024-07-21 20:11:19,479 - AirSimEnvLogger - INFO - Action: [0.24128399 0.24128399 0.24128399 0.24128399], Velocity: (0.2412839901062916, 0.2412839901062916, 0.2412839901062916), Duration: 0.5, Reward: -10.165631261773521, Done: False
2024-07-21 20:11:19,554 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:19,554 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:22,606 - AirSimEnvLogger - INFO - Predictive model loss: 0.006475016474723816
2024-07-21 20:11:27,339 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.738447940911119, Velocity: -0.10156185692976163, Collision: 0, Height: -1.0, Movement: -0.06598722409817646, Smoothness: -0.0, Curiosity: 0.6714843511581421, Exploration: 0.029762302094994517, Total: -9.954132955787747
2024-07-21 20:11:27,464 - AirSimEnvLogger - INFO - Action: [0.32993612 0.32993612 0.32993612 0.32993612], Velocity: (0.3299361204908823, 0.3299361204908823, 0.3299361204908823), Duration: 0.5, Reward: -9.954132955787747, Done: False
2024-07-21 20:11:27,527 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:27,527 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:30,609 - AirSimEnvLogger - INFO - Predictive model loss: 0.010447682812809944
2024-07-21 20:11:35,092 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.585865115491822, Velocity: -0.17252473717466008, Collision: 0, Height: -1.0, Movement: -0.10915302967833017, Smoothness: -0.0, Curiosity: 1.6628444194793701, Exploration: 0.03211217327321446, Total: -9.345198142652656
2024-07-21 20:11:35,171 - AirSimEnvLogger - INFO - Action: [0.54576515 0.54576515 0.54576515 0.54576515], Velocity: (0.5457651483916508, 0.5457651483916508, 0.5457651483916508), Duration: 0.5, Reward: -9.345198142652656, Done: False
2024-07-21 20:11:35,251 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:35,251 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:37,810 - AirSimEnvLogger - INFO - Predictive model loss: 0.02272721566259861
2024-07-21 20:11:42,809 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.899046214893819, Velocity: -0.042202524202264974, Collision: 0, Height: -1.0, Movement: -0.021582902790076866, Smoothness: -0.0, Curiosity: 0.16947093605995178, Exploration: 0.02306291456963665, Total: -10.332957716330055
2024-07-21 20:11:42,918 - AirSimEnvLogger - INFO - Action: [-0.10791451 -0.10791451 -0.10791451 -0.10791451], Velocity: (-0.10791451395038432, -0.10791451395038432, -0.10791451395038432), Duration: 0.5, Reward: -10.332957716330055, Done: False
2024-07-21 20:11:42,981 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:42,981 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:46,012 - AirSimEnvLogger - INFO - Predictive model loss: 0.004509173799306154
2024-07-21 20:11:51,153 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.80524767350589, Velocity: -0.08533424360890268, Collision: 0, Height: -1.0, Movement: -0.05383301907729739, Smoothness: -0.0, Curiosity: 0.5657804012298584, Exploration: 0.0243293672472143, Total: -10.065542023153698
2024-07-21 20:11:51,264 - AirSimEnvLogger - INFO - Action: [0.2691651 0.2691651 0.2691651 0.2691651], Velocity: (0.2691650953864869, 0.2691650953864869, 0.2691650953864869), Duration: 0.5, Reward: -10.065542023153698, Done: False
2024-07-21 20:11:51,328 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:51,328 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:11:54,365 - AirSimEnvLogger - INFO - Predictive model loss: 0.010945504531264305
2024-07-21 20:11:59,531 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.800064169335698, Velocity: -0.06938514377290593, Collision: 0, Height: -1.0, Movement: -0.044412886768339245, Smoothness: -0.0, Curiosity: 0.4855053722858429, Exploration: 0.02265336509812051, Total: -10.091914670736438
2024-07-21 20:11:59,672 - AirSimEnvLogger - INFO - Action: [0.22206443 0.22206443 0.22206443 0.22206443], Velocity: (0.22206443384169622, 0.22206443384169622, 0.22206443384169622), Duration: 0.5, Reward: -10.091914670736438, Done: False
2024-07-21 20:11:59,735 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:11:59,735 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:02,581 - AirSimEnvLogger - INFO - Predictive model loss: 0.01083226315677166
2024-07-21 20:12:07,585 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.062507079533432, Velocity: -0.08388599707999303, Collision: 0, Height: -1.0, Movement: -0.04756557361793609, Smoothness: -0.0, Curiosity: 0.3509995639324188, Exploration: 0.035468251069486055, Total: -10.426613203255116
2024-07-21 20:12:07,644 - AirSimEnvLogger - INFO - Action: [-0.23782787 -0.23782787 -0.23782787 -0.23782787], Velocity: (-0.23782786808968043, -0.23782786808968043, -0.23782786808968043), Duration: 0.5, Reward: -10.426613203255116, Done: False
2024-07-21 20:12:07,692 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:07,692 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:10,669 - AirSimEnvLogger - INFO - Predictive model loss: 0.004855751059949398
2024-07-21 20:12:15,269 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.027585946137888, Velocity: -0.0037514184604571644, Collision: 0, Height: -1.0, Movement: -0.0005100160017009026, Smoothness: -0.0, Curiosity: 0.16903644800186157, Exploration: 0.025936444033818225, Total: -10.439807144160593
2024-07-21 20:12:15,379 - AirSimEnvLogger - INFO - Action: [0.00255008 0.00255008 0.00255008 0.00255008], Velocity: (0.0025500800085045128, 0.0025500800085045128, 0.0025500800085045128), Duration: 0.5, Reward: -10.439807144160593, Done: False
2024-07-21 20:12:15,442 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:15,442 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:18,356 - AirSimEnvLogger - INFO - Predictive model loss: 0.005023459438234568
2024-07-21 20:12:22,669 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.814077763482898, Velocity: -0.11283345588447247, Collision: 0, Height: -1.0, Movement: -0.07154866005849816, Smoothness: -0.0, Curiosity: 0.8584502339363098, Exploration: 0.03993931835385811, Total: -9.940436376792057
2024-07-21 20:12:22,690 - AirSimEnvLogger - INFO - Action: [0.3577433 0.3577433 0.3577433 0.3577433], Velocity: (0.3577433002924908, 0.3577433002924908, 0.3577433002924908), Duration: 0.5, Reward: -9.940436376792057, Done: False
2024-07-21 20:12:22,750 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:22,750 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:25,692 - AirSimEnvLogger - INFO - Predictive model loss: 0.01053526159375906
2024-07-21 20:12:30,860 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.731093623415655, Velocity: -0.10667315268035014, Collision: 0, Height: -1.0, Movement: -0.0711176611278056, Smoothness: -0.0, Curiosity: 0.9425386190414429, Exploration: 0.03781097507471129, Total: -9.812710461332946
2024-07-21 20:12:30,986 - AirSimEnvLogger - INFO - Action: [0.35558831 0.35558831 0.35558831 0.35558831], Velocity: (0.355588305639028, 0.355588305639028, 0.355588305639028), Duration: 0.5, Reward: -9.812710461332946, Done: False
2024-07-21 20:12:31,049 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:31,049 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:33,997 - AirSimEnvLogger - INFO - Predictive model loss: 0.013047143816947937
2024-07-21 20:12:38,896 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.017552819749577, Velocity: -0.09343034264293022, Collision: 0, Height: -1.0, Movement: -0.04060058706533914, Smoothness: -0.0, Curiosity: 0.34993207454681396, Exploration: 0.0328691635448152, Total: -10.386788179795206
2024-07-21 20:12:39,051 - AirSimEnvLogger - INFO - Action: [-0.20300294 -0.20300294 -0.20300294 -0.20300294], Velocity: (-0.20300293532669567, -0.20300293532669567, -0.20300293532669567), Duration: 0.5, Reward: -10.386788179795206, Done: False
2024-07-21 20:12:39,114 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:39,114 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:42,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.004695258568972349
2024-07-21 20:12:47,232 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.29075315536479, Velocity: -0.1922525109765769, Collision: 0, Height: -1.0, Movement: -0.09645971116191152, Smoothness: -0.0, Curiosity: 1.146602749824524, Exploration: 0.07333395545931484, Total: -10.308557215965145
2024-07-21 20:12:47,389 - AirSimEnvLogger - INFO - Action: [-0.48229856 -0.48229856 -0.48229856 -0.48229856], Velocity: (-0.48229855580955755, -0.48229855580955755, -0.48229855580955755), Duration: 0.5, Reward: -10.308557215965145, Done: False
2024-07-21 20:12:47,496 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:47,496 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:50,438 - AirSimEnvLogger - INFO - Predictive model loss: 0.013835412450134754
2024-07-21 20:12:55,345 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.414753749862886, Velocity: -0.1685142992293708, Collision: 0, Height: -1.0, Movement: -0.09982370166364314, Smoothness: -0.0, Curiosity: 1.2768919467926025, Exploration: 0.04660065095751434, Total: -10.361227166056132
2024-07-21 20:12:55,378 - AirSimEnvLogger - INFO - Action: [-0.49911851 -0.49911851 -0.49911851 -0.49911851], Velocity: (-0.4991185083182157, -0.4991185083182157, -0.4991185083182157), Duration: 0.5, Reward: -10.361227166056132, Done: False
2024-07-21 20:12:55,422 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:12:55,422 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:12:58,255 - AirSimEnvLogger - INFO - Predictive model loss: 0.020202213898301125
2024-07-21 20:13:02,950 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.132624181165228, Velocity: -0.031488513688158935, Collision: 0, Height: -1.0, Movement: -0.014760577537700527, Smoothness: -0.0, Curiosity: 0.208810493350029, Exploration: 0.03079660284372764, Total: -10.539279928519319
2024-07-21 20:13:03,123 - AirSimEnvLogger - INFO - Action: [0.07380289 0.07380289 0.07380289 0.07380289], Velocity: (0.07380288768850263, 0.07380288768850263, 0.07380288768850263), Duration: 0.5, Reward: -10.539279928519319, Done: False
2024-07-21 20:13:03,171 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:03,171 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:06,332 - AirSimEnvLogger - INFO - Predictive model loss: 0.003570795990526676
2024-07-21 20:13:11,317 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.230406842709485, Velocity: -0.09408308299567913, Collision: 0, Height: -1.0, Movement: -0.057101011346735064, Smoothness: -0.0, Curiosity: 0.46850448846817017, Exploration: 0.020527454293153603, Total: -10.544800750249282
2024-07-21 20:13:11,505 - AirSimEnvLogger - INFO - Action: [-0.28550506 -0.28550506 -0.28550506 -0.28550506], Velocity: (-0.2855050567336753, -0.2855050567336753, -0.2855050567336753), Duration: 0.5, Reward: -10.544800750249282, Done: False
2024-07-21 20:13:11,568 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:11,568 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:14,386 - AirSimEnvLogger - INFO - Predictive model loss: 0.007740159519016743
2024-07-21 20:13:19,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.407691430566185, Velocity: -0.1888384203430133, Collision: 0, Height: -1.0, Movement: -0.10470992330260948, Smoothness: -0.0, Curiosity: 1.3544540405273438, Exploration: 0.046386839820478275, Total: -10.326077244840185
2024-07-21 20:13:19,633 - AirSimEnvLogger - INFO - Action: [-0.52354962 -0.52354962 -0.52354962 -0.52354962], Velocity: (-0.5235496165130473, -0.5235496165130473, -0.5235496165130473), Duration: 0.5, Reward: -10.326077244840185, Done: False
2024-07-21 20:13:19,694 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:19,694 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:22,357 - AirSimEnvLogger - INFO - Predictive model loss: 0.01973523385822773
2024-07-21 20:13:27,162 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.23907110579186, Velocity: -0.03691320286883909, Collision: 0, Height: -1.0, Movement: -0.023732650297621262, Smoothness: -0.0, Curiosity: 0.1919792890548706, Exploration: 0.006819826776861712, Total: -10.662547362373234
2024-07-21 20:13:27,272 - AirSimEnvLogger - INFO - Action: [-0.11866325 -0.11866325 -0.11866325 -0.11866325], Velocity: (-0.1186632514881063, -0.1186632514881063, -0.1186632514881063), Duration: 0.5, Reward: -10.662547362373234, Done: False
2024-07-21 20:13:27,334 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:27,334 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:30,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.00579258194193244
2024-07-21 20:13:35,097 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.348710978372932, Velocity: -0.1660717384557503, Collision: 0, Height: -1.0, Movement: -0.0828778207390206, Smoothness: -0.0, Curiosity: 0.8812240362167358, Exploration: 0.007216297509041252, Total: -10.497979352064531
2024-07-21 20:13:35,175 - AirSimEnvLogger - INFO - Action: [-0.4143891 -0.4143891 -0.4143891 -0.4143891], Velocity: (-0.41438910369510296, -0.41438910369510296, -0.41438910369510296), Duration: 0.5, Reward: -10.497979352064531, Done: False
2024-07-21 20:13:35,253 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:35,253 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:38,262 - AirSimEnvLogger - INFO - Predictive model loss: 0.01390377152711153
2024-07-21 20:13:43,325 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.252877601997307, Velocity: -0.05812044221228966, Collision: 0, Height: -1.0, Movement: -0.03657860905372584, Smoothness: -0.0, Curiosity: 0.24855385720729828, Exploration: 0.010062892581119287, Total: -10.65930617688895
2024-07-21 20:13:43,451 - AirSimEnvLogger - INFO - Action: [-0.18289305 -0.18289305 -0.18289305 -0.18289305], Velocity: (-0.1828930452686292, -0.1828930452686292, -0.1828930452686292), Duration: 0.5, Reward: -10.65930617688895, Done: False
2024-07-21 20:13:43,528 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:43,528 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:46,408 - AirSimEnvLogger - INFO - Predictive model loss: 0.0060777077451348305
2024-07-21 20:13:51,535 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.938959207588656, Velocity: -0.09370324722385645, Collision: 0, Height: -1.0, Movement: -0.05787011310237903, Smoothness: -0.0, Curiosity: 0.4472541809082031, Exploration: 0.05453490642644248, Total: -10.257063770771433
2024-07-21 20:13:51,709 - AirSimEnvLogger - INFO - Action: [0.28935057 0.28935057 0.28935057 0.28935057], Velocity: (0.28935056551189514, 0.28935056551189514, 0.28935056551189514), Duration: 0.5, Reward: -10.257063770771433, Done: False
2024-07-21 20:13:51,740 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:13:51,740 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:13:54,635 - AirSimEnvLogger - INFO - Predictive model loss: 0.0016257950337603688
2024-07-21 20:13:59,789 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.72592684754172, Velocity: -0.1388269182293812, Collision: 0, Height: -1.0, Movement: -0.08925614319631586, Smoothness: -0.0, Curiosity: 1.048414945602417, Exploration: 0.06355031341717164, Total: -9.767348385491399
2024-07-21 20:13:59,915 - AirSimEnvLogger - INFO - Action: [0.44628072 0.44628072 0.44628072 0.44628072], Velocity: (0.44628071598157926, 0.44628071598157926, 0.44628071598157926), Duration: 0.5, Reward: -9.767348385491399, Done: False
2024-07-21 20:14:00,010 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:00,010 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:02,717 - AirSimEnvLogger - INFO - Predictive model loss: 0.009101694449782372
2024-07-21 20:14:07,608 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.790683068979964, Velocity: -0.054572055242688655, Collision: 0, Height: -1.0, Movement: -0.03594282027278905, Smoothness: -0.0, Curiosity: 0.2921631336212158, Exploration: 0.01624362891888706, Total: -10.172233086034202
2024-07-21 20:14:07,735 - AirSimEnvLogger - INFO - Action: [0.1797141 0.1797141 0.1797141 0.1797141], Velocity: (0.17971410136394522, 0.17971410136394522, 0.17971410136394522), Duration: 0.5, Reward: -10.172233086034202, Done: False
2024-07-21 20:14:07,797 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:07,797 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:10,751 - AirSimEnvLogger - INFO - Predictive model loss: 0.004895500838756561
2024-07-21 20:14:15,695 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.088152489207863, Velocity: -0.09581772513933673, Collision: 0, Height: -1.0, Movement: -0.058188007492847416, Smoothness: -0.0, Curiosity: 0.3976612687110901, Exploration: 0.05005988562706856, Total: -10.433037541045858
2024-07-21 20:14:15,869 - AirSimEnvLogger - INFO - Action: [-0.29094004 -0.29094004 -0.29094004 -0.29094004], Velocity: (-0.2909400374642371, -0.2909400374642371, -0.2909400374642371), Duration: 0.5, Reward: -10.433037541045858, Done: False
2024-07-21 20:14:15,947 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:15,947 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:18,905 - AirSimEnvLogger - INFO - Predictive model loss: 0.0011387659469619393
2024-07-21 20:14:23,985 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.273332841501192, Velocity: -0.18976470361453268, Collision: 0, Height: -1.0, Movement: -0.07937796623459814, Smoothness: -0.0, Curiosity: 0.7878140211105347, Exploration: 0.059312111510038894, Total: -10.470383557074644
2024-07-21 20:14:24,111 - AirSimEnvLogger - INFO - Action: [-0.39688983 -0.39688983 -0.39688983 -0.39688983], Velocity: (-0.3968898311729907, -0.3968898311729907, -0.3968898311729907), Duration: 0.5, Reward: -10.470383557074644, Done: False
2024-07-21 20:14:24,190 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:24,190 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:26,985 - AirSimEnvLogger - INFO - Predictive model loss: 0.0068174125626683235
2024-07-21 20:14:31,475 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.404363247315628, Velocity: -0.16104495415744366, Collision: 0, Height: -1.0, Movement: -0.0985057140934038, Smoothness: -0.0, Curiosity: 1.2064435482025146, Exploration: 0.036832470368469795, Total: -10.384148027628738
2024-07-21 20:14:31,615 - AirSimEnvLogger - INFO - Action: [-0.49252857 -0.49252857 -0.49252857 -0.49252857], Velocity: (-0.49252857046701903, -0.49252857046701903, -0.49252857046701903), Duration: 0.5, Reward: -10.384148027628738, Done: False
2024-07-21 20:14:31,677 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:31,677 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:34,598 - AirSimEnvLogger - INFO - Predictive model loss: 0.013640815392136574
2024-07-21 20:14:39,470 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.108095225917783, Velocity: -0.038731602294108956, Collision: 0, Height: -1.0, Movement: -0.020142792386434086, Smoothness: -0.0, Curiosity: 0.11456402391195297, Exploration: 0.02949409347408437, Total: -10.566294475652688
2024-07-21 20:14:39,611 - AirSimEnvLogger - INFO - Action: [0.10071396 0.10071396 0.10071396 0.10071396], Velocity: (0.10071396193217041, 0.10071396193217041, 0.10071396193217041), Duration: 0.5, Reward: -10.566294475652688, Done: False
2024-07-21 20:14:39,673 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:39,673 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:42,597 - AirSimEnvLogger - INFO - Predictive model loss: 0.0009038071730174124
2024-07-21 20:14:47,419 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.09597103875037, Velocity: -0.03048989889334099, Collision: 0, Height: -1.0, Movement: -0.0151959932234776, Smoothness: -0.0, Curiosity: 0.0716477632522583, Exploration: 0.03654637802209319, Total: -10.56960243028884
2024-07-21 20:14:47,466 - AirSimEnvLogger - INFO - Action: [-0.07597997 -0.07597997 -0.07597997 -0.07597997], Velocity: (-0.075979966117388, -0.075979966117388, -0.075979966117388), Duration: 0.5, Reward: -10.56960243028884, Done: False
2024-07-21 20:14:47,567 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:47,568 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:50,226 - AirSimEnvLogger - INFO - Predictive model loss: 0.0006627020193263888
2024-07-21 20:14:55,114 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.858707825206658, Velocity: -0.09906628735629884, Collision: 0, Height: -1.0, Movement: -0.06316814241871775, Smoothness: -0.0, Curiosity: 0.5171423554420471, Exploration: 0.027648064720544097, Total: -10.150456992461546
2024-07-21 20:14:55,208 - AirSimEnvLogger - INFO - Action: [0.31584071 0.31584071 0.31584071 0.31584071], Velocity: (0.3158407120935887, 0.3158407120935887, 0.3158407120935887), Duration: 0.5, Reward: -10.150456992461546, Done: False
2024-07-21 20:14:55,255 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:14:55,255 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:14:58,219 - AirSimEnvLogger - INFO - Predictive model loss: 0.0037013832479715347
2024-07-21 20:15:02,422 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.811699270361245, Velocity: -0.06988038661132631, Collision: 0, Height: -1.0, Movement: -0.04702461718552083, Smoothness: -0.0, Curiosity: 0.350069522857666, Exploration: 0.034725815180874246, Total: -10.169362000920453
2024-07-21 20:15:02,562 - AirSimEnvLogger - INFO - Action: [0.23512309 0.23512309 0.23512309 0.23512309], Velocity: (0.23512308592760414, 0.23512308592760414, 0.23512308592760414), Duration: 0.5, Reward: -10.169362000920453, Done: False
2024-07-21 20:15:02,623 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:02,624 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:05,571 - AirSimEnvLogger - INFO - Predictive model loss: 0.004610566888004541
2024-07-21 20:15:10,679 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.081454801899675, Velocity: -0.08601610705843864, Collision: 0, Height: -1.0, Movement: -0.05264710903648154, Smoothness: -0.0, Curiosity: 0.3197546899318695, Exploration: 0.034787031148920906, Total: -10.462892815136822
2024-07-21 20:15:10,866 - AirSimEnvLogger - INFO - Action: [-0.26323555 -0.26323555 -0.26323555 -0.26323555], Velocity: (-0.26323554518240766, -0.26323554518240766, -0.26323554518240766), Duration: 0.5, Reward: -10.462892815136822, Done: False
2024-07-21 20:15:10,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:10,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:13,854 - AirSimEnvLogger - INFO - Predictive model loss: 0.0007712848600931466
2024-07-21 20:15:18,518 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.33695478160766, Velocity: -0.17621925905719987, Collision: 0, Height: -1.0, Movement: -0.1024829721474827, Smoothness: -0.0, Curiosity: 1.2386080026626587, Exploration: 0.0671908920249944, Total: -10.30257052861468
2024-07-21 20:15:18,612 - AirSimEnvLogger - INFO - Action: [-0.51241486 -0.51241486 -0.51241486 -0.51241486], Velocity: (-0.5124148607374135, -0.5124148607374135, -0.5124148607374135), Duration: 0.5, Reward: -10.30257052861468, Done: False
2024-07-21 20:15:18,675 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:18,675 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:21,344 - AirSimEnvLogger - INFO - Predictive model loss: 0.009925324469804764
2024-07-21 20:15:25,957 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.510387163283124, Velocity: -0.24261163478954126, Collision: 0, Height: -1.0, Movement: -0.12457068443418447, Smoothness: -0.0, Curiosity: 1.9470527172088623, Exploration: 0.05170587739638417, Total: -10.160282515037604
2024-07-21 20:15:26,034 - AirSimEnvLogger - INFO - Action: [-0.62285342 -0.62285342 -0.62285342 -0.62285342], Velocity: (-0.6228534221709223, -0.6228534221709223, -0.6228534221709223), Duration: 0.5, Reward: -10.160282515037604, Done: False
2024-07-21 20:15:26,128 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:26,128 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:29,062 - AirSimEnvLogger - INFO - Predictive model loss: 0.022782111540436745
2024-07-21 20:15:34,259 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.378816751214423, Velocity: -0.09296574812362185, Collision: 0, Height: -1.0, Movement: -0.057244149372582154, Smoothness: -0.0, Curiosity: 0.5312837958335876, Exploration: 0.010518102614437159, Total: -10.66327852177381
2024-07-21 20:15:34,353 - AirSimEnvLogger - INFO - Action: [-0.28622075 -0.28622075 -0.28622075 -0.28622075], Velocity: (-0.28622074686291077, -0.28622074686291077, -0.28622074686291077), Duration: 0.5, Reward: -10.66327852177381, Done: False
2024-07-21 20:15:34,429 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:34,429 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:37,308 - AirSimEnvLogger - INFO - Predictive model loss: 0.01052812859416008
2024-07-21 20:15:42,063 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.017469434545092, Velocity: -0.08033926582183082, Collision: 0, Height: -1.0, Movement: -0.047537342942950866, Smoothness: -0.0, Curiosity: 0.26976361870765686, Exploration: 0.06490494458485856, Total: -10.414530003479502
2024-07-21 20:15:42,172 - AirSimEnvLogger - INFO - Action: [0.23768671 0.23768671 0.23768671 0.23768671], Velocity: (0.23768671471475433, 0.23768671471475433, 0.23768671471475433), Duration: 0.5, Reward: -10.414530003479502, Done: False
2024-07-21 20:15:42,234 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:42,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:45,209 - AirSimEnvLogger - INFO - Predictive model loss: 0.00110615196172148
2024-07-21 20:15:50,279 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.978096902958839, Velocity: -0.020911055733398003, Collision: 0, Height: -1.0, Movement: -0.013095407802461457, Smoothness: -0.0, Curiosity: 0.04088519513607025, Exploration: 0.04418768601797687, Total: -10.460581836834153
2024-07-21 20:15:50,402 - AirSimEnvLogger - INFO - Action: [0.06547704 0.06547704 0.06547704 0.06547704], Velocity: (0.06547703901230728, 0.06547703901230728, 0.06547703901230728), Duration: 0.5, Reward: -10.460581836834153, Done: False
2024-07-21 20:15:50,494 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:50,494 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:15:53,495 - AirSimEnvLogger - INFO - Predictive model loss: 0.0010883454233407974
2024-07-21 20:15:58,338 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.88084003637211, Velocity: -0.06949539990237831, Collision: 0, Height: -1.0, Movement: -0.038411727109848894, Smoothness: -0.0, Curiosity: 0.19501608610153198, Exploration: 0.013917138424175338, Total: -10.319137438298684
2024-07-21 20:15:58,482 - AirSimEnvLogger - INFO - Action: [0.19205864 0.19205864 0.19205864 0.19205864], Velocity: (0.19205863554924446, 0.19205863554924446, 0.19205863554924446), Duration: 0.5, Reward: -10.319137438298684, Done: False
2024-07-21 20:15:58,543 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:15:58,543 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:01,516 - AirSimEnvLogger - INFO - Predictive model loss: 0.0030191922560334206
2024-07-21 20:16:06,484 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.67742934103931, Velocity: -0.14688598621048243, Collision: 0, Height: -1.0, Movement: -0.0953652811841664, Smoothness: -0.0, Curiosity: 1.113237977027893, Exploration: 0.038797296214094376, Total: -9.696030414506204
2024-07-21 20:16:06,594 - AirSimEnvLogger - INFO - Action: [0.47682641 0.47682641 0.47682641 0.47682641], Velocity: (0.476826405920832, 0.476826405920832, 0.476826405920832), Duration: 0.5, Reward: -9.696030414506204, Done: False
2024-07-21 20:16:06,657 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:06,657 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:09,637 - AirSimEnvLogger - INFO - Predictive model loss: 0.013091162778437138
2024-07-21 20:16:14,453 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.957382358773692, Velocity: -0.050307517848132616, Collision: 0, Height: -1.0, Movement: -0.028476777037158753, Smoothness: -0.0, Curiosity: 0.10315553843975067, Exploration: 0.015790638196500107, Total: -10.430647898542299
2024-07-21 20:16:14,562 - AirSimEnvLogger - INFO - Action: [-0.14238389 -0.14238389 -0.14238389 -0.14238389], Velocity: (-0.14238388518579376, -0.14238388518579376, -0.14238388518579376), Duration: 0.5, Reward: -10.430647898542299, Done: False
2024-07-21 20:16:14,624 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:14,624 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:17,629 - AirSimEnvLogger - INFO - Predictive model loss: 0.0009124440257437527
2024-07-21 20:16:22,804 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.116709779185388, Velocity: -0.07650848284816801, Collision: 0, Height: -1.0, Movement: -0.04563928959775723, Smoothness: -0.0, Curiosity: 0.2530922591686249, Exploration: 0.05612818712911619, Total: -10.521756182559113
2024-07-21 20:16:22,964 - AirSimEnvLogger - INFO - Action: [-0.22819645 -0.22819645 -0.22819645 -0.22819645], Velocity: (-0.22819644798878616, -0.22819644798878616, -0.22819644798878616), Duration: 0.5, Reward: -10.521756182559113, Done: False
2024-07-21 20:16:23,041 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:23,041 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:25,823 - AirSimEnvLogger - INFO - Predictive model loss: 0.0010052763391286135
2024-07-21 20:16:30,931 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.131642233574086, Velocity: -0.047702472687897755, Collision: 0, Height: -1.0, Movement: -0.028011647452954237, Smoothness: -0.0, Curiosity: 0.12661707401275635, Exploration: 0.017952692075049296, Total: -10.591395559241942
2024-07-21 20:16:31,041 - AirSimEnvLogger - INFO - Action: [-0.14005824 -0.14005824 -0.14005824 -0.14005824], Velocity: (-0.14005823726477118, -0.14005823726477118, -0.14005823726477118), Duration: 0.5, Reward: -10.591395559241942, Done: False
2024-07-21 20:16:31,087 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:31,087 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:33,948 - AirSimEnvLogger - INFO - Predictive model loss: 0.0009290826274082065
2024-07-21 20:16:39,289 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.880349291994225, Velocity: -0.09969524709101799, Collision: 0, Height: -1.0, Movement: -0.05908853829401544, Smoothness: -0.0, Curiosity: 0.4265177845954895, Exploration: 0.03517797151212549, Total: -10.215811282768966
2024-07-21 20:16:39,414 - AirSimEnvLogger - INFO - Action: [0.29544269 0.29544269 0.29544269 0.29544269], Velocity: (0.2954426914700772, 0.2954426914700772, 0.2954426914700772), Duration: 0.5, Reward: -10.215811282768966, Done: False
2024-07-21 20:16:39,507 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:39,507 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:42,452 - AirSimEnvLogger - INFO - Predictive model loss: 0.0033293862361460924
2024-07-21 20:16:47,437 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.758258601677941, Velocity: -0.10557119745729486, Collision: 0, Height: -1.0, Movement: -0.06822068304070737, Smoothness: -0.0, Curiosity: 0.6055042743682861, Exploration: 0.046673225124557016, Total: -10.005779486501606
2024-07-21 20:16:47,514 - AirSimEnvLogger - INFO - Action: [0.34110342 0.34110342 0.34110342 0.34110342], Velocity: (0.3411034152035368, 0.3411034152035368, 0.3411034152035368), Duration: 0.5, Reward: -10.005779486501606, Done: False
2024-07-21 20:16:47,576 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:47,576 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:50,611 - AirSimEnvLogger - INFO - Predictive model loss: 0.007443482521921396
2024-07-21 20:16:55,288 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.867379781543836, Velocity: -0.020009376915016402, Collision: 0, Height: -1.0, Movement: -0.013140208573385603, Smoothness: -0.0, Curiosity: 0.07673931121826172, Exploration: 0.008780283528245695, Total: -10.338572778543902
2024-07-21 20:16:55,459 - AirSimEnvLogger - INFO - Action: [0.06570104 0.06570104 0.06570104 0.06570104], Velocity: (0.06570104286692802, 0.06570104286692802, 0.06570104286692802), Duration: 0.5, Reward: -10.338572778543902, Done: False
2024-07-21 20:16:55,536 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:16:55,536 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:16:58,568 - AirSimEnvLogger - INFO - Predictive model loss: 0.0022619422525167465
2024-07-21 20:17:03,656 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.153527724785619, Velocity: -0.11428161483503563, Collision: 0, Height: -1.0, Movement: -0.06958931334254914, Smoothness: -0.0, Curiosity: 0.5468392968177795, Exploration: 0.052011793412354845, Total: -10.43380545644603
2024-07-21 20:17:03,781 - AirSimEnvLogger - INFO - Action: [-0.34794657 -0.34794657 -0.34794657 -0.34794657], Velocity: (-0.3479465667127457, -0.3479465667127457, -0.3479465667127457), Duration: 0.5, Reward: -10.43380545644603, Done: False
2024-07-21 20:17:03,876 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:03,876 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:06,850 - AirSimEnvLogger - INFO - Predictive model loss: 0.002133336616680026
2024-07-21 20:17:12,129 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.392540948895261, Velocity: -0.18981698310247086, Collision: 0, Height: -1.0, Movement: -0.11095407430051651, Smoothness: -0.0, Curiosity: 1.482046127319336, Exploration: 0.06425169439463593, Total: -10.244671445337953
2024-07-21 20:17:12,223 - AirSimEnvLogger - INFO - Action: [-0.55477037 -0.55477037 -0.55477037 -0.55477037], Velocity: (-0.5547703715025826, -0.5547703715025826, -0.5547703715025826), Duration: 0.5, Reward: -10.244671445337953, Done: False
2024-07-21 20:17:12,286 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:12,286 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:15,093 - AirSimEnvLogger - INFO - Predictive model loss: 0.013366161845624447
2024-07-21 20:17:20,086 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.456330090532626, Velocity: -0.20375850913022558, Collision: 0, Height: -1.0, Movement: -0.09908411117255135, Smoothness: -0.0, Curiosity: 1.3063528537750244, Exploration: 0.035544896458250846, Total: -10.40783235003583
2024-07-21 20:17:20,209 - AirSimEnvLogger - INFO - Action: [-0.49542056 -0.49542056 -0.49542056 -0.49542056], Velocity: (-0.49542055586275674, -0.49542055586275674, -0.49542055586275674), Duration: 0.5, Reward: -10.40783235003583, Done: False
2024-07-21 20:17:20,288 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:20,288 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:23,213 - AirSimEnvLogger - INFO - Predictive model loss: 0.01774054579436779
2024-07-21 20:17:28,256 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.463909787943065, Velocity: -0.15184991781490403, Collision: 0, Height: -1.0, Movement: -0.09189256236880741, Smoothness: -0.0, Curiosity: 1.1740951538085938, Exploration: 0.011691518615445783, Total: -10.459638122460012
2024-07-21 20:17:28,366 - AirSimEnvLogger - INFO - Action: [-0.45946281 -0.45946281 -0.45946281 -0.45946281], Velocity: (-0.45946281184403703, -0.45946281184403703, -0.45946281184403703), Duration: 0.5, Reward: -10.459638122460012, Done: False
2024-07-21 20:17:28,429 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:28,429 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:31,335 - AirSimEnvLogger - INFO - Predictive model loss: 0.018301978707313538
2024-07-21 20:17:36,398 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.337885326620084, Velocity: -0.09094163102881062, Collision: 0, Height: -1.0, Movement: -0.048999284920244024, Smoothness: -0.0, Curiosity: 0.43329542875289917, Exploration: 0.016705528487608617, Total: -10.668267250552542
2024-07-21 20:17:36,541 - AirSimEnvLogger - INFO - Action: [-0.24499642 -0.24499642 -0.24499642 -0.24499642], Velocity: (-0.24499642460122012, -0.24499642460122012, -0.24499642460122012), Duration: 0.5, Reward: -10.668267250552542, Done: False
2024-07-21 20:17:36,604 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:36,604 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:39,459 - AirSimEnvLogger - INFO - Predictive model loss: 0.00994583498686552
2024-07-21 20:17:44,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.99358569256857, Velocity: -0.07965255140281083, Collision: 0, Height: -1.0, Movement: -0.04984029570363532, Smoothness: -0.0, Curiosity: 0.2793174088001251, Exploration: 0.05733713110421447, Total: -10.387269867219436
2024-07-21 20:17:44,758 - AirSimEnvLogger - INFO - Action: [0.24920148 0.24920148 0.24920148 0.24920148], Velocity: (0.2492014785181766, 0.2492014785181766, 0.2492014785181766), Duration: 0.5, Reward: -10.387269867219436, Done: False
2024-07-21 20:17:44,837 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:44,837 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:47,850 - AirSimEnvLogger - INFO - Predictive model loss: 0.0011586399050429463
2024-07-21 20:17:53,030 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.131243462468065, Velocity: -0.07184079570352866, Collision: 0, Height: -1.0, Movement: -0.04152967910397764, Smoothness: -0.0, Curiosity: 0.2421128749847412, Exploration: 0.020859184101442774, Total: -10.546088553917569
2024-07-21 20:17:53,139 - AirSimEnvLogger - INFO - Action: [-0.2076484 -0.2076484 -0.2076484 -0.2076484], Velocity: (-0.20764839551988817, -0.20764839551988817, -0.20764839551988817), Duration: 0.5, Reward: -10.546088553917569, Done: False
2024-07-21 20:17:53,234 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:17:53,234 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:17:56,095 - AirSimEnvLogger - INFO - Predictive model loss: 0.00356260035187006
2024-07-21 20:18:00,911 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.963904337997002, Velocity: -0.050790979935119565, Collision: 0, Height: -1.0, Movement: -0.030516839124616415, Smoothness: -0.0, Curiosity: 0.11566314101219177, Exploration: 0.005687861473144289, Total: -10.433382369076298
2024-07-21 20:18:01,052 - AirSimEnvLogger - INFO - Action: [0.1525842 0.1525842 0.1525842 0.1525842], Velocity: (0.15258419562308206, 0.15258419562308206, 0.15258419562308206), Duration: 0.5, Reward: -10.433382369076298, Done: False
2024-07-21 20:18:01,099 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:01,099 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:03,928 - AirSimEnvLogger - INFO - Predictive model loss: 0.0014825102407485247
2024-07-21 20:18:08,946 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.721152439000548, Velocity: -0.1384790712350591, Collision: 0, Height: -1.0, Movement: -0.09141783719155017, Smoothness: -0.0, Curiosity: 0.9579166173934937, Exploration: 0.0541437419609897, Total: -9.809746701248288
2024-07-21 20:18:09,087 - AirSimEnvLogger - INFO - Action: [0.45708919 0.45708919 0.45708919 0.45708919], Velocity: (0.4570891859577508, 0.4570891859577508, 0.4570891859577508), Duration: 0.5, Reward: -9.809746701248288, Done: False
2024-07-21 20:18:09,150 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:09,150 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:12,125 - AirSimEnvLogger - INFO - Predictive model loss: 0.009101825766265392
2024-07-21 20:18:17,340 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.563201804910689, Velocity: -0.1890573575016247, Collision: 0, Height: -1.0, Movement: -0.11263444426105829, Smoothness: -0.0, Curiosity: 1.5442308187484741, Exploration: 0.04874991128904528, Total: -9.387128536455561
2024-07-21 20:18:17,495 - AirSimEnvLogger - INFO - Action: [0.56317222 0.56317222 0.56317222 0.56317222], Velocity: (0.5631722213052914, 0.5631722213052914, 0.5631722213052914), Duration: 0.5, Reward: -9.387128536455561, Done: False
2024-07-21 20:18:17,558 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:17,558 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:20,567 - AirSimEnvLogger - INFO - Predictive model loss: 0.019727477803826332
2024-07-21 20:18:25,504 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.61879635513197, Velocity: -0.10921792008006623, Collision: 0, Height: -1.0, Movement: -0.0711284220340173, Smoothness: -0.0, Curiosity: 0.7098733186721802, Exploration: 0.013647435092578578, Total: -9.8228520110208
2024-07-21 20:18:25,629 - AirSimEnvLogger - INFO - Action: [0.35564211 0.35564211 0.35564211 0.35564211], Velocity: (0.3556421101700865, 0.3556421101700865, 0.3556421101700865), Duration: 0.5, Reward: -9.8228520110208, Done: False
2024-07-21 20:18:25,691 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:25,691 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:27,804 - AirSimEnvLogger - INFO - Predictive model loss: 0.014121280051767826
2024-07-21 20:18:32,829 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.845984762052229, Velocity: -0.01146717528775093, Collision: 0, Height: -1.0, Movement: -0.0008918017209728846, Smoothness: -0.0, Curiosity: 0.044603731483221054, Exploration: 0.03616249907034738, Total: -10.322273164312522
2024-07-21 20:18:32,922 - AirSimEnvLogger - INFO - Action: [0.00445901 0.00445901 0.00445901 0.00445901], Velocity: (0.004459008604864423, 0.004459008604864423, 0.004459008604864423), Duration: 0.5, Reward: -10.322273164312522, Done: False
2024-07-21 20:18:33,032 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:33,032 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:35,866 - AirSimEnvLogger - INFO - Predictive model loss: 0.00261079054325819
2024-07-21 20:18:40,935 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.161510902901666, Velocity: -0.1345283249624009, Collision: 0, Height: -1.0, Movement: -0.0757135167687555, Smoothness: -0.0, Curiosity: 0.6441807150840759, Exploration: 0.06539771220085781, Total: -10.40117651707753
2024-07-21 20:18:41,045 - AirSimEnvLogger - INFO - Action: [-0.37856758 -0.37856758 -0.37856758 -0.37856758], Velocity: (-0.3785675838437775, -0.3785675838437775, -0.3785675838437775), Duration: 0.5, Reward: -10.40117651707753, Done: False
2024-07-21 20:18:41,109 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:41,109 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:44,058 - AirSimEnvLogger - INFO - Predictive model loss: 0.0023780714254826307
2024-07-21 20:18:49,244 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.15227510938061, Velocity: -0.049760955866471154, Collision: 0, Height: -1.0, Movement: -0.030306865273387225, Smoothness: -0.0, Curiosity: 0.1473618447780609, Exploration: 0.0328105230230474, Total: -10.599943246847545
2024-07-21 20:18:49,384 - AirSimEnvLogger - INFO - Action: [-0.15153433 -0.15153433 -0.15153433 -0.15153433], Velocity: (-0.15153432636693612, -0.15153432636693612, -0.15153432636693612), Duration: 0.5, Reward: -10.599943246847545, Done: False
2024-07-21 20:18:49,446 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:49,446 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:52,363 - AirSimEnvLogger - INFO - Predictive model loss: 0.0013134970795363188
2024-07-21 20:18:57,127 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.338134434125987, Velocity: -0.15376275201822853, Collision: 0, Height: -1.0, Movement: -0.09131285026593557, Smoothness: -0.0, Curiosity: 1.0341286659240723, Exploration: 0.02029047176573647, Total: -10.40302466784651
2024-07-21 20:18:57,175 - AirSimEnvLogger - INFO - Action: [-0.45656425 -0.45656425 -0.45656425 -0.45656425], Velocity: (-0.4565642513296778, -0.4565642513296778, -0.4565642513296778), Duration: 0.5, Reward: -10.40302466784651, Done: False
2024-07-21 20:18:57,238 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:18:57,238 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:18:59,986 - AirSimEnvLogger - INFO - Predictive model loss: 0.009672059677541256
2024-07-21 20:19:04,873 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.048400436963634, Velocity: -0.05534019912735057, Collision: 0, Height: -1.0, Movement: -0.030185991153500714, Smoothness: -0.0, Curiosity: 0.12760573625564575, Exploration: 0.01607277705595247, Total: -10.512071712103646
2024-07-21 20:19:05,001 - AirSimEnvLogger - INFO - Action: [0.15092996 0.15092996 0.15092996 0.15092996], Velocity: (0.15092995576750357, 0.15092995576750357, 0.15092995576750357), Duration: 0.5, Reward: -10.512071712103646, Done: False
2024-07-21 20:19:05,047 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:05,047 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:07,846 - AirSimEnvLogger - INFO - Predictive model loss: 0.0005613325047306716
2024-07-21 20:19:13,077 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.784235546721082, Velocity: -0.1275335207505235, Collision: 0, Height: -1.0, Movement: -0.08282991927136096, Smoothness: -0.0, Curiosity: 0.8024149537086487, Exploration: 0.07143864633411087, Total: -9.940790092902333
2024-07-21 20:19:13,203 - AirSimEnvLogger - INFO - Action: [0.4141496 0.4141496 0.4141496 0.4141496], Velocity: (0.41414959635680476, 0.41414959635680476, 0.41414959635680476), Duration: 0.5, Reward: -9.940790092902333, Done: False
2024-07-21 20:19:13,266 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:13,266 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:16,210 - AirSimEnvLogger - INFO - Predictive model loss: 0.0055831740610301495
2024-07-21 20:19:21,082 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.846912600242357, Velocity: -0.039600769566189936, Collision: 0, Height: -1.0, Movement: -0.024526891019290255, Smoothness: -0.0, Curiosity: 0.11084242910146713, Exploration: 0.02114823727574103, Total: -10.309514812121499
2024-07-21 20:19:21,098 - AirSimEnvLogger - INFO - Action: [0.12263446 0.12263446 0.12263446 0.12263446], Velocity: (0.12263445509645127, 0.12263445509645127, 0.12263445509645127), Duration: 0.5, Reward: -10.309514812121499, Done: False
2024-07-21 20:19:21,176 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:21,176 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:24,076 - AirSimEnvLogger - INFO - Predictive model loss: 0.002137690782546997
2024-07-21 20:19:28,470 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.129438585930894, Velocity: -0.10735396132206114, Collision: 0, Height: -1.0, Movement: -0.06389597211959683, Smoothness: -0.0, Curiosity: 0.46254757046699524, Exploration: 0.04900872570989266, Total: -10.44842963342841
2024-07-21 20:19:28,596 - AirSimEnvLogger - INFO - Action: [-0.31947986 -0.31947986 -0.31947986 -0.31947986], Velocity: (-0.3194798605979841, -0.3194798605979841, -0.3194798605979841), Duration: 0.5, Reward: -10.44842963342841, Done: False
2024-07-21 20:19:28,659 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:28,659 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:31,558 - AirSimEnvLogger - INFO - Predictive model loss: 0.0019668240565806627
2024-07-21 20:19:36,298 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.072977620171299, Velocity: -0.012671055313769901, Collision: 0, Height: -1.0, Movement: -0.0075905464822189185, Smoothness: -0.0, Curiosity: 0.037927690893411636, Exploration: 0.024798018596988006, Total: -10.556148753310303
2024-07-21 20:19:36,422 - AirSimEnvLogger - INFO - Action: [-0.03795273 -0.03795273 -0.03795273 -0.03795273], Velocity: (-0.03795273241109459, -0.03795273241109459, -0.03795273241109459), Duration: 0.5, Reward: -10.556148753310303, Done: False
2024-07-21 20:19:36,469 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:36,469 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:39,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.00041622252319939435
2024-07-21 20:19:44,430 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.202192986468376, Velocity: -0.09447259726239532, Collision: 0, Height: -1.0, Movement: -0.05679711216361008, Smoothness: -0.0, Curiosity: 0.40982672572135925, Exploration: 0.007923444305814785, Total: -10.548610944594092
2024-07-21 20:19:44,524 - AirSimEnvLogger - INFO - Action: [-0.28398556 -0.28398556 -0.28398556 -0.28398556], Velocity: (-0.2839855608180504, -0.2839855608180504, -0.2839855608180504), Duration: 0.5, Reward: -10.548610944594092, Done: False
2024-07-21 20:19:44,635 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:44,635 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:47,681 - AirSimEnvLogger - INFO - Predictive model loss: 0.0036293601151555777
2024-07-21 20:19:52,542 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.269741574268446, Velocity: -0.1111132010219444, Collision: 0, Height: -1.0, Movement: -0.062155122129473495, Smoothness: -0.0, Curiosity: 0.5224670171737671, Exploration: 0.027287647203361704, Total: -10.564822648964808
2024-07-21 20:19:52,667 - AirSimEnvLogger - INFO - Action: [-0.31077561 -0.31077561 -0.31077561 -0.31077561], Velocity: (-0.3107756106473675, -0.3107756106473675, -0.3107756106473675), Duration: 0.5, Reward: -10.564822648964808, Done: False
2024-07-21 20:19:52,729 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:19:52,729 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:19:55,680 - AirSimEnvLogger - INFO - Predictive model loss: 0.006274333223700523
2024-07-21 20:20:00,876 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.435065548162044, Velocity: -0.17310114578337474, Collision: 0, Height: -1.0, Movement: -0.10723697869397869, Smoothness: -0.0, Curiosity: 1.4823604822158813, Exploration: 0.030662653212365665, Total: -10.285027047172715
2024-07-21 20:20:01,002 - AirSimEnvLogger - INFO - Action: [-0.53618489 -0.53618489 -0.53618489 -0.53618489], Velocity: (-0.5361848934698934, -0.5361848934698934, -0.5361848934698934), Duration: 0.5, Reward: -10.285027047172715, Done: False
2024-07-21 20:20:01,065 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:01,065 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:04,130 - AirSimEnvLogger - INFO - Predictive model loss: 0.017612280324101448
2024-07-21 20:20:09,210 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.541137443944704, Velocity: -0.23723986983566173, Collision: 0, Height: -1.0, Movement: -0.11968726976920004, Smoothness: -0.0, Curiosity: 1.9564580917358398, Exploration: 0.03769421539693403, Total: -10.18595821689215
2024-07-21 20:20:09,319 - AirSimEnvLogger - INFO - Action: [-0.59843635 -0.59843635 -0.59843635 -0.59843635], Velocity: (-0.5984363488460002, -0.5984363488460002, -0.5984363488460002), Duration: 0.5, Reward: -10.18595821689215, Done: False
2024-07-21 20:20:09,381 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:09,381 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:12,391 - AirSimEnvLogger - INFO - Predictive model loss: 0.02828074060380459
2024-07-21 20:20:17,411 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.637699211055038, Velocity: -0.29121758396990866, Collision: 0, Height: -1.0, Movement: -0.13600305251384195, Smoothness: -0.0, Curiosity: 2.5987114906311035, Exploration: 0.030109363024261072, Total: -9.991530690370972
2024-07-21 20:20:17,520 - AirSimEnvLogger - INFO - Action: [-0.68001526 -0.68001526 -0.68001526 -0.68001526], Velocity: (-0.6800152625692097, -0.6800152625692097, -0.6800152625692097), Duration: 0.5, Reward: -9.991530690370972, Done: False
2024-07-21 20:20:17,613 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:17,613 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:20,597 - AirSimEnvLogger - INFO - Predictive model loss: 0.040716417133808136
2024-07-21 20:20:25,660 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.384625475199886, Velocity: -0.06650873402190167, Collision: 0, Height: -1.0, Movement: -0.04083133008134572, Smoothness: -0.0, Curiosity: 0.4674920439720154, Exploration: 0.023964039214433002, Total: -10.683424145390077
2024-07-21 20:20:25,800 - AirSimEnvLogger - INFO - Action: [-0.20415665 -0.20415665 -0.20415665 -0.20415665], Velocity: (-0.2041566504067286, -0.2041566504067286, -0.2041566504067286), Duration: 0.5, Reward: -10.683424145390077, Done: False
2024-07-21 20:20:25,910 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:25,910 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:28,874 - AirSimEnvLogger - INFO - Predictive model loss: 0.014597387984395027
2024-07-21 20:20:33,688 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.459832665053542, Velocity: -0.16964844230277737, Collision: 0, Height: -1.0, Movement: -0.09657508266991481, Smoothness: -0.0, Curiosity: 1.4414914846420288, Exploration: 0.021948182544736974, Total: -10.32917901564196
2024-07-21 20:20:33,703 - AirSimEnvLogger - INFO - Action: [-0.48287541 -0.48287541 -0.48287541 -0.48287541], Velocity: (-0.482875413349574, -0.482875413349574, -0.482875413349574), Duration: 0.5, Reward: -10.32917901564196, Done: False
2024-07-21 20:20:33,750 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:33,750 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:36,514 - AirSimEnvLogger - INFO - Predictive model loss: 0.025052305310964584
2024-07-21 20:20:41,499 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.37357585584817, Velocity: -0.09737235716573035, Collision: 0, Height: -1.0, Movement: -0.05994389719280653, Smoothness: -0.0, Curiosity: 0.7506651878356934, Exploration: 0.013357319327388649, Total: -10.550252366366992
2024-07-21 20:20:41,576 - AirSimEnvLogger - INFO - Action: [-0.29971949 -0.29971949 -0.29971949 -0.29971949], Velocity: (-0.29971948596403264, -0.29971948596403264, -0.29971948596403264), Duration: 0.5, Reward: -10.550252366366992, Done: False
2024-07-21 20:20:41,637 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:41,637 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:44,585 - AirSimEnvLogger - INFO - Predictive model loss: 0.017990095540881157
2024-07-21 20:20:49,420 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.408160332513114, Velocity: -0.13510953300321216, Collision: 0, Height: -1.0, Movement: -0.08106583091871467, Smoothness: -0.0, Curiosity: 1.1730797290802002, Exploration: 0.009391190346361105, Total: -10.39540357949722
2024-07-21 20:20:49,546 - AirSimEnvLogger - INFO - Action: [-0.40532915 -0.40532915 -0.40532915 -0.40532915], Velocity: (-0.4053291545935733, -0.4053291545935733, -0.4053291545935733), Duration: 0.5, Reward: -10.39540357949722, Done: False
2024-07-21 20:20:49,610 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:49,610 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:20:52,575 - AirSimEnvLogger - INFO - Predictive model loss: 0.023080313578248024
2024-07-21 20:20:57,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.528708517608404, Velocity: -0.19437158025930606, Collision: 0, Height: -1.0, Movement: -0.11510104404517277, Smoothness: -0.0, Curiosity: 2.1231608390808105, Exploration: 0.028054996738995964, Total: -10.07021299325437
2024-07-21 20:20:57,753 - AirSimEnvLogger - INFO - Action: [-0.57550522 -0.57550522 -0.57550522 -0.57550522], Velocity: (-0.5755052202258638, -0.5755052202258638, -0.5755052202258638), Duration: 0.5, Reward: -10.07021299325437, Done: False
2024-07-21 20:20:57,815 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:20:57,815 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:00,827 - AirSimEnvLogger - INFO - Predictive model loss: 0.03503039851784706
2024-07-21 20:21:05,766 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.204631113789857, Velocity: -0.024164184817859827, Collision: 0, Height: -1.0, Movement: -0.004765728778500056, Smoothness: -0.0, Curiosity: 0.29962894320487976, Exploration: 0.0262293396196719, Total: -10.562129439550263
2024-07-21 20:21:05,846 - AirSimEnvLogger - INFO - Action: [0.02382864 0.02382864 0.02382864 0.02382864], Velocity: (0.02382864389250028, 0.02382864389250028, 0.02382864389250028), Duration: 0.5, Reward: -10.562129439550263, Done: False
2024-07-21 20:21:05,941 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:05,941 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:08,982 - AirSimEnvLogger - INFO - Predictive model loss: 0.012651202268898487
2024-07-21 20:21:14,275 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.123823615604943, Velocity: -0.01989408968556024, Collision: 0, Height: -1.0, Movement: -0.008622941043546451, Smoothness: -0.0, Curiosity: 0.2954197824001312, Exploration: 0.0470227640800287, Total: -10.477518510536006
2024-07-21 20:21:14,385 - AirSimEnvLogger - INFO - Action: [-0.04311471 -0.04311471 -0.04311471 -0.04311471], Velocity: (-0.04311470521773225, -0.04311470521773225, -0.04311470521773225), Duration: 0.5, Reward: -10.477518510536006, Done: False
2024-07-21 20:21:14,446 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:14,446 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:17,371 - AirSimEnvLogger - INFO - Predictive model loss: 0.013291693292558193
2024-07-21 20:21:22,323 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.257297861733445, Velocity: -0.10464109782875262, Collision: 0, Height: -1.0, Movement: -0.06337109638580216, Smoothness: -0.0, Curiosity: 0.8941020965576172, Exploration: 0.02014505659557794, Total: -10.364875460688477
2024-07-21 20:21:22,446 - AirSimEnvLogger - INFO - Action: [-0.31685548 -0.31685548 -0.31685548 -0.31685548], Velocity: (-0.31685548192901075, -0.31685548192901075, -0.31685548192901075), Duration: 0.5, Reward: -10.364875460688477, Done: False
2024-07-21 20:21:22,461 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:22,461 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:25,377 - AirSimEnvLogger - INFO - Predictive model loss: 0.018190039321780205
2024-07-21 20:21:30,403 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.976271129481734, Velocity: -0.07151400896595453, Collision: 0, Height: -1.0, Movement: -0.04447386943634088, Smoothness: -0.0, Curiosity: 0.4070177674293518, Exploration: 0.018535094365012623, Total: -10.309259618320667
2024-07-21 20:21:30,543 - AirSimEnvLogger - INFO - Action: [0.22236935 0.22236935 0.22236935 0.22236935], Velocity: (0.22236934718170437, 0.22236934718170437, 0.22236934718170437), Duration: 0.5, Reward: -10.309259618320667, Done: False
2024-07-21 20:21:30,607 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:30,607 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:33,509 - AirSimEnvLogger - INFO - Predictive model loss: 0.014189639128744602
2024-07-21 20:21:38,618 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.054926507502321, Velocity: -0.026539275483561317, Collision: 0, Height: -1.0, Movement: -0.015988719049415314, Smoothness: -0.0, Curiosity: 0.33004704117774963, Exploration: 0.02338073424862838, Total: -10.400095349710442
2024-07-21 20:21:38,681 - AirSimEnvLogger - INFO - Action: [-0.0799436 -0.0799436 -0.0799436 -0.0799436], Velocity: (-0.07994359524707656, -0.07994359524707656, -0.07994359524707656), Duration: 0.5, Reward: -10.400095349710442, Done: False
2024-07-21 20:21:38,744 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:38,744 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:41,743 - AirSimEnvLogger - INFO - Predictive model loss: 0.013569937087595463
2024-07-21 20:21:47,152 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.82407838548129, Velocity: -0.10790434051094905, Collision: 0, Height: -1.0, Movement: -0.0681650581045343, Smoothness: -0.0, Curiosity: 0.6505451202392578, Exploration: 0.016809420595085812, Total: -10.05621261730857
2024-07-21 20:21:47,184 - AirSimEnvLogger - INFO - Action: [0.34082529 0.34082529 0.34082529 0.34082529], Velocity: (0.34082529052267146, 0.34082529052267146, 0.34082529052267146), Duration: 0.5, Reward: -10.05621261730857, Done: False
2024-07-21 20:21:47,262 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:47,262 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:50,339 - AirSimEnvLogger - INFO - Predictive model loss: 0.017568470910191536
2024-07-21 20:21:55,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.609780085002116, Velocity: -0.17312259554200235, Collision: 0, Height: -1.0, Movement: -0.11024194668150909, Smoothness: -0.0, Curiosity: 1.4560407400131226, Exploration: 0.05661730234680809, Total: -9.468021746965345
2024-07-21 20:21:55,647 - AirSimEnvLogger - INFO - Action: [0.55120973 0.55120973 0.55120973 0.55120973], Velocity: (0.5512097334075454, 0.5512097334075454, 0.5512097334075454), Duration: 0.5, Reward: -9.468021746965345, Done: False
2024-07-21 20:21:55,678 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:21:55,678 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:21:58,691 - AirSimEnvLogger - INFO - Predictive model loss: 0.028474153950810432
2024-07-21 20:22:03,559 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.460818473875337, Velocity: -0.19702858759192302, Collision: 0, Height: -1.0, Movement: -0.1312803909699965, Smoothness: -0.0, Curiosity: 2.0641744136810303, Exploration: 0.04374538733206102, Total: -9.031624522461371
2024-07-21 20:22:03,668 - AirSimEnvLogger - INFO - Action: [0.65640195 0.65640195 0.65640195 0.65640195], Velocity: (0.6564019548499824, 0.6564019548499824, 0.6564019548499824), Duration: 0.5, Reward: -9.031624522461371, Done: False
2024-07-21 20:22:03,747 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:03,747 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:06,539 - AirSimEnvLogger - INFO - Predictive model loss: 0.036971431225538254
2024-07-21 20:22:11,552 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.563924386897689, Velocity: -0.12379770763781728, Collision: 0, Height: -1.0, Movement: -0.07557720693552734, Smoothness: -0.0, Curiosity: 0.7978980541229248, Exploration: 0.00994086558877581, Total: -9.732443761230932
2024-07-21 20:22:11,675 - AirSimEnvLogger - INFO - Action: [0.37788603 0.37788603 0.37788603 0.37788603], Velocity: (0.3778860346776367, 0.3778860346776367, 0.3778860346776367), Duration: 0.5, Reward: -9.732443761230932, Done: False
2024-07-21 20:22:11,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:11,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:14,673 - AirSimEnvLogger - INFO - Predictive model loss: 0.02179531566798687
2024-07-21 20:22:20,043 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.488377379655052, Velocity: -0.1703552012083209, Collision: 0, Height: -1.0, Movement: -0.11394802109700562, Smoothness: -0.0, Curiosity: 1.5843346118927002, Exploration: 0.009728007778694045, Total: -9.290836874866823
2024-07-21 20:22:20,207 - AirSimEnvLogger - INFO - Action: [0.56974011 0.56974011 0.56974011 0.56974011], Velocity: (0.5697401054850281, 0.5697401054850281, 0.5697401054850281), Duration: 0.5, Reward: -9.290836874866823, Done: False
2024-07-21 20:22:20,295 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:20,295 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:23,004 - AirSimEnvLogger - INFO - Predictive model loss: 0.025026114657521248
2024-07-21 20:22:27,939 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.854386725604218, Velocity: -0.0410461217815881, Collision: 0, Height: -1.0, Movement: -0.01918540708073915, Smoothness: -0.0, Curiosity: 0.12679435312747955, Exploration: 0.034413531794548174, Total: -10.306548444280436
2024-07-21 20:22:28,064 - AirSimEnvLogger - INFO - Action: [-0.09592704 -0.09592704 -0.09592704 -0.09592704], Velocity: (-0.09592703540369574, -0.09592703540369574, -0.09592703540369574), Duration: 0.5, Reward: -10.306548444280436, Done: False
2024-07-21 20:22:28,142 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:28,142 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:31,413 - AirSimEnvLogger - INFO - Predictive model loss: 0.003344245022162795
2024-07-21 20:22:35,987 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.196521101751612, Velocity: -0.14701715208379268, Collision: 0, Height: -1.0, Movement: -0.08575212116961152, Smoothness: -0.0, Curiosity: 0.9767941236495972, Exploration: 0.08541984715587703, Total: -10.273123858654497
2024-07-21 20:22:36,112 - AirSimEnvLogger - INFO - Action: [-0.42876061 -0.42876061 -0.42876061 -0.42876061], Velocity: (-0.4287606058480576, -0.4287606058480576, -0.4287606058480576), Duration: 0.5, Reward: -10.273123858654497, Done: False
2024-07-21 20:22:36,175 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:36,175 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:39,305 - AirSimEnvLogger - INFO - Predictive model loss: 0.006593162193894386
2024-07-21 20:22:44,016 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.423970248993374, Velocity: -0.18776982088631902, Collision: 0, Height: -1.0, Movement: -0.11372742233636216, Smoothness: -0.0, Curiosity: 1.8082501888275146, Exploration: 0.0640113071190364, Total: -10.112300545832605
2024-07-21 20:22:44,125 - AirSimEnvLogger - INFO - Action: [-0.56863711 -0.56863711 -0.56863711 -0.56863711], Velocity: (-0.5686371116818107, -0.5686371116818107, -0.5686371116818107), Duration: 0.5, Reward: -10.112300545832605, Done: False
2024-07-21 20:22:44,218 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:44,218 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:47,169 - AirSimEnvLogger - INFO - Predictive model loss: 0.02062726579606533
2024-07-21 20:22:52,341 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.364973729640067, Velocity: -0.10069826045240839, Collision: 0, Height: -1.0, Movement: -0.06289063817740596, Smoothness: -0.0, Curiosity: 0.7899945378303528, Exploration: 0.01863855823070684, Total: -10.522886943122694
2024-07-21 20:22:52,468 - AirSimEnvLogger - INFO - Action: [-0.31445319 -0.31445319 -0.31445319 -0.31445319], Velocity: (-0.31445319088702983, -0.31445319088702983, -0.31445319088702983), Duration: 0.5, Reward: -10.522886943122694, Done: False
2024-07-21 20:22:52,532 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:22:52,532 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:22:55,378 - AirSimEnvLogger - INFO - Predictive model loss: 0.015339385718107224
2024-07-21 20:23:00,407 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.306157518135626, Velocity: -0.07905343081283094, Collision: 0, Height: -1.0, Movement: -0.049601101013427566, Smoothness: -0.0, Curiosity: 0.606966495513916, Exploration: 0.016911252371566488, Total: -10.543778845412112
2024-07-21 20:23:00,564 - AirSimEnvLogger - INFO - Action: [-0.24800551 -0.24800551 -0.24800551 -0.24800551], Velocity: (-0.2480055050671378, -0.2480055050671378, -0.2480055050671378), Duration: 0.5, Reward: -10.543778845412112, Done: False
2024-07-21 20:23:00,722 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:00,722 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:03,700 - AirSimEnvLogger - INFO - Predictive model loss: 0.013469203375279903
2024-07-21 20:23:08,972 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.9956016495641, Velocity: -0.07476331064152808, Collision: 0, Height: -1.0, Movement: -0.044646347000000086, Smoothness: -0.0, Curiosity: 0.32009363174438477, Exploration: 0.043798621122427536, Total: -10.368641399488185
2024-07-21 20:23:09,099 - AirSimEnvLogger - INFO - Action: [0.22323174 0.22323174 0.22323174 0.22323174], Velocity: (0.22323173500000043, 0.22323173500000043, 0.22323173500000043), Duration: 0.5, Reward: -10.368641399488185, Done: False
2024-07-21 20:23:09,193 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:09,193 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:12,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.002505091018974781
2024-07-21 20:23:17,148 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.176265120266471, Velocity: -0.09163294052154212, Collision: 0, Height: -1.0, Movement: -0.0538362441292419, Smoothness: -0.0, Curiosity: 0.590944766998291, Exploration: 0.01324955517604522, Total: -10.429342920405812
2024-07-21 20:23:17,195 - AirSimEnvLogger - INFO - Action: [-0.26918122 -0.26918122 -0.26918122 -0.26918122], Velocity: (-0.2691812206462095, -0.2691812206462095, -0.2691812206462095), Duration: 0.5, Reward: -10.429342920405812, Done: False
2024-07-21 20:23:17,241 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:17,241 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:20,138 - AirSimEnvLogger - INFO - Predictive model loss: 0.009886587969958782
2024-07-21 20:23:25,351 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.191037008029964, Velocity: -0.07037453497847102, Collision: 0, Height: -1.0, Movement: -0.03744709480956266, Smoothness: -0.0, Curiosity: 0.4410618543624878, Exploration: 0.03004489454595563, Total: -10.50342907890972
2024-07-21 20:23:25,446 - AirSimEnvLogger - INFO - Action: [-0.18723547 -0.18723547 -0.18723547 -0.18723547], Velocity: (-0.1872354740478133, -0.1872354740478133, -0.1872354740478133), Duration: 0.5, Reward: -10.50342907890972, Done: False
2024-07-21 20:23:25,509 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:25,509 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:28,481 - AirSimEnvLogger - INFO - Predictive model loss: 0.009487166069447994
2024-07-21 20:23:33,575 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.378645020527564, Velocity: -0.16142854495677897, Collision: 0, Height: -1.0, Movement: -0.09488296503402327, Smoothness: -0.0, Curiosity: 1.501624584197998, Exploration: 0.02556626395383634, Total: -10.212922044619589
2024-07-21 20:23:33,731 - AirSimEnvLogger - INFO - Action: [-0.47441483 -0.47441483 -0.47441483 -0.47441483], Velocity: (-0.47441482517011635, -0.47441482517011635, -0.47441482517011635), Duration: 0.5, Reward: -10.212922044619589, Done: False
2024-07-21 20:23:33,793 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:33,794 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:36,696 - AirSimEnvLogger - INFO - Predictive model loss: 0.023401660844683647
2024-07-21 20:23:41,594 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.468614893125887, Velocity: -0.17482832696239223, Collision: 0, Height: -1.0, Movement: -0.09964577552963066, Smoothness: -0.0, Curiosity: 1.7604092359542847, Exploration: 0.03612757591639488, Total: -10.178563500999624
2024-07-21 20:23:41,719 - AirSimEnvLogger - INFO - Action: [-0.49822888 -0.49822888 -0.49822888 -0.49822888], Velocity: (-0.49822887764815327, -0.49822887764815327, -0.49822887764815327), Duration: 0.5, Reward: -10.178563500999624, Done: False
2024-07-21 20:23:41,796 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:41,796 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:44,791 - AirSimEnvLogger - INFO - Predictive model loss: 0.03063318319618702
2024-07-21 20:23:49,798 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.592621993852587, Velocity: -0.20620351793339753, Collision: 0, Height: -1.0, Movement: -0.12598230539405728, Smoothness: -0.0, Curiosity: 2.6854257583618164, Exploration: 0.02816284936663895, Total: -9.859976534304455
2024-07-21 20:23:49,939 - AirSimEnvLogger - INFO - Action: [-0.62991153 -0.62991153 -0.62991153 -0.62991153], Velocity: (-0.6299115269702864, -0.6299115269702864, -0.6299115269702864), Duration: 0.5, Reward: -9.859976534304455, Done: False
2024-07-21 20:23:50,001 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:50,001 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:23:52,982 - AirSimEnvLogger - INFO - Predictive model loss: 0.04572155699133873
2024-07-21 20:23:57,969 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.207927812148672, Velocity: -0.03561498720677489, Collision: 0, Height: -1.0, Movement: -0.013168264932213304, Smoothness: -0.0, Curiosity: 0.43982043862342834, Exploration: 0.034774182722158674, Total: -10.500187076389134
2024-07-21 20:23:58,064 - AirSimEnvLogger - INFO - Action: [0.06584132 0.06584132 0.06584132 0.06584132], Velocity: (0.06584132466106651, 0.06584132466106651, 0.06584132466106651), Duration: 0.5, Reward: -10.500187076389134, Done: False
2024-07-21 20:23:58,126 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:23:58,126 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:00,909 - AirSimEnvLogger - INFO - Predictive model loss: 0.011141650378704071
2024-07-21 20:24:05,928 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.27492453343763, Velocity: -0.09745601803498315, Collision: 0, Height: -1.0, Movement: -0.05424769445917106, Smoothness: -0.0, Curiosity: 0.9335887432098389, Exploration: 0.034653573084970815, Total: -10.355352225679125
2024-07-21 20:24:06,053 - AirSimEnvLogger - INFO - Action: [-0.27123847 -0.27123847 -0.27123847 -0.27123847], Velocity: (-0.2712384722958553, -0.2712384722958553, -0.2712384722958553), Duration: 0.5, Reward: -10.355352225679125, Done: False
2024-07-21 20:24:06,085 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:06,086 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:08,806 - AirSimEnvLogger - INFO - Predictive model loss: 0.017199626192450523
2024-07-21 20:24:13,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.44885603873483, Velocity: -0.17079900304514617, Collision: 0, Height: -1.0, Movement: -0.10328326485882747, Smoothness: -0.0, Curiosity: 2.077057361602783, Exploration: 0.041536415019658976, Total: -9.997747902937963
2024-07-21 20:24:14,123 - AirSimEnvLogger - INFO - Action: [-0.51641632 -0.51641632 -0.51641632 -0.51641632], Velocity: (-0.5164163242941373, -0.5164163242941373, -0.5164163242941373), Duration: 0.5, Reward: -9.997747902937963, Done: False
2024-07-21 20:24:14,249 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:14,249 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:17,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.03014729544520378
2024-07-21 20:24:22,327 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.356528680012547, Velocity: -0.08702726406776798, Collision: 0, Height: -1.0, Movement: -0.052055706502028044, Smoothness: -0.0, Curiosity: 1.0926954746246338, Exploration: 0.01162467947300826, Total: -10.356575209489714
2024-07-21 20:24:22,422 - AirSimEnvLogger - INFO - Action: [-0.26027853 -0.26027853 -0.26027853 -0.26027853], Velocity: (-0.2602785325101402, -0.2602785325101402, -0.2602785325101402), Duration: 0.5, Reward: -10.356575209489714, Done: False
2024-07-21 20:24:22,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:22,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:25,420 - AirSimEnvLogger - INFO - Predictive model loss: 0.018636712804436684
2024-07-21 20:24:30,435 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.485437317447117, Velocity: -0.1674093457032207, Collision: 0, Height: -1.0, Movement: -0.10218727088025598, Smoothness: -0.0, Curiosity: 2.250913381576538, Exploration: 0.0097355668643726, Total: -9.951956913225608
2024-07-21 20:24:30,528 - AirSimEnvLogger - INFO - Action: [-0.51093635 -0.51093635 -0.51093635 -0.51093635], Velocity: (-0.5109363544012798, -0.5109363544012798, -0.5109363544012798), Duration: 0.5, Reward: -9.951956913225608, Done: False
2024-07-21 20:24:30,575 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:30,575 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:33,711 - AirSimEnvLogger - INFO - Predictive model loss: 0.028729159384965897
2024-07-21 20:24:38,791 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.195284212248186, Velocity: -0.020542336682349182, Collision: 0, Height: -1.0, Movement: -0.005591341376049397, Smoothness: -0.0, Curiosity: 0.680039644241333, Exploration: 0.01919795107869957, Total: -10.36225510239056
2024-07-21 20:24:38,853 - AirSimEnvLogger - INFO - Action: [0.02795671 0.02795671 0.02795671 0.02795671], Velocity: (0.027956706880246984, 0.027956706880246984, 0.027956706880246984), Duration: 0.5, Reward: -10.36225510239056, Done: False
2024-07-21 20:24:38,916 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:38,916 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:41,881 - AirSimEnvLogger - INFO - Predictive model loss: 0.007631292566657066
2024-07-21 20:24:46,884 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.896416283851632, Velocity: -0.10573157755684781, Collision: 0, Height: -1.0, Movement: -0.06724989241348857, Smoothness: -0.0, Curiosity: 0.9191684722900391, Exploration: 0.07083175228696086, Total: -9.982256475268994
2024-07-21 20:24:47,009 - AirSimEnvLogger - INFO - Action: [0.33624946 0.33624946 0.33624946 0.33624946], Velocity: (0.33624946206744283, 0.33624946206744283, 0.33624946206744283), Duration: 0.5, Reward: -9.982256475268994, Done: False
2024-07-21 20:24:47,073 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:47,073 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:50,118 - AirSimEnvLogger - INFO - Predictive model loss: 0.006637121085077524
2024-07-21 20:24:54,925 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.740760933363505, Velocity: -0.13303389984052777, Collision: 0, Height: -1.0, Movement: -0.07923360976570425, Smoothness: -0.0, Curiosity: 1.0783147811889648, Exploration: 0.04989704568201597, Total: -9.766064444529455
2024-07-21 20:24:55,068 - AirSimEnvLogger - INFO - Action: [0.39616805 0.39616805 0.39616805 0.39616805], Velocity: (0.3961680488285212, 0.3961680488285212, 0.3961680488285212), Duration: 0.5, Reward: -9.766064444529455, Done: False
2024-07-21 20:24:55,177 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:24:55,178 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:24:58,107 - AirSimEnvLogger - INFO - Predictive model loss: 0.013336333446204662
2024-07-21 20:25:02,573 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.997993048383153, Velocity: -0.04810105463346957, Collision: 0, Height: -1.0, Movement: -0.026059532597692916, Smoothness: -0.0, Curiosity: 0.6473649144172668, Exploration: 0.021237827029773722, Total: -10.196719506345069
2024-07-21 20:25:02,667 - AirSimEnvLogger - INFO - Action: [-0.13029766 -0.13029766 -0.13029766 -0.13029766], Velocity: (-0.13029766298846457, -0.13029766298846457, -0.13029766298846457), Duration: 0.5, Reward: -10.196719506345069, Done: False
2024-07-21 20:25:02,730 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:25:02,730 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:25:05,758 - AirSimEnvLogger - INFO - Predictive model loss: 0.0037898668088018894
2024-07-21 20:25:10,806 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.876611358303563, Velocity: -0.06871770929729708, Collision: 0, Height: -1.0, Movement: -0.043720572970791606, Smoothness: -0.0, Curiosity: 0.6063529849052429, Exploration: 0.015191278453043223, Total: -10.10912752210606
2024-07-21 20:25:10,839 - AirSimEnvLogger - INFO - Action: [0.21860286 0.21860286 0.21860286 0.21860286], Velocity: (0.21860286485395802, 0.21860286485395802, 0.21860286485395802), Duration: 0.5, Reward: -10.10912752210606, Done: False
2024-07-21 20:25:10,901 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:25:10,901 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:25:13,879 - AirSimEnvLogger - INFO - Predictive model loss: 0.009429915808141232
2024-07-21 20:25:37,633 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:25:37,654 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:25:37,655 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "difficulty_increment": 1,
    "difficulty_threshold": 200
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:25:39,244 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:25:39,245 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:25:43,648 - AirSimEnvLogger - ERROR - An error occurred in main: __init__() takes 2 positional arguments but 4 were given
Traceback (most recent call last):
  File "e:\Project\Drone\train.py", line 129, in main
    ppo_agent = PPOAgent(config, logger=logger, drone_controller=env.envs[0].drone_controller)
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 77, in __init__
    self.setup_training_components()
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 162, in setup_training_components
    self.setup_curriculum_learning()
  File "e:\Project\Drone\source\models\ppo\ppo_agent.py", line 181, in setup_curriculum_learning
    self.curriculum = CurriculumLearning(
TypeError: __init__() takes 2 positional arguments but 4 were given
2024-07-21 20:25:44,386 - AirSimEnvLogger - INFO - Environment closed.
2024-07-21 20:25:44,450 - AirSimEnvLogger - INFO - Training script execution finished.
2024-07-21 20:28:52,831 - AirSimEnvLogger - INFO - Initialized with state_dim: 16, action_dim: 4
2024-07-21 20:28:52,858 - AirSimEnvLogger - INFO - Initializing AirSimEnv
2024-07-21 20:28:52,858 - AirSimEnvLogger - INFO - Configuration: {
  "learning_rate": 0.0001,
  "gamma": 0.99,
  "tau": 0.95,
  "batch_size": 64,
  "num_timesteps": 2000000,
  "ppo": {
    "learning_rate": 0.0003,
    "n_steps": 2048,
    "batch_size": 64,
    "n_epochs": 2000,
    "gamma": 0.99,
    "gae_lambda": 0.95,
    "clip_range": 0.3,
    "ent_coef": 0.02,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
    "use_sde": true,
    "sde_sample_freq": 4,
    "tensorboard_log": "logs/tensorboard_logs/ppo_airsim_tensorboard/",
    "verbose": 1,
    "seed": null,
    "device": "auto",
    "continuous": true,
    "save_freq": 10000
  },
  "environment": {
    "env_name": "Africa_001",
    "reward_threshold": 250,
    "max_episode_steps": 1000,
    "state_dim": 16,
    "action_dim": 4,
    "reward_scale": 20,
    "proximity_threshold": 5.0,
    "collision_penalty": 25,
    "height_target": -10,
    "height_tolerance": 1.0,
    "height_penalty": 1,
    "movement_penalty": 0.1,
    "smoothness_penalty": 0.1,
    "duration": 0.1,
    "exploration_area": {
      "x_min": -2000,
      "x_max": 2000,
      "y_min": -2000,
      "y_max": 2000,
      "z_min": -200,
      "z_max": 200
    }
  },
  "policy_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256,
      256
    ],
    "output_size": 4,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": false,
    "num_action_heads": 1
  },
  "critic_network": {
    "input_size": 16,
    "hidden_layers": [
      256,
      256
    ],
    "output_size": 1,
    "use_batch_norm": true,
    "use_dropout": true,
    "dropout_rate": 0.2,
    "use_attention": true
  },
  "exploration": {
    "strategy": "epsilon_decay",
    "initial_epsilon": 1.0,
    "min_epsilon": 0.05,
    "epsilon_decay_rate": 0.9995
  },
  "model_checkpointing": {
    "checkpoint_interval": 10,
    "save_best_only": true,
    "checkpoint_dir": "models/checkpoints"
  },
  "logging": {
    "log_interval": 10,
    "log_dir": "logs/",
    "tensorboard": true,
    "tensorboard_log_dir": "logs/tensorboard_logs",
    "model_save_path": "models/saved_models"
  },
  "advanced_training_techniques": {
    "gradient_clipping": 0.5,
    "use_gae": true,
    "gae_lambda": 0.95,
    "normalize_advantages": true
  },
  "early_stopping": {
    "patience": 10
  },
  "reward_adjustments": {
    "collision_penalty": 50,
    "reward_threshold": 250
  },
  "shared_components": {
    "residual_block": {
      "input_dim": 128,
      "hidden_dim": 128,
      "dropout_rate": 0.2
    },
    "attention_layer": {
      "input_dim": 128,
      "hidden_dim": 128
    }
  },
  "icm": {
    "state_dim": 16,
    "action_dim": 4,
    "image_channels": 3,
    "image_height": 144,
    "image_width": 256,
    "cnn": {
      "input_channels": 3,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    },
    "state_encoder": {
      "hidden_dim": 128
    },
    "forward_model": {
      "hidden_dim": 256
    },
    "inverse_model": {
      "hidden_dim": 256
    }
  },
  "predictive_model": {
    "learning_rate": 0.0001,
    "hidden_layers": [
      256,
      256
    ],
    "cnn_config": {
      "input_channels": 3,
      "image_height": 144,
      "image_width": 256,
      "conv1": {
        "out_channels": 32,
        "kernel_size": 8,
        "stride": 4,
        "padding": 0
      },
      "conv2": {
        "out_channels": 64,
        "kernel_size": 4,
        "stride": 2,
        "padding": 0
      },
      "conv3": {
        "out_channels": 64,
        "kernel_size": 3,
        "stride": 1,
        "padding": 0
      }
    }
  },
  "hrl": {
    "use_hierarchical": true,
    "high_level_policy": {
      "input_size": 16,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    },
    "sub_goal_dim": 3,
    "low_level_policy": {
      "input_size": 19,
      "hidden_layers": [
        256,
        256
      ],
      "output_size": 4
    }
  },
  "curriculum_learning": {
    "use_curriculum": true,
    "initial_difficulty": 1,
    "max_difficulty": 10,
    "difficulty_increment": 0.5,
    "reward_threshold": 50
  },
  "multi_agent": {
    "use_multi_agent": true,
    "num_agents": 2,
    "hidden_layers": [
      256,
      256
    ]
  }
}
2024-07-21 20:28:54,388 - AirSimEnvLogger - INFO - DataVisualizer initialized with Seaborn style set to 'whitegrid'.
2024-07-21 20:28:54,388 - AirSimEnvLogger - INFO - Using device: cpu
2024-07-21 20:29:00,044 - AirSimEnvLogger - INFO - PPOAgent initialized successfully.
2024-07-21 20:29:05,030 - AirSimEnvLogger - INFO - Environment reset and takeoff completed.
2024-07-21 20:29:05,656 - AirSimEnvLogger - INFO - Reset completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:05,656 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:11,671 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.614561102824306, Velocity: -0.17204010711519885, Movement: 0.43252655286410635, Collision: 0, Height: -1.0, Movement Penalty: -0.09988772869110107, Smoothness: -0.0, Curiosity: 1.3084137439727783, Exploration: 0, Total: -8.812741516507623
2024-07-21 20:29:11,813 - AirSimEnvLogger - INFO - Action: [-0.49943864 -0.49943864 -0.49943864 -0.49943864], Velocity: (-0.49943864345550537, -0.49943864345550537, -0.49943864345550537), Duration: 1.0, Reward: -8.812741516507623, Done: False
2024-07-21 20:29:11,874 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:11,874 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:15,361 - AirSimEnvLogger - INFO - Predictive model loss: 0.025348786264657974
2024-07-21 20:29:20,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.09595598087787, Velocity: -0.031859382348853646, Movement: 0.172019898283861, Collision: 0, Height: -1.0, Movement Penalty: -0.03972629383206368, Smoothness: -0.0, Curiosity: 0.35525965690612793, Exploration: 0.8214963628684021, Total: -10.050718459903187
2024-07-21 20:29:21,151 - AirSimEnvLogger - INFO - Action: [-0.19863147 -0.19863147 -0.19863147 -0.19863147], Velocity: (-0.19863146916031837, -0.19863146916031837, -0.19863146916031837), Duration: 1.0, Reward: -10.050718459903187, Done: False
2024-07-21 20:29:21,230 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:21,230 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:24,071 - AirSimEnvLogger - INFO - Predictive model loss: 0.0093222726136446
2024-07-21 20:29:29,864 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.543088378539114, Velocity: -0.04395662180476603, Movement: 0.5055195494825352, Collision: 0, Height: -1.0, Movement Penalty: -0.116744739189744, Smoothness: -0.0, Curiosity: 2.5467934608459473, Exploration: 0.31949596881469305, Total: -8.41480613971358
2024-07-21 20:29:29,880 - AirSimEnvLogger - INFO - Action: [-0.5837237 -0.5837237 -0.5837237 -0.5837237], Velocity: (-0.58372369594872, -0.58372369594872, -0.58372369594872), Duration: 1.0, Reward: -8.41480613971358, Done: False
2024-07-21 20:29:29,958 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:29,958 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:32,802 - AirSimEnvLogger - INFO - Predictive model loss: 0.06253871321678162
2024-07-21 20:29:38,620 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.445492714002768, Velocity: -0.056614282887171095, Movement: 0.31914719788897994, Collision: 0, Height: -1.0, Movement Penalty: -0.07370388824492694, Smoothness: -0.0, Curiosity: 1.7457008361816406, Exploration: 0.20451702925259277, Total: -9.150585725799754
2024-07-21 20:29:38,715 - AirSimEnvLogger - INFO - Action: [-0.36851944 -0.36851944 -0.36851944 -0.36851944], Velocity: (-0.36851944122463465, -0.36851944122463465, -0.36851944122463465), Duration: 1.0, Reward: -9.150585725799754, Done: False
2024-07-21 20:29:38,824 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:38,824 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:41,454 - AirSimEnvLogger - INFO - Predictive model loss: 0.06001318246126175
2024-07-21 20:29:47,760 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.965104107547987, Velocity: -0.023583893375421803, Movement: 0.13292933551547936, Collision: 0, Height: -1.0, Movement Penalty: -0.030698715057224036, Smoothness: -0.0, Curiosity: 0.49454012513160706, Exploration: 0.1996930112599288, Total: -9.923732229720583
2024-07-21 20:29:47,914 - AirSimEnvLogger - INFO - Action: [0.15349358 0.15349358 0.15349358 0.15349358], Velocity: (0.15349357528612018, 0.15349357528612018, 0.15349357528612018), Duration: 1.0, Reward: -9.923732229720583, Done: False
2024-07-21 20:29:48,008 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:48,008 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:50,932 - AirSimEnvLogger - INFO - Predictive model loss: 0.015042601153254509
2024-07-21 20:29:56,777 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.734351846264337, Velocity: -0.039197440587441054, Movement: 0.2816095123579682, Collision: 0, Height: -1.0, Movement Penalty: -0.06503493110649287, Smoothness: -0.0, Curiosity: 0.6387184858322144, Exploration: 0.2868347350637472, Total: -9.526673249249335
2024-07-21 20:29:56,871 - AirSimEnvLogger - INFO - Action: [0.32517466 0.32517466 0.32517466 0.32517466], Velocity: (0.3251746555324644, 0.3251746555324644, 0.3251746555324644), Duration: 1.0, Reward: -9.526673249249335, Done: False
2024-07-21 20:29:56,933 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:29:56,933 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:29:59,753 - AirSimEnvLogger - INFO - Predictive model loss: 0.008507788181304932
2024-07-21 20:30:05,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.41734830386744, Velocity: -0.08504731025089685, Movement: 0.5722110963510545, Collision: 0, Height: -1.0, Movement Penalty: -0.1321464922046289, Smoothness: -0.0, Curiosity: 2.168203115463257, Exploration: 0.2863198516194808, Total: -7.6777600461699596
2024-07-21 20:30:06,054 - AirSimEnvLogger - INFO - Action: [0.66073246 0.66073246 0.66073246 0.66073246], Velocity: (0.6607324610231444, 0.6607324610231444, 0.6607324610231444), Duration: 1.0, Reward: -7.6777600461699596, Done: False
2024-07-21 20:30:06,132 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:06,132 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:08,998 - AirSimEnvLogger - INFO - Predictive model loss: 0.0352761335670948
2024-07-21 20:30:14,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.633585845248902, Velocity: -0.05070123260554181, Movement: 0.28256172830165366, Collision: 0, Height: -1.0, Movement Penalty: -0.06525483595905826, Smoothness: -0.0, Curiosity: 0.6191284656524658, Exploration: 0.15202173206676414, Total: -9.477631280498931
2024-07-21 20:30:15,101 - AirSimEnvLogger - INFO - Action: [0.32627418 0.32627418 0.32627418 0.32627418], Velocity: (0.3262741797952913, 0.3262741797952913, 0.3262741797952913), Duration: 1.0, Reward: -9.477631280498931, Done: False
2024-07-21 20:30:15,164 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:15,164 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:18,240 - AirSimEnvLogger - INFO - Predictive model loss: 0.019899848848581314
2024-07-21 20:30:24,039 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.230179915803276, Velocity: -0.028412664816001222, Movement: 0.2898189099692195, Collision: 0, Height: -1.0, Movement Penalty: -0.06693081027478912, Smoothness: -0.0, Curiosity: 0.6889470219612122, Exploration: 0.3037390916654122, Total: -9.967104926982325
2024-07-21 20:30:24,133 - AirSimEnvLogger - INFO - Action: [-0.33465405 -0.33465405 -0.33465405 -0.33465405], Velocity: (-0.33465405137394555, -0.33465405137394555, -0.33465405137394555), Duration: 1.0, Reward: -9.967104926982325, Done: False
2024-07-21 20:30:24,197 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:24,197 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:26,922 - AirSimEnvLogger - INFO - Predictive model loss: 0.006392229348421097
2024-07-21 20:30:32,915 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.427636607264208, Velocity: -0.06653714220041684, Movement: 0.3827301991316984, Collision: 0, Height: -1.0, Movement Penalty: -0.08838775339827408, Smoothness: -0.0, Curiosity: 1.4803942441940308, Exploration: 0.35054334085511396, Total: -9.36388275771424
2024-07-21 20:30:33,055 - AirSimEnvLogger - INFO - Action: [-0.44193877 -0.44193877 -0.44193877 -0.44193877], Velocity: (-0.44193876699137036, -0.44193876699137036, -0.44193876699137036), Duration: 1.0, Reward: -9.36388275771424, Done: False
2024-07-21 20:30:33,118 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:33,118 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:36,104 - AirSimEnvLogger - INFO - Predictive model loss: 0.026096640154719353
2024-07-21 20:30:41,974 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.718495184924288, Velocity: -0.051860624311785376, Movement: 0.6018784651678085, Collision: 0, Height: -1.0, Movement Penalty: -0.13899787755362922, Smoothness: -0.0, Curiosity: 3.928715229034424, Exploration: 0.283468990541664, Total: -7.215004104149782
2024-07-21 20:30:42,099 - AirSimEnvLogger - INFO - Action: [-0.69498939 -0.69498939 -0.69498939 -0.69498939], Velocity: (-0.6949893877681461, -0.6949893877681461, -0.6949893877681461), Duration: 1.0, Reward: -7.215004104149782, Done: False
2024-07-21 20:30:42,178 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:42,178 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:45,035 - AirSimEnvLogger - INFO - Predictive model loss: 0.09066463261842728
2024-07-21 20:30:51,088 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.591789590965142, Velocity: -0.07682168785255002, Movement: 0.40504462976746264, Collision: 0, Height: -1.0, Movement Penalty: -0.09354105041202275, Smoothness: -0.0, Curiosity: 2.9765429496765137, Exploration: 0.19580740697178026, Total: -8.06795738350894
2024-07-21 20:30:51,167 - AirSimEnvLogger - INFO - Action: [-0.46770525 -0.46770525 -0.46770525 -0.46770525], Velocity: (-0.4677052520601137, -0.4677052520601137, -0.4677052520601137), Duration: 1.0, Reward: -8.06795738350894, Done: False
2024-07-21 20:30:51,230 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:30:51,230 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:30:54,277 - AirSimEnvLogger - INFO - Predictive model loss: 0.08942160755395889
2024-07-21 20:31:00,131 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.359171462692844, Velocity: -0.0333073886754857, Movement: 0.19166784607429332, Collision: 0, Height: -1.0, Movement Penalty: -0.044263793010395604, Smoothness: -0.0, Curiosity: 2.013113021850586, Exploration: 0.1159656159085534, Total: -8.818015486105635
2024-07-21 20:31:00,240 - AirSimEnvLogger - INFO - Action: [-0.22131897 -0.22131897 -0.22131897 -0.22131897], Velocity: (-0.221318965051978, -0.221318965051978, -0.221318965051978), Duration: 1.0, Reward: -8.818015486105635, Done: False
2024-07-21 20:31:00,302 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:00,302 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:03,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.06683352589607239
2024-07-21 20:31:08,937 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.718165560861038, Velocity: -0.10198687916427925, Movement: 0.5288375399613315, Collision: 0, Height: -1.0, Movement Penalty: -0.12212979842170171, Smoothness: -0.0, Curiosity: 5.364080429077148, Exploration: 0.16083729353692058, Total: -5.814318885803092
2024-07-21 20:31:09,014 - AirSimEnvLogger - INFO - Action: [-0.61064899 -0.61064899 -0.61064899 -0.61064899], Velocity: (-0.6106489921085085, -0.6106489921085085, -0.6106489921085085), Duration: 1.0, Reward: -5.814318885803092, Done: False
2024-07-21 20:31:09,077 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:09,077 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:11,835 - AirSimEnvLogger - INFO - Predictive model loss: 0.1640063375234604
2024-07-21 20:31:17,939 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.133665583643483, Velocity: -0.011597582462700698, Movement: 0.06741513329434626, Collision: 0, Height: -1.0, Movement Penalty: -0.015568858141978127, Smoothness: -0.0, Curiosity: 2.0217602252960205, Exploration: 0.08065090177106472, Total: -8.592861719596169
2024-07-21 20:31:18,049 - AirSimEnvLogger - INFO - Action: [0.07784429 0.07784429 0.07784429 0.07784429], Velocity: (0.07784429070989063, 0.07784429070989063, 0.07784429070989063), Duration: 1.0, Reward: -8.592861719596169, Done: False
2024-07-21 20:31:18,112 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:18,112 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:20,965 - AirSimEnvLogger - INFO - Predictive model loss: 0.05900371074676514
2024-07-21 20:31:27,134 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.628781140390766, Velocity: -0.07014283172988799, Movement: 0.4433862766205411, Collision: 0, Height: -1.0, Movement Penalty: -0.10239567446474213, Smoothness: -0.0, Curiosity: 2.0288023948669434, Exploration: 0.420730734986058, Total: -7.999348718155512
2024-07-21 20:31:27,230 - AirSimEnvLogger - INFO - Action: [0.51197837 0.51197837 0.51197837 0.51197837], Velocity: (0.5119783723237106, 0.5119783723237106, 0.5119783723237106), Duration: 1.0, Reward: -7.999348718155512, Done: False
2024-07-21 20:31:27,291 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:27,291 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:30,188 - AirSimEnvLogger - INFO - Predictive model loss: 0.01884705200791359
2024-07-21 20:31:36,101 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.233294417545078, Velocity: -0.03481325820519606, Movement: 0.20491151235037353, Collision: 0, Height: -1.0, Movement Penalty: -0.04732228672621659, Smoothness: -0.0, Curiosity: 2.351763963699341, Exploration: 0.037213424860465485, Total: -8.371426223582406
2024-07-21 20:31:36,211 - AirSimEnvLogger - INFO - Action: [-0.23661143 -0.23661143 -0.23661143 -0.23661143], Velocity: (-0.23661143363108295, -0.23661143363108295, -0.23661143363108295), Duration: 1.0, Reward: -8.371426223582406, Done: False
2024-07-21 20:31:36,304 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:36,304 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:39,109 - AirSimEnvLogger - INFO - Predictive model loss: 0.04590431600809097
2024-07-21 20:31:45,048 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.471190853209238, Velocity: -0.06139418065103781, Movement: 0.3210448876623169, Collision: 0, Height: -1.0, Movement Penalty: -0.07414214092551674, Smoothness: -0.0, Curiosity: 3.6751229763031006, Exploration: 0.3405934902000527, Total: -7.215995578415264
2024-07-21 20:31:45,157 - AirSimEnvLogger - INFO - Action: [-0.3707107 -0.3707107 -0.3707107 -0.3707107], Velocity: (-0.3707107046275837, -0.3707107046275837, -0.3707107046275837), Duration: 1.0, Reward: -7.215995578415264, Done: False
2024-07-21 20:31:45,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:45,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:48,115 - AirSimEnvLogger - INFO - Predictive model loss: 0.07253023982048035
2024-07-21 20:31:53,943 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.236677462290691, Velocity: -0.009131293795834964, Movement: 0.05578784163490582, Collision: 0, Height: -1.0, Movement Penalty: -0.012883650154168437, Smoothness: -0.0, Curiosity: 2.523552179336548, Exploration: 0.03992770019046863, Total: -8.203486568538118
2024-07-21 20:31:54,084 - AirSimEnvLogger - INFO - Action: [-0.06441825 -0.06441825 -0.06441825 -0.06441825], Velocity: (-0.06441825077084218, -0.06441825077084218, -0.06441825077084218), Duration: 1.0, Reward: -8.203486568538118, Done: False
2024-07-21 20:31:54,148 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:31:54,148 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:31:57,057 - AirSimEnvLogger - INFO - Predictive model loss: 0.03453578054904938
2024-07-21 20:32:02,837 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.696362511390001, Velocity: -0.07142676156463436, Movement: 0.399646636408143, Collision: 0, Height: -1.0, Movement Penalty: -0.09229443724438796, Smoothness: -0.0, Curiosity: 2.048673629760742, Exploration: 0.3002641364503965, Total: -8.075967495927186
2024-07-21 20:32:02,917 - AirSimEnvLogger - INFO - Action: [0.46147219 0.46147219 0.46147219 0.46147219], Velocity: (0.46147218622193975, 0.46147218622193975, 0.46147218622193975), Duration: 1.0, Reward: -8.075967495927186, Done: False
2024-07-21 20:32:02,964 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:02,964 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:05,834 - AirSimEnvLogger - INFO - Predictive model loss: 0.013038602657616138
2024-07-21 20:32:11,879 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.3572623042127, Velocity: -0.06857666790060668, Movement: 0.6328034226115539, Collision: 0, Height: -1.0, Movement Penalty: -0.14613969055555884, Smoothness: -0.0, Curiosity: 2.8574273586273193, Exploration: 0.39599839843905177, Total: -6.900090346846621
2024-07-21 20:32:11,989 - AirSimEnvLogger - INFO - Action: [0.73069845 0.73069845 0.73069845 0.73069845], Velocity: (0.7306984527777942, 0.7306984527777942, 0.7306984527777942), Duration: 1.0, Reward: -6.900090346846621, Done: False
2024-07-21 20:32:12,084 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:12,084 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:14,969 - AirSimEnvLogger - INFO - Predictive model loss: 0.0657591000199318
2024-07-21 20:32:20,715 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.21083897288663, Velocity: -0.07486757994109092, Movement: 0.7191677671781368, Collision: 0, Height: -1.0, Movement Penalty: -0.16608468158911976, Smoothness: -0.0, Curiosity: 3.4153294563293457, Exploration: 0.31972586399105996, Total: -6.211818179223069
2024-07-21 20:32:20,840 - AirSimEnvLogger - INFO - Action: [0.83042341 0.83042341 0.83042341 0.83042341], Velocity: (0.8304234079455988, 0.8304234079455988, 0.8304234079455988), Duration: 1.0, Reward: -6.211818179223069, Done: False
2024-07-21 20:32:20,949 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:20,949 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:23,953 - AirSimEnvLogger - INFO - Predictive model loss: 0.14046615362167358
2024-07-21 20:32:30,158 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.347697681168865, Velocity: -0.08510850537275663, Movement: 0.5481472602464594, Collision: 0, Height: -1.0, Movement Penalty: -0.12658918730353969, Smoothness: -0.0, Curiosity: 2.091301679611206, Exploration: 0.20082053315539847, Total: -7.70527210668646
2024-07-21 20:32:30,220 - AirSimEnvLogger - INFO - Action: [0.63294594 0.63294594 0.63294594 0.63294594], Velocity: (0.6329459365176984, 0.6329459365176984, 0.6329459365176984), Duration: 1.0, Reward: -7.70527210668646, Done: False
2024-07-21 20:32:30,283 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:30,283 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:33,231 - AirSimEnvLogger - INFO - Predictive model loss: 0.15162122249603271
2024-07-21 20:32:39,336 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.246746485250018, Velocity: -0.1080877422443835, Movement: 0.7070319254455159, Collision: 0, Height: -1.0, Movement Penalty: -0.1632820289926512, Smoothness: -0.0, Curiosity: 3.7551214694976807, Exploration: 0.21313260465209402, Total: -5.936069835840747
2024-07-21 20:32:39,462 - AirSimEnvLogger - INFO - Action: [0.81641014 0.81641014 0.81641014 0.81641014], Velocity: (0.8164101449632559, 0.8164101449632559, 0.8164101449632559), Duration: 1.0, Reward: -5.936069835840747, Done: False
2024-07-21 20:32:39,493 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:39,493 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:42,301 - AirSimEnvLogger - INFO - Predictive model loss: 0.23846954107284546
2024-07-21 20:32:48,555 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.241814842157757, Velocity: -0.11856956523468636, Movement: 0.7332415330867411, Collision: 0, Height: -1.0, Movement Penalty: -0.16933487860345756, Smoothness: -0.0, Curiosity: 4.795995712280273, Exploration: 0.2843400134651814, Total: -4.874291507973545
2024-07-21 20:32:48,679 - AirSimEnvLogger - INFO - Action: [0.84667439 0.84667439 0.84667439 0.84667439], Velocity: (0.8466743930172878, 0.8466743930172878, 0.8466743930172878), Duration: 1.0, Reward: -4.874291507973545, Done: False
2024-07-21 20:32:48,741 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:48,741 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:32:51,594 - AirSimEnvLogger - INFO - Predictive model loss: 0.31259992718696594
2024-07-21 20:32:57,681 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.753334140564107, Velocity: -0.028753110906492886, Movement: 0.15516288329579186, Collision: 0, Height: -1.0, Movement Penalty: -0.03583333297562557, Smoothness: -0.0, Curiosity: 0.8935874104499817, Exploration: 0.15984342244949196, Total: -9.322051916867792
2024-07-21 20:32:57,759 - AirSimEnvLogger - INFO - Action: [0.17916666 0.17916666 0.17916666 0.17916666], Velocity: (0.17916666487812782, 0.17916666487812782, 0.17916666487812782), Duration: 1.0, Reward: -9.322051916867792, Done: False
2024-07-21 20:32:57,822 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:32:57,822 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:00,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.1797233670949936
2024-07-21 20:33:06,649 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.69983481558925, Velocity: -0.061708460826306946, Movement: 0.3394714406949829, Collision: 0, Height: -1.0, Movement Penalty: -0.07839757106697538, Smoothness: -0.0, Curiosity: 2.0683298110961914, Exploration: 0.14002807121445054, Total: -8.097142178032556
2024-07-21 20:33:06,774 - AirSimEnvLogger - INFO - Action: [0.39198786 0.39198786 0.39198786 0.39198786], Velocity: (0.3919878553348769, 0.3919878553348769, 0.3919878553348769), Duration: 1.0, Reward: -8.097142178032556, Done: False
2024-07-21 20:33:06,789 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:06,789 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:09,667 - AirSimEnvLogger - INFO - Predictive model loss: 0.22192926704883575
2024-07-21 20:33:15,615 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.593621670856056, Velocity: -0.08207795415018806, Movement: 0.49557447386426406, Collision: 0, Height: -1.0, Movement Penalty: -0.114448022355616, Smoothness: -0.0, Curiosity: 3.933405876159668, Exploration: 0.22617683209409475, Total: -6.104246503566576
2024-07-21 20:33:15,709 - AirSimEnvLogger - INFO - Action: [0.57224011 0.57224011 0.57224011 0.57224011], Velocity: (0.57224011177808, 0.57224011177808, 0.57224011177808), Duration: 1.0, Reward: -6.104246503566576, Done: False
2024-07-21 20:33:15,756 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:15,756 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:18,652 - AirSimEnvLogger - INFO - Predictive model loss: 0.2923852205276489
2024-07-21 20:33:24,431 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.619200099135835, Velocity: -0.031369626035921985, Movement: 0.486882147846044, Collision: 0, Height: -1.0, Movement Penalty: -0.11244061564901467, Smoothness: -0.0, Curiosity: 4.807024002075195, Exploration: 0.2050646943878461, Total: -5.256204987081088
2024-07-21 20:33:24,509 - AirSimEnvLogger - INFO - Action: [0.56220308 0.56220308 0.56220308 0.56220308], Velocity: (0.5622030782450733, 0.5622030782450733, 0.5622030782450733), Duration: 1.0, Reward: -5.256204987081088, Done: False
2024-07-21 20:33:24,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:24,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:27,503 - AirSimEnvLogger - INFO - Predictive model loss: 0.3368881940841675
2024-07-21 20:33:33,407 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.632057592610794, Velocity: -0.08885463270755607, Movement: 0.5499820906659039, Collision: 0, Height: -1.0, Movement Penalty: -0.12701292323817312, Smoothness: -0.0, Curiosity: 6.632537841796875, Exploration: 0.19017192757035073, Total: -3.451174639669354
2024-07-21 20:33:33,517 - AirSimEnvLogger - INFO - Action: [0.63506462 0.63506462 0.63506462 0.63506462], Velocity: (0.6350646161908655, 0.6350646161908655, 0.6350646161908655), Duration: 1.0, Reward: -3.451174639669354, Done: False
2024-07-21 20:33:33,548 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:33,548 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:36,490 - AirSimEnvLogger - INFO - Predictive model loss: 0.41913866996765137
2024-07-21 20:33:42,358 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.607485959773204, Velocity: -0.06385151530338198, Movement: 0.7006926191994957, Collision: 0, Height: -1.0, Movement Penalty: -0.16181802892560512, Smoothness: -0.0, Curiosity: 10.20175552368164, Exploration: 0.2623909597631615, Total: 0.1654222853487528
2024-07-21 20:33:42,466 - AirSimEnvLogger - INFO - Action: [0.80909014 0.80909014 0.80909014 0.80909014], Velocity: (0.8090901446280255, 0.8090901446280255, 0.8090901446280255), Duration: 1.0, Reward: 0.1654222853487528, Done: False
2024-07-21 20:33:42,591 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:42,591 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:45,471 - AirSimEnvLogger - INFO - Predictive model loss: 0.5714067816734314
2024-07-21 20:33:51,391 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.668484883932384, Velocity: -0.06178392762268092, Movement: 0.7591360606128643, Collision: 0, Height: -1.0, Movement Penalty: -0.17531496357855572, Smoothness: -0.0, Curiosity: 13.45190715789795, Exploration: 0.2954262440801932, Total: 3.363813484038266
2024-07-21 20:33:51,530 - AirSimEnvLogger - INFO - Action: [0.87657482 0.87657482 0.87657482 0.87657482], Velocity: (0.8765748178927786, 0.8765748178927786, 0.8765748178927786), Duration: 1.0, Reward: 3.363813484038266, Done: False
2024-07-21 20:33:51,594 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:33:51,594 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:33:54,379 - AirSimEnvLogger - INFO - Predictive model loss: 0.7347251772880554
2024-07-21 20:34:00,167 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.244450335714474, Velocity: -0.009071811706331506, Movement: 0.04949332992046609, Collision: 0, Height: -1.0, Movement Penalty: -0.011429994941068822, Smoothness: -0.0, Curiosity: 6.510580062866211, Exploration: 0.21277721310505415, Total: -4.184624626286403
2024-07-21 20:34:00,277 - AirSimEnvLogger - INFO - Action: [-0.05714997 -0.05714997 -0.05714997 -0.05714997], Velocity: (-0.05714997470534411, -0.05714997470534411, -0.05714997470534411), Duration: 1.0, Reward: -4.184624626286403, Done: False
2024-07-21 20:34:00,340 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:00,340 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:03,298 - AirSimEnvLogger - INFO - Predictive model loss: 0.45934391021728516
2024-07-21 20:34:09,108 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.612120937114641, Velocity: -0.07453689279416627, Movement: 0.4500172706556454, Collision: 0, Height: -1.0, Movement Penalty: -0.10392703694120703, Smoothness: -0.0, Curiosity: 5.270089149475098, Exploration: 0.5111645486588634, Total: -5.720878731306237
2024-07-21 20:34:09,217 - AirSimEnvLogger - INFO - Action: [-0.51963518 -0.51963518 -0.51963518 -0.51963518], Velocity: (-0.5196351847060351, -0.5196351847060351, -0.5196351847060351), Duration: 1.0, Reward: -5.720878731306237, Done: False
2024-07-21 20:34:09,311 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:09,311 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:12,147 - AirSimEnvLogger - INFO - Predictive model loss: 0.29111745953559875
2024-07-21 20:34:17,816 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.327188594178757, Velocity: -0.0054144294899325035, Movement: 0.027864507135563585, Collision: 0, Height: -1.0, Movement Penalty: -0.006435032278221554, Smoothness: -0.0, Curiosity: 5.548956394195557, Exploration: 0.0698178697966083, Total: -5.262032018019107
2024-07-21 20:34:17,910 - AirSimEnvLogger - INFO - Action: [0.03217516 0.03217516 0.03217516 0.03217516], Velocity: (0.032175161391107765, 0.032175161391107765, 0.032175161391107765), Duration: 1.0, Reward: -5.262032018019107, Done: False
2024-07-21 20:34:17,988 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:17,988 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:20,820 - AirSimEnvLogger - INFO - Predictive model loss: 0.36896443367004395
2024-07-21 20:34:26,707 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.023732397395483, Velocity: -0.05929601894161146, Movement: 0.4392709190947472, Collision: 0, Height: -1.0, Movement Penalty: -0.10144527335461065, Smoothness: -0.0, Curiosity: 9.56267261505127, Exploration: 0.3710533397877774, Total: -0.8708718240912268
2024-07-21 20:34:26,816 - AirSimEnvLogger - INFO - Action: [0.50722637 0.50722637 0.50722637 0.50722637], Velocity: (0.5072263667730532, 0.5072263667730532, 0.5072263667730532), Duration: 1.0, Reward: -0.8708718240912268, Done: False
2024-07-21 20:34:26,831 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:26,831 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:29,569 - AirSimEnvLogger - INFO - Predictive model loss: 0.5497702956199646
2024-07-21 20:34:35,406 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.228354097405301, Velocity: -0.009123207836564243, Movement: 0.06538471667746071, Collision: 0, Height: -1.0, Movement Penalty: -0.015099953509847741, Smoothness: -0.0, Curiosity: 7.2196879386901855, Exploration: 0.11191087243571692, Total: -3.4822350952586363
2024-07-21 20:34:35,500 - AirSimEnvLogger - INFO - Action: [0.07549977 0.07549977 0.07549977 0.07549977], Velocity: (0.0754997675492387, 0.0754997675492387, 0.0754997675492387), Duration: 1.0, Reward: -3.4822350952586363, Done: False
2024-07-21 20:34:35,594 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:35,594 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:38,468 - AirSimEnvLogger - INFO - Predictive model loss: 0.45757463574409485
2024-07-21 20:34:44,411 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.559011302018716, Velocity: -0.058535092611282466, Movement: 0.369160632436605, Collision: 0, Height: -1.0, Movement Penalty: -0.08525399620459456, Smoothness: -0.0, Curiosity: 5.49835205078125, Exploration: 0.3189610237149578, Total: -5.4840962346053255
2024-07-21 20:34:44,551 - AirSimEnvLogger - INFO - Action: [-0.42626998 -0.42626998 -0.42626998 -0.42626998], Velocity: (-0.42626998102297275, -0.42626998102297275, -0.42626998102297275), Duration: 1.0, Reward: -5.4840962346053255, Done: False
2024-07-21 20:34:44,582 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:44,582 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:47,468 - AirSimEnvLogger - INFO - Predictive model loss: 0.29908254742622375
2024-07-21 20:34:53,336 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.729780587894858, Velocity: -0.09299630109364256, Movement: 0.5630749508423306, Collision: 0, Height: -1.0, Movement Penalty: -0.13003658977710195, Smoothness: -0.0, Curiosity: 4.8486151695251465, Exploration: 0.3781669953094913, Total: -6.289674441328517
2024-07-21 20:34:53,478 - AirSimEnvLogger - INFO - Action: [-0.65018295 -0.65018295 -0.65018295 -0.65018295], Velocity: (-0.6501829488855098, -0.6501829488855098, -0.6501829488855098), Duration: 1.0, Reward: -6.289674441328517, Done: False
2024-07-21 20:34:53,557 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:34:53,557 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:34:56,596 - AirSimEnvLogger - INFO - Predictive model loss: 0.19447672367095947
2024-07-21 20:35:02,594 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.219571352940504, Velocity: -0.02079307160405111, Movement: 0.14868193432486662, Collision: 0, Height: -1.0, Movement Penalty: -0.03433662192243841, Smoothness: -0.0, Curiosity: 5.256308555603027, Exploration: 0.12202802365672506, Total: -5.433628499212574
2024-07-21 20:35:02,736 - AirSimEnvLogger - INFO - Action: [0.17168311 0.17168311 0.17168311 0.17168311], Velocity: (0.17168310961219202, 0.17168310961219202, 0.17168310961219202), Duration: 1.0, Reward: -5.433628499212574, Done: False
2024-07-21 20:35:02,799 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:02,799 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:05,808 - AirSimEnvLogger - INFO - Predictive model loss: 0.3167564570903778
2024-07-21 20:35:11,883 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.934085468094928, Velocity: -0.06690096526670297, Movement: 0.4861979014603195, Collision: 0, Height: -1.0, Movement Penalty: -0.11228259571501864, Smoothness: -0.0, Curiosity: 9.184707641601562, Exploration: 0.46750477678111185, Total: -1.1366154160134438
2024-07-21 20:35:12,010 - AirSimEnvLogger - INFO - Action: [0.56141298 0.56141298 0.56141298 0.56141298], Velocity: (0.5614129785750932, 0.5614129785750932, 0.5614129785750932), Duration: 1.0, Reward: -1.1366154160134438, Done: False
2024-07-21 20:35:12,074 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:12,074 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:15,111 - AirSimEnvLogger - INFO - Predictive model loss: 0.48864027857780457
2024-07-21 20:35:21,569 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.334060565791283, Velocity: -0.0282678085561455, Movement: 0.164224429661156, Collision: 0, Height: -1.0, Movement Penalty: -0.03792600746895247, Smoothness: -0.0, Curiosity: 5.214900970458984, Exploration: 0.04306878290499731, Total: -5.608052139701678
2024-07-21 20:35:21,616 - AirSimEnvLogger - INFO - Action: [-0.18963004 -0.18963004 -0.18963004 -0.18963004], Velocity: (-0.18963003734476236, -0.18963003734476236, -0.18963003734476236), Duration: 1.0, Reward: -5.608052139701678, Done: False
2024-07-21 20:35:21,679 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:21,679 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:24,633 - AirSimEnvLogger - INFO - Predictive model loss: 0.3057638108730316
2024-07-21 20:35:30,466 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.273928937616327, Velocity: -0.005316084805818667, Movement: 0.03276863839532664, Collision: 0, Height: -1.0, Movement Penalty: -0.0075675928794077365, Smoothness: -0.0, Curiosity: 5.776591777801514, Exploration: 0.17832638458545216, Total: -4.9560498869051
2024-07-21 20:35:30,559 - AirSimEnvLogger - INFO - Action: [0.03783796 0.03783796 0.03783796 0.03783796], Velocity: (0.03783796439703868, 0.03783796439703868, 0.03783796439703868), Duration: 1.0, Reward: -4.9560498869051, Done: False
2024-07-21 20:35:30,621 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:30,621 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:33,572 - AirSimEnvLogger - INFO - Predictive model loss: 0.3461875021457672
2024-07-21 20:35:39,498 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.484858129563948, Velocity: -0.05381864989488847, Movement: 0.31104618782361193, Collision: 0, Height: -1.0, Movement Penalty: -0.0718330401081477, Smoothness: -0.0, Curiosity: 4.512038707733154, Exploration: 0.10476846830244262, Total: -6.446474592869489
2024-07-21 20:35:39,620 - AirSimEnvLogger - INFO - Action: [-0.3591652 -0.3591652 -0.3591652 -0.3591652], Velocity: (-0.35916520054073847, -0.35916520054073847, -0.35916520054073847), Duration: 1.0, Reward: -6.446474592869489, Done: False
2024-07-21 20:35:39,636 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:39,636 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:42,502 - AirSimEnvLogger - INFO - Predictive model loss: 0.23288856446743011
2024-07-21 20:35:48,581 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.236403124998064, Velocity: -0.014197940735342241, Movement: 0.09259130616177112, Collision: 0, Height: -1.0, Movement Penalty: -0.021383046214847046, Smoothness: -0.0, Curiosity: 5.395857334136963, Exploration: 0.0277356777153388, Total: -5.333315119304884
2024-07-21 20:35:48,676 - AirSimEnvLogger - INFO - Action: [0.10691523 0.10691523 0.10691523 0.10691523], Velocity: (0.10691523107423523, 0.10691523107423523, 0.10691523107423523), Duration: 1.0, Reward: -5.333315119304884, Done: False
2024-07-21 20:35:48,706 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:48,706 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:35:51,704 - AirSimEnvLogger - INFO - Predictive model loss: 0.3128349184989929
2024-07-21 20:35:57,748 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.141984523649805, Velocity: -0.029710146041511284, Movement: 0.17328141321121446, Collision: 0, Height: -1.0, Movement Penalty: -0.040017628225221376, Smoothness: -0.0, Curiosity: 6.295618057250977, Exploration: 0.2072576787412071, Total: -4.297417631155542
2024-07-21 20:35:57,839 - AirSimEnvLogger - INFO - Action: [0.20008814 0.20008814 0.20008814 0.20008814], Velocity: (0.20008814112610687, 0.20008814112610687, 0.20008814112610687), Duration: 1.0, Reward: -4.297417631155542, Done: False
2024-07-21 20:35:57,871 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:35:57,871 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:00,822 - AirSimEnvLogger - INFO - Predictive model loss: 0.35609742999076843
2024-07-21 20:36:06,391 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.94718331827469, Velocity: -0.031160929583369332, Movement: 0.5196454333412281, Collision: 0, Height: -1.0, Movement Penalty: -0.12000697232908714, Smoothness: -0.0, Curiosity: 10.712013244628906, Exploration: 0.2543556877469361, Total: 0.33296251608090477
2024-07-21 20:36:06,517 - AirSimEnvLogger - INFO - Action: [0.60003486 0.60003486 0.60003486 0.60003486], Velocity: (0.6000348616454356, 0.6000348616454356, 0.6000348616454356), Duration: 1.0, Reward: 0.33296251608090477, Done: False
2024-07-21 20:36:06,566 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:06,566 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:09,560 - AirSimEnvLogger - INFO - Predictive model loss: 0.5408284068107605
2024-07-21 20:36:15,295 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.99825533483957, Velocity: -0.08450328698574917, Movement: 0.49139968748822666, Collision: 0, Height: -1.0, Movement Penalty: -0.11348389674041026, Smoothness: -0.0, Curiosity: 12.248441696166992, Exploration: 0.2656616957368196, Total: 1.8148922305264623
2024-07-21 20:36:15,374 - AirSimEnvLogger - INFO - Action: [0.56741948 0.56741948 0.56741948 0.56741948], Velocity: (0.5674194837020513, 0.5674194837020513, 0.5674194837020513), Duration: 1.0, Reward: 1.8148922305264623, Done: False
2024-07-21 20:36:15,421 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:15,421 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:18,354 - AirSimEnvLogger - INFO - Predictive model loss: 0.6340818405151367
2024-07-21 20:36:24,185 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.193710660980406, Velocity: -0.043961899150728165, Movement: 0.2658523805146375, Collision: 0, Height: -1.0, Movement Penalty: -0.06139597738193151, Smoothness: -0.0, Curiosity: 11.177961349487305, Exploration: 0.09676431590424357, Total: 0.508631635830886
2024-07-21 20:36:24,294 - AirSimEnvLogger - INFO - Action: [0.30697989 0.30697989 0.30697989 0.30697989], Velocity: (0.30697988690965755, 0.30697988690965755, 0.30697988690965755), Duration: 1.0, Reward: 0.508631635830886, Done: False
2024-07-21 20:36:24,324 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:24,324 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:27,223 - AirSimEnvLogger - INFO - Predictive model loss: 0.6135286688804626
2024-07-21 20:36:33,183 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.57625825778699, Velocity: -0.041816943613042265, Movement: 0.27931806823173816, Collision: 0, Height: -1.0, Movement Penalty: -0.06450574475324812, Smoothness: -0.0, Curiosity: 7.924701690673828, Exploration: 0.28557685560168694, Total: -3.083203927510258
2024-07-21 20:36:33,307 - AirSimEnvLogger - INFO - Action: [-0.32252872 -0.32252872 -0.32252872 -0.32252872], Velocity: (-0.3225287237662406, -0.3225287237662406, -0.3225287237662406), Duration: 1.0, Reward: -3.083203927510258, Done: False
2024-07-21 20:36:33,401 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:33,401 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:36,343 - AirSimEnvLogger - INFO - Predictive model loss: 0.408012330532074
2024-07-21 20:36:42,395 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.804228738086106, Velocity: -0.10714034801272092, Movement: 0.5660311405663147, Collision: 0, Height: -1.0, Movement Penalty: -0.13071929255026907, Smoothness: -0.0, Curiosity: 6.808797836303711, Exploration: 0.43521058130916396, Total: -4.392161789478408
2024-07-21 20:36:42,471 - AirSimEnvLogger - INFO - Action: [-0.65359646 -0.65359646 -0.65359646 -0.65359646], Velocity: (-0.6535964627513453, -0.6535964627513453, -0.6535964627513453), Duration: 1.0, Reward: -4.392161789478408, Done: False
2024-07-21 20:36:42,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:42,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:45,444 - AirSimEnvLogger - INFO - Predictive model loss: 0.26166319847106934
2024-07-21 20:36:51,193 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.727325992197185, Velocity: -0.07186379640741575, Movement: 0.45487791083125995, Collision: 0, Height: -1.0, Movement Penalty: -0.10504955370673702, Smoothness: -0.0, Curiosity: 5.325860023498535, Exploration: 0.24789773900626735, Total: -5.840477737704942
2024-07-21 20:36:51,333 - AirSimEnvLogger - INFO - Action: [-0.52524777 -0.52524777 -0.52524777 -0.52524777], Velocity: (-0.525247768533685, -0.525247768533685, -0.525247768533685), Duration: 1.0, Reward: -5.840477737704942, Done: False
2024-07-21 20:36:51,396 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:36:51,396 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:36:54,364 - AirSimEnvLogger - INFO - Predictive model loss: 0.20472322404384613
2024-07-21 20:37:00,095 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.795719978665607, Velocity: -0.0654335074852026, Movement: 0.6251206811943345, Collision: 0, Height: -1.0, Movement Penalty: -0.1443654374254205, Smoothness: -0.0, Curiosity: 4.737236022949219, Exploration: 0.2176811950381485, Total: -6.499626454951638
2024-07-21 20:37:00,143 - AirSimEnvLogger - INFO - Action: [-0.72182719 -0.72182719 -0.72182719 -0.72182719], Velocity: (-0.7218271871271025, -0.7218271871271025, -0.7218271871271025), Duration: 1.0, Reward: -6.499626454951638, Done: False
2024-07-21 20:37:00,236 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:00,237 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:03,229 - AirSimEnvLogger - INFO - Predictive model loss: 0.11787058413028717
2024-07-21 20:37:09,538 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.472580480997737, Velocity: -0.009847223410284848, Movement: 0.2285450403860728, Collision: 0, Height: -1.0, Movement Penalty: -0.052780216235541216, Smoothness: -0.0, Curiosity: 3.004380702972412, Exploration: 0.12687962659289523, Total: -7.934395988713706
2024-07-21 20:37:09,663 - AirSimEnvLogger - INFO - Action: [-0.26390108 -0.26390108 -0.26390108 -0.26390108], Velocity: (-0.26390108117770605, -0.26390108117770605, -0.26390108117770605), Duration: 1.0, Reward: -7.934395988713706, Done: False
2024-07-21 20:37:09,726 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:09,726 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:12,703 - AirSimEnvLogger - INFO - Predictive model loss: 0.10989519953727722
2024-07-21 20:37:18,880 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.129973418088763, Velocity: -0.022872749465423923, Movement: 0.1303600916909289, Collision: 0, Height: -1.0, Movement Penalty: -0.030105373611736843, Smoothness: -0.0, Curiosity: 3.851043701171875, Exploration: 0.27191679894241677, Total: -6.715478402075813
2024-07-21 20:37:18,988 - AirSimEnvLogger - INFO - Action: [0.15052687 0.15052687 0.15052687 0.15052687], Velocity: (0.1505268680586842, 0.1505268680586842, 0.1505268680586842), Duration: 1.0, Reward: -6.715478402075813, Done: False
2024-07-21 20:37:19,050 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:19,050 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:21,728 - AirSimEnvLogger - INFO - Predictive model loss: 0.15106631815433502
2024-07-21 20:37:27,975 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.844957215504504, Velocity: -0.07188397972423234, Movement: 0.4880999418715466, Collision: 0, Height: -1.0, Movement Penalty: -0.11272185313239125, Smoothness: -0.0, Curiosity: 7.556431293487549, Exploration: 0.35726398304827595, Total: -2.7015705385924544
2024-07-21 20:37:28,102 - AirSimEnvLogger - INFO - Action: [0.56360927 0.56360927 0.56360927 0.56360927], Velocity: (0.5636092656619562, 0.5636092656619562, 0.5636092656619562), Duration: 1.0, Reward: -2.7015705385924544, Done: False
2024-07-21 20:37:28,133 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:28,133 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:31,057 - AirSimEnvLogger - INFO - Predictive model loss: 0.26826566457748413
2024-07-21 20:37:37,242 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.746708396872053, Velocity: -0.0928120239054558, Movement: 0.6768146686863947, Collision: 0, Height: -1.0, Movement Penalty: -0.15630365246303096, Smoothness: -0.0, Curiosity: 11.760554313659668, Exploration: 0.3685677276949683, Total: 1.6059375191514917
2024-07-21 20:37:37,369 - AirSimEnvLogger - INFO - Action: [0.78151826 0.78151826 0.78151826 0.78151826], Velocity: (0.7815182623151548, 0.7815182623151548, 0.7815182623151548), Duration: 1.0, Reward: 1.6059375191514917, Done: False
2024-07-21 20:37:37,432 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:37,432 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:40,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.4064730405807495
2024-07-21 20:37:46,128 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.786842463819559, Velocity: -0.06298281456736445, Movement: 0.7710243497441234, Collision: 0, Height: -1.0, Movement Penalty: -0.17806044635061036, Smoothness: -0.0, Curiosity: 15.893390655517578, Exploration: 0.3218872272888035, Total: 5.69319715830658
2024-07-21 20:37:46,223 - AirSimEnvLogger - INFO - Action: [0.89030223 0.89030223 0.89030223 0.89030223], Velocity: (0.8903022317530516, 0.8903022317530516, 0.8903022317530516), Duration: 1.0, Reward: 5.69319715830658, Done: False
2024-07-21 20:37:46,302 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:46,302 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:49,237 - AirSimEnvLogger - INFO - Predictive model loss: 0.5511857271194458
2024-07-21 20:37:55,265 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -9.929591990481132, Velocity: -0.12408319516400437, Movement: 0.7581166788711797, Collision: 0, Height: -1.0, Movement Penalty: -0.17507954744936827, Smoothness: -0.0, Curiosity: 18.839519500732422, Exploration: 0.27643177073695746, Total: 8.47969505932167
2024-07-21 20:37:55,358 - AirSimEnvLogger - INFO - Action: [0.87539774 0.87539774 0.87539774 0.87539774], Velocity: (0.8753977372468413, 0.8753977372468413, 0.8753977372468413), Duration: 1.0, Reward: 8.47969505932167, Done: False
2024-07-21 20:37:55,419 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:37:55,419 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:37:58,355 - AirSimEnvLogger - INFO - Predictive model loss: 0.6697343587875366
2024-07-21 20:38:04,265 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.263634375397082, Velocity: -0.04706222143116578, Movement: 0.3436661914190566, Collision: 0, Height: -1.0, Movement Penalty: -0.07936630725086631, Smoothness: -0.0, Curiosity: 15.329765319824219, Exploration: 0.15520467254911302, Total: 4.605551910143551
2024-07-21 20:38:04,312 - AirSimEnvLogger - INFO - Action: [0.39683154 0.39683154 0.39683154 0.39683154], Velocity: (0.39683153625433154, 0.39683153625433154, 0.39683153625433154), Duration: 1.0, Reward: 4.605551910143551, Done: False
2024-07-21 20:38:04,374 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:04,374 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:07,307 - AirSimEnvLogger - INFO - Predictive model loss: 0.5655396580696106
2024-07-21 20:38:13,484 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.33296159851296, Velocity: -0.06937604397084343, Movement: 0.523187446523115, Collision: 0, Height: -1.0, Movement Penalty: -0.12082496523470135, Smoothness: -0.0, Curiosity: 19.604522705078125, Exploration: 0.1375474253172013, Total: 8.809093169886923
2024-07-21 20:38:13,595 - AirSimEnvLogger - INFO - Action: [0.60412483 0.60412483 0.60412483 0.60412483], Velocity: (0.6041248261735067, 0.6041248261735067, 0.6041248261735067), Duration: 1.0, Reward: 8.809093169886923, Done: False
2024-07-21 20:38:13,656 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:13,656 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:16,435 - AirSimEnvLogger - INFO - Predictive model loss: 0.672691285610199
2024-07-21 20:38:22,791 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.39211654039433, Velocity: -0.1140687740663304, Movement: 0.6879068032293478, Collision: 0, Height: -1.0, Movement Penalty: -0.15886527120873556, Smoothness: -0.0, Curiosity: 25.361896514892578, Exploration: 0.28762792171149937, Total: 14.541401835933
2024-07-21 20:38:22,823 - AirSimEnvLogger - INFO - Action: [0.79432636 0.79432636 0.79432636 0.79432636], Velocity: (0.7943263560436777, 0.7943263560436777, 0.7943263560436777), Duration: 1.0, Reward: 14.541401835933, Done: False
2024-07-21 20:38:22,871 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:22,871 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:25,757 - AirSimEnvLogger - INFO - Predictive model loss: 0.8138813376426697
2024-07-21 20:38:31,673 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.548297300577307, Velocity: -0.08558847229069358, Movement: 0.6905872598186416, Collision: 0, Height: -1.0, Movement Penalty: -0.1594842961420875, Smoothness: -0.0, Curiosity: 29.304359436035156, Exploration: 0.27416499449400933, Total: 18.32750130526597
2024-07-21 20:38:31,783 - AirSimEnvLogger - INFO - Action: [0.79742148 0.79742148 0.79742148 0.79742148], Velocity: (0.7974214807104375, 0.7974214807104375, 0.7974214807104375), Duration: 1.0, Reward: 18.32750130526597, Done: False
2024-07-21 20:38:31,846 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:31,846 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:34,543 - AirSimEnvLogger - INFO - Predictive model loss: 0.8832322359085083
2024-07-21 20:38:40,798 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.717868990196664, Velocity: -0.10713322142737704, Movement: 0.774999480865727, Collision: 0, Height: -1.0, Movement Penalty: -0.17897846355985908, Smoothness: -0.0, Curiosity: 35.26026153564453, Exploration: 0.2650524687817851, Total: 24.111651986666214
2024-07-21 20:38:40,939 - AirSimEnvLogger - INFO - Action: [0.89489232 0.89489232 0.89489232 0.89489232], Velocity: (0.8948923177992953, 0.8948923177992953, 0.8948923177992953), Duration: 1.0, Reward: 24.111651986666214, Done: False
2024-07-21 20:38:40,987 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:40,987 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:43,959 - AirSimEnvLogger - INFO - Predictive model loss: 0.9865942001342773
2024-07-21 20:38:49,912 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -10.938298121809877, Velocity: -0.12641537302044817, Movement: 0.8183541946064129, Collision: 0, Height: -1.0, Movement Penalty: -0.18899080581938876, Smoothness: -0.0, Curiosity: 41.48979568481445, Exploration: 0.2959780429849216, Total: 30.127005159924266
2024-07-21 20:38:50,022 - AirSimEnvLogger - INFO - Action: [0.94495403 0.94495403 0.94495403 0.94495403], Velocity: (0.9449540290969438, 0.9449540290969438, 0.9449540290969438), Duration: 1.0, Reward: 30.127005159924266, Done: False
2024-07-21 20:38:50,100 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:50,100 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:38:52,993 - AirSimEnvLogger - INFO - Predictive model loss: 1.0623544454574585
2024-07-21 20:38:58,967 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.189283326745809, Velocity: -0.10468069653816107, Movement: 0.8359223584918374, Collision: 0, Height: -1.0, Movement Penalty: -0.19304799947875567, Smoothness: -0.0, Curiosity: 47.709228515625, Exploration: 0.2918542569775246, Total: 36.09710872877969
2024-07-21 20:38:59,046 - AirSimEnvLogger - INFO - Action: [0.96524 0.96524 0.96524 0.96524], Velocity: (0.9652399973937783, 0.9652399973937783, 0.9652399973937783), Duration: 1.0, Reward: 36.09710872877969, Done: False
2024-07-21 20:38:59,078 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:38:59,078 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:01,943 - AirSimEnvLogger - INFO - Predictive model loss: 1.0951104164123535
2024-07-21 20:39:07,865 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.466020478113315, Velocity: -0.11306729348107648, Movement: 0.7319357904159625, Collision: 0, Height: -1.0, Movement Penalty: -0.16903333025047096, Smoothness: -0.0, Curiosity: 51.25620651245117, Exploration: 0.2456349425028487, Total: 39.353329687298405
2024-07-21 20:39:07,990 - AirSimEnvLogger - INFO - Action: [0.84516665 0.84516665 0.84516665 0.84516665], Velocity: (0.8451666512523548, 0.8451666512523548, 0.8451666512523548), Duration: 1.0, Reward: 39.353329687298405, Done: False
2024-07-21 20:39:08,069 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:08,069 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:11,122 - AirSimEnvLogger - INFO - Predictive model loss: 1.0189988613128662
2024-07-21 20:39:16,380 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.773712479271538, Velocity: -0.008150325580048795, Movement: 0.04141967172432112, Collision: 0, Height: -1.0, Movement Penalty: -0.009565463447913092, Smoothness: -0.0, Curiosity: 41.93588638305664, Exploration: 0.23938381186902424, Total: 29.717433170274422
2024-07-21 20:39:16,504 - AirSimEnvLogger - INFO - Action: [0.04782732 0.04782732 0.04782732 0.04782732], Velocity: (0.04782731723956546, 0.04782731723956546, 0.04782731723956546), Duration: 1.0, Reward: 29.717433170274422, Done: False
2024-07-21 20:39:16,534 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:16,534 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:19,517 - AirSimEnvLogger - INFO - Predictive model loss: 0.6103212833404541
2024-07-21 20:39:25,681 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.909801881406544, Velocity: -0.045425299134504125, Movement: 0.21791487633683515, Collision: 0, Height: -1.0, Movement Penalty: -0.05032528500539832, Smoothness: -0.0, Curiosity: 38.34402084350586, Exploration: 0.3836525992773081, Total: 26.0232619702041
2024-07-21 20:39:25,805 - AirSimEnvLogger - INFO - Action: [-0.25162643 -0.25162643 -0.25162643 -0.25162643], Velocity: (-0.25162642502699156, -0.25162642502699156, -0.25162642502699156), Duration: 1.0, Reward: 26.0232619702041, Done: False
2024-07-21 20:39:25,821 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:25,821 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:28,375 - AirSimEnvLogger - INFO - Predictive model loss: 0.36582469940185547
2024-07-21 20:39:34,463 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.937564731579188, Velocity: -0.08797284179776198, Movement: 0.4851570859810311, Collision: 0, Height: -1.0, Movement Penalty: -0.11204222967616109, Smoothness: -0.0, Curiosity: 33.68349838256836, Exploration: 0.31489089389603003, Total: 21.32146214960475
2024-07-21 20:39:34,540 - AirSimEnvLogger - INFO - Action: [-0.56021115 -0.56021115 -0.56021115 -0.56021115], Velocity: (-0.5602111483808054, -0.5602111483808054, -0.5602111483808054), Duration: 1.0, Reward: 21.32146214960475, Done: False
2024-07-21 20:39:34,619 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:34,619 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:37,628 - AirSimEnvLogger - INFO - Predictive model loss: 0.20378659665584564
2024-07-21 20:39:43,603 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.748593363936234, Velocity: -0.004647930958857676, Movement: 0.0419951759026203, Collision: 0, Height: -1.0, Movement Penalty: -0.009698370444817406, Smoothness: -0.0, Curiosity: 36.81371307373047, Exploration: 0.05262960986593102, Total: 24.577789866371475
2024-07-21 20:39:43,742 - AirSimEnvLogger - INFO - Action: [0.04849185 0.04849185 0.04849185 0.04849185], Velocity: (0.04849185222408703, 0.04849185222408703, 0.04849185222408703), Duration: 1.0, Reward: 24.577789866371475, Done: False
2024-07-21 20:39:43,805 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:43,805 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:46,762 - AirSimEnvLogger - INFO - Predictive model loss: 0.12009178847074509
2024-07-21 20:39:52,806 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.689436057960004, Velocity: -0.014942918402538204, Movement: 0.08018152187785667, Collision: 0, Height: -1.0, Movement Penalty: -0.018517129296085767, Smoothness: -0.0, Curiosity: 37.74980545043945, Exploration: 0.22374076435905824, Total: 25.612302324856635
2024-07-21 20:39:52,899 - AirSimEnvLogger - INFO - Action: [0.09258565 0.09258565 0.09258565 0.09258565], Velocity: (0.09258564648042883, 0.09258564648042883, 0.09258564648042883), Duration: 1.0, Reward: 25.612302324856635, Done: False
2024-07-21 20:39:52,946 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:39:52,946 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:39:55,960 - AirSimEnvLogger - INFO - Predictive model loss: 0.13534444570541382
2024-07-21 20:40:01,995 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.678525053843462, Velocity: -0.04270806933882148, Movement: 0.24629905542715796, Collision: 0, Height: -1.0, Movement Penalty: -0.056880330380808086, Smoothness: -0.0, Curiosity: 41.33144760131836, Exploration: 0.1340744390974007, Total: 29.185530564102816
2024-07-21 20:40:02,167 - AirSimEnvLogger - INFO - Action: [0.28440165 0.28440165 0.28440165 0.28440165], Velocity: (0.28440165190404043, 0.28440165190404043, 0.28440165190404043), Duration: 1.0, Reward: 29.185530564102816, Done: False
2024-07-21 20:40:02,198 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:02,198 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:05,240 - AirSimEnvLogger - INFO - Predictive model loss: 0.17071551084518433
2024-07-21 20:40:11,161 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.71359810000335, Velocity: -0.06422001719428408, Movement: 0.5518150982109108, Collision: 0, Height: -1.0, Movement Penalty: -0.12743623819798766, Smoothness: -0.0, Curiosity: 49.84965515136719, Exploration: 0.27960286985124827, Total: 37.70747970436073
2024-07-21 20:40:11,254 - AirSimEnvLogger - INFO - Action: [0.63718119 0.63718119 0.63718119 0.63718119], Velocity: (0.6371811909899383, 0.6371811909899383, 0.6371811909899383), Duration: 1.0, Reward: 37.70747970436073, Done: False
2024-07-21 20:40:11,333 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:11,333 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:14,343 - AirSimEnvLogger - INFO - Predictive model loss: 0.13559414446353912
2024-07-21 20:40:20,374 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -11.856851349352352, Velocity: -0.11207911069748834, Movement: 0.6907318610875743, Collision: 0, Height: -1.0, Movement Penalty: -0.1595176903747049, Smoothness: -0.0, Curiosity: 57.88017272949219, Exploration: 0.3376379737323958, Total: 45.606713818393445
2024-07-21 20:40:20,516 - AirSimEnvLogger - INFO - Action: [0.79758845 0.79758845 0.79758845 0.79758845], Velocity: (0.7975884518735245, 0.7975884518735245, 0.7975884518735245), Duration: 1.0, Reward: 45.606713818393445, Done: False
2024-07-21 20:40:20,595 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:20,595 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:23,753 - AirSimEnvLogger - INFO - Predictive model loss: 0.07738036662340164
2024-07-21 20:40:29,847 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.0887010759276, Velocity: -0.11927994301593854, Movement: 0.7678337977910219, Collision: 0, Height: -1.0, Movement Penalty: -0.1773236199390157, Smoothness: -0.0, Curiosity: 66.11954498291016, Exploration: 0.3031180236937841, Total: 53.60746797985177
2024-07-21 20:40:29,864 - AirSimEnvLogger - INFO - Action: [0.8866181 0.8866181 0.8866181 0.8866181], Velocity: (0.8866180996950783, 0.8866180996950783, 0.8866180996950783), Duration: 1.0, Reward: 53.60746797985177, Done: False
2024-07-21 20:40:29,941 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:29,941 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:33,004 - AirSimEnvLogger - INFO - Predictive model loss: 0.02450638823211193
2024-07-21 20:40:39,094 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.341269559671538, Velocity: -0.010754695196674161, Movement: 0.04710929214537045, Collision: 0, Height: -1.0, Movement Penalty: -0.010879425000584942, Smoothness: -0.0, Curiosity: 53.80894088745117, Exploration: 0.2074042164769075, Total: 41.015454416449714
2024-07-21 20:40:39,251 - AirSimEnvLogger - INFO - Action: [-0.05439713 -0.05439713 -0.05439713 -0.05439713], Velocity: (-0.05439712500292471, -0.05439712500292471, -0.05439712500292471), Duration: 1.0, Reward: 41.015454416449714, Done: False
2024-07-21 20:40:39,330 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:39,330 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:42,193 - AirSimEnvLogger - INFO - Predictive model loss: 0.0649547278881073
2024-07-21 20:40:48,132 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.424485895194827, Velocity: -0.03311352461983217, Movement: 0.36900751955166244, Collision: 0, Height: -1.0, Movement Penalty: -0.08521863629845937, Smoothness: -0.0, Curiosity: 62.09170913696289, Exploration: 0.15173310370422025, Total: 49.207862237746056
2024-07-21 20:40:48,241 - AirSimEnvLogger - INFO - Action: [0.42609318 0.42609318 0.42609318 0.42609318], Velocity: (0.4260931814922968, 0.4260931814922968, 0.4260931814922968), Duration: 1.0, Reward: 49.207862237746056, Done: False
2024-07-21 20:40:48,304 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:48,305 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:40:51,223 - AirSimEnvLogger - INFO - Predictive model loss: 0.010988228023052216
2024-07-21 20:40:57,156 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.520101658078964, Velocity: -0.03774630999970765, Movement: 0.270657318194963, Collision: 0, Height: -1.0, Movement Penalty: -0.06250563020720164, Smoothness: -0.0, Curiosity: 63.294742584228516, Exploration: 0.19289470089830482, Total: 50.32187128579989
2024-07-21 20:40:57,298 - AirSimEnvLogger - INFO - Action: [0.31252815 0.31252815 0.31252815 0.31252815], Velocity: (0.3125281510360082, 0.3125281510360082, 0.3125281510360082), Duration: 1.0, Reward: 50.32187128579989, Done: False
2024-07-21 20:40:57,378 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:40:57,378 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:00,207 - AirSimEnvLogger - INFO - Predictive model loss: 0.008154264651238918
2024-07-21 20:41:06,143 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.651181081381475, Velocity: -0.08023716949642344, Movement: 0.5658584289020746, Collision: 0, Height: -1.0, Movement Penalty: -0.1306794064999326, Smoothness: -0.0, Curiosity: 73.27193450927734, Exploration: 0.19656177211876086, Total: 60.17181939532986
2024-07-21 20:41:06,301 - AirSimEnvLogger - INFO - Action: [0.65339703 0.65339703 0.65339703 0.65339703], Velocity: (0.653397032499663, 0.653397032499663, 0.653397032499663), Duration: 1.0, Reward: 60.17181939532986, Done: False
2024-07-21 20:41:06,348 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:06,348 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:09,321 - AirSimEnvLogger - INFO - Predictive model loss: 0.04230218753218651
2024-07-21 20:41:15,306 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.85173464964636, Velocity: -0.11243871105453804, Movement: 0.6770106345025905, Collision: 0, Height: -1.0, Movement Penalty: -0.156348908829724, Smoothness: -0.0, Curiosity: 82.16191864013672, Exploration: 0.3193466519808606, Total: 68.88899688375601
2024-07-21 20:41:15,414 - AirSimEnvLogger - INFO - Action: [0.78174454 0.78174454 0.78174454 0.78174454], Velocity: (0.7817445441486199, 0.7817445441486199, 0.7817445441486199), Duration: 1.0, Reward: 68.88899688375601, Done: False
2024-07-21 20:41:15,491 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:15,491 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:18,399 - AirSimEnvLogger - INFO - Predictive model loss: 0.10877823829650879
2024-07-21 20:41:24,139 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.009798732315442, Velocity: -0.012316445408699883, Movement: 0.057972959977671934, Collision: 0, Height: -1.0, Movement Penalty: -0.013388281619531317, Smoothness: -0.0, Curiosity: 70.23609924316406, Exploration: 0.16587328541471053, Total: 56.76464179554024
2024-07-21 20:41:24,280 - AirSimEnvLogger - INFO - Action: [-0.06694141 -0.06694141 -0.06694141 -0.06694141], Velocity: (-0.06694140809765659, -0.06694140809765659, -0.06694140809765659), Duration: 1.0, Reward: 56.76464179554024, Done: False
2024-07-21 20:41:24,342 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:24,342 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:27,147 - AirSimEnvLogger - INFO - Predictive model loss: 0.04557746276259422
2024-07-21 20:41:32,922 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.076822052943537, Velocity: -0.004668529996036015, Movement: 0.03531760125974579, Collision: 0, Height: -1.0, Movement Penalty: -0.00815625063777844, Smoothness: -0.0, Curiosity: 71.61782836914062, Exploration: 0.2503430868701845, Total: 58.098984712035104
2024-07-21 20:41:33,061 - AirSimEnvLogger - INFO - Action: [0.04078125 0.04078125 0.04078125 0.04078125], Velocity: (0.0407812531888922, 0.0407812531888922, 0.0407812531888922), Duration: 1.0, Reward: 58.098984712035104, Done: False
2024-07-21 20:41:33,077 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:33,077 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:36,074 - AirSimEnvLogger - INFO - Predictive model loss: 0.03434443846344948
2024-07-21 20:41:41,846 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.14794083562725, Velocity: -0.06596999276638357, Movement: 0.4367599645524541, Collision: 0, Height: -1.0, Movement Penalty: -0.1008653932422443, Smoothness: -0.0, Curiosity: 81.97900390625, Exploration: 0.21616104798239175, Total: 68.38489681079828
2024-07-21 20:41:41,955 - AirSimEnvLogger - INFO - Action: [0.50432697 0.50432697 0.50432697 0.50432697], Velocity: (0.5043269662112215, 0.5043269662112215, 0.5043269662112215), Duration: 1.0, Reward: 68.38489681079828, Done: False
2024-07-21 20:41:42,032 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:42,032 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:44,966 - AirSimEnvLogger - INFO - Predictive model loss: 0.04493115469813347
2024-07-21 20:41:50,970 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.31525059206794, Velocity: -0.059911214295022115, Movement: 0.6512316840815188, Collision: 0, Height: -1.0, Movement Penalty: -0.15039551524371128, Smoothness: -0.0, Curiosity: 93.00689697265625, Exploration: 0.36797658652000004, Total: 79.28626455220797
2024-07-21 20:41:51,111 - AirSimEnvLogger - INFO - Action: [0.75197758 0.75197758 0.75197758 0.75197758], Velocity: (0.7519775762185563, 0.7519775762185563, 0.7519775762185563), Duration: 1.0, Reward: 79.28626455220797, Done: False
2024-07-21 20:41:51,190 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:41:51,190 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:41:54,042 - AirSimEnvLogger - INFO - Predictive model loss: 0.06448247283697128
2024-07-21 20:41:59,976 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.422577546016003, Velocity: -0.009643593911179416, Movement: 0.07585668988856765, Collision: 0, Height: -1.0, Movement Penalty: -0.017518352130799397, Smoothness: -0.0, Curiosity: 80.68440246582031, Exploration: 0.12114434966070033, Total: 66.79058452135415
2024-07-21 20:42:00,008 - AirSimEnvLogger - INFO - Action: [-0.08759176 -0.08759176 -0.08759176 -0.08759176], Velocity: (-0.08759176065399699, -0.08759176065399699, -0.08759176065399699), Duration: 1.0, Reward: 66.79058452135415, Done: False
2024-07-21 20:42:00,070 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:00,070 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:03,025 - AirSimEnvLogger - INFO - Predictive model loss: 0.011695997789502144
2024-07-21 20:42:08,986 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.427624432376888, Velocity: -0.06439831047118943, Movement: 0.4051333582114853, Collision: 0, Height: -1.0, Movement Penalty: -0.09356154136843925, Smoothness: -0.0, Curiosity: 73.27825164794922, Exploration: 0.4445055504011453, Total: 59.45636156058286
2024-07-21 20:42:09,097 - AirSimEnvLogger - INFO - Action: [-0.46780771 -0.46780771 -0.46780771 -0.46780771], Velocity: (-0.46780770684219625, -0.46780770684219625, -0.46780770684219625), Duration: 1.0, Reward: 59.45636156058286, Done: False
2024-07-21 20:42:09,128 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:09,128 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:12,082 - AirSimEnvLogger - INFO - Predictive model loss: 0.060604456812143326
2024-07-21 20:42:17,740 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.32052854922621, Velocity: -0.08781680397194759, Movement: 0.481084492516909, Collision: 0, Height: -1.0, Movement Penalty: -0.11110170450303676, Smoothness: -0.0, Curiosity: 67.86429595947266, Exploration: 0.30276210435097034, Total: 54.116421990550826
2024-07-21 20:42:17,882 - AirSimEnvLogger - INFO - Action: [-0.55550852 -0.55550852 -0.55550852 -0.55550852], Velocity: (-0.5555085225151838, -0.5555085225151838, -0.5555085225151838), Duration: 1.0, Reward: 54.116421990550826, Done: False
2024-07-21 20:42:17,961 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:17,961 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:20,759 - AirSimEnvLogger - INFO - Predictive model loss: 0.132494255900383
2024-07-21 20:42:26,433 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.155420044457802, Velocity: -0.06380889684697126, Movement: 0.6529779859345771, Collision: 0, Height: -1.0, Movement Penalty: -0.1507988063816911, Smoothness: -0.0, Curiosity: 60.752830505371094, Exploration: 0.2853149729788383, Total: 47.17266952985372
2024-07-21 20:42:26,544 - AirSimEnvLogger - INFO - Action: [-0.75399403 -0.75399403 -0.75399403 -0.75399403], Velocity: (-0.7539940319084555, -0.7539940319084555, -0.7539940319084555), Duration: 1.0, Reward: 47.17266952985372, Done: False
2024-07-21 20:42:26,589 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:26,589 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:29,292 - AirSimEnvLogger - INFO - Predictive model loss: 0.2118525356054306
2024-07-21 20:42:34,954 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.978778338154914, Velocity: -0.01978017333979383, Movement: 0.10554400352260856, Collision: 0, Height: -1.0, Movement Penalty: -0.024374343538051547, Smoothness: -0.0, Curiosity: 67.76917266845703, Exploration: 0.16020960297806014, Total: 54.32785351037723
2024-07-21 20:42:35,080 - AirSimEnvLogger - INFO - Action: [0.12187172 0.12187172 0.12187172 0.12187172], Velocity: (0.12187171769025773, 0.12187171769025773, 0.12187171769025773), Duration: 1.0, Reward: 54.32785351037723, Done: False
2024-07-21 20:42:35,143 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:35,143 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:37,498 - AirSimEnvLogger - INFO - Predictive model loss: 0.09063160419464111
2024-07-21 20:42:43,290 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.916405680042391, Velocity: -0.0034281781096229887, Movement: 0.02289889726282821, Collision: 0, Height: -1.0, Movement Penalty: -0.005288273799535781, Smoothness: -0.0, Curiosity: 66.00225067138672, Exploration: 0.2610973062503205, Total: 52.64611626268797
2024-07-21 20:42:43,399 - AirSimEnvLogger - INFO - Action: [-0.02644137 -0.02644137 -0.02644137 -0.02644137], Velocity: (-0.026441368997678905, -0.026441368997678905, -0.026441368997678905), Duration: 1.0, Reward: 52.64611626268797, Done: False
2024-07-21 20:42:43,431 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:43,431 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:46,463 - AirSimEnvLogger - INFO - Predictive model loss: 0.04738466441631317
2024-07-21 20:42:51,770 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -12.952572902119352, Velocity: -0.0380258337733952, Movement: 0.40389128700240967, Collision: 0, Height: -1.0, Movement Penalty: -0.09327469730967425, Smoothness: -0.0, Curiosity: 75.87943267822266, Exploration: 0.17555122164374024, Total: 62.473341405229526
2024-07-21 20:42:51,881 - AirSimEnvLogger - INFO - Action: [0.46637349 0.46637349 0.46637349 0.46637349], Velocity: (0.46637348654837124, 0.46637348654837124, 0.46637348654837124), Duration: 1.0, Reward: 62.473341405229526, Done: False
2024-07-21 20:42:51,944 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:42:51,944 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:42:54,622 - AirSimEnvLogger - INFO - Predictive model loss: 0.021914450451731682
2024-07-21 20:43:00,514 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.09262968367997, Velocity: -0.09146587113671896, Movement: 0.6342936457719535, Collision: 0, Height: -1.0, Movement Penalty: -0.1464838428526826, Smoothness: -0.0, Curiosity: 86.57183837890625, Exploration: 0.3769646043067744, Total: 73.07232315608489
2024-07-21 20:43:00,607 - AirSimEnvLogger - INFO - Action: [0.73241921 0.73241921 0.73241921 0.73241921], Velocity: (0.7324192142634129, 0.7324192142634129, 0.7324192142634129), Duration: 1.0, Reward: 73.07232315608489, Done: False
2024-07-21 20:43:00,639 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:00,639 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:03,434 - AirSimEnvLogger - INFO - Predictive model loss: 0.09396776556968689
2024-07-21 20:43:08,961 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.201295426218595, Velocity: -0.011108127602422541, Movement: 0.11010237012331173, Collision: 0, Height: -1.0, Movement Penalty: -0.025427053211643937, Smoothness: -0.0, Curiosity: 74.08613586425781, Exploration: 0.11944470421098348, Total: 60.41390271245895
2024-07-21 20:43:09,085 - AirSimEnvLogger - INFO - Action: [-0.12713527 -0.12713527 -0.12713527 -0.12713527], Velocity: (-0.12713526605821968, -0.12713526605821968, -0.12713526605821968), Duration: 1.0, Reward: 60.41390271245895, Done: False
2024-07-21 20:43:09,148 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:09,148 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:11,949 - AirSimEnvLogger - INFO - Predictive model loss: 0.04469144716858864
2024-07-21 20:43:17,689 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.29735626714909, Velocity: -0.04429920212330423, Movement: 0.311113883270859, Collision: 0, Height: -1.0, Movement Penalty: -0.07184867370202411, Smoothness: -0.0, Curiosity: 82.8777847290039, Exploration: 0.11656230328630093, Total: 69.11043947820524
2024-07-21 20:43:17,799 - AirSimEnvLogger - INFO - Action: [0.35924337 0.35924337 0.35924337 0.35924337], Velocity: (0.3592433685101205, 0.3592433685101205, 0.3592433685101205), Duration: 1.0, Reward: 69.11043947820524, Done: False
2024-07-21 20:43:17,877 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:17,877 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:20,845 - AirSimEnvLogger - INFO - Predictive model loss: 0.11622384935617447
2024-07-21 20:43:26,745 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.403452281177673, Velocity: -0.056588930659579796, Movement: 0.41211158672777454, Collision: 0, Height: -1.0, Movement Penalty: -0.09517309421337777, Smoothness: -0.0, Curiosity: 89.07013702392578, Exploration: 0.2710201800532597, Total: 75.2334695661296
2024-07-21 20:43:26,854 - AirSimEnvLogger - INFO - Action: [0.47586547 0.47586547 0.47586547 0.47586547], Velocity: (0.47586547106688887, 0.47586547106688887, 0.47586547106688887), Duration: 1.0, Reward: 75.2334695661296, Done: False
2024-07-21 20:43:26,916 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:26,916 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:29,847 - AirSimEnvLogger - INFO - Predictive model loss: 0.16801749169826508
2024-07-21 20:43:36,030 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.484459415948239, Velocity: -0.004452096010672549, Movement: 0.049898304113404325, Collision: 0, Height: -1.0, Movement Penalty: -0.01152351972479192, Smoothness: -0.0, Curiosity: 84.06645965576172, Exploration: 0.06084672377162793, Total: 70.09677377939336
2024-07-21 20:43:36,173 - AirSimEnvLogger - INFO - Action: [0.0576176 0.0576176 0.0576176 0.0576176], Velocity: (0.0576175986239596, 0.0576175986239596, 0.0576175986239596), Duration: 1.0, Reward: 70.09677377939336, Done: False
2024-07-21 20:43:36,204 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:36,204 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:39,231 - AirSimEnvLogger - INFO - Predictive model loss: 0.10226933658123016
2024-07-21 20:43:45,235 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.485893030056513, Velocity: -0.07141258472595852, Movement: 0.40595230134068433, Collision: 0, Height: -1.0, Movement Penalty: -0.09375066818287686, Smoothness: -0.0, Curiosity: 75.0792465209961, Exploration: 0.3410616874055629, Total: 61.17461440866652
2024-07-21 20:43:45,360 - AirSimEnvLogger - INFO - Action: [-0.46875334 -0.46875334 -0.46875334 -0.46875334], Velocity: (-0.4687533409143843, -0.4687533409143843, -0.4687533409143843), Duration: 1.0, Reward: 61.17461440866652, Done: False
2024-07-21 20:43:45,423 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:45,423 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:48,431 - AirSimEnvLogger - INFO - Predictive model loss: 0.02464885637164116
2024-07-21 20:43:54,514 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.474251968270412, Velocity: -0.028934435065117114, Movement: 0.20460026672838597, Collision: 0, Height: -1.0, Movement Penalty: -0.04725040762876115, Smoothness: -0.0, Curiosity: 83.9984359741211, Exploration: 0.01180785192753137, Total: 70.0290252018151
2024-07-21 20:43:54,621 - AirSimEnvLogger - INFO - Action: [0.23625204 0.23625204 0.23625204 0.23625204], Velocity: (0.23625203814380574, 0.23625203814380574, 0.23625203814380574), Duration: 1.0, Reward: 70.0290252018151, Done: False
2024-07-21 20:43:54,700 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:43:54,700 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:43:57,753 - AirSimEnvLogger - INFO - Predictive model loss: 0.02419404126703739
2024-07-21 20:44:03,734 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.430937535529777, Velocity: -0.032590968580894344, Movement: 0.1789233228992835, Collision: 0, Height: -1.0, Movement Penalty: -0.04132057145608147, Smoothness: -0.0, Curiosity: 77.31478118896484, Exploration: 0.08710429927934367, Total: 63.405007524889086
2024-07-21 20:44:03,843 - AirSimEnvLogger - INFO - Action: [-0.20660286 -0.20660286 -0.20660286 -0.20660286], Velocity: (-0.20660285728040734, -0.20660285728040734, -0.20660285728040734), Duration: 1.0, Reward: 63.405007524889086, Done: False
2024-07-21 20:44:03,875 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:03,875 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:06,858 - AirSimEnvLogger - INFO - Predictive model loss: 0.0049750301986932755
2024-07-21 20:44:12,896 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.378766338748061, Velocity: -0.04889232488027132, Movement: 0.32034884574541594, Collision: 0, Height: -1.0, Movement Penalty: -0.07398139693028072, Smoothness: -0.0, Curiosity: 72.63774108886719, Exploration: 0.24679339208632478, Total: 58.81870613775017
2024-07-21 20:44:13,022 - AirSimEnvLogger - INFO - Action: [-0.36990698 -0.36990698 -0.36990698 -0.36990698], Velocity: (-0.3699069846514036, -0.3699069846514036, -0.3699069846514036), Duration: 1.0, Reward: 58.81870613775017, Done: False
2024-07-21 20:44:13,085 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:13,085 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:16,105 - AirSimEnvLogger - INFO - Predictive model loss: 0.04087219759821892
2024-07-21 20:44:21,412 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.277797408713612, Velocity: -0.008151089897287571, Movement: 0.3852534219228595, Collision: 0, Height: -1.0, Movement Penalty: -0.08897046674135496, Smoothness: -0.0, Curiosity: 68.26104736328125, Exploration: 0.1928586783301897, Total: 54.53624258729237
2024-07-21 20:44:21,475 - AirSimEnvLogger - INFO - Action: [-0.44485233 -0.44485233 -0.44485233 -0.44485233], Velocity: (-0.4448523337067748, -0.4448523337067748, -0.4448523337067748), Duration: 1.0, Reward: 54.53624258729237, Done: False
2024-07-21 20:44:21,519 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:21,520 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:24,380 - AirSimEnvLogger - INFO - Predictive model loss: 0.09311705082654953
2024-07-21 20:44:30,626 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.155423955712617, Velocity: -0.031951446721156976, Movement: 0.27770801731878103, Collision: 0, Height: -1.0, Movement Penalty: -0.0641339194220462, Smoothness: -0.0, Curiosity: 66.32835388183594, Exploration: 0.11159282906241347, Total: 52.702213294849166
2024-07-21 20:44:30,735 - AirSimEnvLogger - INFO - Action: [-0.3206696 -0.3206696 -0.3206696 -0.3206696], Velocity: (-0.32066959711023096, -0.32066959711023096, -0.32066959711023096), Duration: 1.0, Reward: 52.702213294849166, Done: False
2024-07-21 20:44:30,829 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:30,829 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:33,786 - AirSimEnvLogger - INFO - Predictive model loss: 0.11639398336410522
2024-07-21 20:44:39,919 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.091674719618789, Velocity: -0.053926694720578705, Movement: 0.2923387315255986, Collision: 0, Height: -1.0, Movement Penalty: -0.06751273813634323, Smoothness: -0.0, Curiosity: 75.8900375366211, Exploration: 0.24474109119800985, Total: 62.3564316518512
2024-07-21 20:44:40,076 - AirSimEnvLogger - INFO - Action: [0.33756369 0.33756369 0.33756369 0.33756369], Velocity: (0.33756369068171616, 0.33756369068171616, 0.33756369068171616), Duration: 1.0, Reward: 62.3564316518512, Done: False
2024-07-21 20:44:40,139 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:40,139 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:43,179 - AirSimEnvLogger - INFO - Predictive model loss: 0.04809563606977463
2024-07-21 20:44:49,219 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.109627769642051, Velocity: -0.016568699394424524, Movement: 0.08498366107766958, Collision: 0, Height: -1.0, Movement Penalty: -0.019626135839964982, Smoothness: -0.0, Curiosity: 73.98028564453125, Exploration: 0.20251773530509892, Total: 60.41766472870082
2024-07-21 20:44:49,311 - AirSimEnvLogger - INFO - Action: [0.09813068 0.09813068 0.09813068 0.09813068], Velocity: (0.0981306791998249, 0.0981306791998249, 0.0981306791998249), Duration: 1.0, Reward: 60.41766472870082, Done: False
2024-07-21 20:44:49,389 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:49,389 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:44:52,269 - AirSimEnvLogger - INFO - Predictive model loss: 0.02242211066186428
2024-07-21 20:44:58,566 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.21362552935312, Velocity: -0.04727318902387965, Movement: 0.47240137060810283, Collision: 0, Height: -1.0, Movement Penalty: -0.10909642339445454, Smoothness: -0.0, Curiosity: 84.79529571533203, Exploration: 0.15392583476172664, Total: 71.12393378925962
2024-07-21 20:44:58,676 - AirSimEnvLogger - INFO - Action: [0.54548212 0.54548212 0.54548212 0.54548212], Velocity: (0.5454821169722727, 0.5454821169722727, 0.5454821169722727), Duration: 1.0, Reward: 71.12393378925962, Done: False
2024-07-21 20:44:58,754 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:44:58,754 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:01,770 - AirSimEnvLogger - INFO - Predictive model loss: 0.016884515061974525
2024-07-21 20:45:07,992 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.400989128193, Velocity: -0.1177078169729651, Movement: 0.6689268235596632, Collision: 0, Height: -1.0, Movement Penalty: -0.15448203266013316, Smoothness: -0.0, Curiosity: 95.91716003417969, Exploration: 0.36309927285707655, Total: 82.1043216960148
2024-07-21 20:45:08,115 - AirSimEnvLogger - INFO - Action: [0.77241016 0.77241016 0.77241016 0.77241016], Velocity: (0.7724101633006657, 0.7724101633006657, 0.7724101633006657), Duration: 1.0, Reward: 82.1043216960148, Done: False
2024-07-21 20:45:08,193 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:08,193 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:11,172 - AirSimEnvLogger - INFO - Predictive model loss: 0.0754832774400711
2024-07-21 20:45:17,329 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.580639329025113, Velocity: -0.05071829186025026, Movement: 0.3408100656779404, Collision: 0, Height: -1.0, Movement Penalty: -0.07870671326467718, Smoothness: -0.0, Curiosity: 93.710693359375, Exploration: 0.15145767136627103, Total: 79.66817751814068
2024-07-21 20:45:17,454 - AirSimEnvLogger - INFO - Action: [0.39353357 0.39353357 0.39353357 0.39353357], Velocity: (0.3935335663233859, 0.3935335663233859, 0.3935335663233859), Duration: 1.0, Reward: 79.66817751814068, Done: False
2024-07-21 20:45:17,486 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:17,486 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:20,456 - AirSimEnvLogger - INFO - Predictive model loss: 0.07906585931777954
2024-07-21 20:45:26,578 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.65773732590542, Velocity: -0.03601140073813698, Movement: 0.19268681276979763, Collision: 0, Height: -1.0, Movement Penalty: -0.04449911328877348, Smoothness: -0.0, Curiosity: 84.21617889404297, Exploration: 0.29360194280833385, Total: 70.12709547205303
2024-07-21 20:45:26,734 - AirSimEnvLogger - INFO - Action: [-0.22249557 -0.22249557 -0.22249557 -0.22249557], Velocity: (-0.22249556644386737, -0.22249556644386737, -0.22249556644386737), Duration: 1.0, Reward: 70.12709547205303, Done: False
2024-07-21 20:45:26,811 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:26,811 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:29,794 - AirSimEnvLogger - INFO - Predictive model loss: 0.03653644770383835
2024-07-21 20:45:35,874 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.602187232805024, Velocity: -0.053762958801052675, Movement: 0.5266927156880815, Collision: 0, Height: -1.0, Movement Penalty: -0.12163447247309156, Smoothness: -0.0, Curiosity: 75.7376708984375, Exploration: 0.43015763833996207, Total: 61.74196337053035
2024-07-21 20:45:35,984 - AirSimEnvLogger - INFO - Action: [-0.60817236 -0.60817236 -0.60817236 -0.60817236], Velocity: (-0.6081723623654578, -0.6081723623654578, -0.6081723623654578), Duration: 1.0, Reward: 61.74196337053035, Done: False
2024-07-21 20:45:36,016 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:36,016 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:38,686 - AirSimEnvLogger - INFO - Predictive model loss: 0.029669880867004395
2024-07-21 20:45:44,879 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.511721154495483, Velocity: -0.01318230454412278, Movement: 0.07325235764461709, Collision: 0, Height: -1.0, Movement Penalty: -0.016916907361957768, Smoothness: -0.0, Curiosity: 78.95375061035156, Exploration: 0.086003585880233, Total: 64.9622889265434
2024-07-21 20:45:45,036 - AirSimEnvLogger - INFO - Action: [-0.08458454 -0.08458454 -0.08458454 -0.08458454], Velocity: (-0.08458453680978884, -0.08458453680978884, -0.08458453680978884), Duration: 1.0, Reward: 64.9622889265434, Done: False
2024-07-21 20:45:45,099 - AirSimEnvLogger - INFO - Step completed. Observation keys: dict_keys(['state', 'visual'])
2024-07-21 20:45:45,099 - AirSimEnvLogger - INFO - State shape: (16,), Visual shape: (144, 256, 3)
2024-07-21 20:45:48,034 - AirSimEnvLogger - INFO - Predictive model loss: 0.00861362461000681
2024-07-21 20:45:53,778 - AirSimEnvLogger - INFO - Reward breakdown - Distance: -13.47573277394233, Velocity: -0.02938966704202531, Movement: 0.19028036994814734, Collision: 0, Height: -1.0, Movement Penalty: -0.043943369124425785, Smoothness: -0.0, Curiosity: 84.42588806152344, Exploration: 0.2703925609586606, Total: 70.51407417712792
2024-07-21 20:45:53,842 - AirSimEnvLogger - INFO - Action: [0.21971685 0.21971685 0.21971685 0.21971685], Velocity: (0.2197168456221289, 0.2197168456221289, 0.2197168456221289), Duration: 1.0, Reward: 70.51407417712792, Done: False
